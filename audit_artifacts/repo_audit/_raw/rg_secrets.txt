./docs/COPILOT_GUARD.md:11:- Do NOT remove the autouse behavior for `FIRSTTRY_SHARED_SECRET`.
./docs/demos/README.md:11:We only publish sanitized clips. No proprietary repo names, no secrets.
./docs/licensing.md:62:  - 100% parity with cloud resources (self-hosted runners, secrets, etc.).
./FIRSTTRY_ENTERPRISE_AUDIT_REPORT.md:12:| Team | âš ï¸ Partial Pass | Strong CI/workflow coverage and parity assets exist, but some parity artifacts reference paths or snapshot locations that may not exist locally (e.g., `.firsttry` runtime artifacts). Pre-commit configuration injects a test secret for parity. | Many `.github/workflows/*.yml` are present (supply-chain-audit, parity/ci workflows). `ci/parity.lock.json` exists. See Team Audit section. |
./FIRSTTRY_ENTERPRISE_AUDIT_REPORT.md:13:| Enterprise | âš ï¸ Partial Pass | Dependency scanning (pip-audit) is clean; Bandit surfaced low-risk findings (B110/B404/B607). A shared-secret pattern exists in `src/firsttry/license.py` with an explicit test/dev fallback; this requires manual review. LICENSE file present (Apache-2.0). | Security tooling is wired, but a few policy gaps & SAST findings remain. See Enterprise Audit section. |
./FIRSTTRY_ENTERPRISE_AUDIT_REPORT.md:55:  - `.pre-commit-config.yaml` is present and includes ruff, black, isort, and mypy hooks. There are local hooks that run `ft-pre-commit` and hardcoded test-secret export lines (e.g., `export FT_ENV=test FIRSTTRY_SHARED_SECRET=test-secret-DO-NOT-USE-IN-PROD-0123456789abcdef0123 ...`) used for local parity/dry-run. This is detection-only: it shows tests do inject a deterministic test secret when running parity hooks.
./FIRSTTRY_ENTERPRISE_AUDIT_REPORT.md:83:- Secrets & license gating
./FIRSTTRY_ENTERPRISE_AUDIT_REPORT.md:84:  - `src/firsttry/license.py` contains a shared-secret selection strategy and a typed `DEFAULT_SHARED_SECRET` constant that may be used in test/dev modes. The file implements an import-time guard that raises when `CI=true` and a valid env secret is not present â€” this is a detection of explicit enforcement. However, the code contains fallback semantics used for test/dev runs (detection-only: evidence exists). Key snippets detected:
./FIRSTTRY_ENTERPRISE_AUDIT_REPORT.md:85:    - A typed default: `DEFAULT_SHARED_SECRET: str = os.getenv("DEFAULT_SHARED_SECRET", "DO-NOT-USE-IN-PROD")` (read-only evidence)
./FIRSTTRY_ENTERPRISE_AUDIT_REPORT.md:86:    - Environment lookups for `FIRSTTRY_SHARED_SECRET`, `FT_SHARED_SECRET` and enforcement checks that raise `RuntimeError` if missing in production-like environments.
./FIRSTTRY_ENTERPRISE_AUDIT_REPORT.md:87:  - `.pre-commit-config.yaml` reveals a pre-commit hook that sets `FIRSTTRY_SHARED_SECRET=test-secret-DO-NOT-USE-IN-PROD-0123456789abcdef0123` for parity/dry-run. This demonstrates a practice of using a known test secret during pre-commit runs (detection-only; do not change it here).
./FIRSTTRY_ENTERPRISE_AUDIT_REPORT.md:103:- Partial Pass (âš ï¸): Security controls and scanning are present and active (pip-audit, Bandit, CI gating). A LICENSE is present. Notable findings for follow-up (detection-only): Bandit low findings exist; a shared-secret pattern and pre-commit test-secret injection are present (which is acceptable for parity but must be reviewed for production policies); governance docs like `SECURITY.md` are missing.
./FIRSTTRY_ENTERPRISE_AUDIT_REPORT.md:135:    - Document secret-management policy and review `src/firsttry/license.py` fallback/test secret semantics to ensure production cannot use insecure defaults; ensure pre-commit/test secret injection is restricted to local/test runs only.
./FIRSTTRY_ENTERPRISE_AUDIT_REPORT.md:148:- `src/firsttry/license.py` â€” shared-secret lookup, import-time guard and typed `DEFAULT_SHARED_SECRET` detection.
./FIRSTTRY_READONLY_AUDIT.md:22:- Compatibility alias: `FirstTryConfig` exported at runtime (workaround implemented to avoid literal token in source). Verified via import reflection (`import_reflection.log`) and by targeted tests.
./.github/workflows/hardening-guards.yml:33:      - name: Block hardcoded secrets
./.github/workflows/hardening-guards.yml:35:          echo "Scanning for hardcoded secrets..."
./.github/workflows/hardening-guards.yml:36:          # Search for hardcoded secret patterns (excluding docs, cache, and config files)
./.github/workflows/hardening-guards.yml:51:            -E 'dev-secret-change-me' src/ tests/ || true)
./.github/workflows/hardening-guards.yml:54:            echo "::error::Hardcoded secret detected - use environment variables instead"
./.github/workflows/hardening-guards.yml:58:          echo "No hardcoded secrets found âœ…"
./.github/workflows/hardening-guards.yml:60:      - name: Verify secret enforcement in code
./.github/workflows/hardening-guards.yml:62:          echo "Checking that production requires FIRSTTRY_SHARED_SECRET..."
./.github/workflows/hardening-guards.yml:64:          if ! grep -q 'raise RuntimeError.*FIRSTTRY_SHARED_SECRET.*required in production' src/firsttry/license.py; then
./.github/workflows/hardening-guards.yml:65:            echo "::error file=src/firsttry/license.py::Missing production secret enforcement guard"
./.github/workflows/hardening-guards.yml:68:          echo "Secret enforcement guard present âœ…"
./.github/workflows/nightly-stability.yml:32:          FIRSTTRY_SHARED_SECRET: 'dev-fallback-secret-0123456789012345'
./.github/workflows/release-vsix.yml:36:          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
./.github/workflows/ci-parity.yml:74:          FIRSTTRY_SHARED_SECRET='dev-fallback-secret-0123456789012345' pytest -q --maxfail=1 --ignore=tools
./.github/workflows/ci-parity.yml:78:          FIRSTTRY_SHARED_SECRET: 'dev-fallback-secret-0123456789012345'
./.github/workflows/audit.yml:88:            # Check 3: No hardcoded secrets (exclude source and tests where
./.github/workflows/audit.yml:261:                          'No hardcoded secrets',
./.github/workflows/ci.yml:175:          FIRSTTRY_LICENSE_KEY: ${{ secrets.FIRSTTRY_LICENSE_KEY_TEST }}
./.github/workflows/ci.yml:355:      - name: Check for secrets
./.github/workflows/ci.yml:357:          echo "ðŸ” Checking for hardcoded secrets..."
./.github/workflows/ci-hardened.yml:157:          POSTGRES_PASSWORD: testpass
./.github/workflows/remote-cache.yml:8:  id-token: write
./.github/workflows/remote-cache.yml:31:          # In production, use AWS Secrets Manager or GitHub Secrets
./.github/workflows/remote-cache.yml:66:          # Note: In CI/CD, AWS credentials should be configured via OIDC or GitHub Secrets
./.github/workflows/audit-hardened.yml:162:          - Secrets: No static keys (OIDC auth)
./.github/workflows/remote-cache-hardened.yml:9:  id-token: write  # Required for OIDC token request
./.github/workflows/remote-cache-hardened.yml:27:      id-token: write
./.github/workflows/remote-cache-hardened.yml:35:          # This would be set as an org secret or environment variable
./.github/workflows/remote-cache-hardened.yml:37:          echo "role=${{ secrets.AWS_ROLE_ARN }}" >> $GITHUB_OUTPUT
./.github/workflows/remote-cache-hardened.yml:44:      id-token: write
./.github/workflows/remote-cache-hardened.yml:64:          # Token TTL: 1 hour (default 3600 seconds)
./.github/workflows/remote-cache-hardened.yml:114:      id-token: write
./.github/workflows/remote-cache-hardened.yml:184:      id-token: write
./.github/workflows/remote-cache-hardened.yml:228:      id-token: write
./.github/workflows/ci-gate.yml:18:      FIRSTTRY_SHARED_SECRET: "dev-ci-shared-secret-xxxxxxxxxxxxxxxxxxxxxxxx"
./.github/workflows/publish.yml:31:          user: __token__
./.github/workflows/publish.yml:32:          password: ${{ secrets.PYPI_API_TOKEN }}
./.github/workflows/publish.yml:34:# NOTE: Add a repository secret named PYPI_API_TOKEN with an API token from PyPI.
./.github/workflows/remote-cache-e2e.yml:48:          AWS_SECRET_ACCESS_KEY: test
./.github/workflows/remote-cache-e2e.yml:56:          AWS_SECRET_ACCESS_KEY: test
./.github/workflows/remote-cache-e2e.yml:76:          AWS_SECRET_ACCESS_KEY: test
./PHASE2D_COMPLETE_SUMMARY.md:17:Combined with earlier features (secrets scanning, dependency audit, performance SLO), FirstTry now provides **complete enterprise hardening**.
./PHASE2D_COMPLETE_SUMMARY.md:23:### 2.D.1: Secrets Scanning âœ… VERIFIED
./PHASE2D_COMPLETE_SUMMARY.md:25:- **Coverage:** AWS keys, GitHub tokens, PKI detection
./PHASE2D_COMPLETE_SUMMARY.md:93:PHASE 2.D.1: SECRETS SCANNING ......................... 14 tests âœ…
./PHASE2D_COMPLETE_SUMMARY.md:95:â”œâ”€â”€ GitHub token detection ........................... 3 tests
./PHASE2D_COMPLETE_SUMMARY.md:143:    â”œâ”€â”€ Security: âœ… PROVEN (secrets, scanning, signing)
./PHASE2D_COMPLETE_SUMMARY.md:179:- **Phase 2.D.1 (Secrets):** 14/14 = 100% âœ…
./PHASE2D_COMPLETE_SUMMARY.md:193:âœ… Secrets scanning with 20+ pattern detection  
./PHASE2D_COMPLETE_SUMMARY.md:224:- âœ… `tests/enterprise/test_secrets_scanning.py` (14 tests)
./PHASE2D_COMPLETE_SUMMARY.md:265:| Secrets Scanning | 2.D.1 | 14 | âœ… VERIFIED | 20+ patterns detected |
./PHASE2D_COMPLETE_SUMMARY.md:301:- âœ… Secrets scanning active
./.venv-build/lib/python3.12/site-packages/rich/logging.py:275:    log.warning("password was rejected for admin site.")
./.venv-build/lib/python3.12/site-packages/rich/containers.py:146:                tokens: List[Text] = []
./.venv-build/lib/python3.12/site-packages/rich/containers.py:148:                    tokens.append(word)
./.venv-build/lib/python3.12/site-packages/rich/containers.py:153:                        tokens.append(Text(" " * spaces[index], style=space_style))
./.venv-build/lib/python3.12/site-packages/rich/containers.py:154:                self[line_index] = Text("").join(tokens)
./.venv-build/lib/python3.12/site-packages/rich/console.py:2102:        password: bool = False,
./.venv-build/lib/python3.12/site-packages/rich/console.py:2113:            password: (bool, optional): Hide typed text. Defaults to False.
./.venv-build/lib/python3.12/site-packages/rich/console.py:2121:        if password:
./.venv-build/lib/python3.12/site-packages/rich/syntax.py:27:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/rich/syntax.py:36:    Token,
./.venv-build/lib/python3.12/site-packages/rich/syntax.py:54:TokenType = Tuple[str, ...]
./.venv-build/lib/python3.12/site-packages/rich/syntax.py:62:ANSI_LIGHT: Dict[TokenType, Style] = {
./.venv-build/lib/python3.12/site-packages/rich/syntax.py:63:    Token: Style(),
./.venv-build/lib/python3.12/site-packages/rich/syntax.py:91:ANSI_DARK: Dict[TokenType, Style] = {
./.venv-build/lib/python3.12/site-packages/rich/syntax.py:92:    Token: Style(),
./.venv-build/lib/python3.12/site-packages/rich/syntax.py:128:    def get_style_for_token(self, token_type: TokenType) -> Style:
./.venv-build/lib/python3.12/site-packages/rich/syntax.py:129:        """Get a style for a given Pygments token."""
./.venv-build/lib/python3.12/site-packages/rich/syntax.py:142:        self._style_cache: Dict[TokenType, Style] = {}
./.venv-build/lib/python3.12/site-packages/rich/syntax.py:154:    def get_style_for_token(self, token_type: TokenType) -> Style:
./.venv-build/lib/python3.12/site-packages/rich/syntax.py:157:            return self._style_cache[token_type]
./.venv-build/lib/python3.12/site-packages/rich/syntax.py:160:                pygments_style = self._pygments_style_class.style_for_token(token_type)
./.venv-build/lib/python3.12/site-packages/rich/syntax.py:173:            self._style_cache[token_type] = style
./.venv-build/lib/python3.12/site-packages/rich/syntax.py:183:    def __init__(self, style_map: Dict[TokenType, Style]) -> None:
./.venv-build/lib/python3.12/site-packages/rich/syntax.py:187:        self._style_cache: Dict[TokenType, Style] = {}
./.venv-build/lib/python3.12/site-packages/rich/syntax.py:189:    def get_style_for_token(self, token_type: TokenType) -> Style:
./.venv-build/lib/python3.12/site-packages/rich/syntax.py:192:            return self._style_cache[token_type]
./.venv-build/lib/python3.12/site-packages/rich/syntax.py:198:            token = tuple(token_type)
./.venv-build/lib/python3.12/site-packages/rich/syntax.py:200:            while token:
./.venv-build/lib/python3.12/site-packages/rich/syntax.py:201:                _style = get_style(token)
./.venv-build/lib/python3.12/site-packages/rich/syntax.py:205:                token = token[:-1]
./.venv-build/lib/python3.12/site-packages/rich/syntax.py:206:            self._style_cache[token_type] = style
./.venv-build/lib/python3.12/site-packages/rich/syntax.py:424:    def _get_token_color(self, token_type: TokenType) -> Optional[Color]:
./.venv-build/lib/python3.12/site-packages/rich/syntax.py:425:        """Get a color (if any) for the given token.
./.venv-build/lib/python3.12/site-packages/rich/syntax.py:428:            token_type (TokenType): A token type tuple from Pygments.
./.venv-build/lib/python3.12/site-packages/rich/syntax.py:433:        style = self._theme.get_style_for_token(token_type)
./.venv-build/lib/python3.12/site-packages/rich/syntax.py:489:        _get_theme_style = self._theme.get_style_for_token
./.venv-build/lib/python3.12/site-packages/rich/syntax.py:501:                def line_tokenize() -> Iterable[Tuple[Any, str]]:
./.venv-build/lib/python3.12/site-packages/rich/syntax.py:502:                    """Split tokens to one per line."""
./.venv-build/lib/python3.12/site-packages/rich/syntax.py:505:                    for token_type, token in lexer.get_tokens(code):
./.venv-build/lib/python3.12/site-packages/rich/syntax.py:506:                        while token:
./.venv-build/lib/python3.12/site-packages/rich/syntax.py:507:                            line_token, new_line, token = token.partition("\n")
./.venv-build/lib/python3.12/site-packages/rich/syntax.py:508:                            yield token_type, line_token + new_line
./.venv-build/lib/python3.12/site-packages/rich/syntax.py:510:                def tokens_to_spans() -> Iterable[Tuple[str, Optional[Style]]]:
./.venv-build/lib/python3.12/site-packages/rich/syntax.py:511:                    """Convert tokens to spans."""
./.venv-build/lib/python3.12/site-packages/rich/syntax.py:512:                    tokens = iter(line_tokenize())
./.venv-build/lib/python3.12/site-packages/rich/syntax.py:516:                    # Skip over tokens until line start
./.venv-build/lib/python3.12/site-packages/rich/syntax.py:519:                            _token_type, token = next(tokens)
./.venv-build/lib/python3.12/site-packages/rich/syntax.py:522:                        yield (token, None)
./.venv-build/lib/python3.12/site-packages/rich/syntax.py:523:                        if token.endswith("\n"):
./.venv-build/lib/python3.12/site-packages/rich/syntax.py:526:                    for token_type, token in tokens:
./.venv-build/lib/python3.12/site-packages/rich/syntax.py:527:                        yield (token, _get_theme_style(token_type))
./.venv-build/lib/python3.12/site-packages/rich/syntax.py:528:                        if token.endswith("\n"):
./.venv-build/lib/python3.12/site-packages/rich/syntax.py:533:                text.append_tokens(tokens_to_spans())
./.venv-build/lib/python3.12/site-packages/rich/syntax.py:536:                text.append_tokens(
./.venv-build/lib/python3.12/site-packages/rich/syntax.py:537:                    (token, _get_theme_style(token_type))
./.venv-build/lib/python3.12/site-packages/rich/syntax.py:538:                    for token_type, token in lexer.get_tokens(code)
./.venv-build/lib/python3.12/site-packages/rich/syntax.py:572:        foreground_color = self._get_token_color(Token.Text)
./.venv-build/lib/python3.12/site-packages/rich/syntax.py:600:                self._theme.get_style_for_token(Token.Text),
./.venv-build/lib/python3.12/site-packages/rich/syntax.py:606:                self._theme.get_style_for_token(Token.Text),
./.venv-build/lib/python3.12/site-packages/rich/syntax.py:670:                + self._theme.get_style_for_token(Comment)
./.venv-build/lib/python3.12/site-packages/rich/syntax.py:704:                + self._theme.get_style_for_token(Comment)
./.venv-build/lib/python3.12/site-packages/rich/prompt.py:37:        password (bool, optional): Enable password input. Defaults to False.
./.venv-build/lib/python3.12/site-packages/rich/prompt.py:57:        password: bool = False,
./.venv-build/lib/python3.12/site-packages/rich/prompt.py:67:        self.password = password
./.venv-build/lib/python3.12/site-packages/rich/prompt.py:81:        password: bool = False,
./.venv-build/lib/python3.12/site-packages/rich/prompt.py:97:        password: bool = False,
./.venv-build/lib/python3.12/site-packages/rich/prompt.py:111:        password: bool = False,
./.venv-build/lib/python3.12/site-packages/rich/prompt.py:127:            password (bool, optional): Enable password input. Defaults to False.
./.venv-build/lib/python3.12/site-packages/rich/prompt.py:137:            password=password,
./.venv-build/lib/python3.12/site-packages/rich/prompt.py:188:        password: bool,
./.venv-build/lib/python3.12/site-packages/rich/prompt.py:196:            password (bool): Enable password entry.
./.venv-build/lib/python3.12/site-packages/rich/prompt.py:201:        return console.input(prompt, password=password, stream=stream)
./.venv-build/lib/python3.12/site-packages/rich/prompt.py:278:            value = self.get_input(self.console, prompt, self.password, stream=stream)
./.venv-build/lib/python3.12/site-packages/rich/prompt.py:366:            password = Prompt.ask(
./.venv-build/lib/python3.12/site-packages/rich/prompt.py:367:                "Please enter a password [cyan](must be at least 5 characters)",
./.venv-build/lib/python3.12/site-packages/rich/prompt.py:368:                password=True,
./.venv-build/lib/python3.12/site-packages/rich/prompt.py:370:            if len(password) >= 5:
./.venv-build/lib/python3.12/site-packages/rich/prompt.py:372:            print("[prompt.invalid]password too short")
./.venv-build/lib/python3.12/site-packages/rich/prompt.py:373:        print(f"password={password!r}")
./.venv-build/lib/python3.12/site-packages/rich/markdown.py:7:from markdown_it.token import Token
./.venv-build/lib/python3.12/site-packages/rich/markdown.py:29:    def create(cls, markdown: Markdown, token: Token) -> MarkdownElement:
./.venv-build/lib/python3.12/site-packages/rich/markdown.py:34:            token (Token): A node from markdown-it.
./.venv-build/lib/python3.12/site-packages/rich/markdown.py:112:    def create(cls, markdown: Markdown, token: Token) -> Paragraph:
./.venv-build/lib/python3.12/site-packages/rich/markdown.py:127:    def create(cls, markdown: Markdown, token: Token) -> Heading:
./.venv-build/lib/python3.12/site-packages/rich/markdown.py:128:        return cls(token.tag)
./.venv-build/lib/python3.12/site-packages/rich/markdown.py:162:    def create(cls, markdown: Markdown, token: Token) -> CodeBlock:
./.venv-build/lib/python3.12/site-packages/rich/markdown.py:163:        node_info = token.info or ""
./.venv-build/lib/python3.12/site-packages/rich/markdown.py:283:    def create(cls, markdown: Markdown, token: Token) -> MarkdownElement:
./.venv-build/lib/python3.12/site-packages/rich/markdown.py:284:        style = str(token.attrs.get("style")) or ""
./.venv-build/lib/python3.12/site-packages/rich/markdown.py:313:    def create(cls, markdown: Markdown, token: Token) -> ListElement:
./.venv-build/lib/python3.12/site-packages/rich/markdown.py:314:        return cls(token.type, int(token.attrs.get("start", 1)))
./.venv-build/lib/python3.12/site-packages/rich/markdown.py:381:    def create(cls, markdown: Markdown, token: Token) -> MarkdownElement:
./.venv-build/lib/python3.12/site-packages/rich/markdown.py:382:        url = token.attrs.get("href", "#")
./.venv-build/lib/python3.12/site-packages/rich/markdown.py:383:        return cls(token.content, str(url))
./.venv-build/lib/python3.12/site-packages/rich/markdown.py:396:    def create(cls, markdown: Markdown, token: Token) -> MarkdownElement:
./.venv-build/lib/python3.12/site-packages/rich/markdown.py:401:            token (Any): A token from markdown-it.
./.venv-build/lib/python3.12/site-packages/rich/markdown.py:406:        return cls(str(token.attrs.get("src", "")), markdown.hyperlinks)
./.venv-build/lib/python3.12/site-packages/rich/markdown.py:532:    def _flatten_tokens(self, tokens: Iterable[Token]) -> Iterable[Token]:
./.venv-build/lib/python3.12/site-packages/rich/markdown.py:533:        """Flattens the token stream."""
./.venv-build/lib/python3.12/site-packages/rich/markdown.py:534:        for token in tokens:
./.venv-build/lib/python3.12/site-packages/rich/markdown.py:535:            is_fence = token.type == "fence"
./.venv-build/lib/python3.12/site-packages/rich/markdown.py:536:            is_image = token.tag == "img"
./.venv-build/lib/python3.12/site-packages/rich/markdown.py:537:            if token.children and not (is_image or is_fence):
./.venv-build/lib/python3.12/site-packages/rich/markdown.py:538:                yield from self._flatten_tokens(token.children)
./.venv-build/lib/python3.12/site-packages/rich/markdown.py:540:                yield token
./.venv-build/lib/python3.12/site-packages/rich/markdown.py:553:        tokens = self.parsed
./.venv-build/lib/python3.12/site-packages/rich/markdown.py:558:        for token in self._flatten_tokens(tokens):
./.venv-build/lib/python3.12/site-packages/rich/markdown.py:559:            node_type = token.type
./.venv-build/lib/python3.12/site-packages/rich/markdown.py:560:            tag = token.tag
./.venv-build/lib/python3.12/site-packages/rich/markdown.py:562:            entering = token.nesting == 1
./.venv-build/lib/python3.12/site-packages/rich/markdown.py:563:            exiting = token.nesting == -1
./.venv-build/lib/python3.12/site-packages/rich/markdown.py:564:            self_closing = token.nesting == 0
./.venv-build/lib/python3.12/site-packages/rich/markdown.py:567:                context.on_text(token.content, node_type)
./.venv-build/lib/python3.12/site-packages/rich/markdown.py:573:                href = str(token.attrs.get("href", ""))
./.venv-build/lib/python3.12/site-packages/rich/markdown.py:579:                    context.stack.push(Link.create(self, token))
./.venv-build/lib/python3.12/site-packages/rich/markdown.py:598:                    # If it's an opening inline token e.g. strong, em, etc.
./.venv-build/lib/python3.12/site-packages/rich/markdown.py:608:                    if token.content:
./.venv-build/lib/python3.12/site-packages/rich/markdown.py:609:                        context.on_text(token.content, node_type)
./.venv-build/lib/python3.12/site-packages/rich/markdown.py:613:                element_class = self.elements.get(token.type) or UnknownElement
./.venv-build/lib/python3.12/site-packages/rich/markdown.py:614:                element = element_class.create(self, token)
./.venv-build/lib/python3.12/site-packages/rich/markdown.py:634:                    text = token.content
./.venv-build/lib/python3.12/site-packages/rich/_emoji_codes.py:156:    "japanese_secret_button": "ãŠ™",
./.venv-build/lib/python3.12/site-packages/rich/_emoji_codes.py:2882:    "secret": "ãŠ™",
./.venv-build/lib/python3.12/site-packages/rich/text.py:1013:    def append_tokens(self, tokens: Iterable[Tuple[str, Optional[StyleType]]]) -> "Text":
./.venv-build/lib/python3.12/site-packages/rich/text.py:1017:            tokens (Iterable[Tuple[str, Optional[StyleType]]]): An iterable of tuples containing str content and style.
./.venv-build/lib/python3.12/site-packages/rich/text.py:1026:        for content, style in tokens:
./.venv-build/lib/python3.12/site-packages/rich/ansi.py:20:class _AnsiToken(NamedTuple):
./.venv-build/lib/python3.12/site-packages/rich/ansi.py:21:    """Result of ansi tokenized string."""
./.venv-build/lib/python3.12/site-packages/rich/ansi.py:28:def _ansi_tokenize(ansi_text: str) -> Iterable[_AnsiToken]:
./.venv-build/lib/python3.12/site-packages/rich/ansi.py:29:    """Tokenize a string in to plain text and ANSI codes.
./.venv-build/lib/python3.12/site-packages/rich/ansi.py:35:        AnsiToken: A named tuple of (plain, sgr, osc)
./.venv-build/lib/python3.12/site-packages/rich/ansi.py:45:            yield _AnsiToken(ansi_text[position:start])
./.venv-build/lib/python3.12/site-packages/rich/ansi.py:51:                yield _AnsiToken("", sgr[1:-1], osc)
./.venv-build/lib/python3.12/site-packages/rich/ansi.py:53:            yield _AnsiToken("", sgr, osc)
./.venv-build/lib/python3.12/site-packages/rich/ansi.py:56:        yield _AnsiToken(ansi_text[position:])
./.venv-build/lib/python3.12/site-packages/rich/ansi.py:153:        for plain_text, sgr, osc in _ansi_tokenize(line):
./.venv-build/lib/python3.12/site-packages/rich/pretty.py:414:    def iter_tokens(self) -> Iterable[str]:
./.venv-build/lib/python3.12/site-packages/rich/pretty.py:415:        """Generate tokens for this node."""
./.venv-build/lib/python3.12/site-packages/rich/pretty.py:425:                    yield from self.children[0].iter_tokens()
./.venv-build/lib/python3.12/site-packages/rich/pretty.py:429:                        yield from child.iter_tokens()
./.venv-build/lib/python3.12/site-packages/rich/pretty.py:447:        for token in self.iter_tokens():
./.venv-build/lib/python3.12/site-packages/rich/pretty.py:448:            total_length += cell_len(token)
./.venv-build/lib/python3.12/site-packages/rich/pretty.py:454:        repr_text = "".join(self.iter_tokens())
./.venv-build/lib/python3.12/site-packages/rich/traceback.py:24:from pygments.token import Comment, Keyword, Name, Number, Operator, String
./.venv-build/lib/python3.12/site-packages/rich/traceback.py:25:from pygments.token import Text as TextToken
./.venv-build/lib/python3.12/site-packages/rich/traceback.py:26:from pygments.token import Token
./.venv-build/lib/python3.12/site-packages/rich/traceback.py:598:        token_style = theme.get_style_for_token
./.venv-build/lib/python3.12/site-packages/rich/traceback.py:602:                "pretty": token_style(TextToken),
./.venv-build/lib/python3.12/site-packages/rich/traceback.py:603:                "pygments.text": token_style(Token),
./.venv-build/lib/python3.12/site-packages/rich/traceback.py:604:                "pygments.string": token_style(String),
./.venv-build/lib/python3.12/site-packages/rich/traceback.py:605:                "pygments.function": token_style(Name.Function),
./.venv-build/lib/python3.12/site-packages/rich/traceback.py:606:                "pygments.number": token_style(Number),
./.venv-build/lib/python3.12/site-packages/rich/traceback.py:607:                "repr.indent": token_style(Comment) + Style(dim=True),
./.venv-build/lib/python3.12/site-packages/rich/traceback.py:608:                "repr.str": token_style(String),
./.venv-build/lib/python3.12/site-packages/rich/traceback.py:609:                "repr.brace": token_style(TextToken) + Style(bold=True),
./.venv-build/lib/python3.12/site-packages/rich/traceback.py:610:                "repr.number": token_style(Number),
./.venv-build/lib/python3.12/site-packages/rich/traceback.py:611:                "repr.bool_true": token_style(Keyword.Constant),
./.venv-build/lib/python3.12/site-packages/rich/traceback.py:612:                "repr.bool_false": token_style(Keyword.Constant),
./.venv-build/lib/python3.12/site-packages/rich/traceback.py:613:                "repr.none": token_style(Keyword.Constant),
./.venv-build/lib/python3.12/site-packages/rich/traceback.py:614:                "scope.border": token_style(String.Delimiter),
./.venv-build/lib/python3.12/site-packages/rich/traceback.py:615:                "scope.equals": token_style(Operator),
./.venv-build/lib/python3.12/site-packages/rich/traceback.py:616:                "scope.key": token_style(Name),
./.venv-build/lib/python3.12/site-packages/rich/traceback.py:617:                "scope.key.special": token_style(Name.Constant) + Style(dim=True),
./.venv-build/lib/python3.12/site-packages/rfc3986/exceptions.py:81:class PasswordForbidden(ValidationError):
./.venv-build/lib/python3.12/site-packages/rfc3986/exceptions.py:82:    """Exception raised when a URL has a password in the userinfo section."""
./.venv-build/lib/python3.12/site-packages/rfc3986/exceptions.py:87:        super().__init__('"{}" contained a password when validation forbade it'.format(unsplit()))
./.venv-build/lib/python3.12/site-packages/rfc3986/_mixin.py:23:            ``{'userinfo': 'username:password', 'host': 'www.example.com',
./.venv-build/lib/python3.12/site-packages/rfc3986/validators.py:54:        self.allow_password = True
./.venv-build/lib/python3.12/site-packages/rfc3986/validators.py:116:    def allow_use_of_password(self):
./.venv-build/lib/python3.12/site-packages/rfc3986/validators.py:117:        """Allow passwords to be present in the URI.
./.venv-build/lib/python3.12/site-packages/rfc3986/validators.py:126:        self.allow_password = True
./.venv-build/lib/python3.12/site-packages/rfc3986/validators.py:129:    def forbid_use_of_password(self):
./.venv-build/lib/python3.12/site-packages/rfc3986/validators.py:130:        """Prevent passwords from being included in the URI.
./.venv-build/lib/python3.12/site-packages/rfc3986/validators.py:139:        self.allow_password = False
./.venv-build/lib/python3.12/site-packages/rfc3986/validators.py:197:        :raises PasswordForbidden:
./.venv-build/lib/python3.12/site-packages/rfc3986/validators.py:198:            When a password is present in the userinfo component but is
./.venv-build/lib/python3.12/site-packages/rfc3986/validators.py:203:        if not self.allow_password:
./.venv-build/lib/python3.12/site-packages/rfc3986/validators.py:204:            check_password(uri)
./.venv-build/lib/python3.12/site-packages/rfc3986/validators.py:222:def check_password(uri):
./.venv-build/lib/python3.12/site-packages/rfc3986/validators.py:223:    """Assert that there is no password present in the uri."""
./.venv-build/lib/python3.12/site-packages/rfc3986/validators.py:230:    raise exceptions.PasswordForbidden(uri)
./.venv-build/lib/python3.12/site-packages/rfc3986/uri.py:51:        ``username:password@example.com:443``, etc.
./.venv-build/lib/python3.12/site-packages/rfc3986/normalizers.py:44:def normalize_password(password):
./.venv-build/lib/python3.12/site-packages/rfc3986/normalizers.py:45:    """Normalize a password to make safe for userinfo."""
./.venv-build/lib/python3.12/site-packages/rfc3986/normalizers.py:46:    return compat.urlquote(password)
./.venv-build/lib/python3.12/site-packages/rfc3986/builder.py:120:    def add_credentials(self, username, password):
./.venv-build/lib/python3.12/site-packages/rfc3986/builder.py:137:        if password is not None:
./.venv-build/lib/python3.12/site-packages/rfc3986/builder.py:140:                normalizers.normalize_password(password),
./.venv-build/lib/python3.12/site-packages/urllib3/connectionpool.py:966:    ``ca_cert_dir``, ``ssl_version``, ``key_password`` are only used if :mod:`ssl`
./.venv-build/lib/python3.12/site-packages/urllib3/connectionpool.py:988:        key_password: str | None = None,
./.venv-build/lib/python3.12/site-packages/urllib3/connectionpool.py:1014:        self.key_password = key_password
./.venv-build/lib/python3.12/site-packages/urllib3/connectionpool.py:1065:            key_password=self.key_password,
./.venv-build/lib/python3.12/site-packages/urllib3/poolmanager.py:48:    "key_password",
./.venv-build/lib/python3.12/site-packages/urllib3/poolmanager.py:72:    key_key_password: str | None
./.venv-build/lib/python3.12/site-packages/urllib3/connection.py:389:                f"Method cannot contain non-token characters {method!r} (found at least {match.group()!r})"
./.venv-build/lib/python3.12/site-packages/urllib3/connection.py:629:        key_password: str | None = None,
./.venv-build/lib/python3.12/site-packages/urllib3/connection.py:644:        self.key_password = key_password
./.venv-build/lib/python3.12/site-packages/urllib3/connection.py:670:        key_password: str | None = None,
./.venv-build/lib/python3.12/site-packages/urllib3/connection.py:699:        self.key_password = key_password
./.venv-build/lib/python3.12/site-packages/urllib3/connection.py:789:                key_password=self.key_password,
./.venv-build/lib/python3.12/site-packages/urllib3/connection.py:866:            key_password=None,
./.venv-build/lib/python3.12/site-packages/urllib3/connection.py:892:    key_password: str | None,
./.venv-build/lib/python3.12/site-packages/urllib3/connection.py:961:        key_password=key_password,
./.venv-build/lib/python3.12/site-packages/urllib3/util/timeout.py:17:    token = -1
./.venv-build/lib/python3.12/site-packages/urllib3/util/timeout.py:20:_DEFAULT_TIMEOUT: Final[_TYPE_DEFAULT] = _TYPE_DEFAULT.token
./.venv-build/lib/python3.12/site-packages/urllib3/util/request.py:48:    token = 0
./.venv-build/lib/python3.12/site-packages/urllib3/util/request.py:51:_FAILEDTELL: Final[_TYPE_FAILEDTELL] = _TYPE_FAILEDTELL.token
./.venv-build/lib/python3.12/site-packages/urllib3/util/request.py:91:        Colon-separated username:password string for 'authorization: basic ...'
./.venv-build/lib/python3.12/site-packages/urllib3/util/request.py:95:        Colon-separated username:password string for 'proxy-authorization: basic ...'
./.venv-build/lib/python3.12/site-packages/urllib3/util/ssl_.py:386:    key_password: str | None = ...,
./.venv-build/lib/python3.12/site-packages/urllib3/util/ssl_.py:404:    key_password: str | None = ...,
./.venv-build/lib/python3.12/site-packages/urllib3/util/ssl_.py:421:    key_password: str | None = None,
./.venv-build/lib/python3.12/site-packages/urllib3/util/ssl_.py:442:    :param key_password:
./.venv-build/lib/python3.12/site-packages/urllib3/util/ssl_.py:443:        Optional password if the keyfile is encrypted.
./.venv-build/lib/python3.12/site-packages/urllib3/util/ssl_.py:469:    if keyfile and key_password is None and _is_key_file_encrypted(keyfile):
./.venv-build/lib/python3.12/site-packages/urllib3/util/ssl_.py:470:        raise SSLError("Client private key is encrypted, password is required")
./.venv-build/lib/python3.12/site-packages/urllib3/util/ssl_.py:473:        if key_password is None:
./.venv-build/lib/python3.12/site-packages/urllib3/util/ssl_.py:476:            context.load_cert_chain(certfile, keyfile, key_password)
./.venv-build/lib/python3.12/site-packages/urllib3/util/url.py:180:            print( urllib3.util.Url("https", "username:password",
./.venv-build/lib/python3.12/site-packages/urllib3/util/url.py:184:            # "https://username:password@host.com:80/path?query#fragment"
./.venv-build/lib/python3.12/site-packages/urllib3/_base_connection.py:136:        key_password: str | None
./.venv-build/lib/python3.12/site-packages/urllib3/_base_connection.py:162:            key_password: str | None = None,
./.venv-build/lib/python3.12/site-packages/urllib3/contrib/pyopenssl.py:472:        password: str | None = None,
./.venv-build/lib/python3.12/site-packages/urllib3/contrib/pyopenssl.py:476:            if password is not None:
./.venv-build/lib/python3.12/site-packages/urllib3/contrib/pyopenssl.py:477:                if not isinstance(password, bytes):
./.venv-build/lib/python3.12/site-packages/urllib3/contrib/pyopenssl.py:478:                    password = password.encode("utf-8")  # type: ignore[assignment]
./.venv-build/lib/python3.12/site-packages/urllib3/contrib/pyopenssl.py:479:                self._ctx.set_passwd_cb(lambda *_: password)
./.venv-build/lib/python3.12/site-packages/urllib3/contrib/emscripten/connection.py:169:    key_password: str | None
./.venv-build/lib/python3.12/site-packages/urllib3/contrib/emscripten/connection.py:201:        key_password: str | None = None,
./.venv-build/lib/python3.12/site-packages/urllib3/contrib/emscripten/connection.py:217:        self.key_password = key_password
./.venv-build/lib/python3.12/site-packages/urllib3/contrib/emscripten/connection.py:240:        key_password: str | None = None,
./.venv-build/lib/python3.12/site-packages/urllib3/contrib/socks.py:14:- Usernames and passwords for the SOCKS proxy
./.venv-build/lib/python3.12/site-packages/urllib3/contrib/socks.py:31:When connecting to a SOCKS5 proxy the ``username`` and ``password`` portion
./.venv-build/lib/python3.12/site-packages/urllib3/contrib/socks.py:32:of the ``proxy_url`` will be sent as the username/password to authenticate
./.venv-build/lib/python3.12/site-packages/urllib3/contrib/socks.py:37:    proxy_url="socks5h://<username>:<password>@proxy-host"
./.venv-build/lib/python3.12/site-packages/urllib3/contrib/socks.py:80:    password: str | None
./.venv-build/lib/python3.12/site-packages/urllib3/contrib/socks.py:116:                proxy_password=self._socks_options["password"],
./.venv-build/lib/python3.12/site-packages/urllib3/contrib/socks.py:182:        password: str | None = None,
./.venv-build/lib/python3.12/site-packages/urllib3/contrib/socks.py:189:        if username is None and password is None and parsed.auth is not None:
./.venv-build/lib/python3.12/site-packages/urllib3/contrib/socks.py:192:                username, password = split
./.venv-build/lib/python3.12/site-packages/urllib3/contrib/socks.py:215:            "password": password,
./.venv-build/lib/python3.12/site-packages/keyring/devpi_client.py:23:def devpiclient_get_password(url, username):
./.venv-build/lib/python3.12/site-packages/keyring/devpi_client.py:25:    >>> pluggy._hooks.varnames(devpiclient_get_password)
./.venv-build/lib/python3.12/site-packages/keyring/devpi_client.py:29:    return keyring.get_password(url, username)
./.venv-build/lib/python3.12/site-packages/keyring/core.py:61:def get_password(service_name: str, username: str) -> typing.Optional[str]:
./.venv-build/lib/python3.12/site-packages/keyring/core.py:62:    """Get password from the specified service."""
./.venv-build/lib/python3.12/site-packages/keyring/core.py:63:    return get_keyring().get_password(service_name, username)
./.venv-build/lib/python3.12/site-packages/keyring/core.py:66:def set_password(service_name: str, username: str, password: str) -> None:
./.venv-build/lib/python3.12/site-packages/keyring/core.py:67:    """Set password for the user in the specified service."""
./.venv-build/lib/python3.12/site-packages/keyring/core.py:68:    get_keyring().set_password(service_name, username, password)
./.venv-build/lib/python3.12/site-packages/keyring/core.py:71:def delete_password(service_name: str, username: str) -> None:
./.venv-build/lib/python3.12/site-packages/keyring/core.py:72:    """Delete the password for the user in the specified service."""
./.venv-build/lib/python3.12/site-packages/keyring/core.py:73:    get_keyring().delete_password(service_name, username)
./.venv-build/lib/python3.12/site-packages/keyring/core.py:126:    ...      'keyring.backends.SecretService.Keyring',
./.venv-build/lib/python3.12/site-packages/keyring/credentials.py:14:    def password(self) -> str: ...
./.venv-build/lib/python3.12/site-packages/keyring/credentials.py:17:        return dict(username=self.username, password=self.password)
./.venv-build/lib/python3.12/site-packages/keyring/credentials.py:23:    def __init__(self, username: str, password: str):
./.venv-build/lib/python3.12/site-packages/keyring/credentials.py:25:        self._password = password
./.venv-build/lib/python3.12/site-packages/keyring/credentials.py:32:    def password(self) -> str:
./.venv-build/lib/python3.12/site-packages/keyring/credentials.py:33:        return self._password
./.venv-build/lib/python3.12/site-packages/keyring/credentials.py:37:    def __init__(self, password: str):
./.venv-build/lib/python3.12/site-packages/keyring/credentials.py:38:        self._password = password
./.venv-build/lib/python3.12/site-packages/keyring/credentials.py:45:        return dict(password=self.password)
./.venv-build/lib/python3.12/site-packages/keyring/credentials.py:84:    def password(self) -> str:
./.venv-build/lib/python3.12/site-packages/keyring/http.py:2:urllib2.HTTPPasswordMgr object using the keyring, for use with the
./.venv-build/lib/python3.12/site-packages/keyring/http.py:7:    handlers = [urllib2.HTTPBasicAuthHandler(PasswordMgr())]
./.venv-build/lib/python3.12/site-packages/keyring/http.py:11:This will prompt for a password if one is required and isn't already
./.venv-build/lib/python3.12/site-packages/keyring/http.py:17:from . import delete_password, get_password, set_password
./.venv-build/lib/python3.12/site-packages/keyring/http.py:20:class PasswordMgr:
./.venv-build/lib/python3.12/site-packages/keyring/http.py:24:    def add_password(self, realm, authuri, password):
./.venv-build/lib/python3.12/site-packages/keyring/http.py:26:        set_password(realm, user, password)
./.venv-build/lib/python3.12/site-packages/keyring/http.py:28:    def find_user_password(self, realm, authuri):
./.venv-build/lib/python3.12/site-packages/keyring/http.py:30:        password = get_password(realm, user)
./.venv-build/lib/python3.12/site-packages/keyring/http.py:31:        if password is None:
./.venv-build/lib/python3.12/site-packages/keyring/http.py:32:            prompt = f"password for {user}@{realm} for {authuri}: "
./.venv-build/lib/python3.12/site-packages/keyring/http.py:33:            password = getpass.getpass(prompt)
./.venv-build/lib/python3.12/site-packages/keyring/http.py:34:            set_password(realm, user, password)
./.venv-build/lib/python3.12/site-packages/keyring/http.py:35:        return user, password
./.venv-build/lib/python3.12/site-packages/keyring/http.py:37:    def clear_password(self, realm, authuri):
./.venv-build/lib/python3.12/site-packages/keyring/http.py:39:        delete_password(realm, user)
./.venv-build/lib/python3.12/site-packages/keyring/backend.py:36:    Wraps set_password to validate the username.
./.venv-build/lib/python3.12/site-packages/keyring/backend.py:42:        cls._validate_username_in_set_password()
./.venv-build/lib/python3.12/site-packages/keyring/backend.py:51:    def _validate_username_in_set_password(cls):
./.venv-build/lib/python3.12/site-packages/keyring/backend.py:53:        Wrap ``set_password`` such to validate the passed username.
./.venv-build/lib/python3.12/site-packages/keyring/backend.py:55:        orig = cls.set_password
./.venv-build/lib/python3.12/site-packages/keyring/backend.py:62:        cls.set_password = wrapper
./.venv-build/lib/python3.12/site-packages/keyring/backend.py:125:    def get_password(self, service: str, username: str) -> str | None:
./.venv-build/lib/python3.12/site-packages/keyring/backend.py:126:        """Get password of the username for the service"""
./.venv-build/lib/python3.12/site-packages/keyring/backend.py:142:    def set_password(self, service: str, username: str, password: str) -> None:
./.venv-build/lib/python3.12/site-packages/keyring/backend.py:143:        """Set password for the username of the service.
./.venv-build/lib/python3.12/site-packages/keyring/backend.py:145:        If the backend cannot store passwords, raise
./.venv-build/lib/python3.12/site-packages/keyring/backend.py:146:        PasswordSetError.
./.venv-build/lib/python3.12/site-packages/keyring/backend.py:148:        raise errors.PasswordSetError("reason")
./.venv-build/lib/python3.12/site-packages/keyring/backend.py:151:    #  delete_password
./.venv-build/lib/python3.12/site-packages/keyring/backend.py:153:    def delete_password(self, service: str, username: str) -> None:
./.venv-build/lib/python3.12/site-packages/keyring/backend.py:154:        """Delete the password for the username of the service.
./.venv-build/lib/python3.12/site-packages/keyring/backend.py:156:        If the backend cannot delete passwords, raise
./.venv-build/lib/python3.12/site-packages/keyring/backend.py:157:        PasswordDeleteError.
./.venv-build/lib/python3.12/site-packages/keyring/backend.py:159:        raise errors.PasswordDeleteError("reason")
./.venv-build/lib/python3.12/site-packages/keyring/backend.py:169:        """Gets the username and password for the service.
./.venv-build/lib/python3.12/site-packages/keyring/backend.py:178:            password = self.get_password(service, username)
./.venv-build/lib/python3.12/site-packages/keyring/backend.py:179:            if password is not None:
./.venv-build/lib/python3.12/site-packages/keyring/backend.py:180:                return credentials.SimpleCredential(username, password)
./.venv-build/lib/python3.12/site-packages/keyring/cli.py:1:"""Simple command line interface to get/set password from a keyring"""
./.venv-build/lib/python3.12/site-packages/keyring/cli.py:15:    delete_password,
./.venv-build/lib/python3.12/site-packages/keyring/cli.py:17:    get_password,
./.venv-build/lib/python3.12/site-packages/keyring/cli.py:19:    set_password,
./.venv-build/lib/python3.12/site-packages/keyring/cli.py:56:        self.parser._get_modes = ["password", "creds"]
./.venv-build/lib/python3.12/site-packages/keyring/cli.py:61:            default="password",
./.venv-build/lib/python3.12/site-packages/keyring/cli.py:64:            'password' requires a username and will return only the password.
./.venv-build/lib/python3.12/site-packages/keyring/cli.py:65:            'creds' does not require a username and will return both the username and password separated by a newline.
./.venv-build/lib/python3.12/site-packages/keyring/cli.py:67:            Default is 'password'
./.venv-build/lib/python3.12/site-packages/keyring/cli.py:142:    def _get_password(self) -> credentials.Credential | None:
./.venv-build/lib/python3.12/site-packages/keyring/cli.py:143:        password = get_password(self.service, self.username)
./.venv-build/lib/python3.12/site-packages/keyring/cli.py:144:        return credentials.AnonymousCredential(password) if password is not None else None
./.venv-build/lib/python3.12/site-packages/keyring/cli.py:147:        password = self.input_password(f"Password for '{self.username}' in '{self.service}': ")
./.venv-build/lib/python3.12/site-packages/keyring/cli.py:148:        set_password(self.service, self.username, password)
./.venv-build/lib/python3.12/site-packages/keyring/cli.py:151:        delete_password(self.service, self.username)
./.venv-build/lib/python3.12/site-packages/keyring/cli.py:180:    def input_password(self, prompt):
./.venv-build/lib/python3.12/site-packages/keyring/cli.py:181:        """Retrieve password from input."""
./.venv-build/lib/python3.12/site-packages/keyring/cli.py:186:        """Return password from pipe if not on TTY, else False."""
./.venv-build/lib/python3.12/site-packages/keyring/backends/macOS/api.py:138:def find_generic_password(kc_name, service, username, not_found_ok=False):
./.venv-build/lib/python3.12/site-packages/keyring/backends/macOS/api.py:140:        kSecClass=k_("kSecClassGenericPassword"),
./.venv-build/lib/python3.12/site-packages/keyring/backends/macOS/api.py:144:        kSecReturnData=True,
./.venv-build/lib/python3.12/site-packages/keyring/backends/macOS/api.py:158:def set_generic_password(name, service, username, password):
./.venv-build/lib/python3.12/site-packages/keyring/backends/macOS/api.py:160:        delete_generic_password(name, service, username)
./.venv-build/lib/python3.12/site-packages/keyring/backends/macOS/api.py:163:        kSecClass=k_("kSecClassGenericPassword"),
./.venv-build/lib/python3.12/site-packages/keyring/backends/macOS/api.py:166:        kSecValueData=password,
./.venv-build/lib/python3.12/site-packages/keyring/backends/macOS/api.py:173:def delete_generic_password(name, service, username):
./.venv-build/lib/python3.12/site-packages/keyring/backends/macOS/api.py:175:        kSecClass=k_("kSecClassGenericPassword"),
./.venv-build/lib/python3.12/site-packages/keyring/backends/macOS/__init__.py:8:from ...errors import KeyringError, KeyringLocked, PasswordDeleteError, PasswordSetError
./.venv-build/lib/python3.12/site-packages/keyring/backends/macOS/__init__.py:44:    def set_password(self, service, username, password):
./.venv-build/lib/python3.12/site-packages/keyring/backends/macOS/__init__.py:49:            api.set_generic_password(self.keychain, service, username, password)
./.venv-build/lib/python3.12/site-packages/keyring/backends/macOS/__init__.py:51:            raise KeyringLocked(f"Can't store password on keychain: {e}") from e
./.venv-build/lib/python3.12/site-packages/keyring/backends/macOS/__init__.py:53:            raise PasswordSetError(f"Can't store password on keychain: {e}") from e
./.venv-build/lib/python3.12/site-packages/keyring/backends/macOS/__init__.py:56:    def get_password(self, service, username):
./.venv-build/lib/python3.12/site-packages/keyring/backends/macOS/__init__.py:61:            return api.find_generic_password(self.keychain, service, username)
./.venv-build/lib/python3.12/site-packages/keyring/backends/macOS/__init__.py:65:            raise KeyringLocked(f"Can't get password from keychain: {e}") from e
./.venv-build/lib/python3.12/site-packages/keyring/backends/macOS/__init__.py:67:            raise KeyringError(f"Can't get password from keychain: {e}") from e
./.venv-build/lib/python3.12/site-packages/keyring/backends/macOS/__init__.py:70:    def delete_password(self, service, username):
./.venv-build/lib/python3.12/site-packages/keyring/backends/macOS/__init__.py:75:            return api.delete_generic_password(self.keychain, service, username)
./.venv-build/lib/python3.12/site-packages/keyring/backends/macOS/__init__.py:77:            raise PasswordDeleteError(f"Can't delete password in keychain: {e}") from e
./.venv-build/lib/python3.12/site-packages/keyring/backends/libsecret.py:9:    PasswordDeleteError,
./.venv-build/lib/python3.12/site-packages/keyring/backends/libsecret.py:10:    PasswordSetError,
./.venv-build/lib/python3.12/site-packages/keyring/backends/libsecret.py:18:    gi.require_version("Secret", "1")
./.venv-build/lib/python3.12/site-packages/keyring/backends/libsecret.py:19:    from gi.repository import Secret
./.venv-build/lib/python3.12/site-packages/keyring/backends/libsecret.py:29:    """libsecret Keyring"""
./.venv-build/lib/python3.12/site-packages/keyring/backends/libsecret.py:35:        return Secret.Schema.new(
./.venv-build/lib/python3.12/site-packages/keyring/backends/libsecret.py:36:            "org.freedesktop.Secret.Generic",
./.venv-build/lib/python3.12/site-packages/keyring/backends/libsecret.py:37:            Secret.SchemaFlags.NONE,
./.venv-build/lib/python3.12/site-packages/keyring/backends/libsecret.py:39:                Secret.SchemaAttributeType.STRING,
./.venv-build/lib/python3.12/site-packages/keyring/backends/libsecret.py:40:                Secret.SchemaAttributeType.STRING,
./.venv-build/lib/python3.12/site-packages/keyring/backends/libsecret.py:41:                application=Secret.SchemaAttributeType.STRING,
./.venv-build/lib/python3.12/site-packages/keyring/backends/libsecret.py:47:        return Secret.COLLECTION_DEFAULT
./.venv-build/lib/python3.12/site-packages/keyring/backends/libsecret.py:52:            raise RuntimeError("libsecret required")
./.venv-build/lib/python3.12/site-packages/keyring/backends/libsecret.py:54:        # Make sure there is actually a secret service running
./.venv-build/lib/python3.12/site-packages/keyring/backends/libsecret.py:56:            Secret.Service.get_sync(Secret.ServiceFlags.OPEN_SESSION, None)
./.venv-build/lib/python3.12/site-packages/keyring/backends/libsecret.py:58:            raise RuntimeError("Can't open a session to the secret service") from error
./.venv-build/lib/python3.12/site-packages/keyring/backends/libsecret.py:62:    def get_password(self, service, username):
./.venv-build/lib/python3.12/site-packages/keyring/backends/libsecret.py:63:        """Get password of the username for the service"""
./.venv-build/lib/python3.12/site-packages/keyring/backends/libsecret.py:66:            items = Secret.password_search_sync(
./.venv-build/lib/python3.12/site-packages/keyring/backends/libsecret.py:67:                self.schema, attributes, Secret.SearchFlags.UNLOCK, None
./.venv-build/lib/python3.12/site-packages/keyring/backends/libsecret.py:76:                return item.retrieve_secret_sync().get_text()
./.venv-build/lib/python3.12/site-packages/keyring/backends/libsecret.py:78:                quark = GLib.quark_try_string("secret-error")
./.venv-build/lib/python3.12/site-packages/keyring/backends/libsecret.py:79:                if error.matches(quark, Secret.Error.IS_LOCKED):
./.venv-build/lib/python3.12/site-packages/keyring/backends/libsecret.py:83:    def set_password(self, service, username, password):
./.venv-build/lib/python3.12/site-packages/keyring/backends/libsecret.py:84:        """Set password for the username of the service"""
./.venv-build/lib/python3.12/site-packages/keyring/backends/libsecret.py:86:        label = f"Password for '{username}' on '{service}'"
./.venv-build/lib/python3.12/site-packages/keyring/backends/libsecret.py:88:            stored = Secret.password_store_sync(
./.venv-build/lib/python3.12/site-packages/keyring/backends/libsecret.py:89:                self.schema, attributes, self.collection, label, password, None
./.venv-build/lib/python3.12/site-packages/keyring/backends/libsecret.py:92:            quark = GLib.quark_try_string("secret-error")
./.venv-build/lib/python3.12/site-packages/keyring/backends/libsecret.py:93:            if error.matches(quark, Secret.Error.IS_LOCKED):
./.venv-build/lib/python3.12/site-packages/keyring/backends/libsecret.py:100:            raise PasswordSetError("Failed to store password!")
./.venv-build/lib/python3.12/site-packages/keyring/backends/libsecret.py:102:    def delete_password(self, service, username):
./.venv-build/lib/python3.12/site-packages/keyring/backends/libsecret.py:103:        """Delete the stored password (only the first one)"""
./.venv-build/lib/python3.12/site-packages/keyring/backends/libsecret.py:106:            items = Secret.password_search_sync(
./.venv-build/lib/python3.12/site-packages/keyring/backends/libsecret.py:107:                self.schema, attributes, Secret.SearchFlags.UNLOCK, None
./.venv-build/lib/python3.12/site-packages/keyring/backends/libsecret.py:116:                removed = Secret.password_clear_sync(self.schema, item.get_attributes(), None)
./.venv-build/lib/python3.12/site-packages/keyring/backends/libsecret.py:118:                quark = GLib.quark_try_string("secret-error")
./.venv-build/lib/python3.12/site-packages/keyring/backends/libsecret.py:119:                if error.matches(quark, Secret.Error.IS_LOCKED):
./.venv-build/lib/python3.12/site-packages/keyring/backends/libsecret.py:123:        raise PasswordDeleteError("No such password!")
./.venv-build/lib/python3.12/site-packages/keyring/backends/libsecret.py:126:        """Get the first username and password for a service.
./.venv-build/lib/python3.12/site-packages/keyring/backends/libsecret.py:129:        The username can be omitted, but if there is one, it will use get_password
./.venv-build/lib/python3.12/site-packages/keyring/backends/libsecret.py:130:        and return a SimpleCredential containing  the username and password
./.venv-build/lib/python3.12/site-packages/keyring/backends/libsecret.py:131:        Otherwise, it will return the first username and password combo that it finds.
./.venv-build/lib/python3.12/site-packages/keyring/backends/libsecret.py:135:            items = Secret.password_search_sync(self.schema, query, Secret.SearchFlags.UNLOCK, None)
./.venv-build/lib/python3.12/site-packages/keyring/backends/libsecret.py:144:                return SimpleCredential(username, item.retrieve_secret_sync().get_text())
./.venv-build/lib/python3.12/site-packages/keyring/backends/libsecret.py:146:                quark = GLib.quark_try_string("secret-error")
./.venv-build/lib/python3.12/site-packages/keyring/backends/libsecret.py:147:                if error.matches(quark, Secret.Error.IS_LOCKED):
./.venv-build/lib/python3.12/site-packages/keyring/backends/null.py:10:    >>> kr.get_password('svc', 'user')
./.venv-build/lib/python3.12/site-packages/keyring/backends/null.py:17:    def get_password(self, service, username, password=None):
./.venv-build/lib/python3.12/site-packages/keyring/backends/null.py:20:    set_password = delete_password = get_password
./.venv-build/lib/python3.12/site-packages/keyring/backends/Windows.py:10:from ..errors import PasswordDeleteError
./.venv-build/lib/python3.12/site-packages/keyring/backends/Windows.py:67:    WinVaultKeyring stores encrypted passwords using the Windows Credential
./.venv-build/lib/python3.12/site-packages/keyring/backends/Windows.py:77:    Passwords are stored under the service name unless there is a collision
./.venv-build/lib/python3.12/site-packages/keyring/backends/Windows.py:78:    (another password with the same service name but different user name),
./.venv-build/lib/python3.12/site-packages/keyring/backends/Windows.py:79:    in which case the previous password is moved into a compound name:
./.venv-build/lib/python3.12/site-packages/keyring/backends/Windows.py:98:    def get_password(self, service, username):
./.venv-build/lib/python3.12/site-packages/keyring/backends/Windows.py:103:        # first attempt to get the password under the service name
./.venv-build/lib/python3.12/site-packages/keyring/backends/Windows.py:119:    def set_password(self, service, username, password):
./.venv-build/lib/python3.12/site-packages/keyring/backends/Windows.py:122:            # resave the existing password using a compound target
./.venv-build/lib/python3.12/site-packages/keyring/backends/Windows.py:125:            self._set_password(
./.venv-build/lib/python3.12/site-packages/keyring/backends/Windows.py:130:        self._set_password(service, username, str(password))
./.venv-build/lib/python3.12/site-packages/keyring/backends/Windows.py:132:    def _set_password(self, target, username, password):
./.venv-build/lib/python3.12/site-packages/keyring/backends/Windows.py:137:            CredentialBlob=password,
./.venv-build/lib/python3.12/site-packages/keyring/backends/Windows.py:143:    def delete_password(self, service, username):
./.venv-build/lib/python3.12/site-packages/keyring/backends/Windows.py:150:                self._delete_password(target)
./.venv-build/lib/python3.12/site-packages/keyring/backends/Windows.py:152:            raise PasswordDeleteError(service)
./.venv-build/lib/python3.12/site-packages/keyring/backends/Windows.py:154:    def _delete_password(self, target):
./.venv-build/lib/python3.12/site-packages/keyring/backends/SecretService.py:13:    PasswordDeleteError,
./.venv-build/lib/python3.12/site-packages/keyring/backends/SecretService.py:17:    import secretstorage
./.venv-build/lib/python3.12/site-packages/keyring/backends/SecretService.py:18:    import secretstorage.exceptions as exceptions
./.venv-build/lib/python3.12/site-packages/keyring/backends/SecretService.py:29:    """Secret Service Keyring"""
./.venv-build/lib/python3.12/site-packages/keyring/backends/SecretService.py:36:            secretstorage.__name__  # noqa: B018
./.venv-build/lib/python3.12/site-packages/keyring/backends/SecretService.py:38:            raise RuntimeError("SecretStorage required")
./.venv-build/lib/python3.12/site-packages/keyring/backends/SecretService.py:39:        if secretstorage.__version_tuple__ < (3, 2):
./.venv-build/lib/python3.12/site-packages/keyring/backends/SecretService.py:40:            raise RuntimeError("SecretStorage 3.2 or newer required")
./.venv-build/lib/python3.12/site-packages/keyring/backends/SecretService.py:42:            with closing(secretstorage.dbus_init()) as connection:
./.venv-build/lib/python3.12/site-packages/keyring/backends/SecretService.py:43:                if not secretstorage.check_service_availability(connection):
./.venv-build/lib/python3.12/site-packages/keyring/backends/SecretService.py:45:                        "The Secret Service daemon is neither running nor "
./.venv-build/lib/python3.12/site-packages/keyring/backends/SecretService.py:48:        except exceptions.SecretStorageException as e:
./.venv-build/lib/python3.12/site-packages/keyring/backends/SecretService.py:49:            raise RuntimeError(f"Unable to initialize SecretService: {e}") from e
./.venv-build/lib/python3.12/site-packages/keyring/backends/SecretService.py:57:        bus = secretstorage.dbus_init()
./.venv-build/lib/python3.12/site-packages/keyring/backends/SecretService.py:60:                collection = secretstorage.Collection(bus, self.preferred_collection)
./.venv-build/lib/python3.12/site-packages/keyring/backends/SecretService.py:62:                collection = secretstorage.get_default_collection(bus)
./.venv-build/lib/python3.12/site-packages/keyring/backends/SecretService.py:63:        except exceptions.SecretStorageException as e:
./.venv-build/lib/python3.12/site-packages/keyring/backends/SecretService.py:77:    def get_password(self, service, username):
./.venv-build/lib/python3.12/site-packages/keyring/backends/SecretService.py:78:        """Get password of the username for the service"""
./.venv-build/lib/python3.12/site-packages/keyring/backends/SecretService.py:84:                return item.get_secret().decode("utf-8")
./.venv-build/lib/python3.12/site-packages/keyring/backends/SecretService.py:86:    def set_password(self, service, username, password):
./.venv-build/lib/python3.12/site-packages/keyring/backends/SecretService.py:87:        """Set password for the username of the service"""
./.venv-build/lib/python3.12/site-packages/keyring/backends/SecretService.py:90:        label = f"Password for '{username}' on '{service}'"
./.venv-build/lib/python3.12/site-packages/keyring/backends/SecretService.py:92:            collection.create_item(label, attributes, password, replace=True)
./.venv-build/lib/python3.12/site-packages/keyring/backends/SecretService.py:94:    def delete_password(self, service, username):
./.venv-build/lib/python3.12/site-packages/keyring/backends/SecretService.py:95:        """Delete the stored password (only the first one)"""
./.venv-build/lib/python3.12/site-packages/keyring/backends/SecretService.py:101:        raise PasswordDeleteError("No such password!")
./.venv-build/lib/python3.12/site-packages/keyring/backends/SecretService.py:104:        """Gets the first username and password for a service.
./.venv-build/lib/python3.12/site-packages/keyring/backends/SecretService.py:107:        The username can be omitted, but if there is one, it will use get_password
./.venv-build/lib/python3.12/site-packages/keyring/backends/SecretService.py:108:        and return a SimpleCredential containing  the username and password
./.venv-build/lib/python3.12/site-packages/keyring/backends/SecretService.py:109:        Otherwise, it will return the first username and password combo that it finds.
./.venv-build/lib/python3.12/site-packages/keyring/backends/SecretService.py:120:                return SimpleCredential(username, item.get_secret().decode("utf-8"))
./.venv-build/lib/python3.12/site-packages/keyring/backends/kwallet.py:8:from ..errors import InitError, KeyringLocked, PasswordDeleteError, PasswordSetError
./.venv-build/lib/python3.12/site-packages/keyring/backends/kwallet.py:63:            entry_list = self.iface.readPasswordList(self.handle, old_folder, "*@*", self.appid)
./.venv-build/lib/python3.12/site-packages/keyring/backends/kwallet.py:67:                password = entry[1]
./.venv-build/lib/python3.12/site-packages/keyring/backends/kwallet.py:70:                ret = self.iface.writePassword(self.handle, service, username, password, self.appid)
./.venv-build/lib/python3.12/site-packages/keyring/backends/kwallet.py:74:            entry_list = self.iface.readPasswordList(self.handle, old_folder, "*", self.appid)
./.venv-build/lib/python3.12/site-packages/keyring/backends/kwallet.py:97:    def get_password(self, service, username):
./.venv-build/lib/python3.12/site-packages/keyring/backends/kwallet.py:98:        """Get password of the username for the service"""
./.venv-build/lib/python3.12/site-packages/keyring/backends/kwallet.py:104:        password = self.iface.readPassword(self.handle, service, username, self.appid)
./.venv-build/lib/python3.12/site-packages/keyring/backends/kwallet.py:105:        return str(password)
./.venv-build/lib/python3.12/site-packages/keyring/backends/kwallet.py:108:        """Gets the first username and password for a service.
./.venv-build/lib/python3.12/site-packages/keyring/backends/kwallet.py:112:        get_password.
./.venv-build/lib/python3.12/site-packages/keyring/backends/kwallet.py:113:        Otherwise, it will return the first username and password combo that it finds.
./.venv-build/lib/python3.12/site-packages/keyring/backends/kwallet.py:123:            password = self.iface.readPassword(self.handle, service, username, self.appid)
./.venv-build/lib/python3.12/site-packages/keyring/backends/kwallet.py:124:            return SimpleCredential(str(username), str(password))
./.venv-build/lib/python3.12/site-packages/keyring/backends/kwallet.py:126:    def set_password(self, service, username, password):
./.venv-build/lib/python3.12/site-packages/keyring/backends/kwallet.py:127:        """Set password for the username of the service"""
./.venv-build/lib/python3.12/site-packages/keyring/backends/kwallet.py:130:            raise PasswordSetError("Cancelled by user")
./.venv-build/lib/python3.12/site-packages/keyring/backends/kwallet.py:131:        self.iface.writePassword(self.handle, service, username, password, self.appid)
./.venv-build/lib/python3.12/site-packages/keyring/backends/kwallet.py:133:    def delete_password(self, service, username):
./.venv-build/lib/python3.12/site-packages/keyring/backends/kwallet.py:134:        """Delete the password for the username of the service."""
./.venv-build/lib/python3.12/site-packages/keyring/backends/kwallet.py:137:            raise PasswordDeleteError("Cancelled by user")
./.venv-build/lib/python3.12/site-packages/keyring/backends/kwallet.py:139:            raise PasswordDeleteError("Password not found")
./.venv-build/lib/python3.12/site-packages/keyring/backends/fail.py:11:    >>> kr.get_password('svc', 'user')
./.venv-build/lib/python3.12/site-packages/keyring/backends/fail.py:21:    def get_password(self, service, username, password=None):
./.venv-build/lib/python3.12/site-packages/keyring/backends/fail.py:30:    set_password = delete_password = get_password
./.venv-build/lib/python3.12/site-packages/keyring/backends/chainer.py:3:discover passwords in each.
./.venv-build/lib/python3.12/site-packages/keyring/backends/chainer.py:45:    def get_password(self, service, username):
./.venv-build/lib/python3.12/site-packages/keyring/backends/chainer.py:47:            password = keyring.get_password(service, username)
./.venv-build/lib/python3.12/site-packages/keyring/backends/chainer.py:48:            if password is not None:
./.venv-build/lib/python3.12/site-packages/keyring/backends/chainer.py:49:                return password
./.venv-build/lib/python3.12/site-packages/keyring/backends/chainer.py:51:    def set_password(self, service, username, password):
./.venv-build/lib/python3.12/site-packages/keyring/backends/chainer.py:54:                return keyring.set_password(service, username, password)
./.venv-build/lib/python3.12/site-packages/keyring/backends/chainer.py:58:    def delete_password(self, service, username):
./.venv-build/lib/python3.12/site-packages/keyring/backends/chainer.py:61:                return keyring.delete_password(service, username)
./.venv-build/lib/python3.12/site-packages/keyring/testing/backend.py:33:    """Test for the keyring's basic functions. password_set and password_get"""
./.venv-build/lib/python3.12/site-packages/keyring/testing/backend.py:45:            self.keyring.delete_password(*item)
./.venv-build/lib/python3.12/site-packages/keyring/testing/backend.py:47:    def set_password(self, service, username, password):
./.venv-build/lib/python3.12/site-packages/keyring/testing/backend.py:48:        # set the password and save the result so the test runner can clean
./.venv-build/lib/python3.12/site-packages/keyring/testing/backend.py:50:        self.keyring.set_password(service, username, password)
./.venv-build/lib/python3.12/site-packages/keyring/testing/backend.py:53:    def check_set_get(self, service, username, password):
./.venv-build/lib/python3.12/site-packages/keyring/testing/backend.py:56:        # for the non-existent password
./.venv-build/lib/python3.12/site-packages/keyring/testing/backend.py:57:        assert keyring.get_password(service, username) is None
./.venv-build/lib/python3.12/site-packages/keyring/testing/backend.py:60:        self.set_password(service, username, password)
./.venv-build/lib/python3.12/site-packages/keyring/testing/backend.py:61:        assert keyring.get_password(service, username) == password
./.venv-build/lib/python3.12/site-packages/keyring/testing/backend.py:63:        # for the empty password
./.venv-build/lib/python3.12/site-packages/keyring/testing/backend.py:64:        self.set_password(service, username, "")
./.venv-build/lib/python3.12/site-packages/keyring/testing/backend.py:65:        assert keyring.get_password(service, username) == ""
./.venv-build/lib/python3.12/site-packages/keyring/testing/backend.py:67:    def test_password_set_get(self):
./.venv-build/lib/python3.12/site-packages/keyring/testing/backend.py:68:        password = random_string(20)
./.venv-build/lib/python3.12/site-packages/keyring/testing/backend.py:71:        self.check_set_get(service, username, password)
./.venv-build/lib/python3.12/site-packages/keyring/testing/backend.py:76:        self.keyring.set_password(service, username, "")
./.venv-build/lib/python3.12/site-packages/keyring/testing/backend.py:77:        self.keyring.set_password(service, username, "non-blank")
./.venv-build/lib/python3.12/site-packages/keyring/testing/backend.py:80:        password = random_string(20, self.DIFFICULT_CHARS)
./.venv-build/lib/python3.12/site-packages/keyring/testing/backend.py:83:        self.check_set_get(service, username, password)
./.venv-build/lib/python3.12/site-packages/keyring/testing/backend.py:86:        password = random_string(20, self.DIFFICULT_CHARS)
./.venv-build/lib/python3.12/site-packages/keyring/testing/backend.py:89:        self.keyring.set_password(service, username, password)
./.venv-build/lib/python3.12/site-packages/keyring/testing/backend.py:90:        self.keyring.delete_password(service, username)
./.venv-build/lib/python3.12/site-packages/keyring/testing/backend.py:91:        assert self.keyring.get_password(service, username) is None
./.venv-build/lib/python3.12/site-packages/keyring/testing/backend.py:96:        with pytest.raises(errors.PasswordDeleteError):
./.venv-build/lib/python3.12/site-packages/keyring/testing/backend.py:97:            self.keyring.delete_password(service, username)
./.venv-build/lib/python3.12/site-packages/keyring/testing/backend.py:102:        password = random_string(20, self.DIFFICULT_CHARS)
./.venv-build/lib/python3.12/site-packages/keyring/testing/backend.py:104:        self.keyring.set_password(service, username1, password)
./.venv-build/lib/python3.12/site-packages/keyring/testing/backend.py:105:        self.set_password(service, username2, password)
./.venv-build/lib/python3.12/site-packages/keyring/testing/backend.py:106:        self.keyring.delete_password(service, username1)
./.venv-build/lib/python3.12/site-packages/keyring/testing/backend.py:107:        assert self.keyring.get_password(service, username2) == password
./.venv-build/lib/python3.12/site-packages/keyring/testing/backend.py:113:        password = random_string(20, UNICODE_CHARS)
./.venv-build/lib/python3.12/site-packages/keyring/testing/backend.py:116:        self.check_set_get(service, username, password)
./.venv-build/lib/python3.12/site-packages/keyring/testing/backend.py:124:        password = random_string(20, source)
./.venv-build/lib/python3.12/site-packages/keyring/testing/backend.py:127:        self.check_set_get(service, username, password)
./.venv-build/lib/python3.12/site-packages/keyring/testing/backend.py:131:        Issue #47 reports that WinVault isn't storing passwords for
./.venv-build/lib/python3.12/site-packages/keyring/testing/backend.py:137:        self.set_password("service1", "user1", "password1")
./.venv-build/lib/python3.12/site-packages/keyring/testing/backend.py:138:        self.set_password("service1", "user2", "password2")
./.venv-build/lib/python3.12/site-packages/keyring/testing/backend.py:139:        assert keyring.get_password("service1", "user1") == "password1"
./.venv-build/lib/python3.12/site-packages/keyring/testing/backend.py:140:        assert keyring.get_password("service1", "user2") == "password2"
./.venv-build/lib/python3.12/site-packages/keyring/testing/backend.py:141:        self.set_password("service2", "user3", "password3")
./.venv-build/lib/python3.12/site-packages/keyring/testing/backend.py:142:        assert keyring.get_password("service1", "user1") == "password1"
./.venv-build/lib/python3.12/site-packages/keyring/testing/backend.py:150:        self.set_password("service1", "user1", "password1")
./.venv-build/lib/python3.12/site-packages/keyring/testing/backend.py:151:        self.set_password("service1", "user2", "password2")
./.venv-build/lib/python3.12/site-packages/keyring/testing/backend.py:154:        assert cred is None or (cred.username, cred.password) in (
./.venv-build/lib/python3.12/site-packages/keyring/testing/backend.py:155:            ("user1", "password1"),
./.venv-build/lib/python3.12/site-packages/keyring/testing/backend.py:156:            ("user2", "password2"),
./.venv-build/lib/python3.12/site-packages/keyring/testing/backend.py:161:        assert (cred.username, cred.password) in (
./.venv-build/lib/python3.12/site-packages/keyring/testing/backend.py:162:            ("user1", "password1"),
./.venv-build/lib/python3.12/site-packages/keyring/testing/backend.py:163:            ("user2", "password2"),
./.venv-build/lib/python3.12/site-packages/keyring/testing/backend.py:169:            self.set_password("service1", "", "password1")
./.venv-build/lib/python3.12/site-packages/keyring/testing/backend.py:170:        assert self.keyring.get_password("service1", "") == "password1"
./.venv-build/lib/python3.12/site-packages/keyring/testing/backend.py:191:        password_1 = "password1"
./.venv-build/lib/python3.12/site-packages/keyring/testing/backend.py:192:        password_2 = "password2"
./.venv-build/lib/python3.12/site-packages/keyring/testing/backend.py:193:        self.set_password(service, "user1", password_1)
./.venv-build/lib/python3.12/site-packages/keyring/testing/backend.py:194:        self.set_password(service, "user2", password_2)
./.venv-build/lib/python3.12/site-packages/keyring/testing/backend.py:196:        assert keyring.get_credential(service, "user1").password == password_1
./.venv-build/lib/python3.12/site-packages/keyring/testing/backend.py:197:        assert keyring.get_credential(service, "user2").password == password_2
./.venv-build/lib/python3.12/site-packages/keyring/__init__.py:2:    delete_password,
./.venv-build/lib/python3.12/site-packages/keyring/__init__.py:5:    get_password,
./.venv-build/lib/python3.12/site-packages/keyring/__init__.py:7:    set_password,
./.venv-build/lib/python3.12/site-packages/keyring/__init__.py:13:    "set_password",
./.venv-build/lib/python3.12/site-packages/keyring/__init__.py:14:    "get_password",
./.venv-build/lib/python3.12/site-packages/keyring/__init__.py:15:    "delete_password",
./.venv-build/lib/python3.12/site-packages/keyring/errors.py:9:class PasswordSetError(KeyringError):
./.venv-build/lib/python3.12/site-packages/keyring/errors.py:10:    """Raised when the password can't be set."""
./.venv-build/lib/python3.12/site-packages/keyring/errors.py:13:class PasswordDeleteError(KeyringError):
./.venv-build/lib/python3.12/site-packages/keyring/errors.py:14:    """Raised when the password can't be deleted."""
./.venv-build/lib/python3.12/site-packages/requests_toolbelt/adapters/x509.py:13:    load_pem_private_key,
./.venv-build/lib/python3.12/site-packages/requests_toolbelt/adapters/x509.py:14:    load_der_private_key,
./.venv-build/lib/python3.12/site-packages/requests_toolbelt/adapters/x509.py:68:    :param password:
./.venv-build/lib/python3.12/site-packages/requests_toolbelt/adapters/x509.py:92:        password = kwargs.pop("password", None)
./.venv-build/lib/python3.12/site-packages/requests_toolbelt/adapters/x509.py:95:        password_bytes = None
./.venv-build/lib/python3.12/site-packages/requests_toolbelt/adapters/x509.py:110:        if isinstance(password, bytes):
./.venv-build/lib/python3.12/site-packages/requests_toolbelt/adapters/x509.py:111:            password_bytes = password
./.venv-build/lib/python3.12/site-packages/requests_toolbelt/adapters/x509.py:112:        elif password:
./.venv-build/lib/python3.12/site-packages/requests_toolbelt/adapters/x509.py:113:            password_bytes = password.encode("utf8")
./.venv-build/lib/python3.12/site-packages/requests_toolbelt/adapters/x509.py:115:        self.ssl_context = create_ssl_context(cert_bytes, pk_bytes, password_bytes, encoding)
./.venv-build/lib/python3.12/site-packages/requests_toolbelt/adapters/x509.py:163:def create_ssl_context(cert_byes, pk_bytes, password=None, encoding=Encoding.PEM):
./.venv-build/lib/python3.12/site-packages/requests_toolbelt/adapters/x509.py:164:    """Create an SSL Context with the supplied cert/password.
./.venv-build/lib/python3.12/site-packages/requests_toolbelt/adapters/x509.py:170:    :param password array of bytes containing the passphrase to be used
./.venv-build/lib/python3.12/site-packages/requests_toolbelt/adapters/x509.py:184:        key = load_pem_private_key(pk_bytes, password, backend)
./.venv-build/lib/python3.12/site-packages/requests_toolbelt/adapters/x509.py:187:        key = load_der_private_key(pk_bytes, password, backend)
./.venv-build/lib/python3.12/site-packages/requests_toolbelt/auth/guess.py:12:    def __init__(self, username, password):
./.venv-build/lib/python3.12/site-packages/requests_toolbelt/auth/guess.py:14:        self.password = password
./.venv-build/lib/python3.12/site-packages/requests_toolbelt/auth/guess.py:32:        self.auth = auth.HTTPBasicAuth(self.username, self.password)
./.venv-build/lib/python3.12/site-packages/requests_toolbelt/auth/guess.py:41:        self.auth = auth_compat.HTTPDigestAuth(self.username, self.password)
./.venv-build/lib/python3.12/site-packages/requests_toolbelt/auth/guess.py:88:    def __init__(self, username=None, password=None, proxy_username=None, proxy_password=None):
./.venv-build/lib/python3.12/site-packages/requests_toolbelt/auth/guess.py:89:        super(GuessProxyAuth, self).__init__(username, password)
./.venv-build/lib/python3.12/site-packages/requests_toolbelt/auth/guess.py:91:        self.proxy_password = proxy_password
./.venv-build/lib/python3.12/site-packages/requests_toolbelt/auth/guess.py:106:        self.proxy_auth = auth.HTTPProxyAuth(self.proxy_username, self.proxy_password)
./.venv-build/lib/python3.12/site-packages/requests_toolbelt/auth/guess.py:116:            username=self.proxy_username, password=self.proxy_password
./.venv-build/lib/python3.12/site-packages/requests_toolbelt/auth/handler.py:30:            'https://api.github.com': ('sigmavirus24', 'fakepassword'),
./.venv-build/lib/python3.12/site-packages/requests_toolbelt/auth/handler.py:31:            'https://example.com': HTTPDigestAuth('username', 'password')
./.venv-build/lib/python3.12/site-packages/requests_toolbelt/auth/handler.py:82:            that domain. For example: ``('username', 'password')`` or
./.venv-build/lib/python3.12/site-packages/requests_toolbelt/auth/handler.py:83:            ``requests.HTTPDigestAuth('username', 'password')``
./.venv-build/lib/python3.12/site-packages/requests_toolbelt/auth/handler.py:88:            a.add_strategy('https://api.github.com', ('username', 'password'))
./.venv-build/lib/python3.12/site-packages/requests_toolbelt/auth/http_proxy_digest.py:16:        new username and password. i.e., retry build_digest_header
./.venv-build/lib/python3.12/site-packages/requests_toolbelt/auth/http_proxy_digest.py:75:                    raise IOError("User or password is invalid")
./.venv-build/lib/python3.12/site-packages/packaging/markers.py:15:from ._tokenizer import ParserSyntaxError
./.venv-build/lib/python3.12/site-packages/packaging/_tokenizer.py:12:class Token:
./.venv-build/lib/python3.12/site-packages/packaging/_tokenizer.py:91:class Tokenizer:
./.venv-build/lib/python3.12/site-packages/packaging/_tokenizer.py:92:    """Context-sensitive token parsing.
./.venv-build/lib/python3.12/site-packages/packaging/_tokenizer.py:94:    Provides methods to examine the input stream to check whether the next token
./.venv-build/lib/python3.12/site-packages/packaging/_tokenizer.py:108:        self.next_token: Token | None = None
./.venv-build/lib/python3.12/site-packages/packaging/_tokenizer.py:112:        """Move beyond provided token name, if at current position."""
./.venv-build/lib/python3.12/site-packages/packaging/_tokenizer.py:117:        """Check whether the next token has the provided name.
./.venv-build/lib/python3.12/site-packages/packaging/_tokenizer.py:119:        By default, if the check succeeds, the token *must* be read before
./.venv-build/lib/python3.12/site-packages/packaging/_tokenizer.py:120:        another check. If `peek` is set to `True`, the token is not loaded and
./.venv-build/lib/python3.12/site-packages/packaging/_tokenizer.py:124:            self.next_token is None
./.venv-build/lib/python3.12/site-packages/packaging/_tokenizer.py:125:        ), f"Cannot check for {name!r}, already have {self.next_token!r}"
./.venv-build/lib/python3.12/site-packages/packaging/_tokenizer.py:126:        assert name in self.rules, f"Unknown token name: {name!r}"
./.venv-build/lib/python3.12/site-packages/packaging/_tokenizer.py:134:            self.next_token = Token(name, match[0], self.position)
./.venv-build/lib/python3.12/site-packages/packaging/_tokenizer.py:137:    def expect(self, name: str, *, expected: str) -> Token:
./.venv-build/lib/python3.12/site-packages/packaging/_tokenizer.py:138:        """Expect a certain token name next, failing with a syntax error otherwise.
./.venv-build/lib/python3.12/site-packages/packaging/_tokenizer.py:140:        The token is *not* read.
./.venv-build/lib/python3.12/site-packages/packaging/_tokenizer.py:146:    def read(self) -> Token:
./.venv-build/lib/python3.12/site-packages/packaging/_tokenizer.py:147:        """Consume the next token and return it."""
./.venv-build/lib/python3.12/site-packages/packaging/_tokenizer.py:148:        token = self.next_token
./.venv-build/lib/python3.12/site-packages/packaging/_tokenizer.py:149:        assert token is not None
./.venv-build/lib/python3.12/site-packages/packaging/_tokenizer.py:151:        self.position += len(token.text)
./.venv-build/lib/python3.12/site-packages/packaging/_tokenizer.py:152:        self.next_token = None
./.venv-build/lib/python3.12/site-packages/packaging/_tokenizer.py:154:        return token
./.venv-build/lib/python3.12/site-packages/packaging/_tokenizer.py:175:    def enclosing_tokens(self, open_token: str, close_token: str, *, around: str) -> Iterator[None]:
./.venv-build/lib/python3.12/site-packages/packaging/_tokenizer.py:176:        if self.check(open_token):
./.venv-build/lib/python3.12/site-packages/packaging/_tokenizer.py:187:        if not self.check(close_token):
./.venv-build/lib/python3.12/site-packages/packaging/_tokenizer.py:189:                f"Expected matching {close_token} for {open_token}, after {around}",
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:12:from ._tokenizer import DEFAULT_RULES, Tokenizer
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:62:    return _parse_requirement(Tokenizer(source, rules=DEFAULT_RULES))
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:65:def _parse_requirement(tokenizer: Tokenizer) -> ParsedRequirement:
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:69:    tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:71:    name_token = tokenizer.expect(
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:74:    name = name_token.text
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:75:    tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:77:    extras = _parse_extras(tokenizer)
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:78:    tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:80:    url, specifier, marker = _parse_requirement_details(tokenizer)
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:81:    tokenizer.expect("END", expected="end of dependency specifier")
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:87:    tokenizer: Tokenizer,
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:98:    if tokenizer.check("AT"):
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:99:        tokenizer.read()
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:100:        tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:102:        url_start = tokenizer.position
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:103:        url = tokenizer.expect("URL", expected="URL after @").text
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:104:        if tokenizer.check("END", peek=True):
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:107:        tokenizer.expect("WS", expected="whitespace after URL")
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:110:        if tokenizer.check("END", peek=True):
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:114:            tokenizer, span_start=url_start, after="URL and whitespace"
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:117:        specifier_start = tokenizer.position
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:118:        specifier = _parse_specifier(tokenizer)
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:119:        tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:121:        if tokenizer.check("END", peek=True):
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:125:            tokenizer,
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:133:def _parse_requirement_marker(tokenizer: Tokenizer, *, span_start: int, after: str) -> MarkerList:
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:138:    if not tokenizer.check("SEMICOLON"):
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:139:        tokenizer.raise_syntax_error(
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:143:    tokenizer.read()
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:145:    marker = _parse_marker(tokenizer)
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:146:    tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:151:def _parse_extras(tokenizer: Tokenizer) -> list[str]:
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:155:    if not tokenizer.check("LEFT_BRACKET", peek=True):
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:158:    with tokenizer.enclosing_tokens(
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:163:        tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:164:        extras = _parse_extras_list(tokenizer)
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:165:        tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:170:def _parse_extras_list(tokenizer: Tokenizer) -> list[str]:
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:176:    if not tokenizer.check("IDENTIFIER"):
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:179:    extras.append(tokenizer.read().text)
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:182:        tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:183:        if tokenizer.check("IDENTIFIER", peek=True):
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:184:            tokenizer.raise_syntax_error("Expected comma between extra names")
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:185:        elif not tokenizer.check("COMMA"):
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:188:        tokenizer.read()
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:189:        tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:191:        extra_token = tokenizer.expect("IDENTIFIER", expected="extra name after comma")
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:192:        extras.append(extra_token.text)
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:197:def _parse_specifier(tokenizer: Tokenizer) -> str:
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:202:    with tokenizer.enclosing_tokens(
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:207:        tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:208:        parsed_specifiers = _parse_version_many(tokenizer)
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:209:        tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:214:def _parse_version_many(tokenizer: Tokenizer) -> str:
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:219:    while tokenizer.check("SPECIFIER"):
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:220:        span_start = tokenizer.position
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:221:        parsed_specifiers += tokenizer.read().text
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:222:        if tokenizer.check("VERSION_PREFIX_TRAIL", peek=True):
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:223:            tokenizer.raise_syntax_error(
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:226:                span_end=tokenizer.position + 1,
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:228:        if tokenizer.check("VERSION_LOCAL_LABEL_TRAIL", peek=True):
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:229:            tokenizer.raise_syntax_error(
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:232:                span_end=tokenizer.position,
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:234:        tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:235:        if not tokenizer.check("COMMA"):
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:237:        parsed_specifiers += tokenizer.read().text
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:238:        tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:247:    return _parse_full_marker(Tokenizer(source, rules=DEFAULT_RULES))
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:250:def _parse_full_marker(tokenizer: Tokenizer) -> MarkerList:
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:251:    retval = _parse_marker(tokenizer)
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:252:    tokenizer.expect("END", expected="end of marker expression")
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:256:def _parse_marker(tokenizer: Tokenizer) -> MarkerList:
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:260:    expression = [_parse_marker_atom(tokenizer)]
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:261:    while tokenizer.check("BOOLOP"):
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:262:        token = tokenizer.read()
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:263:        expr_right = _parse_marker_atom(tokenizer)
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:264:        expression.extend((token.text, expr_right))
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:268:def _parse_marker_atom(tokenizer: Tokenizer) -> MarkerAtom:
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:274:    tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:275:    if tokenizer.check("LEFT_PARENTHESIS", peek=True):
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:276:        with tokenizer.enclosing_tokens(
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:281:            tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:282:            marker: MarkerAtom = _parse_marker(tokenizer)
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:283:            tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:285:        marker = _parse_marker_item(tokenizer)
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:286:    tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:290:def _parse_marker_item(tokenizer: Tokenizer) -> MarkerItem:
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:294:    tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:295:    marker_var_left = _parse_marker_var(tokenizer)
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:296:    tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:297:    marker_op = _parse_marker_op(tokenizer)
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:298:    tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:299:    marker_var_right = _parse_marker_var(tokenizer)
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:300:    tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:304:def _parse_marker_var(tokenizer: Tokenizer) -> MarkerVar:
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:308:    if tokenizer.check("VARIABLE"):
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:309:        return process_env_var(tokenizer.read().text.replace(".", "_"))
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:310:    elif tokenizer.check("QUOTED_STRING"):
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:311:        return process_python_str(tokenizer.read().text)
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:313:        tokenizer.raise_syntax_error(message="Expected a marker variable or quoted string")
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:328:def _parse_marker_op(tokenizer: Tokenizer) -> Op:
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:332:    if tokenizer.check("IN"):
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:333:        tokenizer.read()
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:335:    elif tokenizer.check("NOT"):
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:336:        tokenizer.read()
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:337:        tokenizer.expect("WS", expected="whitespace after 'not'")
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:338:        tokenizer.expect("IN", expected="'in' after 'not'")
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:340:    elif tokenizer.check("OP"):
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:341:        return Op(tokenizer.read().text)
./.venv-build/lib/python3.12/site-packages/packaging/_parser.py:343:        return tokenizer.raise_syntax_error(
./.venv-build/lib/python3.12/site-packages/packaging/licenses/__init__.py:67:    # Pad any parentheses so tokenization can be achieved by merely splitting on
./.venv-build/lib/python3.12/site-packages/packaging/licenses/__init__.py:81:    tokens = license_expression.split()
./.venv-build/lib/python3.12/site-packages/packaging/licenses/__init__.py:86:    python_tokens = []
./.venv-build/lib/python3.12/site-packages/packaging/licenses/__init__.py:87:    for token in tokens:
./.venv-build/lib/python3.12/site-packages/packaging/licenses/__init__.py:88:        if token not in {"or", "and", "with", "(", ")"}:
./.venv-build/lib/python3.12/site-packages/packaging/licenses/__init__.py:89:            python_tokens.append("False")
./.venv-build/lib/python3.12/site-packages/packaging/licenses/__init__.py:90:        elif token == "with":
./.venv-build/lib/python3.12/site-packages/packaging/licenses/__init__.py:91:            python_tokens.append("or")
./.venv-build/lib/python3.12/site-packages/packaging/licenses/__init__.py:92:        elif token == "(" and python_tokens and python_tokens[-1] not in {"or", "and"}:
./.venv-build/lib/python3.12/site-packages/packaging/licenses/__init__.py:96:            python_tokens.append(token)
./.venv-build/lib/python3.12/site-packages/packaging/licenses/__init__.py:98:    python_expression = " ".join(python_tokens)
./.venv-build/lib/python3.12/site-packages/packaging/licenses/__init__.py:109:    normalized_tokens = []
./.venv-build/lib/python3.12/site-packages/packaging/licenses/__init__.py:110:    for token in tokens:
./.venv-build/lib/python3.12/site-packages/packaging/licenses/__init__.py:111:        if token in {"or", "and", "with", "(", ")"}:
./.venv-build/lib/python3.12/site-packages/packaging/licenses/__init__.py:112:            normalized_tokens.append(token.upper())
./.venv-build/lib/python3.12/site-packages/packaging/licenses/__init__.py:115:        if normalized_tokens and normalized_tokens[-1] == "WITH":
./.venv-build/lib/python3.12/site-packages/packaging/licenses/__init__.py:116:            if token not in EXCEPTIONS:
./.venv-build/lib/python3.12/site-packages/packaging/licenses/__init__.py:117:                message = f"Unknown license exception: {token!r}"
./.venv-build/lib/python3.12/site-packages/packaging/licenses/__init__.py:120:            normalized_tokens.append(EXCEPTIONS[token]["id"])
./.venv-build/lib/python3.12/site-packages/packaging/licenses/__init__.py:122:            if token.endswith("+"):
./.venv-build/lib/python3.12/site-packages/packaging/licenses/__init__.py:123:                final_token = token[:-1]
./.venv-build/lib/python3.12/site-packages/packaging/licenses/__init__.py:126:                final_token = token
./.venv-build/lib/python3.12/site-packages/packaging/licenses/__init__.py:129:            if final_token.startswith("licenseref-"):
./.venv-build/lib/python3.12/site-packages/packaging/licenses/__init__.py:130:                if not license_ref_allowed.match(final_token):
./.venv-build/lib/python3.12/site-packages/packaging/licenses/__init__.py:131:                    message = f"Invalid licenseref: {final_token!r}"
./.venv-build/lib/python3.12/site-packages/packaging/licenses/__init__.py:133:                normalized_tokens.append(license_refs[final_token] + suffix)
./.venv-build/lib/python3.12/site-packages/packaging/licenses/__init__.py:135:                if final_token not in LICENSES:
./.venv-build/lib/python3.12/site-packages/packaging/licenses/__init__.py:136:                    message = f"Unknown license: {final_token!r}"
./.venv-build/lib/python3.12/site-packages/packaging/licenses/__init__.py:138:                normalized_tokens.append(LICENSES[final_token]["id"] + suffix)
./.venv-build/lib/python3.12/site-packages/packaging/licenses/__init__.py:140:    normalized_expression = " ".join(normalized_tokens)
./.venv-build/lib/python3.12/site-packages/packaging/requirements.py:9:from ._tokenizer import ParserSyntaxError
./.venv-build/lib/python3.12/site-packages/cffi/_imp_emulation.py:17:    import tokenize
./.venv-build/lib/python3.12/site-packages/cffi/_imp_emulation.py:72:                encoding = tokenize.detect_encoding(file.readline)[0]
./.venv-build/lib/python3.12/site-packages/requests/auth.py:25:def _basic_auth_str(username, password):
./.venv-build/lib/python3.12/site-packages/requests/auth.py:45:    if not isinstance(password, basestring):
./.venv-build/lib/python3.12/site-packages/requests/auth.py:47:            "Non-string passwords will no longer be supported in Requests "
./.venv-build/lib/python3.12/site-packages/requests/auth.py:50:            "problems.".format(type(password)),
./.venv-build/lib/python3.12/site-packages/requests/auth.py:53:        password = str(password)
./.venv-build/lib/python3.12/site-packages/requests/auth.py:59:    if isinstance(password, str):
./.venv-build/lib/python3.12/site-packages/requests/auth.py:60:        password = password.encode("latin1")
./.venv-build/lib/python3.12/site-packages/requests/auth.py:62:    authstr = "Basic " + to_native_string(b64encode(b":".join((username, password))).strip())
./.venv-build/lib/python3.12/site-packages/requests/auth.py:77:    def __init__(self, username, password):
./.venv-build/lib/python3.12/site-packages/requests/auth.py:79:        self.password = password
./.venv-build/lib/python3.12/site-packages/requests/auth.py:85:                self.password == getattr(other, "password", None),
./.venv-build/lib/python3.12/site-packages/requests/auth.py:93:        r.headers["Authorization"] = _basic_auth_str(self.username, self.password)
./.venv-build/lib/python3.12/site-packages/requests/auth.py:101:        r.headers["Proxy-Authorization"] = _basic_auth_str(self.username, self.password)
./.venv-build/lib/python3.12/site-packages/requests/auth.py:108:    def __init__(self, username, password):
./.venv-build/lib/python3.12/site-packages/requests/auth.py:110:        self.password = password
./.venv-build/lib/python3.12/site-packages/requests/auth.py:187:        A1 = f"{self.username}:{realm}:{self.password}"
./.venv-build/lib/python3.12/site-packages/requests/auth.py:305:                self.password == getattr(other, "password", None),
./.venv-build/lib/python3.12/site-packages/requests/utils.py:237:                # Return with login / password
./.venv-build/lib/python3.12/site-packages/requests/utils.py:378:    >>> parse_list_header('token, "quoted value"')
./.venv-build/lib/python3.12/site-packages/requests/utils.py:379:    ['token', 'quoted value']
./.venv-build/lib/python3.12/site-packages/requests/utils.py:508:    tokens = header.split(";")
./.venv-build/lib/python3.12/site-packages/requests/utils.py:509:    content_type, params = tokens[0].strip(), tokens[1:]
./.venv-build/lib/python3.12/site-packages/requests/utils.py:1006:    username,password.
./.venv-build/lib/python3.12/site-packages/requests/utils.py:1013:        auth = (unquote(parsed.username), unquote(parsed.password))
./.venv-build/lib/python3.12/site-packages/requests/sessions.py:317:            username, password = get_auth_from_url(new_proxies[scheme])
./.venv-build/lib/python3.12/site-packages/requests/sessions.py:319:            username, password = None, None
./.venv-build/lib/python3.12/site-packages/requests/sessions.py:323:        if not scheme.startswith("https") and username and password:
./.venv-build/lib/python3.12/site-packages/requests/sessions.py:324:            headers["Proxy-Authorization"] = _basic_auth_str(username, password)
./.venv-build/lib/python3.12/site-packages/requests/adapters.py:251:            username, password = get_auth_from_url(proxy)
./.venv-build/lib/python3.12/site-packages/requests/adapters.py:255:                password=password,
./.venv-build/lib/python3.12/site-packages/requests/adapters.py:568:        username, password = get_auth_from_url(proxy)
./.venv-build/lib/python3.12/site-packages/requests/adapters.py:571:            headers["Proxy-Authorization"] = _basic_auth_str(username, password)
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/logging.py:275:    log.warning("password was rejected for admin site.")
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/containers.py:146:                tokens: List[Text] = []
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/containers.py:148:                    tokens.append(word)
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/containers.py:153:                        tokens.append(Text(" " * spaces[index], style=space_style))
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/containers.py:154:                self[line_index] = Text("").join(tokens)
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/console.py:2106:        password: bool = False,
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/console.py:2117:            password: (bool, optional): Hide typed text. Defaults to False.
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/console.py:2125:        if password:
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:27:from pip._vendor.pygments.token import (
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:36:    Token,
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:54:TokenType = Tuple[str, ...]
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:62:ANSI_LIGHT: Dict[TokenType, Style] = {
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:63:    Token: Style(),
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:91:ANSI_DARK: Dict[TokenType, Style] = {
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:92:    Token: Style(),
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:128:    def get_style_for_token(self, token_type: TokenType) -> Style:
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:129:        """Get a style for a given Pygments token."""
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:142:        self._style_cache: Dict[TokenType, Style] = {}
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:154:    def get_style_for_token(self, token_type: TokenType) -> Style:
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:157:            return self._style_cache[token_type]
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:160:                pygments_style = self._pygments_style_class.style_for_token(token_type)
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:173:            self._style_cache[token_type] = style
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:183:    def __init__(self, style_map: Dict[TokenType, Style]) -> None:
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:187:        self._style_cache: Dict[TokenType, Style] = {}
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:189:    def get_style_for_token(self, token_type: TokenType) -> Style:
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:192:            return self._style_cache[token_type]
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:198:            token = tuple(token_type)
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:200:            while token:
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:201:                _style = get_style(token)
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:205:                token = token[:-1]
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:206:            self._style_cache[token_type] = style
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:424:    def _get_token_color(self, token_type: TokenType) -> Optional[Color]:
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:425:        """Get a color (if any) for the given token.
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:428:            token_type (TokenType): A token type tuple from Pygments.
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:433:        style = self._theme.get_style_for_token(token_type)
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:489:        _get_theme_style = self._theme.get_style_for_token
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:501:                def line_tokenize() -> Iterable[Tuple[Any, str]]:
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:502:                    """Split tokens to one per line."""
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:505:                    for token_type, token in lexer.get_tokens(code):
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:506:                        while token:
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:507:                            line_token, new_line, token = token.partition("\n")
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:508:                            yield token_type, line_token + new_line
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:510:                def tokens_to_spans() -> Iterable[Tuple[str, Optional[Style]]]:
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:511:                    """Convert tokens to spans."""
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:512:                    tokens = iter(line_tokenize())
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:516:                    # Skip over tokens until line start
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:519:                            _token_type, token = next(tokens)
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:522:                        yield (token, None)
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:523:                        if token.endswith("\n"):
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:526:                    for token_type, token in tokens:
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:527:                        yield (token, _get_theme_style(token_type))
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:528:                        if token.endswith("\n"):
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:533:                text.append_tokens(tokens_to_spans())
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:536:                text.append_tokens(
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:537:                    (token, _get_theme_style(token_type))
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:538:                    for token_type, token in lexer.get_tokens(code)
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:572:        foreground_color = self._get_token_color(Token.Text)
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:600:                self._theme.get_style_for_token(Token.Text),
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:606:                self._theme.get_style_for_token(Token.Text),
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:670:                + self._theme.get_style_for_token(Comment)
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:704:                + self._theme.get_style_for_token(Comment)
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/prompt.py:37:        password (bool, optional): Enable password input. Defaults to False.
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/prompt.py:57:        password: bool = False,
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/prompt.py:67:        self.password = password
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/prompt.py:81:        password: bool = False,
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/prompt.py:97:        password: bool = False,
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/prompt.py:111:        password: bool = False,
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/prompt.py:127:            password (bool, optional): Enable password input. Defaults to False.
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/prompt.py:137:            password=password,
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/prompt.py:188:        password: bool,
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/prompt.py:196:            password (bool): Enable password entry.
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/prompt.py:201:        return console.input(prompt, password=password, stream=stream)
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/prompt.py:278:            value = self.get_input(self.console, prompt, self.password, stream=stream)
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/prompt.py:366:            password = Prompt.ask(
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/prompt.py:367:                "Please enter a password [cyan](must be at least 5 characters)",
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/prompt.py:368:                password=True,
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/prompt.py:370:            if len(password) >= 5:
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/prompt.py:372:            print("[prompt.invalid]password too short")
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/prompt.py:373:        print(f"password={password!r}")
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/_emoji_codes.py:156:    "japanese_secret_button": "ãŠ™",
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/_emoji_codes.py:2882:    "secret": "ãŠ™",
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/text.py:1013:    def append_tokens(self, tokens: Iterable[Tuple[str, Optional[StyleType]]]) -> "Text":
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/text.py:1017:            tokens (Iterable[Tuple[str, Optional[StyleType]]]): An iterable of tuples containing str content and style.
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/text.py:1026:        for content, style in tokens:
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/ansi.py:20:class _AnsiToken(NamedTuple):
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/ansi.py:21:    """Result of ansi tokenized string."""
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/ansi.py:28:def _ansi_tokenize(ansi_text: str) -> Iterable[_AnsiToken]:
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/ansi.py:29:    """Tokenize a string in to plain text and ANSI codes.
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/ansi.py:35:        AnsiToken: A named tuple of (plain, sgr, osc)
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/ansi.py:45:            yield _AnsiToken(ansi_text[position:start])
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/ansi.py:51:                yield _AnsiToken("", sgr[1:-1], osc)
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/ansi.py:53:            yield _AnsiToken("", sgr, osc)
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/ansi.py:56:        yield _AnsiToken(ansi_text[position:])
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/ansi.py:153:        for plain_text, sgr, osc in _ansi_tokenize(line):
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/pretty.py:414:    def iter_tokens(self) -> Iterable[str]:
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/pretty.py:415:        """Generate tokens for this node."""
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/pretty.py:425:                    yield from self.children[0].iter_tokens()
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/pretty.py:429:                        yield from child.iter_tokens()
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/pretty.py:447:        for token in self.iter_tokens():
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/pretty.py:448:            total_length += cell_len(token)
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/pretty.py:454:        repr_text = "".join(self.iter_tokens())
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/traceback.py:24:from pip._vendor.pygments.token import Comment, Keyword, Name, Number, Operator, String
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/traceback.py:25:from pip._vendor.pygments.token import Text as TextToken
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/traceback.py:26:from pip._vendor.pygments.token import Token
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/traceback.py:598:        token_style = theme.get_style_for_token
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/traceback.py:602:                "pretty": token_style(TextToken),
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/traceback.py:603:                "pygments.text": token_style(Token),
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/traceback.py:604:                "pygments.string": token_style(String),
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/traceback.py:605:                "pygments.function": token_style(Name.Function),
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/traceback.py:606:                "pygments.number": token_style(Number),
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/traceback.py:607:                "repr.indent": token_style(Comment) + Style(dim=True),
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/traceback.py:608:                "repr.str": token_style(String),
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/traceback.py:609:                "repr.brace": token_style(TextToken) + Style(bold=True),
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/traceback.py:610:                "repr.number": token_style(Number),
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/traceback.py:611:                "repr.bool_true": token_style(Keyword.Constant),
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/traceback.py:612:                "repr.bool_false": token_style(Keyword.Constant),
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/traceback.py:613:                "repr.none": token_style(Keyword.Constant),
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/traceback.py:614:                "scope.border": token_style(String.Delimiter),
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/traceback.py:615:                "scope.equals": token_style(Operator),
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/traceback.py:616:                "scope.key": token_style(Name),
./.venv-build/lib/python3.12/site-packages/pip/_vendor/rich/traceback.py:617:                "scope.key.special": token_style(Name.Constant) + Style(dim=True),
./.venv-build/lib/python3.12/site-packages/pip/_vendor/urllib3/packages/six.py:426:    MovedAttribute("HTTPPasswordMgr", "urllib2", "urllib.request"),
./.venv-build/lib/python3.12/site-packages/pip/_vendor/urllib3/packages/six.py:427:    MovedAttribute("HTTPPasswordMgrWithDefaultRealm", "urllib2", "urllib.request"),
./.venv-build/lib/python3.12/site-packages/pip/_vendor/urllib3/connectionpool.py:913:    ``ca_cert_dir``, ``ssl_version``, ``key_password`` are only used if :mod:`ssl`
./.venv-build/lib/python3.12/site-packages/pip/_vendor/urllib3/connectionpool.py:936:        key_password=None,
./.venv-build/lib/python3.12/site-packages/pip/_vendor/urllib3/connectionpool.py:962:        self.key_password = key_password
./.venv-build/lib/python3.12/site-packages/pip/_vendor/urllib3/connectionpool.py:978:                key_password=self.key_password,
./.venv-build/lib/python3.12/site-packages/pip/_vendor/urllib3/connectionpool.py:1032:            key_password=self.key_password,
./.venv-build/lib/python3.12/site-packages/pip/_vendor/urllib3/poolmanager.py:36:    "key_password",
./.venv-build/lib/python3.12/site-packages/pip/_vendor/urllib3/poolmanager.py:52:    "key_key_password",  # str
./.venv-build/lib/python3.12/site-packages/pip/_vendor/urllib3/connection.py:212:                "Method cannot contain non-token characters %r (found at least %r)"
./.venv-build/lib/python3.12/site-packages/pip/_vendor/urllib3/connection.py:303:        key_password=None,
./.venv-build/lib/python3.12/site-packages/pip/_vendor/urllib3/connection.py:314:        self.key_password = key_password
./.venv-build/lib/python3.12/site-packages/pip/_vendor/urllib3/connection.py:327:        key_password=None,
./.venv-build/lib/python3.12/site-packages/pip/_vendor/urllib3/connection.py:348:        self.key_password = key_password
./.venv-build/lib/python3.12/site-packages/pip/_vendor/urllib3/connection.py:417:            key_password=self.key_password,
./.venv-build/lib/python3.12/site-packages/pip/_vendor/urllib3/util/request.py:45:        Colon-separated username:password string for 'authorization: basic ...'
./.venv-build/lib/python3.12/site-packages/pip/_vendor/urllib3/util/request.py:49:        Colon-separated username:password string for 'proxy-authorization: basic ...'
./.venv-build/lib/python3.12/site-packages/pip/_vendor/urllib3/util/ssl_.py:371:    key_password=None,
./.venv-build/lib/python3.12/site-packages/pip/_vendor/urllib3/util/ssl_.py:390:    :param key_password:
./.venv-build/lib/python3.12/site-packages/pip/_vendor/urllib3/util/ssl_.py:391:        Optional password if the keyfile is encrypted.
./.venv-build/lib/python3.12/site-packages/pip/_vendor/urllib3/util/ssl_.py:418:    if keyfile and key_password is None and _is_key_file_encrypted(keyfile):
./.venv-build/lib/python3.12/site-packages/pip/_vendor/urllib3/util/ssl_.py:419:        raise SSLError("Client private key is encrypted, password is required")
./.venv-build/lib/python3.12/site-packages/pip/_vendor/urllib3/util/ssl_.py:422:        if key_password is None:
./.venv-build/lib/python3.12/site-packages/pip/_vendor/urllib3/util/ssl_.py:425:            context.load_cert_chain(certfile, keyfile, key_password)
./.venv-build/lib/python3.12/site-packages/pip/_vendor/urllib3/util/url.py:142:            >>> Url('http', 'username:password', 'host.com', 80,
./.venv-build/lib/python3.12/site-packages/pip/_vendor/urllib3/util/url.py:144:            'http://username:password@host.com:80/path?query#fragment'
./.venv-build/lib/python3.12/site-packages/pip/_vendor/urllib3/contrib/securetransport.py:863:    def load_cert_chain(self, certfile, keyfile=None, password=None):
./.venv-build/lib/python3.12/site-packages/pip/_vendor/urllib3/contrib/securetransport.py:866:        self._client_cert_passphrase = password
./.venv-build/lib/python3.12/site-packages/pip/_vendor/urllib3/contrib/_securetransport/low_level.py:212:    credentials. This keychain uses a one-time password and a temporary file to
./.venv-build/lib/python3.12/site-packages/pip/_vendor/urllib3/contrib/_securetransport/low_level.py:224:    # some random bytes to password-protect the keychain we're creating, so we
./.venv-build/lib/python3.12/site-packages/pip/_vendor/urllib3/contrib/_securetransport/low_level.py:228:    password = base64.b16encode(random_bytes[8:])  # Must be valid UTF-8
./.venv-build/lib/python3.12/site-packages/pip/_vendor/urllib3/contrib/_securetransport/low_level.py:236:        keychain_path, len(password), password, False, None, ctypes.byref(keychain)
./.venv-build/lib/python3.12/site-packages/pip/_vendor/urllib3/contrib/pyopenssl.py:468:    def load_cert_chain(self, certfile, keyfile=None, password=None):
./.venv-build/lib/python3.12/site-packages/pip/_vendor/urllib3/contrib/pyopenssl.py:470:        if password is not None:
./.venv-build/lib/python3.12/site-packages/pip/_vendor/urllib3/contrib/pyopenssl.py:471:            if not isinstance(password, six.binary_type):
./.venv-build/lib/python3.12/site-packages/pip/_vendor/urllib3/contrib/pyopenssl.py:472:                password = password.encode("utf-8")
./.venv-build/lib/python3.12/site-packages/pip/_vendor/urllib3/contrib/pyopenssl.py:473:            self._ctx.set_passwd_cb(lambda *_: password)
./.venv-build/lib/python3.12/site-packages/pip/_vendor/urllib3/contrib/ntlmpool.py:39:        pw is the password for the user.
./.venv-build/lib/python3.12/site-packages/pip/_vendor/urllib3/contrib/ntlmpool.py:105:                raise Exception("Server rejected request: wrong username or password")
./.venv-build/lib/python3.12/site-packages/pip/_vendor/urllib3/contrib/socks.py:15:- Usernames and passwords for the SOCKS proxy
./.venv-build/lib/python3.12/site-packages/pip/_vendor/urllib3/contrib/socks.py:32:When connecting to a SOCKS5 proxy the ``username`` and ``password`` portion
./.venv-build/lib/python3.12/site-packages/pip/_vendor/urllib3/contrib/socks.py:33:of the ``proxy_url`` will be sent as the username/password to authenticate
./.venv-build/lib/python3.12/site-packages/pip/_vendor/urllib3/contrib/socks.py:38:    proxy_url="socks5h://<username>:<password>@proxy-host"
./.venv-build/lib/python3.12/site-packages/pip/_vendor/urllib3/contrib/socks.py:102:                proxy_password=self._socks_options["password"],
./.venv-build/lib/python3.12/site-packages/pip/_vendor/urllib3/contrib/socks.py:169:        password=None,
./.venv-build/lib/python3.12/site-packages/pip/_vendor/urllib3/contrib/socks.py:176:        if username is None and password is None and parsed.auth is not None:
./.venv-build/lib/python3.12/site-packages/pip/_vendor/urllib3/contrib/socks.py:179:                username, password = split
./.venv-build/lib/python3.12/site-packages/pip/_vendor/urllib3/contrib/socks.py:202:            "password": password,
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/markers.py:15:from ._tokenizer import ParserSyntaxError
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_tokenizer.py:12:class Token:
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_tokenizer.py:91:class Tokenizer:
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_tokenizer.py:92:    """Context-sensitive token parsing.
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_tokenizer.py:94:    Provides methods to examine the input stream to check whether the next token
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_tokenizer.py:108:        self.next_token: Token | None = None
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_tokenizer.py:112:        """Move beyond provided token name, if at current position."""
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_tokenizer.py:117:        """Check whether the next token has the provided name.
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_tokenizer.py:119:        By default, if the check succeeds, the token *must* be read before
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_tokenizer.py:120:        another check. If `peek` is set to `True`, the token is not loaded and
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_tokenizer.py:124:            self.next_token is None
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_tokenizer.py:125:        ), f"Cannot check for {name!r}, already have {self.next_token!r}"
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_tokenizer.py:126:        assert name in self.rules, f"Unknown token name: {name!r}"
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_tokenizer.py:134:            self.next_token = Token(name, match[0], self.position)
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_tokenizer.py:137:    def expect(self, name: str, *, expected: str) -> Token:
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_tokenizer.py:138:        """Expect a certain token name next, failing with a syntax error otherwise.
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_tokenizer.py:140:        The token is *not* read.
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_tokenizer.py:146:    def read(self) -> Token:
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_tokenizer.py:147:        """Consume the next token and return it."""
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_tokenizer.py:148:        token = self.next_token
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_tokenizer.py:149:        assert token is not None
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_tokenizer.py:151:        self.position += len(token.text)
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_tokenizer.py:152:        self.next_token = None
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_tokenizer.py:154:        return token
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_tokenizer.py:175:    def enclosing_tokens(self, open_token: str, close_token: str, *, around: str) -> Iterator[None]:
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_tokenizer.py:176:        if self.check(open_token):
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_tokenizer.py:187:        if not self.check(close_token):
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_tokenizer.py:189:                f"Expected matching {close_token} for {open_token}, after {around}",
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:12:from ._tokenizer import DEFAULT_RULES, Tokenizer
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:62:    return _parse_requirement(Tokenizer(source, rules=DEFAULT_RULES))
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:65:def _parse_requirement(tokenizer: Tokenizer) -> ParsedRequirement:
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:69:    tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:71:    name_token = tokenizer.expect(
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:74:    name = name_token.text
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:75:    tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:77:    extras = _parse_extras(tokenizer)
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:78:    tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:80:    url, specifier, marker = _parse_requirement_details(tokenizer)
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:81:    tokenizer.expect("END", expected="end of dependency specifier")
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:87:    tokenizer: Tokenizer,
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:98:    if tokenizer.check("AT"):
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:99:        tokenizer.read()
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:100:        tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:102:        url_start = tokenizer.position
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:103:        url = tokenizer.expect("URL", expected="URL after @").text
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:104:        if tokenizer.check("END", peek=True):
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:107:        tokenizer.expect("WS", expected="whitespace after URL")
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:110:        if tokenizer.check("END", peek=True):
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:114:            tokenizer, span_start=url_start, after="URL and whitespace"
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:117:        specifier_start = tokenizer.position
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:118:        specifier = _parse_specifier(tokenizer)
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:119:        tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:121:        if tokenizer.check("END", peek=True):
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:125:            tokenizer,
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:133:def _parse_requirement_marker(tokenizer: Tokenizer, *, span_start: int, after: str) -> MarkerList:
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:138:    if not tokenizer.check("SEMICOLON"):
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:139:        tokenizer.raise_syntax_error(
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:143:    tokenizer.read()
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:145:    marker = _parse_marker(tokenizer)
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:146:    tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:151:def _parse_extras(tokenizer: Tokenizer) -> list[str]:
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:155:    if not tokenizer.check("LEFT_BRACKET", peek=True):
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:158:    with tokenizer.enclosing_tokens(
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:163:        tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:164:        extras = _parse_extras_list(tokenizer)
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:165:        tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:170:def _parse_extras_list(tokenizer: Tokenizer) -> list[str]:
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:176:    if not tokenizer.check("IDENTIFIER"):
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:179:    extras.append(tokenizer.read().text)
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:182:        tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:183:        if tokenizer.check("IDENTIFIER", peek=True):
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:184:            tokenizer.raise_syntax_error("Expected comma between extra names")
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:185:        elif not tokenizer.check("COMMA"):
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:188:        tokenizer.read()
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:189:        tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:191:        extra_token = tokenizer.expect("IDENTIFIER", expected="extra name after comma")
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:192:        extras.append(extra_token.text)
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:197:def _parse_specifier(tokenizer: Tokenizer) -> str:
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:202:    with tokenizer.enclosing_tokens(
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:207:        tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:208:        parsed_specifiers = _parse_version_many(tokenizer)
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:209:        tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:214:def _parse_version_many(tokenizer: Tokenizer) -> str:
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:219:    while tokenizer.check("SPECIFIER"):
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:220:        span_start = tokenizer.position
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:221:        parsed_specifiers += tokenizer.read().text
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:222:        if tokenizer.check("VERSION_PREFIX_TRAIL", peek=True):
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:223:            tokenizer.raise_syntax_error(
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:226:                span_end=tokenizer.position + 1,
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:228:        if tokenizer.check("VERSION_LOCAL_LABEL_TRAIL", peek=True):
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:229:            tokenizer.raise_syntax_error(
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:232:                span_end=tokenizer.position,
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:234:        tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:235:        if not tokenizer.check("COMMA"):
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:237:        parsed_specifiers += tokenizer.read().text
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:238:        tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:247:    return _parse_full_marker(Tokenizer(source, rules=DEFAULT_RULES))
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:250:def _parse_full_marker(tokenizer: Tokenizer) -> MarkerList:
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:251:    retval = _parse_marker(tokenizer)
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:252:    tokenizer.expect("END", expected="end of marker expression")
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:256:def _parse_marker(tokenizer: Tokenizer) -> MarkerList:
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:260:    expression = [_parse_marker_atom(tokenizer)]
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:261:    while tokenizer.check("BOOLOP"):
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:262:        token = tokenizer.read()
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:263:        expr_right = _parse_marker_atom(tokenizer)
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:264:        expression.extend((token.text, expr_right))
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:268:def _parse_marker_atom(tokenizer: Tokenizer) -> MarkerAtom:
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:274:    tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:275:    if tokenizer.check("LEFT_PARENTHESIS", peek=True):
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:276:        with tokenizer.enclosing_tokens(
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:281:            tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:282:            marker: MarkerAtom = _parse_marker(tokenizer)
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:283:            tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:285:        marker = _parse_marker_item(tokenizer)
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:286:    tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:290:def _parse_marker_item(tokenizer: Tokenizer) -> MarkerItem:
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:294:    tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:295:    marker_var_left = _parse_marker_var(tokenizer)
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:296:    tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:297:    marker_op = _parse_marker_op(tokenizer)
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:298:    tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:299:    marker_var_right = _parse_marker_var(tokenizer)
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:300:    tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:304:def _parse_marker_var(tokenizer: Tokenizer) -> MarkerVar:
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:308:    if tokenizer.check("VARIABLE"):
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:309:        return process_env_var(tokenizer.read().text.replace(".", "_"))
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:310:    elif tokenizer.check("QUOTED_STRING"):
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:311:        return process_python_str(tokenizer.read().text)
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:313:        tokenizer.raise_syntax_error(message="Expected a marker variable or quoted string")
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:328:def _parse_marker_op(tokenizer: Tokenizer) -> Op:
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:332:    if tokenizer.check("IN"):
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:333:        tokenizer.read()
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:335:    elif tokenizer.check("NOT"):
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:336:        tokenizer.read()
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:337:        tokenizer.expect("WS", expected="whitespace after 'not'")
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:338:        tokenizer.expect("IN", expected="'in' after 'not'")
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:340:    elif tokenizer.check("OP"):
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:341:        return Op(tokenizer.read().text)
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:343:        return tokenizer.raise_syntax_error(
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/licenses/__init__.py:67:    # Pad any parentheses so tokenization can be achieved by merely splitting on
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/licenses/__init__.py:81:    tokens = license_expression.split()
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/licenses/__init__.py:86:    python_tokens = []
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/licenses/__init__.py:87:    for token in tokens:
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/licenses/__init__.py:88:        if token not in {"or", "and", "with", "(", ")"}:
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/licenses/__init__.py:89:            python_tokens.append("False")
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/licenses/__init__.py:90:        elif token == "with":
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/licenses/__init__.py:91:            python_tokens.append("or")
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/licenses/__init__.py:92:        elif token == "(" and python_tokens and python_tokens[-1] not in {"or", "and"}:
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/licenses/__init__.py:96:            python_tokens.append(token)
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/licenses/__init__.py:98:    python_expression = " ".join(python_tokens)
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/licenses/__init__.py:109:    normalized_tokens = []
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/licenses/__init__.py:110:    for token in tokens:
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/licenses/__init__.py:111:        if token in {"or", "and", "with", "(", ")"}:
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/licenses/__init__.py:112:            normalized_tokens.append(token.upper())
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/licenses/__init__.py:115:        if normalized_tokens and normalized_tokens[-1] == "WITH":
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/licenses/__init__.py:116:            if token not in EXCEPTIONS:
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/licenses/__init__.py:117:                message = f"Unknown license exception: {token!r}"
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/licenses/__init__.py:120:            normalized_tokens.append(EXCEPTIONS[token]["id"])
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/licenses/__init__.py:122:            if token.endswith("+"):
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/licenses/__init__.py:123:                final_token = token[:-1]
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/licenses/__init__.py:126:                final_token = token
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/licenses/__init__.py:129:            if final_token.startswith("licenseref-"):
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/licenses/__init__.py:130:                if not license_ref_allowed.match(final_token):
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/licenses/__init__.py:131:                    message = f"Invalid licenseref: {final_token!r}"
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/licenses/__init__.py:133:                normalized_tokens.append(license_refs[final_token] + suffix)
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/licenses/__init__.py:135:                if final_token not in LICENSES:
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/licenses/__init__.py:136:                    message = f"Unknown license: {final_token!r}"
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/licenses/__init__.py:138:                normalized_tokens.append(LICENSES[final_token]["id"] + suffix)
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/licenses/__init__.py:140:    normalized_expression = " ".join(normalized_tokens)
./.venv-build/lib/python3.12/site-packages/pip/_vendor/packaging/requirements.py:9:from ._tokenizer import ParserSyntaxError
./.venv-build/lib/python3.12/site-packages/pip/_vendor/requests/auth.py:25:def _basic_auth_str(username, password):
./.venv-build/lib/python3.12/site-packages/pip/_vendor/requests/auth.py:45:    if not isinstance(password, basestring):
./.venv-build/lib/python3.12/site-packages/pip/_vendor/requests/auth.py:47:            "Non-string passwords will no longer be supported in Requests "
./.venv-build/lib/python3.12/site-packages/pip/_vendor/requests/auth.py:50:            "problems.".format(type(password)),
./.venv-build/lib/python3.12/site-packages/pip/_vendor/requests/auth.py:53:        password = str(password)
./.venv-build/lib/python3.12/site-packages/pip/_vendor/requests/auth.py:59:    if isinstance(password, str):
./.venv-build/lib/python3.12/site-packages/pip/_vendor/requests/auth.py:60:        password = password.encode("latin1")
./.venv-build/lib/python3.12/site-packages/pip/_vendor/requests/auth.py:62:    authstr = "Basic " + to_native_string(b64encode(b":".join((username, password))).strip())
./.venv-build/lib/python3.12/site-packages/pip/_vendor/requests/auth.py:77:    def __init__(self, username, password):
./.venv-build/lib/python3.12/site-packages/pip/_vendor/requests/auth.py:79:        self.password = password
./.venv-build/lib/python3.12/site-packages/pip/_vendor/requests/auth.py:85:                self.password == getattr(other, "password", None),
./.venv-build/lib/python3.12/site-packages/pip/_vendor/requests/auth.py:93:        r.headers["Authorization"] = _basic_auth_str(self.username, self.password)
./.venv-build/lib/python3.12/site-packages/pip/_vendor/requests/auth.py:101:        r.headers["Proxy-Authorization"] = _basic_auth_str(self.username, self.password)
./.venv-build/lib/python3.12/site-packages/pip/_vendor/requests/auth.py:108:    def __init__(self, username, password):
./.venv-build/lib/python3.12/site-packages/pip/_vendor/requests/auth.py:110:        self.password = password
./.venv-build/lib/python3.12/site-packages/pip/_vendor/requests/auth.py:187:        A1 = f"{self.username}:{realm}:{self.password}"
./.venv-build/lib/python3.12/site-packages/pip/_vendor/requests/auth.py:305:                self.password == getattr(other, "password", None),
./.venv-build/lib/python3.12/site-packages/pip/_vendor/requests/utils.py:237:                # Return with login / password
./.venv-build/lib/python3.12/site-packages/pip/_vendor/requests/utils.py:378:    >>> parse_list_header('token, "quoted value"')
./.venv-build/lib/python3.12/site-packages/pip/_vendor/requests/utils.py:379:    ['token', 'quoted value']
./.venv-build/lib/python3.12/site-packages/pip/_vendor/requests/utils.py:508:    tokens = header.split(";")
./.venv-build/lib/python3.12/site-packages/pip/_vendor/requests/utils.py:509:    content_type, params = tokens[0].strip(), tokens[1:]
./.venv-build/lib/python3.12/site-packages/pip/_vendor/requests/utils.py:1006:    username,password.
./.venv-build/lib/python3.12/site-packages/pip/_vendor/requests/utils.py:1013:        auth = (unquote(parsed.username), unquote(parsed.password))
./.venv-build/lib/python3.12/site-packages/pip/_vendor/requests/sessions.py:317:            username, password = get_auth_from_url(new_proxies[scheme])
./.venv-build/lib/python3.12/site-packages/pip/_vendor/requests/sessions.py:319:            username, password = None, None
./.venv-build/lib/python3.12/site-packages/pip/_vendor/requests/sessions.py:323:        if not scheme.startswith("https") and username and password:
./.venv-build/lib/python3.12/site-packages/pip/_vendor/requests/sessions.py:324:            headers["Proxy-Authorization"] = _basic_auth_str(username, password)
./.venv-build/lib/python3.12/site-packages/pip/_vendor/requests/adapters.py:251:            username, password = get_auth_from_url(proxy)
./.venv-build/lib/python3.12/site-packages/pip/_vendor/requests/adapters.py:255:                password=password,
./.venv-build/lib/python3.12/site-packages/pip/_vendor/requests/adapters.py:568:        username, password = get_auth_from_url(proxy)
./.venv-build/lib/python3.12/site-packages/pip/_vendor/requests/adapters.py:571:            headers["Proxy-Authorization"] = _basic_auth_str(username, password)
./.venv-build/lib/python3.12/site-packages/pip/_vendor/truststore/_api.py:31:_PasswordType: typing.TypeAlias = str | bytes | typing.Callable[[], str | bytes]
./.venv-build/lib/python3.12/site-packages/pip/_vendor/truststore/_api.py:165:        password: _PasswordType | None = None,
./.venv-build/lib/python3.12/site-packages/pip/_vendor/truststore/_api.py:167:        return self._ctx.load_cert_chain(certfile=certfile, keyfile=keyfile, password=password)
./.venv-build/lib/python3.12/site-packages/pip/_vendor/distro/distro.py:1105:        tokens = list(lexer)
./.venv-build/lib/python3.12/site-packages/pip/_vendor/distro/distro.py:1106:        for token in tokens:
./.venv-build/lib/python3.12/site-packages/pip/_vendor/distro/distro.py:1110:            # stripped, etc.), so the tokens are now either:
./.venv-build/lib/python3.12/site-packages/pip/_vendor/distro/distro.py:1113:            # Ignore any tokens that are not variable assignments
./.venv-build/lib/python3.12/site-packages/pip/_vendor/distro/distro.py:1114:            if "=" in token:
./.venv-build/lib/python3.12/site-packages/pip/_vendor/distro/distro.py:1115:                k, v = token.split("=", 1)
./.venv-build/lib/python3.12/site-packages/pip/_vendor/distlib/util.py:849:    username = password = None
./.venv-build/lib/python3.12/site-packages/pip/_vendor/distlib/util.py:855:            username, password = prefix.split(":", 1)
./.venv-build/lib/python3.12/site-packages/pip/_vendor/distlib/util.py:858:    if password:
./.venv-build/lib/python3.12/site-packages/pip/_vendor/distlib/util.py:859:        password = unquote(password)
./.venv-build/lib/python3.12/site-packages/pip/_vendor/distlib/util.py:860:    return username, password, netloc
./.venv-build/lib/python3.12/site-packages/pip/_vendor/distlib/util.py:1866:                            ("password", None),
./.venv-build/lib/python3.12/site-packages/pip/_vendor/distlib/util.py:1892:                    "password": config.get(server, "password"),
./.venv-build/lib/python3.12/site-packages/pip/_vendor/distlib/util.py:1899:    def update(self, username, password):
./.venv-build/lib/python3.12/site-packages/pip/_vendor/distlib/util.py:1907:        config.set("pypi", "password", password)
./.venv-build/lib/python3.12/site-packages/pip/_vendor/distlib/util.py:1920:    PyPIRCFile().update(index.username, index.password)
./.venv-build/lib/python3.12/site-packages/pip/_vendor/distlib/compat.py:50:        HTTPPasswordMgr,
./.venv-build/lib/python3.12/site-packages/pip/_vendor/distlib/compat.py:106:        HTTPPasswordMgr,
./.venv-build/lib/python3.12/site-packages/pip/_vendor/distlib/compat.py:400:    from tokenize import detect_encoding
./.venv-build/lib/python3.12/site-packages/pip/_vendor/distlib/compat.py:407:        """Imitates get_normal_name in tokenizer.c."""
./.venv-build/lib/python3.12/site-packages/pip/_vendor/distlib/compat.py:422:        in the same way as the tokenize() generator.
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/formatters/_mapping.py:10:        "Format tokens with BBcodes. These formatting codes are used by many bulletin boards, so you can highlight your sourcecode with pygments before posting it there.",
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/formatters/_mapping.py:31:        "Format tokens with groff escapes to change their color and font style.",
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/formatters/_mapping.py:38:        "Format tokens as HTML 4 ``<span>`` tags. By default, the content is enclosed in a ``<pre>`` tag, itself wrapped in a ``<div>`` tag (but see the `nowrap` option). The ``<div>``'s CSS class can be set by the `cssclass` option.",
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/formatters/_mapping.py:45:        "Format tokens with IRC color sequences",
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/formatters/_mapping.py:66:        "Format tokens as LaTeX code. This needs the `fancyvrb` and `color` standard packages.",
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/formatters/_mapping.py:80:        "Format tokens as Pango Markup code. It can then be rendered to an SVG.",
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/formatters/_mapping.py:82:    "RawTokenFormatter": (
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/formatters/_mapping.py:84:        "Raw tokens",
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/formatters/_mapping.py:85:        ("raw", "tokens"),
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/formatters/_mapping.py:87:        "Format tokens as a raw representation for storing token streams.",
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/formatters/_mapping.py:94:        "Format tokens as RTF markup. This formatter automatically outputs full RTF documents with color information and other useful stuff. Perfect for Copy and Paste into Microsoft(R) Word(R) documents.",
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/formatters/_mapping.py:101:        "Format tokens as an SVG graphics file.  This formatter is still experimental. Each line of code is a ``<text>`` element with explicit ``x`` and ``y`` coordinates containing ``<tspan>`` elements with the individual token styles.",
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/formatters/_mapping.py:108:        "Format tokens with ANSI color sequences, for output in a 256-color terminal or console.  Like in `TerminalFormatter` color sequences are terminated at newlines, so that paging the output works correctly.",
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/formatters/_mapping.py:115:        "Format tokens with ANSI color sequences, for output in a text console. Color sequences are terminated at newlines, so that paging the output works correctly.",
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/formatters/_mapping.py:122:        "Format tokens with ANSI color sequences, for output in a true-color terminal or console.  Like in `TerminalFormatter` color sequences are terminated at newlines, so that paging the output works correctly.",
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/formatters/_mapping.py:129:        "Format tokens as appropriate for a new testcase.",
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/formatter.py:27:    Converts a token stream to text.
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/formatter.py:58:        convert the Unicode token strings to byte strings in the
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/formatter.py:114:    def format(self, tokensource, outfile):
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/formatter.py:116:        This method must format the tokens from the `tokensource` iterable and
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/formatter.py:119:        Formatter options can control how exactly the tokens are converted.
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/formatter.py:124:        return self.format_unencoded(tokensource, outfile)
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/token.py:2:pygments.token
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/token.py:5:Basic token types and the standard tokens.
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/token.py:12:class _TokenType(tuple):
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/token.py:34:        new = _TokenType(self + (val,))
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/token.py:41:        return "Token" + (self and "." or "") + ".".join(self)
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/token.py:52:Token = _TokenType()
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/token.py:54:# Special token types
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/token.py:55:Text = Token.Text
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/token.py:57:Escape = Token.Escape
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/token.py:58:Error = Token.Error
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/token.py:60:Other = Token.Other
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/token.py:62:# Common token types for source code
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/token.py:63:Keyword = Token.Keyword
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/token.py:64:Name = Token.Name
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/token.py:65:Literal = Token.Literal
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/token.py:68:Punctuation = Token.Punctuation
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/token.py:69:Operator = Token.Operator
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/token.py:70:Comment = Token.Comment
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/token.py:73:Generic = Token.Generic
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/token.py:75:# String and some others are not direct children of Token.
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/token.py:77:Token.Token = Token
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/token.py:78:Token.String = String
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/token.py:79:Token.Number = Number
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/token.py:82:def is_token_subtype(ttype, other):
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/token.py:91:def string_to_tokentype(s):
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/token.py:93:    Convert a string into a token type::
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/token.py:95:        >>> string_to_token('String.Double')
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/token.py:96:        Token.Literal.String.Double
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/token.py:97:        >>> string_to_token('Token.Literal.Number')
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/token.py:98:        Token.Literal.Number
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/token.py:99:        >>> string_to_token('')
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/token.py:100:        Token
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/token.py:102:    Tokens that are already tokens are returned unchanged:
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/token.py:104:        >>> string_to_token(String)
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/token.py:105:        Token.Literal.String
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/token.py:107:    if isinstance(s, _TokenType):
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/token.py:110:        return Token
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/token.py:111:    node = Token
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/token.py:117:# Map standard token types to short names, used in CSS class naming.
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/token.py:121:    Token: "",
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/style.py:11:from pip._vendor.pygments.token import Token, STANDARD_TYPES
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/style.py:61:        for token in STANDARD_TYPES:
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/style.py:62:            if token not in obj.styles:
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/style.py:63:                obj.styles[token] = ""
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/style.py:83:            for token in ttype.split():
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/style.py:84:                if token in _styles:
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/style.py:86:                ndef = _styles.get(token.parent, None)
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/style.py:87:                styledefs = obj.styles.get(token, "").split()
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/style.py:88:                if not ndef or token is None:
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/style.py:90:                elif "noinherit" in styledefs and token is not Token:
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/style.py:91:                    ndef = _styles[Token][:]
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/style.py:94:                _styles[token] = ndef
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/style.py:95:                for styledef in obj.styles.get(token, "").split():
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/style.py:125:    def style_for_token(cls, token):
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/style.py:126:        t = cls._styles[token]
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/style.py:158:    def styles_token(cls, ttype):
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/style.py:162:        for token in cls._styles:
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/style.py:163:            yield token, cls.style_for_token(token)
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/style.py:188:    #: Style definitions for individual token types.
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexers/_mapping.py:3116:    "RawTokenLexer": (
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexers/_mapping.py:3118:        "Raw token data",
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexers/_mapping.py:3121:        ("application/x-pygments-tokens",),
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexers/python.py:25:from pip._vendor.pygments.token import (
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexers/python.py:135:    tokens = {
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexers/python.py:800:    tokens = {
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexers/python.py:1278:    Code tokens are output as ``Token.Other.Code``, traceback tokens as
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexers/python.py:1279:    ``Token.Other.Traceback``.
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexers/python.py:1281:    tokens = {
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexers/python.py:1352:        # different tokens.  TODO: DelegatingLexer should support this
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexers/python.py:1354:        # distinguishing tokens. Then we wouldn't need this intermediary
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexers/python.py:1379:    tokens = {
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexers/python.py:1457:    tokens = {
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexers/python.py:1511:    tokens = {
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexers/python.py:1852:    tokens = {
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexers/python.py:2423:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexers/python.py:2424:        for index, token, value in PythonLexer.get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexers/python.py:2425:            if token is Name and value in self.EXTRA_KEYWORDS:
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexers/python.py:2428:                yield index, token, value
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:17:from pip._vendor.pygments.token import Error, Text, Other, Whitespace, _TokenType
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:276:    def get_tokens(self, text, unfiltered=False):
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:280:        iterable of ``(tokentype, value)`` pairs from `text`.
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:284:        (`stripnl`, `stripall` and so on), and then yields all tokens
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:285:        from `get_tokens_unprocessed()`, with the ``index`` dropped.
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:293:            for _, t, v in self.get_tokens_unprocessed(text):
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:301:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:304:        ``(index, tokentype, value)`` tuples where ``index`` is the starting
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:305:        position of the token within the input text.
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:317:    lexer, afterwards all ``Other`` tokens are lexed using the root
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:329:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:333:        for i, t, v in self.language_lexer.get_tokens_unprocessed(text):
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:343:        return do_insertions(insertions, self.root_lexer.get_tokens_unprocessed(buffered))
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:420:            elif type(action) is _TokenType:
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:481:            for i, t, v in lx.get_tokens_unprocessed(match.group(), **gt_kwargs):
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:494:            for i, t, v in lx.get_tokens_unprocessed(match.group(), **gt_kwargs):
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:505:    For example default('#pop') is equivalent to ('', Token, '#pop')
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:534:    Metaclass for RegexLexer, creates the self._tokens attribute from
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:535:    self.tokens on the first instantiation.
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:539:        """Preprocess the regular expression component of a token definition."""
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:544:    def _process_token(cls, token):
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:545:        """Preprocess the token component of a token definition."""
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:546:        assert type(token) is _TokenType or callable(
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:547:            token
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:548:        ), f"token type must be simple type or callable, not {token!r}"
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:549:        return token
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:552:        """Preprocess the state transition action of a token definition."""
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:569:            itokens = []
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:572:                itokens.extend(cls._process_state(unprocessed, processed, istate))
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:573:            processed[tmp_state] = itokens
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:591:        tokens = processed[state] = []
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:597:                tokens.extend(cls._process_state(unprocessed, processed, str(tdef)))
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:606:                tokens.append((re.compile("").match, None, new_state))
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:618:            token = cls._process_token(tdef[1])
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:625:            tokens.append((rex, token, new_state))
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:626:        return tokens
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:628:    def process_tokendef(cls, name, tokendefs=None):
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:629:        """Preprocess a dictionary of token definitions."""
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:630:        processed = cls._all_tokens[name] = {}
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:631:        tokendefs = tokendefs or cls.tokens[name]
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:632:        for state in list(tokendefs):
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:633:            cls._process_state(tokendefs, processed, state)
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:636:    def get_tokendefs(cls):
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:638:        Merge tokens from superclasses in MRO order, returning a single tokendef
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:648:        tokens = {}
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:651:            toks = c.__dict__.get("tokens", {})
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:654:                curitems = tokens.get(state)
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:660:                    tokens[state] = items
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:683:        return tokens
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:686:        """Instantiate cls after preprocessing its token definitions."""
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:687:        if "_tokens" not in cls.__dict__:
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:688:            cls._all_tokens = {}
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:690:            if hasattr(cls, "token_variants") and cls.token_variants:
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:694:                cls._tokens = cls.process_tokendef("", cls.get_tokendefs())
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:713:    #: Dict of ``{'state': [(regex, tokentype, new_state), ...], ...}``
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:732:    tokens = {}
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:734:    def get_tokens_unprocessed(self, text, stack=("root",)):
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:736:        Split ``text`` into (tokentype, text) pairs.
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:741:        tokendefs = self._tokens
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:743:        statetokens = tokendefs[statestack[-1]]
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:745:            for rexmatch, action, new_state in statetokens:
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:749:                        if type(action) is _TokenType:
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:777:                        statetokens = tokendefs[statestack[-1]]
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:780:                # We are here only if all state tokens have been considered
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:786:                        statetokens = tokendefs["root"]
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:816:    def get_tokens_unprocessed(self, text=None, context=None):
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:818:        Split ``text`` into (tokentype, text) pairs.
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:821:        tokendefs = self._tokens
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:824:            statetokens = tokendefs["root"]
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:827:            statetokens = tokendefs[ctx.stack[-1]]
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:830:            for rexmatch, action, new_state in statetokens:
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:834:                        if type(action) is _TokenType:
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:841:                                statetokens = tokendefs[ctx.stack[-1]]
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:864:                        statetokens = tokendefs[ctx.stack[-1]]
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:873:                        statetokens = tokendefs["root"]
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:883:def do_insertions(insertions, tokens):
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:888:    ``insertions`` is a list of ``(index, itokens)`` pairs.
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:889:    Each ``itokens`` iterable should be inserted at position
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:890:    ``index`` into the token stream given by the ``tokens``
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:893:    The result is a combined token stream.
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:899:        index, itokens = next(insertions)
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:902:        yield from tokens
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:908:    # iterate over the token stream where we want to insert
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:909:    # the tokens from the insertion list.
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:910:    for i, t, v in tokens:
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:920:            for it_index, it_token, it_value in itokens:
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:921:                yield realpos, it_token, it_value
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:925:                index, itokens = next(insertions)
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:933:    # leftover tokens
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:935:        # no normal tokens, set realpos to zero
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:937:        for p, t, v in itokens:
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:941:            index, itokens = next(insertions)
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:975:    def get_tokens_unprocessed(self, text, stack=("root",)):
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:978:        yield from RegexLexer.get_tokens_unprocessed(self, text, stack)
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/__init__.py:39:    and return an iterable of tokens. Currently, this only calls
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/__init__.py:40:    `lexer.get_tokens()`.
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/__init__.py:43:        return lexer.get_tokens(code)
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/__init__.py:53:def format(tokens, formatter, outfile=None):  # pylint: disable=redefined-builtin
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/__init__.py:55:    Format ``tokens`` (an iterable of tokens) with the formatter ``formatter``
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/__init__.py:65:            formatter.format(tokens, realoutfile)
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/__init__.py:68:            formatter.format(tokens, outfile)
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/filters/__init__.py:14:from pip._vendor.pygments.token import (
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/filters/__init__.py:21:    string_to_tokentype,
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/filters/__init__.py:723:    """Highlight a normal Name (and Name.*) token with a different token type.
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/filters/__init__.py:729:            tokentype=Name.Function,
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/filters/__init__.py:733:    as functions. `Name.Function` is the default token type.
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/filters/__init__.py:738:      A list of names that should be given the different token type.
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/filters/__init__.py:740:    `tokentype` : TokenType or string
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/filters/__init__.py:741:      A token type or a string containing a token type name that is
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/filters/__init__.py:749:        tokentype = options.get("tokentype")
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/filters/__init__.py:750:        if tokentype:
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/filters/__init__.py:751:            self.tokentype = string_to_tokentype(tokentype)
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/filters/__init__.py:753:            self.tokentype = Name.Function
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/filters/__init__.py:758:                yield self.tokentype, value
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/filters/__init__.py:763:class ErrorToken(Exception):
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/filters/__init__.py:767:class RaiseOnErrorTokenFilter(Filter):
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/filters/__init__.py:768:    """Raise an exception when the lexer generates an error token.
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/filters/__init__.py:774:      The default is `pygments.filters.ErrorToken`.
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/filters/__init__.py:781:        self.exception = options.get("excclass", ErrorToken)
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/filters/__init__.py:818:    `wstokentype` : bool
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/filters/__init__.py:819:      If true, give whitespace the special `Whitespace` token type.  This allows
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/filters/__init__.py:839:        self.wstt = get_bool_opt(options, "wstokentype", True)
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/filters/__init__.py:901:            # Remove ``left`` tokens from first line, ``n`` from all others.
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/filters/__init__.py:912:class TokenMergeFilter(Filter):
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/filters/__init__.py:913:    """Merges consecutive tokens with the same token type in the output
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/filters/__init__.py:941:    "raiseonerror": RaiseOnErrorTokenFilter,
./.venv-build/lib/python3.12/site-packages/pip/_vendor/pygments/filters/__init__.py:944:    "tokenmerge": TokenMergeFilter,
./.venv-build/lib/python3.12/site-packages/pip/_internal/vcs/versioncontrol.py:363:        information can be provided via the --username and --password options
./.venv-build/lib/python3.12/site-packages/pip/_internal/vcs/versioncontrol.py:367:        Returns: (netloc, (username, password)).
./.venv-build/lib/python3.12/site-packages/pip/_internal/vcs/versioncontrol.py:377:        Returns: (url, rev, (username, password)).
./.venv-build/lib/python3.12/site-packages/pip/_internal/vcs/versioncontrol.py:402:    def make_rev_args(username: str | None, password: HiddenText | None) -> CommandArgs:
./.venv-build/lib/python3.12/site-packages/pip/_internal/vcs/versioncontrol.py:413:        secret_url, rev, user_pass = self.get_url_rev_and_auth(url.secret)
./.venv-build/lib/python3.12/site-packages/pip/_internal/vcs/versioncontrol.py:414:        username, secret_password = user_pass
./.venv-build/lib/python3.12/site-packages/pip/_internal/vcs/versioncontrol.py:415:        password: HiddenText | None = None
./.venv-build/lib/python3.12/site-packages/pip/_internal/vcs/versioncontrol.py:416:        if secret_password is not None:
./.venv-build/lib/python3.12/site-packages/pip/_internal/vcs/versioncontrol.py:417:            password = hide_value(secret_password)
./.venv-build/lib/python3.12/site-packages/pip/_internal/vcs/versioncontrol.py:418:        extra_args = self.make_rev_args(username, password)
./.venv-build/lib/python3.12/site-packages/pip/_internal/vcs/versioncontrol.py:421:        return hide_url(secret_url), rev_options
./.venv-build/lib/python3.12/site-packages/pip/_internal/vcs/versioncontrol.py:511:            if self.compare_urls(existing_url, url.secret):
./.venv-build/lib/python3.12/site-packages/pip/_internal/vcs/mercurial.py:77:            config.set("paths", "default", url.secret)
./.venv-build/lib/python3.12/site-packages/pip/_internal/vcs/subversion.py:80:        --username and --password options instead of via the URL.
./.venv-build/lib/python3.12/site-packages/pip/_internal/vcs/subversion.py:83:            # The --username and --password options can't be used for
./.venv-build/lib/python3.12/site-packages/pip/_internal/vcs/subversion.py:98:    def make_rev_args(username: str | None, password: HiddenText | None) -> CommandArgs:
./.venv-build/lib/python3.12/site-packages/pip/_internal/vcs/subversion.py:102:        if password:
./.venv-build/lib/python3.12/site-packages/pip/_internal/vcs/subversion.py:103:            extra_args += ["--password", password]
./.venv-build/lib/python3.12/site-packages/pip/_internal/vcs/subversion.py:160:                # is being used to prompt for passwords, because passwords
./.venv-build/lib/python3.12/site-packages/pip/_internal/vcs/subversion.py:269:        # the user can be prompted for a password, if required.
./.venv-build/lib/python3.12/site-packages/pip/_internal/models/link.py:422:            # includes a username and password.
./.venv-build/lib/python3.12/site-packages/pip/_internal/models/direct_url.py:171:        """url with user:password part removed unless it is formed with
./.venv-build/lib/python3.12/site-packages/pip/_internal/req/req_file.py:440:    tokens = line.split(" ")
./.venv-build/lib/python3.12/site-packages/pip/_internal/req/req_file.py:442:    options = tokens[:]
./.venv-build/lib/python3.12/site-packages/pip/_internal/req/req_file.py:443:    for token in tokens:
./.venv-build/lib/python3.12/site-packages/pip/_internal/req/req_file.py:444:        if token.startswith(("-", "--")):
./.venv-build/lib/python3.12/site-packages/pip/_internal/req/req_file.py:447:            args.append(token)
./.venv-build/lib/python3.12/site-packages/pip/_internal/commands/completion.py:41:                (commandline --current-process --tokenize --cut-at-cursor) \\
./.venv-build/lib/python3.12/site-packages/pip/_internal/commands/completion.py:42:                (commandline --current-token --cut-at-cursor)
./.venv-build/lib/python3.12/site-packages/pip/_internal/utils/wheel.py:71:        # and RuntimeError for password-protected files
./.venv-build/lib/python3.12/site-packages/pip/_internal/utils/subprocess.py:55:    return [arg.secret if isinstance(arg, HiddenText) else arg for arg in args]
./.venv-build/lib/python3.12/site-packages/pip/_internal/utils/misc.py:236:def ask_password(message: str) -> str:
./.venv-build/lib/python3.12/site-packages/pip/_internal/utils/misc.py:237:    """Ask for a password interactively."""
./.venv-build/lib/python3.12/site-packages/pip/_internal/utils/misc.py:425:    Returns: (netloc, (username, password)).
./.venv-build/lib/python3.12/site-packages/pip/_internal/utils/misc.py:432:    # the password attribute of urlsplit()'s return value).
./.venv-build/lib/python3.12/site-packages/pip/_internal/utils/misc.py:438:        # using the password attribute of the return value)
./.venv-build/lib/python3.12/site-packages/pip/_internal/utils/misc.py:456:        - "accesstoken@example.com" returns "****@example.com"
./.venv-build/lib/python3.12/site-packages/pip/_internal/utils/misc.py:458:    netloc, (user, password) = split_auth_from_netloc(netloc)
./.venv-build/lib/python3.12/site-packages/pip/_internal/utils/misc.py:461:    if password is None:
./.venv-build/lib/python3.12/site-packages/pip/_internal/utils/misc.py:463:        password = ""
./.venv-build/lib/python3.12/site-packages/pip/_internal/utils/misc.py:466:        password = ":****"
./.venv-build/lib/python3.12/site-packages/pip/_internal/utils/misc.py:467:    return f"{user}{password}@{netloc}"
./.venv-build/lib/python3.12/site-packages/pip/_internal/utils/misc.py:504:    Returns: (url_without_auth, netloc, (username, password))
./.venv-build/lib/python3.12/site-packages/pip/_internal/utils/misc.py:511:    """Return a copy of url with 'username:password@' removed."""
./.venv-build/lib/python3.12/site-packages/pip/_internal/utils/misc.py:518:    """Replace the password in a given url with ****."""
./.venv-build/lib/python3.12/site-packages/pip/_internal/utils/misc.py:523:    """Replace the password in a given requirement url with ****."""
./.venv-build/lib/python3.12/site-packages/pip/_internal/utils/misc.py:531:    secret: str
./.venv-build/lib/python3.12/site-packages/pip/_internal/utils/misc.py:547:        return self.secret == other.secret
./.venv-build/lib/python3.12/site-packages/pip/_internal/network/auth.py:30:    ask_password,
./.venv-build/lib/python3.12/site-packages/pip/_internal/network/auth.py:44:    password: str
./.venv-build/lib/python3.12/site-packages/pip/_internal/network/auth.py:56:    def save_auth_info(self, url: str, username: str, password: str) -> None: ...
./.venv-build/lib/python3.12/site-packages/pip/_internal/network/auth.py:67:    def save_auth_info(self, url: str, username: str, password: str) -> None:
./.venv-build/lib/python3.12/site-packages/pip/_internal/network/auth.py:89:                return cred.username, cred.password
./.venv-build/lib/python3.12/site-packages/pip/_internal/network/auth.py:93:            logger.debug("Getting password from keyring for %s", url)
./.venv-build/lib/python3.12/site-packages/pip/_internal/network/auth.py:94:            password = self.keyring.get_password(url, username)
./.venv-build/lib/python3.12/site-packages/pip/_internal/network/auth.py:95:            if password:
./.venv-build/lib/python3.12/site-packages/pip/_internal/network/auth.py:96:                return username, password
./.venv-build/lib/python3.12/site-packages/pip/_internal/network/auth.py:99:    def save_auth_info(self, url: str, username: str, password: str) -> None:
./.venv-build/lib/python3.12/site-packages/pip/_internal/network/auth.py:100:        self.keyring.set_password(url, username, password)
./.venv-build/lib/python3.12/site-packages/pip/_internal/network/auth.py:121:            password = self._get_password(url, username)
./.venv-build/lib/python3.12/site-packages/pip/_internal/network/auth.py:122:            if password is not None:
./.venv-build/lib/python3.12/site-packages/pip/_internal/network/auth.py:123:                return username, password
./.venv-build/lib/python3.12/site-packages/pip/_internal/network/auth.py:126:    def save_auth_info(self, url: str, username: str, password: str) -> None:
./.venv-build/lib/python3.12/site-packages/pip/_internal/network/auth.py:127:        return self._set_password(url, username, password)
./.venv-build/lib/python3.12/site-packages/pip/_internal/network/auth.py:129:    def _get_password(self, service_name: str, username: str) -> str | None:
./.venv-build/lib/python3.12/site-packages/pip/_internal/network/auth.py:130:        """Mirror the implementation of keyring.get_password using cli"""
./.venv-build/lib/python3.12/site-packages/pip/_internal/network/auth.py:147:    def _set_password(self, service_name: str, username: str, password: str) -> None:
./.venv-build/lib/python3.12/site-packages/pip/_internal/network/auth.py:148:        """Mirror the implementation of keyring.set_password using cli"""
./.venv-build/lib/python3.12/site-packages/pip/_internal/network/auth.py:155:            input=f"{password}{os.linesep}".encode(),
./.venv-build/lib/python3.12/site-packages/pip/_internal/network/auth.py:234:        self.passwords: dict[str, AuthInfo] = {}
./.venv-build/lib/python3.12/site-packages/pip/_internal/network/auth.py:293:        The provided url should have had its username and password
./.venv-build/lib/python3.12/site-packages/pip/_internal/network/auth.py:344:        url, netloc, url_user_password = split_auth_netloc_from_url(
./.venv-build/lib/python3.12/site-packages/pip/_internal/network/auth.py:349:        username, password = url_user_password
./.venv-build/lib/python3.12/site-packages/pip/_internal/network/auth.py:350:        if username is not None and password is not None:
./.venv-build/lib/python3.12/site-packages/pip/_internal/network/auth.py:352:            return url_user_password
./.venv-build/lib/python3.12/site-packages/pip/_internal/network/auth.py:360:                index_url, _, index_url_user_password = index_info
./.venv-build/lib/python3.12/site-packages/pip/_internal/network/auth.py:364:        if index_url and index_url_user_password[0] is not None:
./.venv-build/lib/python3.12/site-packages/pip/_internal/network/auth.py:365:            username, password = index_url_user_password
./.venv-build/lib/python3.12/site-packages/pip/_internal/network/auth.py:366:            if username is not None and password is not None:
./.venv-build/lib/python3.12/site-packages/pip/_internal/network/auth.py:368:                return index_url_user_password
./.venv-build/lib/python3.12/site-packages/pip/_internal/network/auth.py:377:        # If we don't have a password and keyring is available, use it.
./.venv-build/lib/python3.12/site-packages/pip/_internal/network/auth.py:390:        return username, password
./.venv-build/lib/python3.12/site-packages/pip/_internal/network/auth.py:398:        Returns (url_without_credentials, username, password). Note
./.venv-build/lib/python3.12/site-packages/pip/_internal/network/auth.py:400:        function may return a different username and password.
./.venv-build/lib/python3.12/site-packages/pip/_internal/network/auth.py:405:        username, password = self._get_new_credentials(original_url)
./.venv-build/lib/python3.12/site-packages/pip/_internal/network/auth.py:408:        # Do this if either the username or the password is missing.
./.venv-build/lib/python3.12/site-packages/pip/_internal/network/auth.py:410:        # the username in the index url, but the password comes from keyring.
./.venv-build/lib/python3.12/site-packages/pip/_internal/network/auth.py:411:        if (username is None or password is None) and netloc in self.passwords:
./.venv-build/lib/python3.12/site-packages/pip/_internal/network/auth.py:412:            un, pw = self.passwords[netloc]
./.venv-build/lib/python3.12/site-packages/pip/_internal/network/auth.py:416:                username, password = un, pw
./.venv-build/lib/python3.12/site-packages/pip/_internal/network/auth.py:418:        if username is not None or password is not None:
./.venv-build/lib/python3.12/site-packages/pip/_internal/network/auth.py:419:            # Convert the username and password if they're None, so that
./.venv-build/lib/python3.12/site-packages/pip/_internal/network/auth.py:424:            password = password or ""
./.venv-build/lib/python3.12/site-packages/pip/_internal/network/auth.py:427:            self.passwords[netloc] = (username, password)
./.venv-build/lib/python3.12/site-packages/pip/_internal/network/auth.py:431:            (username is not None and password is not None)
./.venv-build/lib/python3.12/site-packages/pip/_internal/network/auth.py:433:            or (username is None and password is None)
./.venv-build/lib/python3.12/site-packages/pip/_internal/network/auth.py:436:        return url, username, password
./.venv-build/lib/python3.12/site-packages/pip/_internal/network/auth.py:440:        url, username, password = self._get_url_and_credentials(req.url)
./.venv-build/lib/python3.12/site-packages/pip/_internal/network/auth.py:445:        if username is not None and password is not None:
./.venv-build/lib/python3.12/site-packages/pip/_internal/network/auth.py:447:            req = HTTPBasicAuth(username, password)(req)
./.venv-build/lib/python3.12/site-packages/pip/_internal/network/auth.py:455:    def _prompt_for_password(self, netloc: str) -> tuple[str | None, str | None, bool]:
./.venv-build/lib/python3.12/site-packages/pip/_internal/network/auth.py:463:        password = ask_password("Password: ")
./.venv-build/lib/python3.12/site-packages/pip/_internal/network/auth.py:464:        return username, password, True
./.venv-build/lib/python3.12/site-packages/pip/_internal/network/auth.py:467:    def _should_save_password_to_keyring(self) -> bool:
./.venv-build/lib/python3.12/site-packages/pip/_internal/network/auth.py:478:        username, password = None, None
./.venv-build/lib/python3.12/site-packages/pip/_internal/network/auth.py:482:            username, password = self._get_new_credentials(
./.venv-build/lib/python3.12/site-packages/pip/_internal/network/auth.py:489:        if not self.prompting and not username and not password:
./.venv-build/lib/python3.12/site-packages/pip/_internal/network/auth.py:494:        # Prompt the user for a new username and password
./.venv-build/lib/python3.12/site-packages/pip/_internal/network/auth.py:496:        if not username and not password:
./.venv-build/lib/python3.12/site-packages/pip/_internal/network/auth.py:497:            username, password, save = self._prompt_for_password(parsed.netloc)
./.venv-build/lib/python3.12/site-packages/pip/_internal/network/auth.py:499:        # Store the new username and password to use for future requests
./.venv-build/lib/python3.12/site-packages/pip/_internal/network/auth.py:501:        if username is not None and password is not None:
./.venv-build/lib/python3.12/site-packages/pip/_internal/network/auth.py:502:            self.passwords[parsed.netloc] = (username, password)
./.venv-build/lib/python3.12/site-packages/pip/_internal/network/auth.py:504:            # Prompt to save the password to keyring
./.venv-build/lib/python3.12/site-packages/pip/_internal/network/auth.py:505:            if save and self._should_save_password_to_keyring():
./.venv-build/lib/python3.12/site-packages/pip/_internal/network/auth.py:509:                    password=password,
./.venv-build/lib/python3.12/site-packages/pip/_internal/network/auth.py:519:        # Add our new username and password to the request
./.venv-build/lib/python3.12/site-packages/pip/_internal/network/auth.py:520:        req = HTTPBasicAuth(username or "", password or "")(resp.request)
./.venv-build/lib/python3.12/site-packages/pip/_internal/network/auth.py:552:                self.keyring_provider.save_auth_info(creds.url, creds.username, creds.password)
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/markers.py:21:from ._tokenizer import ParserSyntaxError
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_tokenizer.py:10:class Token:
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_tokenizer.py:88:class Tokenizer:
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_tokenizer.py:89:    """Context-sensitive token parsing.
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_tokenizer.py:91:    Provides methods to examine the input stream to check whether the next token
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_tokenizer.py:105:        self.next_token: Optional[Token] = None
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_tokenizer.py:109:        """Move beyond provided token name, if at current position."""
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_tokenizer.py:114:        """Check whether the next token has the provided name.
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_tokenizer.py:116:        By default, if the check succeeds, the token *must* be read before
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_tokenizer.py:117:        another check. If `peek` is set to `True`, the token is not loaded and
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_tokenizer.py:121:            self.next_token is None
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_tokenizer.py:122:        ), f"Cannot check for {name!r}, already have {self.next_token!r}"
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_tokenizer.py:123:        assert name in self.rules, f"Unknown token name: {name!r}"
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_tokenizer.py:131:            self.next_token = Token(name, match[0], self.position)
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_tokenizer.py:134:    def expect(self, name: str, *, expected: str) -> Token:
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_tokenizer.py:135:        """Expect a certain token name next, failing with a syntax error otherwise.
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_tokenizer.py:137:        The token is *not* read.
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_tokenizer.py:143:    def read(self) -> Token:
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_tokenizer.py:144:        """Consume the next token and return it."""
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_tokenizer.py:145:        token = self.next_token
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_tokenizer.py:146:        assert token is not None
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_tokenizer.py:148:        self.position += len(token.text)
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_tokenizer.py:149:        self.next_token = None
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_tokenizer.py:151:        return token
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_tokenizer.py:172:    def enclosing_tokens(self, open_token: str, close_token: str, *, around: str) -> Iterator[None]:
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_tokenizer.py:173:        if self.check(open_token):
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_tokenizer.py:184:        if not self.check(close_token):
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_tokenizer.py:186:                f"Expected matching {close_token} for {open_token}, after {around}",
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:10:from ._tokenizer import DEFAULT_RULES, Tokenizer
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:64:    return _parse_requirement(Tokenizer(source, rules=DEFAULT_RULES))
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:67:def _parse_requirement(tokenizer: Tokenizer) -> ParsedRequirement:
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:71:    tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:73:    name_token = tokenizer.expect(
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:76:    name = name_token.text
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:77:    tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:79:    extras = _parse_extras(tokenizer)
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:80:    tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:82:    url, specifier, marker = _parse_requirement_details(tokenizer)
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:83:    tokenizer.expect("END", expected="end of dependency specifier")
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:89:    tokenizer: Tokenizer,
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:100:    if tokenizer.check("AT"):
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:101:        tokenizer.read()
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:102:        tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:104:        url_start = tokenizer.position
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:105:        url = tokenizer.expect("URL", expected="URL after @").text
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:106:        if tokenizer.check("END", peek=True):
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:109:        tokenizer.expect("WS", expected="whitespace after URL")
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:112:        if tokenizer.check("END", peek=True):
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:116:            tokenizer, span_start=url_start, after="URL and whitespace"
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:119:        specifier_start = tokenizer.position
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:120:        specifier = _parse_specifier(tokenizer)
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:121:        tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:123:        if tokenizer.check("END", peek=True):
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:127:            tokenizer,
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:135:def _parse_requirement_marker(tokenizer: Tokenizer, *, span_start: int, after: str) -> MarkerList:
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:140:    if not tokenizer.check("SEMICOLON"):
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:141:        tokenizer.raise_syntax_error(
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:145:    tokenizer.read()
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:147:    marker = _parse_marker(tokenizer)
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:148:    tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:153:def _parse_extras(tokenizer: Tokenizer) -> List[str]:
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:157:    if not tokenizer.check("LEFT_BRACKET", peek=True):
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:160:    with tokenizer.enclosing_tokens(
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:165:        tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:166:        extras = _parse_extras_list(tokenizer)
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:167:        tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:172:def _parse_extras_list(tokenizer: Tokenizer) -> List[str]:
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:178:    if not tokenizer.check("IDENTIFIER"):
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:181:    extras.append(tokenizer.read().text)
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:184:        tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:185:        if tokenizer.check("IDENTIFIER", peek=True):
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:186:            tokenizer.raise_syntax_error("Expected comma between extra names")
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:187:        elif not tokenizer.check("COMMA"):
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:190:        tokenizer.read()
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:191:        tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:193:        extra_token = tokenizer.expect("IDENTIFIER", expected="extra name after comma")
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:194:        extras.append(extra_token.text)
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:199:def _parse_specifier(tokenizer: Tokenizer) -> str:
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:204:    with tokenizer.enclosing_tokens(
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:209:        tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:210:        parsed_specifiers = _parse_version_many(tokenizer)
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:211:        tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:216:def _parse_version_many(tokenizer: Tokenizer) -> str:
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:221:    while tokenizer.check("SPECIFIER"):
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:222:        span_start = tokenizer.position
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:223:        parsed_specifiers += tokenizer.read().text
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:224:        if tokenizer.check("VERSION_PREFIX_TRAIL", peek=True):
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:225:            tokenizer.raise_syntax_error(
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:228:                span_end=tokenizer.position + 1,
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:230:        if tokenizer.check("VERSION_LOCAL_LABEL_TRAIL", peek=True):
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:231:            tokenizer.raise_syntax_error(
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:234:                span_end=tokenizer.position,
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:236:        tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:237:        if not tokenizer.check("COMMA"):
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:239:        parsed_specifiers += tokenizer.read().text
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:240:        tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:249:    return _parse_full_marker(Tokenizer(source, rules=DEFAULT_RULES))
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:252:def _parse_full_marker(tokenizer: Tokenizer) -> MarkerList:
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:253:    retval = _parse_marker(tokenizer)
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:254:    tokenizer.expect("END", expected="end of marker expression")
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:258:def _parse_marker(tokenizer: Tokenizer) -> MarkerList:
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:262:    expression = [_parse_marker_atom(tokenizer)]
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:263:    while tokenizer.check("BOOLOP"):
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:264:        token = tokenizer.read()
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:265:        expr_right = _parse_marker_atom(tokenizer)
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:266:        expression.extend((token.text, expr_right))
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:270:def _parse_marker_atom(tokenizer: Tokenizer) -> MarkerAtom:
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:276:    tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:277:    if tokenizer.check("LEFT_PARENTHESIS", peek=True):
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:278:        with tokenizer.enclosing_tokens(
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:283:            tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:284:            marker: MarkerAtom = _parse_marker(tokenizer)
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:285:            tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:287:        marker = _parse_marker_item(tokenizer)
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:288:    tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:292:def _parse_marker_item(tokenizer: Tokenizer) -> MarkerItem:
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:296:    tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:297:    marker_var_left = _parse_marker_var(tokenizer)
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:298:    tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:299:    marker_op = _parse_marker_op(tokenizer)
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:300:    tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:301:    marker_var_right = _parse_marker_var(tokenizer)
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:302:    tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:306:def _parse_marker_var(tokenizer: Tokenizer) -> MarkerVar:
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:310:    if tokenizer.check("VARIABLE"):
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:311:        return process_env_var(tokenizer.read().text.replace(".", "_"))
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:312:    elif tokenizer.check("QUOTED_STRING"):
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:313:        return process_python_str(tokenizer.read().text)
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:315:        tokenizer.raise_syntax_error(message="Expected a marker variable or quoted string")
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:330:def _parse_marker_op(tokenizer: Tokenizer) -> Op:
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:334:    if tokenizer.check("IN"):
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:335:        tokenizer.read()
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:337:    elif tokenizer.check("NOT"):
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:338:        tokenizer.read()
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:339:        tokenizer.expect("WS", expected="whitespace after 'not'")
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:340:        tokenizer.expect("IN", expected="'in' after 'not'")
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:342:    elif tokenizer.check("OP"):
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:343:        return Op(tokenizer.read().text)
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:345:        return tokenizer.raise_syntax_error(
./.venv-build/lib/python3.12/site-packages/wheel/vendored/packaging/requirements.py:8:from ._tokenizer import ParserSyntaxError
./.venv-build/lib/python3.12/site-packages/pycparser/plyparser.py:57:    def _token_coord(self, p, token_idx):
./.venv-build/lib/python3.12/site-packages/pycparser/plyparser.py:59:        with 'token_idx'. The coordinate includes the 'lineno' and
./.venv-build/lib/python3.12/site-packages/pycparser/plyparser.py:62:        last_cr = p.lexer.lexer.lexdata.rfind("\n", 0, p.lexpos(token_idx))
./.venv-build/lib/python3.12/site-packages/pycparser/plyparser.py:65:        column = p.lexpos(token_idx) - (last_cr)
./.venv-build/lib/python3.12/site-packages/pycparser/plyparser.py:66:        return self._coord(p.lineno(token_idx), column)
./.venv-build/lib/python3.12/site-packages/pycparser/ply/lex.py:52:# This regular expression is used to match valid token names
./.venv-build/lib/python3.12/site-packages/pycparser/ply/lex.py:56:# Exception thrown when invalid token encountered and no default error
./.venv-build/lib/python3.12/site-packages/pycparser/ply/lex.py:64:# Token class.  This class is used to represent the tokens produced.
./.venv-build/lib/python3.12/site-packages/pycparser/ply/lex.py:65:class LexToken(object):
./.venv-build/lib/python3.12/site-packages/pycparser/ply/lex.py:67:        return "LexToken(%s,%r,%d,%d)" % (
./.venv-build/lib/python3.12/site-packages/pycparser/ply/lex.py:115:#    token()          -  Get the next token
./.venv-build/lib/python3.12/site-packages/pycparser/ply/lex.py:145:        self.lextokens = None  # List of valid tokens
./.venv-build/lib/python3.12/site-packages/pycparser/ply/lex.py:193:            tf.write("_lextokens    = set(%s)\n" % repr(tuple(sorted(self.lextokens))))
./.venv-build/lib/python3.12/site-packages/pycparser/ply/lex.py:234:        self.lextokens = lextab._lextokens
./.venv-build/lib/python3.12/site-packages/pycparser/ply/lex.py:237:        self.lextokens_all = self.lextokens | set(self.lexliterals)
./.venv-build/lib/python3.12/site-packages/pycparser/ply/lex.py:317:    # opttoken() - Return the next token from the Lexer
./.venv-build/lib/python3.12/site-packages/pycparser/ply/lex.py:323:    def token(self):
./.venv-build/lib/python3.12/site-packages/pycparser/ply/lex.py:342:                # Create a token for return
./.venv-build/lib/python3.12/site-packages/pycparser/ply/lex.py:343:                tok = LexToken()
./.venv-build/lib/python3.12/site-packages/pycparser/ply/lex.py:352:                    # If no token type was set, it's an ignored token
./.venv-build/lib/python3.12/site-packages/pycparser/ply/lex.py:362:                # If token is processed by a function, call it
./.venv-build/lib/python3.12/site-packages/pycparser/ply/lex.py:364:                tok.lexer = self  # Set additional attributes useful in token rules
./.venv-build/lib/python3.12/site-packages/pycparser/ply/lex.py:370:                # Every function must return a token, if nothing, we just move to next token
./.venv-build/lib/python3.12/site-packages/pycparser/ply/lex.py:376:                # Verify type of the token.  If not in the token map, raise an error
./.venv-build/lib/python3.12/site-packages/pycparser/ply/lex.py:378:                    if newtok.type not in self.lextokens_all:
./.venv-build/lib/python3.12/site-packages/pycparser/ply/lex.py:380:                            "%s:%d: Rule '%s' returned an unknown token type '%s'"
./.venv-build/lib/python3.12/site-packages/pycparser/ply/lex.py:394:                    tok = LexToken()
./.venv-build/lib/python3.12/site-packages/pycparser/ply/lex.py:404:                    tok = LexToken()
./.venv-build/lib/python3.12/site-packages/pycparser/ply/lex.py:430:            tok = LexToken()
./.venv-build/lib/python3.12/site-packages/pycparser/ply/lex.py:450:        t = self.token()
./.venv-build/lib/python3.12/site-packages/pycparser/ply/lex.py:470:# or as a .regex attribute attached by the @TOKEN decorator.
./.venv-build/lib/python3.12/site-packages/pycparser/ply/lex.py:564:# def _statetoken(s,names)
./.venv-build/lib/python3.12/site-packages/pycparser/ply/lex.py:567:# state names, this function returns a tuple (states,tokenname) where states
./.venv-build/lib/python3.12/site-packages/pycparser/ply/lex.py:568:# is a tuple of state names and tokenname is the name of the token.  For example,
./.venv-build/lib/python3.12/site-packages/pycparser/ply/lex.py:571:def _statetoken(s, names):
./.venv-build/lib/python3.12/site-packages/pycparser/ply/lex.py:586:    tokenname = "_".join(parts[i:])
./.venv-build/lib/python3.12/site-packages/pycparser/ply/lex.py:587:    return (states, tokenname)
./.venv-build/lib/python3.12/site-packages/pycparser/ply/lex.py:600:        self.tokens = []
./.venv-build/lib/python3.12/site-packages/pycparser/ply/lex.py:609:        self.get_tokens()
./.venv-build/lib/python3.12/site-packages/pycparser/ply/lex.py:616:        self.validate_tokens()
./.venv-build/lib/python3.12/site-packages/pycparser/ply/lex.py:621:    # Get the tokens map
./.venv-build/lib/python3.12/site-packages/pycparser/ply/lex.py:622:    def get_tokens(self):
./.venv-build/lib/python3.12/site-packages/pycparser/ply/lex.py:623:        tokens = self.ldict.get("tokens", None)
./.venv-build/lib/python3.12/site-packages/pycparser/ply/lex.py:624:        if not tokens:
./.venv-build/lib/python3.12/site-packages/pycparser/ply/lex.py:625:            self.log.error("No token list is defined")
./.venv-build/lib/python3.12/site-packages/pycparser/ply/lex.py:629:        if not isinstance(tokens, (list, tuple)):
./.venv-build/lib/python3.12/site-packages/pycparser/ply/lex.py:630:            self.log.error("tokens must be a list or tuple")
./.venv-build/lib/python3.12/site-packages/pycparser/ply/lex.py:634:        if not tokens:
./.venv-build/lib/python3.12/site-packages/pycparser/ply/lex.py:635:            self.log.error("tokens is empty")
./.venv-build/lib/python3.12/site-packages/pycparser/ply/lex.py:639:        self.tokens = tokens
./.venv-build/lib/python3.12/site-packages/pycparser/ply/lex.py:641:    # Validate the tokens
./.venv-build/lib/python3.12/site-packages/pycparser/ply/lex.py:642:    def validate_tokens(self):
./.venv-build/lib/python3.12/site-packages/pycparser/ply/lex.py:644:        for n in self.tokens:
./.venv-build/lib/python3.12/site-packages/pycparser/ply/lex.py:646:                self.log.error("Bad token name '%s'", n)
./.venv-build/lib/python3.12/site-packages/pycparser/ply/lex.py:649:                self.log.warning("Token '%s' multiply defined", n)
./.venv-build/lib/python3.12/site-packages/pycparser/ply/lex.py:713:        self.toknames = {}  # Mapping of symbols to token names
./.venv-build/lib/python3.12/site-packages/pycparser/ply/lex.py:731:            states, tokname = _statetoken(f, self.stateinfo)
./.venv-build/lib/python3.12/site-packages/pycparser/ply/lex.py:856:                if tokname not in self.tokens and tokname.find("ignore_") < 0:
./.venv-build/lib/python3.12/site-packages/pycparser/ply/lex.py:857:                    self.log.error("Rule '%s' defined for an unspecified token %s", name, tokname)
./.venv-build/lib/python3.12/site-packages/pycparser/ply/lex.py:976:    global token, input
./.venv-build/lib/python3.12/site-packages/pycparser/ply/lex.py:1016:            token = lexobj.token
./.venv-build/lib/python3.12/site-packages/pycparser/ply/lex.py:1026:        debuglog.info("lex: tokens   = %r", linfo.tokens)
./.venv-build/lib/python3.12/site-packages/pycparser/ply/lex.py:1030:    # Build a dictionary of valid token names
./.venv-build/lib/python3.12/site-packages/pycparser/ply/lex.py:1031:    lexobj.lextokens = set()
./.venv-build/lib/python3.12/site-packages/pycparser/ply/lex.py:1032:    for n in linfo.tokens:
./.venv-build/lib/python3.12/site-packages/pycparser/ply/lex.py:1033:        lexobj.lextokens.add(n)
./.venv-build/lib/python3.12/site-packages/pycparser/ply/lex.py:1041:    lexobj.lextokens_all = lexobj.lextokens | set(lexobj.lexliterals)
./.venv-build/lib/python3.12/site-packages/pycparser/ply/lex.py:1125:    # Create global versions of the token() and input() functions
./.venv-build/lib/python3.12/site-packages/pycparser/ply/lex.py:1126:    token = lexobj.token
./.venv-build/lib/python3.12/site-packages/pycparser/ply/lex.py:1180:        _token = lexer.token
./.venv-build/lib/python3.12/site-packages/pycparser/ply/lex.py:1182:        _token = token
./.venv-build/lib/python3.12/site-packages/pycparser/ply/lex.py:1185:        tok = _token()
./.venv-build/lib/python3.12/site-packages/pycparser/ply/lex.py:1192:# @TOKEN(regex)
./.venv-build/lib/python3.12/site-packages/pycparser/ply/lex.py:1199:def TOKEN(r):
./.venv-build/lib/python3.12/site-packages/pycparser/ply/lex.py:1210:# Alternative spelling of the TOKEN decorator
./.venv-build/lib/python3.12/site-packages/pycparser/ply/lex.py:1211:Token = TOKEN
./.venv-build/lib/python3.12/site-packages/pycparser/ply/ctokens.py:2:# ctokens.py
./.venv-build/lib/python3.12/site-packages/pycparser/ply/ctokens.py:4:# Token specifications for symbols in ANSI C and C++.  This file is
./.venv-build/lib/python3.12/site-packages/pycparser/ply/ctokens.py:5:# meant to be used as a library in other tokenizers.
./.venv-build/lib/python3.12/site-packages/pycparser/ply/ctokens.py:10:tokens = [
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:20:# Default preprocessor lexer definitions.   These tokens are enough to get
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:24:tokens = (
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:153:#    .value     - Macro value (a list of tokens)
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:158:# When a macro is created, the macro replacement token sequence is
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:192:        # Probe the lexer for selected tokens
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:201:    # tokenize()
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:203:    # Utility function. Given a string of text, tokenize into a list of tokens
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:206:    def tokenize(self, text):
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:207:        tokens = []
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:210:            tok = self.lexer.token()
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:213:            tokens.append(tok)
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:214:        return tokens
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:229:    # the token types of symbols that are important to the preprocessor.
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:231:    # with any suitable lexer regardless of how tokens have been named.
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:235:        # Determine the token type for identifiers
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:237:        tok = self.lexer.token()
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:243:        # Determine the token type for integers
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:245:        tok = self.lexer.token()
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:252:        # Determine the token type for strings enclosed in double quotes
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:254:        tok = self.lexer.token()
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:260:        # Determine the token type for whitespace--if any
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:262:        tok = self.lexer.token()
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:268:        # Determine the token type for newlines
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:270:        tok = self.lexer.token()
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:273:            print("Couldn't determine token for newlines")
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:283:            tok = self.lexer.token()
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:321:            tok = lex.token()
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:333:    # tokenstrip()
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:335:    # Remove leading/trailing whitespace tokens from a token list
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:338:    def tokenstrip(self, tokens):
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:340:        while i < len(tokens) and tokens[i].type in self.t_WS:
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:342:        del tokens[:i]
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:343:        i = len(tokens) - 1
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:344:        while i >= 0 and tokens[i].type in self.t_WS:
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:346:        del tokens[i + 1 :]
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:347:        return tokens
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:352:    # Collects comma separated arguments from a list of tokens.   The arguments
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:353:    # must be enclosed in parenthesis.  Returns a tuple (tokencount,args,positions)
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:354:    # where tokencount is the number of tokens consumed, args is a list of arguments,
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:356:    # argument.  Each argument is represented by a list of tokens.
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:365:    def collect_args(self, tokenlist):
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:370:        tokenlen = len(tokenlist)
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:374:        while (i < tokenlen) and (tokenlist[i].type in self.t_WS):
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:377:        if (i < tokenlen) and (tokenlist[i].value == "("):
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:380:            self.error(self.source, tokenlist[0].lineno, "Missing '(' in macro arguments")
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:385:        while i < tokenlen:
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:386:            t = tokenlist[i]
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:394:                        args.append(self.tokenstrip(current_arg))
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:399:                args.append(self.tokenstrip(current_arg))
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:407:        self.error(self.source, tokenlist[-1].lineno, "Missing ')' in macro arguments")
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:413:    # Examine the macro value (token sequence) and identify patch points
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:461:    # Given a Macro and list of arguments (each a token list), this method
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:462:    # returns an expanded version of a macro.  The return value is a token sequence
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:463:    # representing the replacement macro tokens
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:467:        # Make a copy of the macro token sequence
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:512:    # Given a list of tokens, this function performs macro expansion.
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:517:    def expand_macros(self, tokens, expanded=None):
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:521:        while i < len(tokens):
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:522:            t = tokens[i]
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:534:                        tokens[i : i + 1] = ex
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:539:                        while j < len(tokens) and tokens[j].type in self.t_WS:
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:541:                        if tokens[j].value == "(":
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:542:                            tokcount, args, positions = self.collect_args(tokens[j:])
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:571:                                        args[len(m.arglist) - 1] = tokens[
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:581:                                tokens[i : j + tokcount] = rep
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:590:        return tokens
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:595:    # Evaluate an expression token sequence for the purposes of evaluating
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:599:    def evalexpr(self, tokens):
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:600:        # tokens = tokenize(line)
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:603:        while i < len(tokens):
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:604:            if tokens[i].type == self.t_ID and tokens[i].value == "defined":
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:608:                while j < len(tokens):
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:609:                    if tokens[j].type in self.t_WS:
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:612:                    elif tokens[j].type == self.t_ID:
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:613:                        if tokens[j].value in self.macros:
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:619:                    elif tokens[j].value == "(":
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:621:                    elif tokens[j].value == ")":
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:624:                        self.error(self.source, tokens[i].lineno, "Malformed defined()")
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:626:                tokens[i].type = self.t_INTEGER
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:627:                tokens[i].value = self.t_INTEGER_TYPE(result)
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:628:                del tokens[i + 1 : j + 1]
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:630:        tokens = self.expand_macros(tokens)
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:631:        for i, t in enumerate(tokens):
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:633:                tokens[i] = copy.copy(t)
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:634:                tokens[i].type = self.t_INTEGER
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:635:                tokens[i].value = self.t_INTEGER_TYPE("0L")
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:637:                tokens[i] = copy.copy(t)
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:639:                tokens[i].value = str(tokens[i].value)
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:640:                while tokens[i].value[-1] not in "0123456789abcdefABCDEF":
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:641:                    tokens[i].value = tokens[i].value[:-1]
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:643:        expr = "".join([str(x.value) for x in tokens])
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:650:            self.error(self.source, tokens[0].lineno, "Couldn't evaluate expression")
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:682:                # insert necessary whitespace instead of eaten tokens
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:687:                dirtokens = self.tokenstrip(x[i + 1 :])
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:688:                if dirtokens:
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:689:                    name = dirtokens[0].value
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:690:                    args = self.tokenstrip(dirtokens[1:])
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:755:                        self.error(self.source, dirtokens[0].lineno, "Misplaced #elif")
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:766:                        self.error(self.source, dirtokens[0].lineno, "Misplaced #else")
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:772:                        self.error(self.source, dirtokens[0].lineno, "Misplaced #endif")
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:792:    def include(self, tokens):
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:794:        if not tokens:
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:796:        if tokens:
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:797:            if tokens[0].value != "<" and tokens[0].type != self.t_STRING:
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:798:                tokens = self.expand_macros(tokens)
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:800:            if tokens[0].value == "<":
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:803:                while i < len(tokens):
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:804:                    if tokens[i].value == ">":
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:810:                filename = "".join([x.value for x in tokens[1:i]])
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:812:            elif tokens[0].type == self.t_STRING:
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:813:                filename = tokens[0].value[1:-1]
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:841:    def define(self, tokens):
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:842:        if isinstance(tokens, STRING_TYPES):
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:843:            tokens = self.tokenize(tokens)
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:845:        linetok = tokens
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:857:                m = Macro(name.value, self.tokenstrip(linetok[2:]))
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:887:                    mvalue = self.tokenstrip(linetok[1 + tokcount :])
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:911:    def undef(self, tokens):
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:912:        id = tokens[0].value
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:928:    # token()
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:930:    # Method to return individual tokens
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:932:    def token(self):
./.venv-build/lib/python3.12/site-packages/pycparser/ply/cpp.py:957:        tok = p.token()
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:168:_token = None
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:170:_warnmsg = """PLY: Don't use global functions errok(), token(), and restart() in p_error().
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:175:        # Use parser.errok(), parser.token(), parser.restart()
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:192:def token():
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:194:    return _token()
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:198:def call_errorfunc(errorfunc, token, parser):
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:199:    global _errok, _token, _restart
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:201:    _token = parser.token
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:203:    r = errorfunc(token)
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:205:        del _errok, _token, _restart
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:324:    # the next look-ahead token.  This delayed invocation of the tokenizer can be useful in
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:339:    def parse(self, input=None, lexer=None, debug=False, tracking=False, tokenfunc=None):
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:343:            return self.parsedebug(input, lexer, debug, tracking, tokenfunc)
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:345:            return self.parseopt(input, lexer, debug, tracking, tokenfunc)
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:347:            return self.parseopt_notrack(input, lexer, debug, tracking, tokenfunc)
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:363:    def parsedebug(self, input=None, lexer=None, debug=False, tracking=False, tokenfunc=None):
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:392:        if tokenfunc is None:
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:393:            # Tokenize function
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:394:            get_token = lexer.token
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:396:            get_token = tokenfunc
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:398:        # Set the parser() token method (sometimes used in error recovery)
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:399:        self.token = get_token
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:409:        errtoken = None  # Err token
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:421:            # the next token off of the lookaheadstack or from the lexer
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:431:                        lookahead = get_token()  # Get the next token
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:538:                            lookaheadstack.append(lookahead)  # Save the current lookahead token
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:581:                            lookaheadstack.append(lookahead)  # Save the current lookahead token
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:613:                # this, we are going to push the current token onto
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:614:                # the tokenstack and replace it with an 'error' token.
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:618:                # In addition to pushing the error token, we call call
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:625:                    errtoken = lookahead
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:626:                    if errtoken.type == "$end":
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:627:                        errtoken = None  # End of file!
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:629:                        if errtoken and not hasattr(errtoken, "lexer"):
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:630:                            errtoken.lexer = lexer
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:632:                        tok = call_errorfunc(self.errorfunc, errtoken, self)
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:636:                            # returned token is the next lookahead
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:638:                            errtoken = None
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:641:                        if errtoken:
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:642:                            if hasattr(errtoken, "lineno"):
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:648:                                    "yacc: Syntax error at line %d, token=%s\n"
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:649:                                    % (lineno, errtoken.type)
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:652:                                sys.stderr.write("yacc: Syntax error, token=%s" % errtoken.type)
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:661:                # entire parse has been rolled back and we're completely hosed.   The token is
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:666:                    errtoken = None
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:673:                # at the end of the file. nuke the top entry and generate an error token
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:729:    def parseopt(self, input=None, lexer=None, debug=False, tracking=False, tokenfunc=None):
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:754:        if tokenfunc is None:
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:755:            # Tokenize function
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:756:            get_token = lexer.token
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:758:            get_token = tokenfunc
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:760:        # Set the parser() token method (sometimes used in error recovery)
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:761:        self.token = get_token
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:771:        errtoken = None  # Err token
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:783:            # the next token off of the lookaheadstack or from the lexer
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:788:                        lookahead = get_token()  # Get the next token
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:858:                            lookaheadstack.append(lookahead)  # Save the current lookahead token
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:898:                            lookaheadstack.append(lookahead)  # Save the current lookahead token
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:917:                # this, we are going to push the current token onto
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:918:                # the tokenstack and replace it with an 'error' token.
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:922:                # In addition to pushing the error token, we call call
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:929:                    errtoken = lookahead
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:930:                    if errtoken.type == "$end":
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:931:                        errtoken = None  # End of file!
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:933:                        if errtoken and not hasattr(errtoken, "lexer"):
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:934:                            errtoken.lexer = lexer
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:936:                        tok = call_errorfunc(self.errorfunc, errtoken, self)
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:940:                            # returned token is the next lookahead
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:942:                            errtoken = None
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:945:                        if errtoken:
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:946:                            if hasattr(errtoken, "lineno"):
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:952:                                    "yacc: Syntax error at line %d, token=%s\n"
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:953:                                    % (lineno, errtoken.type)
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:956:                                sys.stderr.write("yacc: Syntax error, token=%s" % errtoken.type)
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:965:                # entire parse has been rolled back and we're completely hosed.   The token is
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:970:                    errtoken = None
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:977:                # at the end of the file. nuke the top entry and generate an error token
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:1033:    def parseopt_notrack(self, input=None, lexer=None, debug=False, tracking=False, tokenfunc=None):
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:1058:        if tokenfunc is None:
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:1059:            # Tokenize function
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:1060:            get_token = lexer.token
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:1062:            get_token = tokenfunc
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:1064:        # Set the parser() token method (sometimes used in error recovery)
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:1065:        self.token = get_token
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:1075:        errtoken = None  # Err token
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:1087:            # the next token off of the lookaheadstack or from the lexer
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:1092:                        lookahead = get_token()  # Get the next token
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:1152:                            lookaheadstack.append(lookahead)  # Save the current lookahead token
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:1186:                            lookaheadstack.append(lookahead)  # Save the current lookahead token
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:1205:                # this, we are going to push the current token onto
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:1206:                # the tokenstack and replace it with an 'error' token.
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:1210:                # In addition to pushing the error token, we call call
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:1217:                    errtoken = lookahead
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:1218:                    if errtoken.type == "$end":
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:1219:                        errtoken = None  # End of file!
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:1221:                        if errtoken and not hasattr(errtoken, "lexer"):
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:1222:                            errtoken.lexer = lexer
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:1224:                        tok = call_errorfunc(self.errorfunc, errtoken, self)
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:1228:                            # returned token is the next lookahead
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:1230:                            errtoken = None
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:1233:                        if errtoken:
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:1234:                            if hasattr(errtoken, "lineno"):
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:1240:                                    "yacc: Syntax error at line %d, token=%s\n"
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:1241:                                    % (lineno, errtoken.type)
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:1244:                                sys.stderr.write("yacc: Syntax error, token=%s" % errtoken.type)
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:1253:                # entire parse has been rolled back and we're completely hosed.   The token is
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:1258:                    errtoken = None
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:1265:                # at the end of the file. nuke the top entry and generate an error token
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:1588:                "%s:%d: Illegal rule name %r. Already defined as a token" % (file, line, prodname)
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:1597:        # Look for literal tokens
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:1604:                            "%s:%d: Literal token %s in rule %r may only be a single character"
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:1778:    # Find all symbols that were used the grammar, but not defined as tokens or
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:2641:                                    # Shift precedence comes from the token
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:2717:                                    # Shift precedence comes from the token
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:3029:# start symbol, error function, tokens, precedence list, action functions,
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:3037:        self.tokens = None
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:3051:        self.get_tokens()
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:3059:        self.validate_tokens()
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:3073:            if self.tokens:
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:3074:                parts.append(" ".join(self.tokens))
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:3158:    # Get the tokens map
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:3159:    def get_tokens(self):
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:3160:        tokens = self.pdict.get("tokens")
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:3161:        if not tokens:
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:3162:            self.log.error("No token list is defined")
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:3166:        if not isinstance(tokens, (list, tuple)):
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:3167:            self.log.error("tokens must be a list or tuple")
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:3171:        if not tokens:
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:3172:            self.log.error("tokens is empty")
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:3176:        self.tokens = tokens
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:3178:    # Validate the tokens
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:3179:    def validate_tokens(self):
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:3180:        # Validate the tokens.
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:3181:        if "error" in self.tokens:
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:3182:            self.log.error("Illegal token name 'error'. Is a reserved word")
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:3187:        for n in self.tokens:
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:3189:                self.log.warning("Token %r multiply defined", n)
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:3450:    grammar = Grammar(pinfo.tokens)
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:3485:            "%s:%d: Symbol %r used, but not defined as a token or a rule",
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:3498:            errorlog.warning("Token %r defined, but not used", term)
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:3515:        errorlog.warning("There is 1 unused token")
./.venv-build/lib/python3.12/site-packages/pycparser/ply/yacc.py:3517:        errorlog.warning("There are %d unused tokens", len(unused_terminals))
./.venv-build/lib/python3.12/site-packages/pycparser/c_lexer.py:12:from .ply.lex import TOKEN
./.venv-build/lib/python3.12/site-packages/pycparser/c_lexer.py:17:    input text with input(), and call token() to get new
./.venv-build/lib/python3.12/site-packages/pycparser/c_lexer.py:18:    tokens.
./.venv-build/lib/python3.12/site-packages/pycparser/c_lexer.py:48:        # Keeps track of the last token returned from self.token()
./.venv-build/lib/python3.12/site-packages/pycparser/c_lexer.py:49:        self.last_token = None
./.venv-build/lib/python3.12/site-packages/pycparser/c_lexer.py:74:    def token(self):
./.venv-build/lib/python3.12/site-packages/pycparser/c_lexer.py:75:        self.last_token = self.lexer.token()
./.venv-build/lib/python3.12/site-packages/pycparser/c_lexer.py:76:        return self.last_token
./.venv-build/lib/python3.12/site-packages/pycparser/c_lexer.py:78:    def find_tok_column(self, token):
./.venv-build/lib/python3.12/site-packages/pycparser/c_lexer.py:79:        """Find the column of the token in its line."""
./.venv-build/lib/python3.12/site-packages/pycparser/c_lexer.py:80:        last_cr = self.lexer.lexdata.rfind("\n", 0, token.lexpos)
./.venv-build/lib/python3.12/site-packages/pycparser/c_lexer.py:81:        return token.lexpos - last_cr
./.venv-build/lib/python3.12/site-packages/pycparser/c_lexer.py:88:    def _error(self, msg, token):
./.venv-build/lib/python3.12/site-packages/pycparser/c_lexer.py:89:        location = self._make_tok_location(token)
./.venv-build/lib/python3.12/site-packages/pycparser/c_lexer.py:93:    def _make_tok_location(self, token):
./.venv-build/lib/python3.12/site-packages/pycparser/c_lexer.py:94:        return (token.lineno, self.find_tok_column(token))
./.venv-build/lib/python3.12/site-packages/pycparser/c_lexer.py:159:    ## All the tokens recognized by the lexer
./.venv-build/lib/python3.12/site-packages/pycparser/c_lexer.py:161:    tokens = (
./.venv-build/lib/python3.12/site-packages/pycparser/c_lexer.py:250:    ## Regexes for use in tokens
./.venv-build/lib/python3.12/site-packages/pycparser/c_lexer.py:388:    @TOKEN(string_literal)
./.venv-build/lib/python3.12/site-packages/pycparser/c_lexer.py:395:    @TOKEN(decimal_constant)
./.venv-build/lib/python3.12/site-packages/pycparser/c_lexer.py:520:    # lookahead token.  If we open a new scope in brace_open, then TT has
./.venv-build/lib/python3.12/site-packages/pycparser/c_lexer.py:525:    @TOKEN(r"\{")
./.venv-build/lib/python3.12/site-packages/pycparser/c_lexer.py:530:    @TOKEN(r"\}")
./.venv-build/lib/python3.12/site-packages/pycparser/c_lexer.py:542:    @TOKEN(floating_constant)
./.venv-build/lib/python3.12/site-packages/pycparser/c_lexer.py:546:    @TOKEN(hex_floating_constant)
./.venv-build/lib/python3.12/site-packages/pycparser/c_lexer.py:550:    @TOKEN(hex_constant)
./.venv-build/lib/python3.12/site-packages/pycparser/c_lexer.py:554:    @TOKEN(bin_constant)
./.venv-build/lib/python3.12/site-packages/pycparser/c_lexer.py:558:    @TOKEN(bad_octal_constant)
./.venv-build/lib/python3.12/site-packages/pycparser/c_lexer.py:563:    @TOKEN(unsupported_c_style_comment)
./.venv-build/lib/python3.12/site-packages/pycparser/c_lexer.py:568:    @TOKEN(unsupported_cxx_style_comment)
./.venv-build/lib/python3.12/site-packages/pycparser/c_lexer.py:573:    @TOKEN(octal_constant)
./.venv-build/lib/python3.12/site-packages/pycparser/c_lexer.py:577:    @TOKEN(decimal_constant)
./.venv-build/lib/python3.12/site-packages/pycparser/c_lexer.py:584:    @TOKEN(multicharacter_constant)
./.venv-build/lib/python3.12/site-packages/pycparser/c_lexer.py:588:    @TOKEN(char_const)
./.venv-build/lib/python3.12/site-packages/pycparser/c_lexer.py:592:    @TOKEN(wchar_const)
./.venv-build/lib/python3.12/site-packages/pycparser/c_lexer.py:596:    @TOKEN(u8char_const)
./.venv-build/lib/python3.12/site-packages/pycparser/c_lexer.py:600:    @TOKEN(u16char_const)
./.venv-build/lib/python3.12/site-packages/pycparser/c_lexer.py:604:    @TOKEN(u32char_const)
./.venv-build/lib/python3.12/site-packages/pycparser/c_lexer.py:608:    @TOKEN(unmatched_quote)
./.venv-build/lib/python3.12/site-packages/pycparser/c_lexer.py:613:    @TOKEN(bad_char_const)
./.venv-build/lib/python3.12/site-packages/pycparser/c_lexer.py:618:    @TOKEN(wstring_literal)
./.venv-build/lib/python3.12/site-packages/pycparser/c_lexer.py:622:    @TOKEN(u8string_literal)
./.venv-build/lib/python3.12/site-packages/pycparser/c_lexer.py:626:    @TOKEN(u16string_literal)
./.venv-build/lib/python3.12/site-packages/pycparser/c_lexer.py:630:    @TOKEN(u32string_literal)
./.venv-build/lib/python3.12/site-packages/pycparser/c_lexer.py:636:    @TOKEN(bad_string_literal)
./.venv-build/lib/python3.12/site-packages/pycparser/c_lexer.py:641:    @TOKEN(identifier)
./.venv-build/lib/python3.12/site-packages/pycparser/lextab.py:3:_lextokens = set(
./.venv-build/lib/python3.12/site-packages/pycparser/c_parser.py:86:        self.tokens = self.clex.tokens
./.venv-build/lib/python3.12/site-packages/pycparser/c_parser.py:127:        # Keeps track of the last token given to yacc (the lookahead token)
./.venv-build/lib/python3.12/site-packages/pycparser/c_parser.py:128:        self._last_yielded_token = None
./.venv-build/lib/python3.12/site-packages/pycparser/c_parser.py:146:        self._last_yielded_token = None
./.venv-build/lib/python3.12/site-packages/pycparser/c_parser.py:206:    def _get_yacc_lookahead_token(self):
./.venv-build/lib/python3.12/site-packages/pycparser/c_parser.py:207:        """We need access to yacc's lookahead token in certain cases.
./.venv-build/lib/python3.12/site-packages/pycparser/c_parser.py:208:        This is the last token yacc requested from the lexer, so we
./.venv-build/lib/python3.12/site-packages/pycparser/c_parser.py:211:        return self.clex.last_token
./.venv-build/lib/python3.12/site-packages/pycparser/c_parser.py:470:    def _select_struct_union_class(self, token):
./.venv-build/lib/python3.12/site-packages/pycparser/c_parser.py:471:        """Given a token (either STRUCT or UNION), selects the
./.venv-build/lib/python3.12/site-packages/pycparser/c_parser.py:474:        if token == "struct":
./.venv-build/lib/python3.12/site-packages/pycparser/c_parser.py:554:            p[0] = [c_ast.StaticAssert(p[3], None, self._token_coord(p, 1))]
./.venv-build/lib/python3.12/site-packages/pycparser/c_parser.py:556:            p[0] = [c_ast.StaticAssert(p[3], p[5], self._token_coord(p, 1))]
./.venv-build/lib/python3.12/site-packages/pycparser/c_parser.py:560:        self._parse_error("Directives not supported yet", self._token_coord(p, 1))
./.venv-build/lib/python3.12/site-packages/pycparser/c_parser.py:573:            p[0] = c_ast.Pragma(p[3], self._token_coord(p, 2))
./.venv-build/lib/python3.12/site-packages/pycparser/c_parser.py:575:            p[0] = c_ast.Pragma(p[2], self._token_coord(p, 2))
./.venv-build/lib/python3.12/site-packages/pycparser/c_parser.py:577:            p[0] = c_ast.Pragma("", self._token_coord(p, 1))
./.venv-build/lib/python3.12/site-packages/pycparser/c_parser.py:594:            type=[c_ast.IdentifierType(["int"], coord=self._token_coord(p, 1))],
./.venv-build/lib/python3.12/site-packages/pycparser/c_parser.py:670:            p[0] = c_ast.Compound(block_items=p[1] + [p[2]], coord=self._token_coord(p, 1))
./.venv-build/lib/python3.12/site-packages/pycparser/c_parser.py:737:    # the parser's lookahead asked for the token after SEMI, which
./.venv-build/lib/python3.12/site-packages/pycparser/c_parser.py:844:        p[0] = c_ast.IdentifierType([p[1]], coord=self._token_coord(p, 1))
./.venv-build/lib/python3.12/site-packages/pycparser/c_parser.py:932:        p[0] = klass(name=p[2], decls=None, coord=self._token_coord(p, 2))
./.venv-build/lib/python3.12/site-packages/pycparser/c_parser.py:941:            p[0] = klass(name=None, decls=[], coord=self._token_coord(p, 2))
./.venv-build/lib/python3.12/site-packages/pycparser/c_parser.py:943:            p[0] = klass(name=None, decls=p[3], coord=self._token_coord(p, 2))
./.venv-build/lib/python3.12/site-packages/pycparser/c_parser.py:954:            p[0] = klass(name=p[2], decls=[], coord=self._token_coord(p, 2))
./.venv-build/lib/python3.12/site-packages/pycparser/c_parser.py:956:            p[0] = klass(name=p[2], decls=p[4], coord=self._token_coord(p, 2))
./.venv-build/lib/python3.12/site-packages/pycparser/c_parser.py:1040:        p[0] = c_ast.Enum(p[2], None, self._token_coord(p, 1))
./.venv-build/lib/python3.12/site-packages/pycparser/c_parser.py:1044:        p[0] = c_ast.Enum(None, p[3], self._token_coord(p, 1))
./.venv-build/lib/python3.12/site-packages/pycparser/c_parser.py:1050:        p[0] = c_ast.Enum(p[2], p[4], self._token_coord(p, 1))
./.venv-build/lib/python3.12/site-packages/pycparser/c_parser.py:1069:        p[0] = c_ast.Alignas(p[3], self._token_coord(p, 1))
./.venv-build/lib/python3.12/site-packages/pycparser/c_parser.py:1076:            enumerator = c_ast.Enumerator(p[1], None, self._token_coord(p, 1))
./.venv-build/lib/python3.12/site-packages/pycparser/c_parser.py:1078:            enumerator = c_ast.Enumerator(p[1], p[3], self._token_coord(p, 1))
./.venv-build/lib/python3.12/site-packages/pycparser/c_parser.py:1107:            coord=self._token_coord(p, 1),
./.venv-build/lib/python3.12/site-packages/pycparser/c_parser.py:1151:            dim=c_ast.ID(p[4], self._token_coord(p, 4)),
./.venv-build/lib/python3.12/site-packages/pycparser/c_parser.py:1165:        # To see why _get_yacc_lookahead_token is needed, consider:
./.venv-build/lib/python3.12/site-packages/pycparser/c_parser.py:1170:        # yacc's lookahead token.  We don't know if we're declaring or
./.venv-build/lib/python3.12/site-packages/pycparser/c_parser.py:1172:        # trigger a rule on that token, then TT will have already been read
./.venv-build/lib/python3.12/site-packages/pycparser/c_parser.py:1176:        if self._get_yacc_lookahead_token().type == "LBRACE":
./.venv-build/lib/python3.12/site-packages/pycparser/c_parser.py:1189:        coord = self._token_coord(p, 1)
./.venv-build/lib/python3.12/site-packages/pycparser/c_parser.py:1226:            p[1].params.append(c_ast.EllipsisParam(self._token_coord(p, 3)))
./.venv-build/lib/python3.12/site-packages/pycparser/c_parser.py:1256:            spec["type"] = [c_ast.IdentifierType(["int"], coord=self._token_coord(p, 1))]
./.venv-build/lib/python3.12/site-packages/pycparser/c_parser.py:1263:            spec["type"] = [c_ast.IdentifierType(["int"], coord=self._token_coord(p, 1))]
./.venv-build/lib/python3.12/site-packages/pycparser/c_parser.py:1284:                coord=self._token_coord(p, 2),
./.venv-build/lib/python3.12/site-packages/pycparser/c_parser.py:1310:            p[0] = c_ast.InitList([], self._token_coord(p, 1))
./.venv-build/lib/python3.12/site-packages/pycparser/c_parser.py:1352:            coord=self._token_coord(p, 2),
./.venv-build/lib/python3.12/site-packages/pycparser/c_parser.py:1392:            coord=self._token_coord(p, 1),
./.venv-build/lib/python3.12/site-packages/pycparser/c_parser.py:1399:            dim=c_ast.ID(p[3], self._token_coord(p, 3)),
./.venv-build/lib/python3.12/site-packages/pycparser/c_parser.py:1410:            dim=c_ast.ID(p[3], self._token_coord(p, 3)),
./.venv-build/lib/python3.12/site-packages/pycparser/c_parser.py:1412:            coord=self._token_coord(p, 1),
./.venv-build/lib/python3.12/site-packages/pycparser/c_parser.py:1426:            coord=self._token_coord(p, 1),
./.venv-build/lib/python3.12/site-packages/pycparser/c_parser.py:1439:            coord=self._token_coord(p, 1),
./.venv-build/lib/python3.12/site-packages/pycparser/c_parser.py:1462:        p[0] = c_ast.Compound(block_items=p[2], coord=self._token_coord(p, 1))
./.venv-build/lib/python3.12/site-packages/pycparser/c_parser.py:1466:        p[0] = c_ast.Label(p[1], p[3], self._token_coord(p, 1))
./.venv-build/lib/python3.12/site-packages/pycparser/c_parser.py:1470:        p[0] = c_ast.Case(p[2], [p[4]], self._token_coord(p, 1))
./.venv-build/lib/python3.12/site-packages/pycparser/c_parser.py:1474:        p[0] = c_ast.Default([p[3]], self._token_coord(p, 1))
./.venv-build/lib/python3.12/site-packages/pycparser/c_parser.py:1479:            p[1], c_ast.EmptyStatement(self._token_coord(p, 1)), self._token_coord(p, 1)
./.venv-build/lib/python3.12/site-packages/pycparser/c_parser.py:1486:            [c_ast.EmptyStatement(self._token_coord(p, 2))],
./.venv-build/lib/python3.12/site-packages/pycparser/c_parser.py:1487:            self._token_coord(p, 1),
./.venv-build/lib/python3.12/site-packages/pycparser/c_parser.py:1493:            [c_ast.EmptyStatement(self._token_coord(p, 1))], self._token_coord(p, 1)
./.venv-build/lib/python3.12/site-packages/pycparser/c_parser.py:1498:        p[0] = c_ast.If(p[3], p[5], None, self._token_coord(p, 1))
./.venv-build/lib/python3.12/site-packages/pycparser/c_parser.py:1502:        p[0] = c_ast.If(p[3], p[5], p[7], self._token_coord(p, 1))
./.venv-build/lib/python3.12/site-packages/pycparser/c_parser.py:1506:        p[0] = fix_switch_cases(c_ast.Switch(p[3], p[5], self._token_coord(p, 1)))
./.venv-build/lib/python3.12/site-packages/pycparser/c_parser.py:1510:        p[0] = c_ast.While(p[3], p[5], self._token_coord(p, 1))
./.venv-build/lib/python3.12/site-packages/pycparser/c_parser.py:1514:        p[0] = c_ast.DoWhile(p[5], p[2], self._token_coord(p, 1))
./.venv-build/lib/python3.12/site-packages/pycparser/c_parser.py:1518:        p[0] = c_ast.For(p[3], p[5], p[7], p[9], self._token_coord(p, 1))
./.venv-build/lib/python3.12/site-packages/pycparser/c_parser.py:1523:            c_ast.DeclList(p[3], self._token_coord(p, 1)),
./.venv-build/lib/python3.12/site-packages/pycparser/c_parser.py:1527:            self._token_coord(p, 1),
./.venv-build/lib/python3.12/site-packages/pycparser/c_parser.py:1532:        p[0] = c_ast.Goto(p[2], self._token_coord(p, 1))
./.venv-build/lib/python3.12/site-packages/pycparser/c_parser.py:1536:        p[0] = c_ast.Break(self._token_coord(p, 1))
./.venv-build/lib/python3.12/site-packages/pycparser/c_parser.py:1540:        p[0] = c_ast.Continue(self._token_coord(p, 1))
./.venv-build/lib/python3.12/site-packages/pycparser/c_parser.py:1546:        p[0] = c_ast.Return(p[2] if len(p) == 4 else None, self._token_coord(p, 1))
./.venv-build/lib/python3.12/site-packages/pycparser/c_parser.py:1551:            p[0] = c_ast.EmptyStatement(self._token_coord(p, 2))
./.venv-build/lib/python3.12/site-packages/pycparser/c_parser.py:1574:        p[0] = c_ast.IdentifierType([p[1]], coord=self._token_coord(p, 1))
./.venv-build/lib/python3.12/site-packages/pycparser/c_parser.py:1650:        p[0] = c_ast.Cast(p[2], p[4], self._token_coord(p, 1))
./.venv-build/lib/python3.12/site-packages/pycparser/c_parser.py:1668:        p[0] = c_ast.UnaryOp(p[1], p[2] if len(p) == 3 else p[3], self._token_coord(p, 1))
./.venv-build/lib/python3.12/site-packages/pycparser/c_parser.py:1700:        field = c_ast.ID(p[3], self._token_coord(p, 3))
./.venv-build/lib/python3.12/site-packages/pycparser/c_parser.py:1735:        coord = self._token_coord(p, 1)
./.venv-build/lib/python3.12/site-packages/pycparser/c_parser.py:1764:        p[0] = c_ast.ID(p[1], self._token_coord(p, 1))
./.venv-build/lib/python3.12/site-packages/pycparser/c_parser.py:1786:        p[0] = c_ast.Constant(prefix + "int", p[1], self._token_coord(p, 1))
./.venv-build/lib/python3.12/site-packages/pycparser/c_parser.py:1799:        p[0] = c_ast.Constant(t, p[1], self._token_coord(p, 1))
./.venv-build/lib/python3.12/site-packages/pycparser/c_parser.py:1808:        p[0] = c_ast.Constant("char", p[1], self._token_coord(p, 1))
./.venv-build/lib/python3.12/site-packages/pycparser/c_parser.py:1820:            p[0] = c_ast.Constant("string", p[1], self._token_coord(p, 1))
./.venv-build/lib/python3.12/site-packages/pycparser/c_parser.py:1836:            p[0] = c_ast.Constant("string", p[1], self._token_coord(p, 1))
./.venv-build/lib/python3.12/site-packages/pycparser/c_parser.py:1857:        # _get_yacc_lookahead_token still works!
./.venv-build/lib/python3.12/site-packages/jeepney/low_level.py:354:    token = sig.pop(0)
./.venv-build/lib/python3.12/site-packages/jeepney/low_level.py:355:    if token == "a":
./.venv-build/lib/python3.12/site-packages/jeepney/low_level.py:357:    if token == "v":
./.venv-build/lib/python3.12/site-packages/jeepney/low_level.py:359:    elif token == "(":
./.venv-build/lib/python3.12/site-packages/jeepney/low_level.py:365:    elif token == "{":
./.venv-build/lib/python3.12/site-packages/jeepney/low_level.py:371:    elif token in ")}":
./.venv-build/lib/python3.12/site-packages/jeepney/low_level.py:374:        return simple_types[token]
./.venv-build/lib/python3.12/site-packages/jeepney/tests/test_bindgen.py:7:sample_file = os.path.join(os.path.dirname(__file__), "secrets_introspect.xml")
./.venv-build/lib/python3.12/site-packages/jeepney/tests/test_bindgen.py:15:        xml, path="/org/freedesktop/secrets", bus_name="org.freedesktop.secrets", fh=sio
./.venv-build/lib/python3.12/site-packages/jeepney/tests/test_bindgen.py:26:    assert Service.interface == "org.freedesktop.Secret.Service"
./.venv-build/lib/python3.12/site-packages/jeepney/tests/test_bindgen.py:29:    assert msg.header.fields[HeaderFields.destination] == "org.freedesktop.secrets"
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/markers.py:15:from ._tokenizer import ParserSyntaxError
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_tokenizer.py:12:class Token:
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_tokenizer.py:90:class Tokenizer:
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_tokenizer.py:91:    """Context-sensitive token parsing.
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_tokenizer.py:93:    Provides methods to examine the input stream to check whether the next token
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_tokenizer.py:107:        self.next_token: Token | None = None
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_tokenizer.py:111:        """Move beyond provided token name, if at current position."""
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_tokenizer.py:116:        """Check whether the next token has the provided name.
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_tokenizer.py:118:        By default, if the check succeeds, the token *must* be read before
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_tokenizer.py:119:        another check. If `peek` is set to `True`, the token is not loaded and
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_tokenizer.py:123:            self.next_token is None
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_tokenizer.py:124:        ), f"Cannot check for {name!r}, already have {self.next_token!r}"
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_tokenizer.py:125:        assert name in self.rules, f"Unknown token name: {name!r}"
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_tokenizer.py:133:            self.next_token = Token(name, match[0], self.position)
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_tokenizer.py:136:    def expect(self, name: str, *, expected: str) -> Token:
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_tokenizer.py:137:        """Expect a certain token name next, failing with a syntax error otherwise.
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_tokenizer.py:139:        The token is *not* read.
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_tokenizer.py:145:    def read(self) -> Token:
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_tokenizer.py:146:        """Consume the next token and return it."""
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_tokenizer.py:147:        token = self.next_token
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_tokenizer.py:148:        assert token is not None
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_tokenizer.py:150:        self.position += len(token.text)
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_tokenizer.py:151:        self.next_token = None
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_tokenizer.py:153:        return token
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_tokenizer.py:174:    def enclosing_tokens(self, open_token: str, close_token: str, *, around: str) -> Iterator[None]:
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_tokenizer.py:175:        if self.check(open_token):
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_tokenizer.py:186:        if not self.check(close_token):
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_tokenizer.py:188:                f"Expected matching {close_token} for {open_token}, after {around}",
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:12:from ._tokenizer import DEFAULT_RULES, Tokenizer
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:62:    return _parse_requirement(Tokenizer(source, rules=DEFAULT_RULES))
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:65:def _parse_requirement(tokenizer: Tokenizer) -> ParsedRequirement:
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:69:    tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:71:    name_token = tokenizer.expect(
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:74:    name = name_token.text
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:75:    tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:77:    extras = _parse_extras(tokenizer)
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:78:    tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:80:    url, specifier, marker = _parse_requirement_details(tokenizer)
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:81:    tokenizer.expect("END", expected="end of dependency specifier")
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:87:    tokenizer: Tokenizer,
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:98:    if tokenizer.check("AT"):
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:99:        tokenizer.read()
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:100:        tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:102:        url_start = tokenizer.position
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:103:        url = tokenizer.expect("URL", expected="URL after @").text
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:104:        if tokenizer.check("END", peek=True):
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:107:        tokenizer.expect("WS", expected="whitespace after URL")
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:110:        if tokenizer.check("END", peek=True):
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:114:            tokenizer, span_start=url_start, after="URL and whitespace"
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:117:        specifier_start = tokenizer.position
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:118:        specifier = _parse_specifier(tokenizer)
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:119:        tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:121:        if tokenizer.check("END", peek=True):
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:125:            tokenizer,
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:133:def _parse_requirement_marker(tokenizer: Tokenizer, *, span_start: int, after: str) -> MarkerList:
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:138:    if not tokenizer.check("SEMICOLON"):
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:139:        tokenizer.raise_syntax_error(
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:143:    tokenizer.read()
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:145:    marker = _parse_marker(tokenizer)
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:146:    tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:151:def _parse_extras(tokenizer: Tokenizer) -> list[str]:
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:155:    if not tokenizer.check("LEFT_BRACKET", peek=True):
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:158:    with tokenizer.enclosing_tokens(
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:163:        tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:164:        extras = _parse_extras_list(tokenizer)
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:165:        tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:170:def _parse_extras_list(tokenizer: Tokenizer) -> list[str]:
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:176:    if not tokenizer.check("IDENTIFIER"):
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:179:    extras.append(tokenizer.read().text)
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:182:        tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:183:        if tokenizer.check("IDENTIFIER", peek=True):
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:184:            tokenizer.raise_syntax_error("Expected comma between extra names")
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:185:        elif not tokenizer.check("COMMA"):
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:188:        tokenizer.read()
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:189:        tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:191:        extra_token = tokenizer.expect("IDENTIFIER", expected="extra name after comma")
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:192:        extras.append(extra_token.text)
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:197:def _parse_specifier(tokenizer: Tokenizer) -> str:
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:202:    with tokenizer.enclosing_tokens(
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:207:        tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:208:        parsed_specifiers = _parse_version_many(tokenizer)
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:209:        tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:214:def _parse_version_many(tokenizer: Tokenizer) -> str:
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:219:    while tokenizer.check("SPECIFIER"):
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:220:        span_start = tokenizer.position
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:221:        parsed_specifiers += tokenizer.read().text
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:222:        if tokenizer.check("VERSION_PREFIX_TRAIL", peek=True):
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:223:            tokenizer.raise_syntax_error(
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:226:                span_end=tokenizer.position + 1,
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:228:        if tokenizer.check("VERSION_LOCAL_LABEL_TRAIL", peek=True):
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:229:            tokenizer.raise_syntax_error(
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:232:                span_end=tokenizer.position,
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:234:        tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:235:        if not tokenizer.check("COMMA"):
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:237:        parsed_specifiers += tokenizer.read().text
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:238:        tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:247:    return _parse_full_marker(Tokenizer(source, rules=DEFAULT_RULES))
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:250:def _parse_full_marker(tokenizer: Tokenizer) -> MarkerList:
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:251:    retval = _parse_marker(tokenizer)
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:252:    tokenizer.expect("END", expected="end of marker expression")
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:256:def _parse_marker(tokenizer: Tokenizer) -> MarkerList:
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:260:    expression = [_parse_marker_atom(tokenizer)]
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:261:    while tokenizer.check("BOOLOP"):
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:262:        token = tokenizer.read()
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:263:        expr_right = _parse_marker_atom(tokenizer)
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:264:        expression.extend((token.text, expr_right))
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:268:def _parse_marker_atom(tokenizer: Tokenizer) -> MarkerAtom:
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:274:    tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:275:    if tokenizer.check("LEFT_PARENTHESIS", peek=True):
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:276:        with tokenizer.enclosing_tokens(
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:281:            tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:282:            marker: MarkerAtom = _parse_marker(tokenizer)
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:283:            tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:285:        marker = _parse_marker_item(tokenizer)
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:286:    tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:290:def _parse_marker_item(tokenizer: Tokenizer) -> MarkerItem:
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:294:    tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:295:    marker_var_left = _parse_marker_var(tokenizer)
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:296:    tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:297:    marker_op = _parse_marker_op(tokenizer)
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:298:    tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:299:    marker_var_right = _parse_marker_var(tokenizer)
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:300:    tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:304:def _parse_marker_var(tokenizer: Tokenizer) -> MarkerVar:
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:308:    if tokenizer.check("VARIABLE"):
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:309:        return process_env_var(tokenizer.read().text.replace(".", "_"))
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:310:    elif tokenizer.check("QUOTED_STRING"):
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:311:        return process_python_str(tokenizer.read().text)
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:313:        tokenizer.raise_syntax_error(message="Expected a marker variable or quoted string")
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:328:def _parse_marker_op(tokenizer: Tokenizer) -> Op:
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:332:    if tokenizer.check("IN"):
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:333:        tokenizer.read()
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:335:    elif tokenizer.check("NOT"):
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:336:        tokenizer.read()
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:337:        tokenizer.expect("WS", expected="whitespace after 'not'")
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:338:        tokenizer.expect("IN", expected="'in' after 'not'")
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:340:    elif tokenizer.check("OP"):
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:341:        return Op(tokenizer.read().text)
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:343:        return tokenizer.raise_syntax_error(
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/licenses/__init__.py:67:    # Pad any parentheses so tokenization can be achieved by merely splitting on
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/licenses/__init__.py:81:    tokens = license_expression.split()
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/licenses/__init__.py:86:    python_tokens = []
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/licenses/__init__.py:87:    for token in tokens:
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/licenses/__init__.py:88:        if token not in {"or", "and", "with", "(", ")"}:
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/licenses/__init__.py:89:            python_tokens.append("False")
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/licenses/__init__.py:90:        elif token == "with":
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/licenses/__init__.py:91:            python_tokens.append("or")
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/licenses/__init__.py:92:        elif token == "(" and python_tokens and python_tokens[-1] not in {"or", "and"}:
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/licenses/__init__.py:96:            python_tokens.append(token)
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/licenses/__init__.py:98:    python_expression = " ".join(python_tokens)
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/licenses/__init__.py:109:    normalized_tokens = []
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/licenses/__init__.py:110:    for token in tokens:
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/licenses/__init__.py:111:        if token in {"or", "and", "with", "(", ")"}:
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/licenses/__init__.py:112:            normalized_tokens.append(token.upper())
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/licenses/__init__.py:115:        if normalized_tokens and normalized_tokens[-1] == "WITH":
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/licenses/__init__.py:116:            if token not in EXCEPTIONS:
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/licenses/__init__.py:117:                message = f"Unknown license exception: {token!r}"
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/licenses/__init__.py:120:            normalized_tokens.append(EXCEPTIONS[token]["id"])
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/licenses/__init__.py:122:            if token.endswith("+"):
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/licenses/__init__.py:123:                final_token = token[:-1]
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/licenses/__init__.py:126:                final_token = token
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/licenses/__init__.py:129:            if final_token.startswith("licenseref-"):
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/licenses/__init__.py:130:                if not license_ref_allowed.match(final_token):
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/licenses/__init__.py:131:                    message = f"Invalid licenseref: {final_token!r}"
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/licenses/__init__.py:133:                normalized_tokens.append(license_refs[final_token] + suffix)
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/licenses/__init__.py:135:                if final_token not in LICENSES:
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/licenses/__init__.py:136:                    message = f"Unknown license: {final_token!r}"
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/licenses/__init__.py:138:                normalized_tokens.append(LICENSES[final_token]["id"] + suffix)
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/licenses/__init__.py:140:    normalized_expression = " ".join(normalized_tokens)
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/packaging/requirements.py:9:from ._tokenizer import ParserSyntaxError
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/markers.py:21:from ._tokenizer import ParserSyntaxError
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_tokenizer.py:10:class Token:
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_tokenizer.py:88:class Tokenizer:
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_tokenizer.py:89:    """Context-sensitive token parsing.
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_tokenizer.py:91:    Provides methods to examine the input stream to check whether the next token
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_tokenizer.py:105:        self.next_token: Optional[Token] = None
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_tokenizer.py:109:        """Move beyond provided token name, if at current position."""
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_tokenizer.py:114:        """Check whether the next token has the provided name.
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_tokenizer.py:116:        By default, if the check succeeds, the token *must* be read before
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_tokenizer.py:117:        another check. If `peek` is set to `True`, the token is not loaded and
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_tokenizer.py:121:            self.next_token is None
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_tokenizer.py:122:        ), f"Cannot check for {name!r}, already have {self.next_token!r}"
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_tokenizer.py:123:        assert name in self.rules, f"Unknown token name: {name!r}"
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_tokenizer.py:131:            self.next_token = Token(name, match[0], self.position)
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_tokenizer.py:134:    def expect(self, name: str, *, expected: str) -> Token:
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_tokenizer.py:135:        """Expect a certain token name next, failing with a syntax error otherwise.
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_tokenizer.py:137:        The token is *not* read.
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_tokenizer.py:143:    def read(self) -> Token:
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_tokenizer.py:144:        """Consume the next token and return it."""
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_tokenizer.py:145:        token = self.next_token
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_tokenizer.py:146:        assert token is not None
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_tokenizer.py:148:        self.position += len(token.text)
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_tokenizer.py:149:        self.next_token = None
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_tokenizer.py:151:        return token
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_tokenizer.py:172:    def enclosing_tokens(self, open_token: str, close_token: str, *, around: str) -> Iterator[None]:
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_tokenizer.py:173:        if self.check(open_token):
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_tokenizer.py:184:        if not self.check(close_token):
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_tokenizer.py:186:                f"Expected matching {close_token} for {open_token}, after {around}",
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:10:from ._tokenizer import DEFAULT_RULES, Tokenizer
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:64:    return _parse_requirement(Tokenizer(source, rules=DEFAULT_RULES))
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:67:def _parse_requirement(tokenizer: Tokenizer) -> ParsedRequirement:
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:71:    tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:73:    name_token = tokenizer.expect(
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:76:    name = name_token.text
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:77:    tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:79:    extras = _parse_extras(tokenizer)
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:80:    tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:82:    url, specifier, marker = _parse_requirement_details(tokenizer)
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:83:    tokenizer.expect("END", expected="end of dependency specifier")
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:89:    tokenizer: Tokenizer,
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:100:    if tokenizer.check("AT"):
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:101:        tokenizer.read()
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:102:        tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:104:        url_start = tokenizer.position
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:105:        url = tokenizer.expect("URL", expected="URL after @").text
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:106:        if tokenizer.check("END", peek=True):
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:109:        tokenizer.expect("WS", expected="whitespace after URL")
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:112:        if tokenizer.check("END", peek=True):
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:116:            tokenizer, span_start=url_start, after="URL and whitespace"
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:119:        specifier_start = tokenizer.position
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:120:        specifier = _parse_specifier(tokenizer)
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:121:        tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:123:        if tokenizer.check("END", peek=True):
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:127:            tokenizer,
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:135:def _parse_requirement_marker(tokenizer: Tokenizer, *, span_start: int, after: str) -> MarkerList:
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:140:    if not tokenizer.check("SEMICOLON"):
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:141:        tokenizer.raise_syntax_error(
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:145:    tokenizer.read()
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:147:    marker = _parse_marker(tokenizer)
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:148:    tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:153:def _parse_extras(tokenizer: Tokenizer) -> List[str]:
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:157:    if not tokenizer.check("LEFT_BRACKET", peek=True):
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:160:    with tokenizer.enclosing_tokens(
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:165:        tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:166:        extras = _parse_extras_list(tokenizer)
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:167:        tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:172:def _parse_extras_list(tokenizer: Tokenizer) -> List[str]:
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:178:    if not tokenizer.check("IDENTIFIER"):
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:181:    extras.append(tokenizer.read().text)
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:184:        tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:185:        if tokenizer.check("IDENTIFIER", peek=True):
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:186:            tokenizer.raise_syntax_error("Expected comma between extra names")
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:187:        elif not tokenizer.check("COMMA"):
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:190:        tokenizer.read()
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:191:        tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:193:        extra_token = tokenizer.expect("IDENTIFIER", expected="extra name after comma")
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:194:        extras.append(extra_token.text)
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:199:def _parse_specifier(tokenizer: Tokenizer) -> str:
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:204:    with tokenizer.enclosing_tokens(
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:209:        tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:210:        parsed_specifiers = _parse_version_many(tokenizer)
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:211:        tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:216:def _parse_version_many(tokenizer: Tokenizer) -> str:
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:221:    while tokenizer.check("SPECIFIER"):
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:222:        span_start = tokenizer.position
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:223:        parsed_specifiers += tokenizer.read().text
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:224:        if tokenizer.check("VERSION_PREFIX_TRAIL", peek=True):
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:225:            tokenizer.raise_syntax_error(
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:228:                span_end=tokenizer.position + 1,
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:230:        if tokenizer.check("VERSION_LOCAL_LABEL_TRAIL", peek=True):
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:231:            tokenizer.raise_syntax_error(
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:234:                span_end=tokenizer.position,
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:236:        tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:237:        if not tokenizer.check("COMMA"):
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:239:        parsed_specifiers += tokenizer.read().text
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:240:        tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:249:    return _parse_full_marker(Tokenizer(source, rules=DEFAULT_RULES))
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:252:def _parse_full_marker(tokenizer: Tokenizer) -> MarkerList:
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:253:    retval = _parse_marker(tokenizer)
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:254:    tokenizer.expect("END", expected="end of marker expression")
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:258:def _parse_marker(tokenizer: Tokenizer) -> MarkerList:
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:262:    expression = [_parse_marker_atom(tokenizer)]
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:263:    while tokenizer.check("BOOLOP"):
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:264:        token = tokenizer.read()
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:265:        expr_right = _parse_marker_atom(tokenizer)
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:266:        expression.extend((token.text, expr_right))
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:270:def _parse_marker_atom(tokenizer: Tokenizer) -> MarkerAtom:
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:276:    tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:277:    if tokenizer.check("LEFT_PARENTHESIS", peek=True):
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:278:        with tokenizer.enclosing_tokens(
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:283:            tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:284:            marker: MarkerAtom = _parse_marker(tokenizer)
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:285:            tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:287:        marker = _parse_marker_item(tokenizer)
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:288:    tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:292:def _parse_marker_item(tokenizer: Tokenizer) -> MarkerItem:
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:296:    tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:297:    marker_var_left = _parse_marker_var(tokenizer)
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:298:    tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:299:    marker_op = _parse_marker_op(tokenizer)
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:300:    tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:301:    marker_var_right = _parse_marker_var(tokenizer)
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:302:    tokenizer.consume("WS")
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:306:def _parse_marker_var(tokenizer: Tokenizer) -> MarkerVar:
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:310:    if tokenizer.check("VARIABLE"):
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:311:        return process_env_var(tokenizer.read().text.replace(".", "_"))
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:312:    elif tokenizer.check("QUOTED_STRING"):
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:313:        return process_python_str(tokenizer.read().text)
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:315:        tokenizer.raise_syntax_error(message="Expected a marker variable or quoted string")
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:330:def _parse_marker_op(tokenizer: Tokenizer) -> Op:
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:334:    if tokenizer.check("IN"):
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:335:        tokenizer.read()
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:337:    elif tokenizer.check("NOT"):
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:338:        tokenizer.read()
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:339:        tokenizer.expect("WS", expected="whitespace after 'not'")
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:340:        tokenizer.expect("IN", expected="'in' after 'not'")
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:342:    elif tokenizer.check("OP"):
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:343:        return Op(tokenizer.read().text)
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:345:        return tokenizer.raise_syntax_error(
./.venv-build/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/requirements.py:8:from ._tokenizer import ParserSyntaxError
./.venv-build/lib/python3.12/site-packages/setuptools/tests/config/test_expand.py:47:    secrets = Path(str(dir_) + "secrets")
./.venv-build/lib/python3.12/site-packages/setuptools/tests/config/test_expand.py:48:    secrets.mkdir(exist_ok=True)
./.venv-build/lib/python3.12/site-packages/setuptools/tests/config/test_expand.py:49:    write_files({"secrets.txt": "secret keys"}, secrets)
./.venv-build/lib/python3.12/site-packages/setuptools/tests/config/test_expand.py:59:        cannot_access_secrets_msg = r"Cannot access '.*secrets\.txt'"
./.venv-build/lib/python3.12/site-packages/setuptools/tests/config/test_expand.py:60:        with pytest.raises(DistutilsOptionError, match=cannot_access_secrets_msg):
./.venv-build/lib/python3.12/site-packages/setuptools/tests/config/test_expand.py:61:            expand.read_files(["../dir_secrets/secrets.txt"])
./.venv-build/lib/python3.12/site-packages/setuptools/build_meta.py:38:import tokenize
./.venv-build/lib/python3.12/site-packages/setuptools/build_meta.py:135:    return tokenize.open(setup_script)
./.venv-build/lib/python3.12/site-packages/setuptools/_imp.py:9:import tokenize
./.venv-build/lib/python3.12/site-packages/setuptools/_imp.py:62:            file = tokenize.open(path)
./.venv-build/lib/python3.12/site-packages/setuptools/launch.py:10:import tokenize
./.venv-build/lib/python3.12/site-packages/setuptools/launch.py:27:    open_ = getattr(tokenize, "open", open)
./.venv-build/lib/python3.12/site-packages/setuptools/_distutils/core.py:13:import tokenize
./.venv-build/lib/python3.12/site-packages/setuptools/_distutils/core.py:265:            # tokenize.open supports automatic encoding detection
./.venv-build/lib/python3.12/site-packages/setuptools/_distutils/core.py:266:            with tokenize.open(script_name) as f:
./.venv-build/lib/python3.12/site-packages/setuptools/_distutils/util.py:177:            # password database, do nothing
./.venv-build/lib/python3.12/site-packages/setuptools/_distutils/dist.py:235:        self.password = ""
./.venv-build/lib/python3.12/site-packages/setuptools/_distutils/command/build_scripts.py:7:import tokenize
./.venv-build/lib/python3.12/site-packages/setuptools/_distutils/command/build_scripts.py:91:            f = tokenize.open(script)
./.venv-build/lib/python3.12/site-packages/more_itertools/more.py:858:    Suppose Alice, Bob, Carol, and Dave are playing Secret Santa.
./.venv-build/lib/python3.12/site-packages/more_itertools/more.py:883:    Consider the Secret Santa example with two *different* people who have
./.venv-build/lib/python3.12/site-packages/secretstorage/collection.py:1:# SecretStorage module for Python
./.venv-build/lib/python3.12/site-packages/secretstorage/collection.py:2:# Access passwords using the SecretService DBus API
./.venv-build/lib/python3.12/site-packages/secretstorage/collection.py:6:"""Collection is a place where secret items are stored. Normally, only
./.venv-build/lib/python3.12/site-packages/secretstorage/collection.py:23:from secretstorage.defines import SS_PATH, SS_PREFIX
./.venv-build/lib/python3.12/site-packages/secretstorage/collection.py:24:from secretstorage.dhcrypto import Session
./.venv-build/lib/python3.12/site-packages/secretstorage/collection.py:25:from secretstorage.exceptions import (
./.venv-build/lib/python3.12/site-packages/secretstorage/collection.py:30:from secretstorage.item import Item
./.venv-build/lib/python3.12/site-packages/secretstorage/collection.py:31:from secretstorage.util import (
./.venv-build/lib/python3.12/site-packages/secretstorage/collection.py:34:    format_secret,
./.venv-build/lib/python3.12/site-packages/secretstorage/collection.py:41:DEFAULT_COLLECTION = "/org/freedesktop/secrets/aliases/default"
./.venv-build/lib/python3.12/site-packages/secretstorage/collection.py:42:SESSION_COLLECTION = "/org/freedesktop/secrets/collection/session"
./.venv-build/lib/python3.12/site-packages/secretstorage/collection.py:67:        :exc:`~secretstorage.exceptions.LockedException`."""
./.venv-build/lib/python3.12/site-packages/secretstorage/collection.py:124:        secret: bytes,
./.venv-build/lib/python3.12/site-packages/secretstorage/collection.py:128:        """Creates a new :class:`~secretstorage.item.Item` with given
./.venv-build/lib/python3.12/site-packages/secretstorage/collection.py:129:        `label` (unicode string), `attributes` (dictionary) and `secret`
./.venv-build/lib/python3.12/site-packages/secretstorage/collection.py:132:        sets the content type of the secret (``text/plain`` by default).
./.venv-build/lib/python3.12/site-packages/secretstorage/collection.py:137:        _secret = format_secret(self.session, secret, content_type)
./.venv-build/lib/python3.12/site-packages/secretstorage/collection.py:143:            "CreateItem", "a{sv}(oayays)b", properties, _secret, replace
./.venv-build/lib/python3.12/site-packages/secretstorage/collection.py:167:    :raises: :exc:`~secretstorage.exceptions.PromptDismissedException`
./.venv-build/lib/python3.12/site-packages/secretstorage/collection.py:229:    :exc:`~secretstorage.exceptions.ItemNotFoundException`."""
./.venv-build/lib/python3.12/site-packages/secretstorage/exceptions.py:1:# SecretStorage module for Python
./.venv-build/lib/python3.12/site-packages/secretstorage/exceptions.py:2:# Access passwords using the SecretService DBus API
./.venv-build/lib/python3.12/site-packages/secretstorage/exceptions.py:6:"""All secretstorage functions may raise various exceptions when
./.venv-build/lib/python3.12/site-packages/secretstorage/exceptions.py:8::exc:`SecretStorageException` class."""
./.venv-build/lib/python3.12/site-packages/secretstorage/exceptions.py:11:class SecretStorageException(Exception):
./.venv-build/lib/python3.12/site-packages/secretstorage/exceptions.py:15:class SecretServiceNotAvailableException(SecretStorageException):
./.venv-build/lib/python3.12/site-packages/secretstorage/exceptions.py:16:    """Raised by :class:`~secretstorage.item.Item` or
./.venv-build/lib/python3.12/site-packages/secretstorage/exceptions.py:17:    :class:`~secretstorage.collection.Collection` constructors, or by
./.venv-build/lib/python3.12/site-packages/secretstorage/exceptions.py:18:    other functions in the :mod:`secretstorage.collection` module, when
./.venv-build/lib/python3.12/site-packages/secretstorage/exceptions.py:19:    the Secret Service API is not available."""
./.venv-build/lib/python3.12/site-packages/secretstorage/exceptions.py:22:class LockedException(SecretStorageException):
./.venv-build/lib/python3.12/site-packages/secretstorage/exceptions.py:24:    is locked. Use :meth:`~secretstorage.collection.Collection.is_locked`
./.venv-build/lib/python3.12/site-packages/secretstorage/exceptions.py:26:    :meth:`~secretstorage.collection.Collection.unlock` to unlock it.
./.venv-build/lib/python3.12/site-packages/secretstorage/exceptions.py:30:class ItemNotFoundException(SecretStorageException):
./.venv-build/lib/python3.12/site-packages/secretstorage/exceptions.py:34:    >>> import secretstorage
./.venv-build/lib/python3.12/site-packages/secretstorage/exceptions.py:35:    >>> connection = secretstorage.dbus_init()
./.venv-build/lib/python3.12/site-packages/secretstorage/exceptions.py:38:    ...     item = secretstorage.Item(connection, item_path)
./.venv-build/lib/python3.12/site-packages/secretstorage/exceptions.py:39:    ... except secretstorage.ItemNotFoundException:
./.venv-build/lib/python3.12/site-packages/secretstorage/defines.py:1:# SecretStorage module for Python
./.venv-build/lib/python3.12/site-packages/secretstorage/defines.py:2:# Access passwords using the SecretService DBus API
./.venv-build/lib/python3.12/site-packages/secretstorage/defines.py:8:SS_PREFIX = "org.freedesktop.Secret."
./.venv-build/lib/python3.12/site-packages/secretstorage/defines.py:9:SS_PATH = "/org/freedesktop/secrets"
./.venv-build/lib/python3.12/site-packages/secretstorage/defines.py:17:DBUS_NO_SUCH_OBJECT = "org.freedesktop.Secret.Error.NoSuchObject"
./.venv-build/lib/python3.12/site-packages/secretstorage/util.py:1:# SecretStorage module for Python
./.venv-build/lib/python3.12/site-packages/secretstorage/util.py:2:# Access passwords using the SecretService DBus API
./.venv-build/lib/python3.12/site-packages/secretstorage/util.py:25:from secretstorage.defines import (
./.venv-build/lib/python3.12/site-packages/secretstorage/util.py:38:from secretstorage.dhcrypto import Session, int_to_bytes
./.venv-build/lib/python3.12/site-packages/secretstorage/util.py:39:from secretstorage.exceptions import (
./.venv-build/lib/python3.12/site-packages/secretstorage/util.py:41:    SecretServiceNotAvailableException,
./.venv-build/lib/python3.12/site-packages/secretstorage/util.py:44:BUS_NAME = "org.freedesktop.secrets"
./.venv-build/lib/python3.12/site-packages/secretstorage/util.py:52:    properties, and converts error responses to SecretStorage
./.venv-build/lib/python3.12/site-packages/secretstorage/util.py:79:                raise SecretServiceNotAvailableException(data) from resp
./.venv-build/lib/python3.12/site-packages/secretstorage/util.py:97:    """Returns a new Secret Service session."""
./.venv-build/lib/python3.12/site-packages/secretstorage/util.py:121:def format_secret(
./.venv-build/lib/python3.12/site-packages/secretstorage/util.py:122:    session: Session, secret: bytes, content_type: str
./.venv-build/lib/python3.12/site-packages/secretstorage/util.py:124:    """Formats `secret` to make possible to pass it to the
./.venv-build/lib/python3.12/site-packages/secretstorage/util.py:125:    Secret Service API."""
./.venv-build/lib/python3.12/site-packages/secretstorage/util.py:126:    if isinstance(secret, str):
./.venv-build/lib/python3.12/site-packages/secretstorage/util.py:127:        secret = secret.encode("utf-8")
./.venv-build/lib/python3.12/site-packages/secretstorage/util.py:128:    elif not isinstance(secret, bytes):
./.venv-build/lib/python3.12/site-packages/secretstorage/util.py:129:        raise TypeError("secret must be bytes")
./.venv-build/lib/python3.12/site-packages/secretstorage/util.py:132:        return (session.object_path, b"", secret, content_type)
./.venv-build/lib/python3.12/site-packages/secretstorage/util.py:135:    padding = 0x10 - (len(secret) & 0xF)
./.venv-build/lib/python3.12/site-packages/secretstorage/util.py:136:    secret += bytes((padding,) * padding)
./.venv-build/lib/python3.12/site-packages/secretstorage/util.py:140:    encrypted_secret = encryptor.update(secret) + encryptor.finalize()
./.venv-build/lib/python3.12/site-packages/secretstorage/util.py:141:    return (session.object_path, aes_iv, encrypted_secret, content_type)
./.venv-build/lib/python3.12/site-packages/secretstorage/__init__.py:1:# SecretStorage module for Python
./.venv-build/lib/python3.12/site-packages/secretstorage/__init__.py:2:# Access passwords using the SecretService DBus API
./.venv-build/lib/python3.12/site-packages/secretstorage/__init__.py:6:"""This file provides quick access to all SecretStorage API. Please
./.venv-build/lib/python3.12/site-packages/secretstorage/__init__.py:13:from secretstorage.collection import (
./.venv-build/lib/python3.12/site-packages/secretstorage/__init__.py:22:from secretstorage.exceptions import (
./.venv-build/lib/python3.12/site-packages/secretstorage/__init__.py:26:    SecretServiceNotAvailableException,
./.venv-build/lib/python3.12/site-packages/secretstorage/__init__.py:27:    SecretStorageException,
./.venv-build/lib/python3.12/site-packages/secretstorage/__init__.py:29:from secretstorage.item import Item
./.venv-build/lib/python3.12/site-packages/secretstorage/__init__.py:30:from secretstorage.util import add_match_rules
./.venv-build/lib/python3.12/site-packages/secretstorage/__init__.py:41:    "SecretServiceNotAvailableException",
./.venv-build/lib/python3.12/site-packages/secretstorage/__init__.py:42:    "SecretStorageException",
./.venv-build/lib/python3.12/site-packages/secretstorage/__init__.py:57:    then be passed to various SecretStorage functions, such as
./.venv-build/lib/python3.12/site-packages/secretstorage/__init__.py:58:    :func:`~secretstorage.collection.get_default_collection`.
./.venv-build/lib/python3.12/site-packages/secretstorage/__init__.py:69:              collection = secretstorage.get_default_collection(conn)
./.venv-build/lib/python3.12/site-packages/secretstorage/__init__.py:89:        raise SecretServiceNotAvailableException(reason) from ex
./.venv-build/lib/python3.12/site-packages/secretstorage/__init__.py:91:        raise SecretServiceNotAvailableException(str(ex)) from ex
./.venv-build/lib/python3.12/site-packages/secretstorage/__init__.py:95:    """Returns True if the Secret Service daemon is either running or
./.venv-build/lib/python3.12/site-packages/secretstorage/__init__.py:100:    from secretstorage.util import BUS_NAME
./.venv-build/lib/python3.12/site-packages/secretstorage/item.py:1:# SecretStorage module for Python
./.venv-build/lib/python3.12/site-packages/secretstorage/item.py:2:# Access passwords using the SecretService DBus API
./.venv-build/lib/python3.12/site-packages/secretstorage/item.py:6:"""SecretStorage item contains a *secret*, some *attributes* and a
./.venv-build/lib/python3.12/site-packages/secretstorage/item.py:8:secret is possible only when the :doc:`collection <collection>` storing
./.venv-build/lib/python3.12/site-packages/secretstorage/item.py:10::meth:`~secretstorage.collection.Collection.unlock` method."""
./.venv-build/lib/python3.12/site-packages/secretstorage/item.py:16:from secretstorage.defines import SS_PREFIX
./.venv-build/lib/python3.12/site-packages/secretstorage/item.py:17:from secretstorage.dhcrypto import Session
./.venv-build/lib/python3.12/site-packages/secretstorage/item.py:18:from secretstorage.exceptions import LockedException, PromptDismissedException
./.venv-build/lib/python3.12/site-packages/secretstorage/item.py:19:from secretstorage.util import (
./.venv-build/lib/python3.12/site-packages/secretstorage/item.py:22:    format_secret,
./.venv-build/lib/python3.12/site-packages/secretstorage/item.py:31:    """Represents a secret item."""
./.venv-build/lib/python3.12/site-packages/secretstorage/item.py:53:        :exc:`~secretstorage.exceptions.LockedException`."""
./.venv-build/lib/python3.12/site-packages/secretstorage/item.py:101:    def get_secret(self) -> bytes:
./.venv-build/lib/python3.12/site-packages/secretstorage/item.py:102:        """Returns item secret (bytestring)."""
./.venv-build/lib/python3.12/site-packages/secretstorage/item.py:106:        (secret,) = self._item.call("GetSecret", "o", self.session.object_path)
./.venv-build/lib/python3.12/site-packages/secretstorage/item.py:108:            return bytes(secret[2])
./.venv-build/lib/python3.12/site-packages/secretstorage/item.py:111:        aes_iv = bytes(secret[1])
./.venv-build/lib/python3.12/site-packages/secretstorage/item.py:113:        encrypted_secret = secret[2]
./.venv-build/lib/python3.12/site-packages/secretstorage/item.py:114:        padded_secret = decryptor.update(bytes(encrypted_secret)) + decryptor.finalize()
./.venv-build/lib/python3.12/site-packages/secretstorage/item.py:115:        assert isinstance(padded_secret, bytes)
./.venv-build/lib/python3.12/site-packages/secretstorage/item.py:116:        return padded_secret[: -padded_secret[-1]]
./.venv-build/lib/python3.12/site-packages/secretstorage/item.py:118:    def get_secret_content_type(self) -> str:
./.venv-build/lib/python3.12/site-packages/secretstorage/item.py:119:        """Returns content type of item secret (string)."""
./.venv-build/lib/python3.12/site-packages/secretstorage/item.py:123:        (secret,) = self._item.call("GetSecret", "o", self.session.object_path)
./.venv-build/lib/python3.12/site-packages/secretstorage/item.py:124:        return str(secret[3])
./.venv-build/lib/python3.12/site-packages/secretstorage/item.py:126:    def set_secret(self, secret: bytes, content_type: str = "text/plain") -> None:
./.venv-build/lib/python3.12/site-packages/secretstorage/item.py:127:        """Sets item secret to `secret`. If `content_type` is given,
./.venv-build/lib/python3.12/site-packages/secretstorage/item.py:128:        also sets the content type of the secret (``text/plain`` by
./.venv-build/lib/python3.12/site-packages/secretstorage/item.py:133:        _secret = format_secret(self.session, secret, content_type)
./.venv-build/lib/python3.12/site-packages/secretstorage/item.py:134:        self._item.call("SetSecret", "(oayays)", _secret)
./.venv-build/lib/python3.12/site-packages/secretstorage/dhcrypto.py:1:# SecretStorage module for Python
./.venv-build/lib/python3.12/site-packages/secretstorage/dhcrypto.py:2:# Access passwords using the SecretService DBus API
./.venv-build/lib/python3.12/site-packages/secretstorage/dhcrypto.py:7:to implement dh-ietf1024-sha256-aes128-cbc-pkcs7 secret encryption
./.venv-build/lib/python3.12/site-packages/secretstorage/dhcrypto.py:161:        self.my_private_key = int.from_bytes(os.urandom(0x80), "big")
./.venv-build/lib/python3.12/site-packages/secretstorage/dhcrypto.py:162:        self.my_public_key = pow(2, self.my_private_key, DH_PRIME_1024)
./.venv-build/lib/python3.12/site-packages/secretstorage/dhcrypto.py:165:        common_secret_int = pow(server_public_key, self.my_private_key, DH_PRIME_1024)
./.venv-build/lib/python3.12/site-packages/secretstorage/dhcrypto.py:166:        common_secret = int_to_bytes(common_secret_int)
./.venv-build/lib/python3.12/site-packages/secretstorage/dhcrypto.py:168:        common_secret = b"\x00" * (0x80 - len(common_secret)) + common_secret
./.venv-build/lib/python3.12/site-packages/secretstorage/dhcrypto.py:171:        pseudo_random_key = hmac.new(salt, common_secret, sha256).digest()
./.venv-build/lib/python3.12/site-packages/id/__main__.py:52:        help="decode the OIDC token into JSON",
./.venv-build/lib/python3.12/site-packages/id/__main__.py:76:    from . import decode_oidc_token, detect_credential
./.venv-build/lib/python3.12/site-packages/id/__main__.py:78:    token = detect_credential(args.audience)
./.venv-build/lib/python3.12/site-packages/id/__main__.py:79:    if token and args.decode:
./.venv-build/lib/python3.12/site-packages/id/__main__.py:80:        header, payload, signature = decode_oidc_token(token)
./.venv-build/lib/python3.12/site-packages/id/__main__.py:84:        print(token)
./.venv-build/lib/python3.12/site-packages/id/_internal/oidc/ambient.py:35:_GCP_TOKEN_REQUEST_URL = "http://metadata/computeMetadata/v1/instance/service-accounts/default/token"  # noqa # nosec B105
./.venv-build/lib/python3.12/site-packages/id/_internal/oidc/ambient.py:39:_GCP_GENERATEIDTOKEN_REQUEST_URL = (
./.venv-build/lib/python3.12/site-packages/id/_internal/oidc/ambient.py:40:    "https://iamcredentials.googleapis.com/v1/projects/-/serviceAccounts/{}:generateIdToken"  # noqa
./.venv-build/lib/python3.12/site-packages/id/_internal/oidc/ambient.py:62:    # to a special URL with a special bearer token. Both are stored in
./.venv-build/lib/python3.12/site-packages/id/_internal/oidc/ambient.py:64:    req_token = os.getenv("ACTIONS_ID_TOKEN_REQUEST_TOKEN")
./.venv-build/lib/python3.12/site-packages/id/_internal/oidc/ambient.py:65:    if not req_token:
./.venv-build/lib/python3.12/site-packages/id/_internal/oidc/ambient.py:67:            "GitHub: missing or insufficient OIDC token permissions, the "
./.venv-build/lib/python3.12/site-packages/id/_internal/oidc/ambient.py:68:            "ACTIONS_ID_TOKEN_REQUEST_TOKEN environment variable was unset"
./.venv-build/lib/python3.12/site-packages/id/_internal/oidc/ambient.py:70:    req_url = os.getenv("ACTIONS_ID_TOKEN_REQUEST_URL")
./.venv-build/lib/python3.12/site-packages/id/_internal/oidc/ambient.py:73:            "GitHub: missing or insufficient OIDC token permissions, the "
./.venv-build/lib/python3.12/site-packages/id/_internal/oidc/ambient.py:74:            "ACTIONS_ID_TOKEN_REQUEST_URL environment variable was unset"
./.venv-build/lib/python3.12/site-packages/id/_internal/oidc/ambient.py:77:    logger.debug("GitHub: requesting OIDC token")
./.venv-build/lib/python3.12/site-packages/id/_internal/oidc/ambient.py:81:        headers={"Authorization": f"bearer {req_token}"},
./.venv-build/lib/python3.12/site-packages/id/_internal/oidc/ambient.py:88:            f"GitHub: OIDC token request failed (code={resp.status_code}, "
./.venv-build/lib/python3.12/site-packages/id/_internal/oidc/ambient.py:92:        raise AmbientCredentialError("GitHub: OIDC token request timed out")
./.venv-build/lib/python3.12/site-packages/id/_internal/oidc/ambient.py:99:            raise ValueError("OIDC token is not a string")
./.venv-build/lib/python3.12/site-packages/id/_internal/oidc/ambient.py:103:    logger.debug("GitHub: successfully requested OIDC token")
./.venv-build/lib/python3.12/site-packages/id/_internal/oidc/ambient.py:122:        logger.debug("GCP: requesting access token")
./.venv-build/lib/python3.12/site-packages/id/_internal/oidc/ambient.py:124:            _GCP_TOKEN_REQUEST_URL,
./.venv-build/lib/python3.12/site-packages/id/_internal/oidc/ambient.py:133:                f"GCP: access token request failed (code={resp.status_code}, "
./.venv-build/lib/python3.12/site-packages/id/_internal/oidc/ambient.py:137:            raise AmbientCredentialError("GCP: access token request timed out")
./.venv-build/lib/python3.12/site-packages/id/_internal/oidc/ambient.py:139:        access_token = resp.json().get("access_token")
./.venv-build/lib/python3.12/site-packages/id/_internal/oidc/ambient.py:141:        if not access_token:
./.venv-build/lib/python3.12/site-packages/id/_internal/oidc/ambient.py:142:            raise AmbientCredentialError("GCP: access token missing from response")
./.venv-build/lib/python3.12/site-packages/id/_internal/oidc/ambient.py:145:            _GCP_GENERATEIDTOKEN_REQUEST_URL.format(service_account_name),
./.venv-build/lib/python3.12/site-packages/id/_internal/oidc/ambient.py:148:                "Authorization": f"Bearer {access_token}",
./.venv-build/lib/python3.12/site-packages/id/_internal/oidc/ambient.py:153:        logger.debug("GCP: requesting OIDC token")
./.venv-build/lib/python3.12/site-packages/id/_internal/oidc/ambient.py:158:                f"GCP: OIDC token request failed (code={resp.status_code}, "
./.venv-build/lib/python3.12/site-packages/id/_internal/oidc/ambient.py:162:            raise AmbientCredentialError("GCP: OIDC token request timed out")
./.venv-build/lib/python3.12/site-packages/id/_internal/oidc/ambient.py:164:        oidc_token: str = resp.json().get("token")
./.venv-build/lib/python3.12/site-packages/id/_internal/oidc/ambient.py:166:        if not oidc_token:
./.venv-build/lib/python3.12/site-packages/id/_internal/oidc/ambient.py:167:            raise AmbientCredentialError("GCP: OIDC token missing from response")
./.venv-build/lib/python3.12/site-packages/id/_internal/oidc/ambient.py:169:        logger.debug("GCP: successfully requested OIDC token")
./.venv-build/lib/python3.12/site-packages/id/_internal/oidc/ambient.py:170:        return oidc_token
./.venv-build/lib/python3.12/site-packages/id/_internal/oidc/ambient.py:186:        logger.debug("GCP: requesting OIDC token")
./.venv-build/lib/python3.12/site-packages/id/_internal/oidc/ambient.py:198:                f"GCP: OIDC token request failed (code={resp.status_code}, "
./.venv-build/lib/python3.12/site-packages/id/_internal/oidc/ambient.py:202:            raise AmbientCredentialError("GCP: OIDC token request timed out")
./.venv-build/lib/python3.12/site-packages/id/_internal/oidc/ambient.py:204:        logger.debug("GCP: successfully requested OIDC token")
./.venv-build/lib/python3.12/site-packages/id/_internal/oidc/ambient.py:215:    the agent encounters an error when generating an OIDC token.
./.venv-build/lib/python3.12/site-packages/id/_internal/oidc/ambient.py:229:    # Now query the agent for a token.
./.venv-build/lib/python3.12/site-packages/id/_internal/oidc/ambient.py:246:        ["buildkite-agent", "oidc", "request-token", "--audience", audience],
./.venv-build/lib/python3.12/site-packages/id/_internal/oidc/ambient.py:264:    `<AUD>_ID_TOKEN`  where `<AUD>` is the uppercased audience argument where all
./.venv-build/lib/python3.12/site-packages/id/_internal/oidc/ambient.py:268:    As an example, audience "sigstore" would require variable SIGSTORE_ID_TOKEN,
./.venv-build/lib/python3.12/site-packages/id/_internal/oidc/ambient.py:270:    HTTP___TEST_AUDIENCE_ID_TOKEN.
./.venv-build/lib/python3.12/site-packages/id/_internal/oidc/ambient.py:274:    Raises if the environment is GitLab, but the `<AUD>_ID_TOKEN` environment
./.venv-build/lib/python3.12/site-packages/id/_internal/oidc/ambient.py:285:    var_name = f"{sanitized_audience}_ID_TOKEN"
./.venv-build/lib/python3.12/site-packages/id/_internal/oidc/ambient.py:286:    token = os.getenv(var_name)
./.venv-build/lib/python3.12/site-packages/id/_internal/oidc/ambient.py:287:    if not token:
./.venv-build/lib/python3.12/site-packages/id/_internal/oidc/ambient.py:290:    logger.debug(f"GitLab: Found token in environment variable {var_name}")
./.venv-build/lib/python3.12/site-packages/id/_internal/oidc/ambient.py:291:    return token
./.venv-build/lib/python3.12/site-packages/id/__init__.py:16:API for retrieving OIDC tokens.
./.venv-build/lib/python3.12/site-packages/id/__init__.py:29:    Raised on any OIDC token format or claim error.
./.venv-build/lib/python3.12/site-packages/id/__init__.py:47:    to retrieve an OIDC token.
./.venv-build/lib/python3.12/site-packages/id/__init__.py:83:def decode_oidc_token(token: str) -> tuple[str, str, str]:
./.venv-build/lib/python3.12/site-packages/id/__init__.py:84:    # Split the token into its three parts: header, payload, and signature
./.venv-build/lib/python3.12/site-packages/id/__init__.py:85:    header, payload, signature = token.split(".")
./.venv-build/lib/python3.12/site-packages/pygments/formatters/rtf.py:22:    Format tokens as RTF markup. This formatter automatically outputs full RTF
./.venv-build/lib/python3.12/site-packages/pygments/formatters/rtf.py:193:    def _split_tokens_on_newlines(self, tokensource):
./.venv-build/lib/python3.12/site-packages/pygments/formatters/rtf.py:195:        Split tokens containing newline characters into multiple token
./.venv-build/lib/python3.12/site-packages/pygments/formatters/rtf.py:199:        for ttype, value in tokensource:
./.venv-build/lib/python3.12/site-packages/pygments/formatters/rtf.py:280:    def format_unencoded(self, tokensource, outfile):
./.venv-build/lib/python3.12/site-packages/pygments/formatters/rtf.py:284:        tokensource = self._split_tokens_on_newlines(tokensource)
./.venv-build/lib/python3.12/site-packages/pygments/formatters/rtf.py:286:        # first pass of tokens to count lines, needed for line numbering
./.venv-build/lib/python3.12/site-packages/pygments/formatters/rtf.py:289:            tokens = []  # for copying the token source generator
./.venv-build/lib/python3.12/site-packages/pygments/formatters/rtf.py:290:            for ttype, value in tokensource:
./.venv-build/lib/python3.12/site-packages/pygments/formatters/rtf.py:291:                tokens.append((ttype, value))
./.venv-build/lib/python3.12/site-packages/pygments/formatters/rtf.py:298:            tokensource = tokens
./.venv-build/lib/python3.12/site-packages/pygments/formatters/rtf.py:303:        for ttype, value in tokensource:
./.venv-build/lib/python3.12/site-packages/pygments/formatters/rtf.py:315:            while not self.style.styles_token(ttype) and ttype.parent:
./.venv-build/lib/python3.12/site-packages/pygments/formatters/rtf.py:317:            style = self.style.style_for_token(ttype)
./.venv-build/lib/python3.12/site-packages/pygments/formatters/img.py:513:        Get the correct color for the token from the style.
./.venv-build/lib/python3.12/site-packages/pygments/formatters/img.py:523:        Get the correct background color for the token from the style.
./.venv-build/lib/python3.12/site-packages/pygments/formatters/img.py:564:    def _create_drawables(self, tokensource):
./.venv-build/lib/python3.12/site-packages/pygments/formatters/img.py:566:        Create drawables for the token content.
./.venv-build/lib/python3.12/site-packages/pygments/formatters/img.py:570:        for ttype, value in tokensource:
./.venv-build/lib/python3.12/site-packages/pygments/formatters/img.py:631:    def format(self, tokensource, outfile):
./.venv-build/lib/python3.12/site-packages/pygments/formatters/img.py:633:        Format ``tokensource``, an iterable of ``(tokentype, tokenstring)``
./.venv-build/lib/python3.12/site-packages/pygments/formatters/img.py:636:        This implementation calculates where it should draw each token on the
./.venv-build/lib/python3.12/site-packages/pygments/formatters/img.py:639:        self._create_drawables(tokensource)
./.venv-build/lib/python3.12/site-packages/pygments/formatters/groff.py:20:    Format tokens with groff escapes to change their color and font style.
./.venv-build/lib/python3.12/site-packages/pygments/formatters/groff.py:131:    def format_unencoded(self, tokensource, outfile):
./.venv-build/lib/python3.12/site-packages/pygments/formatters/groff.py:139:        for ttype, value in tokensource:
./.venv-build/lib/python3.12/site-packages/pygments/formatters/_mapping.py:10:        "Format tokens with BBcodes. These formatting codes are used by many bulletin boards, so you can highlight your sourcecode with pygments before posting it there.",
./.venv-build/lib/python3.12/site-packages/pygments/formatters/_mapping.py:31:        "Format tokens with groff escapes to change their color and font style.",
./.venv-build/lib/python3.12/site-packages/pygments/formatters/_mapping.py:38:        "Format tokens as HTML 4 ``<span>`` tags. By default, the content is enclosed in a ``<pre>`` tag, itself wrapped in a ``<div>`` tag (but see the `nowrap` option). The ``<div>``'s CSS class can be set by the `cssclass` option.",
./.venv-build/lib/python3.12/site-packages/pygments/formatters/_mapping.py:45:        "Format tokens with IRC color sequences",
./.venv-build/lib/python3.12/site-packages/pygments/formatters/_mapping.py:66:        "Format tokens as LaTeX code. This needs the `fancyvrb` and `color` standard packages.",
./.venv-build/lib/python3.12/site-packages/pygments/formatters/_mapping.py:80:        "Format tokens as Pango Markup code. It can then be rendered to an SVG.",
./.venv-build/lib/python3.12/site-packages/pygments/formatters/_mapping.py:82:    "RawTokenFormatter": (
./.venv-build/lib/python3.12/site-packages/pygments/formatters/_mapping.py:84:        "Raw tokens",
./.venv-build/lib/python3.12/site-packages/pygments/formatters/_mapping.py:85:        ("raw", "tokens"),
./.venv-build/lib/python3.12/site-packages/pygments/formatters/_mapping.py:87:        "Format tokens as a raw representation for storing token streams.",
./.venv-build/lib/python3.12/site-packages/pygments/formatters/_mapping.py:94:        "Format tokens as RTF markup. This formatter automatically outputs full RTF documents with color information and other useful stuff. Perfect for Copy and Paste into Microsoft(R) Word(R) documents.",
./.venv-build/lib/python3.12/site-packages/pygments/formatters/_mapping.py:101:        "Format tokens as an SVG graphics file.  This formatter is still experimental. Each line of code is a ``<text>`` element with explicit ``x`` and ``y`` coordinates containing ``<tspan>`` elements with the individual token styles.",
./.venv-build/lib/python3.12/site-packages/pygments/formatters/_mapping.py:108:        "Format tokens with ANSI color sequences, for output in a 256-color terminal or console.  Like in `TerminalFormatter` color sequences are terminated at newlines, so that paging the output works correctly.",
./.venv-build/lib/python3.12/site-packages/pygments/formatters/_mapping.py:115:        "Format tokens with ANSI color sequences, for output in a text console. Color sequences are terminated at newlines, so that paging the output works correctly.",
./.venv-build/lib/python3.12/site-packages/pygments/formatters/_mapping.py:122:        "Format tokens with ANSI color sequences, for output in a true-color terminal or console.  Like in `TerminalFormatter` color sequences are terminated at newlines, so that paging the output works correctly.",
./.venv-build/lib/python3.12/site-packages/pygments/formatters/_mapping.py:129:        "Format tokens as appropriate for a new testcase.",
./.venv-build/lib/python3.12/site-packages/pygments/formatters/pangomarkup.py:30:    Format tokens as Pango Markup code. It can then be rendered to an SVG.
./.venv-build/lib/python3.12/site-packages/pygments/formatters/pangomarkup.py:44:        for token, style in self.style:
./.venv-build/lib/python3.12/site-packages/pygments/formatters/pangomarkup.py:59:            self.styles[token] = (start, end)
./.venv-build/lib/python3.12/site-packages/pygments/formatters/pangomarkup.py:61:    def format_unencoded(self, tokensource, outfile):
./.venv-build/lib/python3.12/site-packages/pygments/formatters/pangomarkup.py:67:        for ttype, value in tokensource:
./.venv-build/lib/python3.12/site-packages/pygments/formatters/latex.py:15:from pygments.token import Token, STANDARD_TYPES
./.venv-build/lib/python3.12/site-packages/pygments/formatters/latex.py:65:# each token type defined in the current style.  That obviously is
./.venv-build/lib/python3.12/site-packages/pygments/formatters/latex.py:71:# specific token type, thus falling back to the parent token type if one
./.venv-build/lib/python3.12/site-packages/pygments/formatters/latex.py:73:# forms given in token.STANDARD_TYPES.
./.venv-build/lib/python3.12/site-packages/pygments/formatters/latex.py:151:    Format tokens as LaTeX code. This needs the `fancyvrb` and `color`
./.venv-build/lib/python3.12/site-packages/pygments/formatters/latex.py:179:        If set to ``True``, don't wrap the tokens at all, not even inside a
./.venv-build/lib/python3.12/site-packages/pygments/formatters/latex.py:226:        in comment tokens is not escaped so that LaTeX can render it (default:
./.venv-build/lib/python3.12/site-packages/pygments/formatters/latex.py:283:        t2n = self.ttype2name = {Token: ""}
./.venv-build/lib/python3.12/site-packages/pygments/formatters/latex.py:343:    def format_unencoded(self, tokensource, outfile):
./.venv-build/lib/python3.12/site-packages/pygments/formatters/latex.py:367:        for ttype, value in tokensource:
./.venv-build/lib/python3.12/site-packages/pygments/formatters/latex.py:368:            if ttype in Token.Comment:
./.venv-build/lib/python3.12/site-packages/pygments/formatters/latex.py:406:            elif ttype not in Token.Escape:
./.venv-build/lib/python3.12/site-packages/pygments/formatters/latex.py:409:            while ttype is not Token:
./.venv-build/lib/python3.12/site-packages/pygments/formatters/latex.py:458:    strings and comments. All other consecutive tokens are merged and
./.venv-build/lib/python3.12/site-packages/pygments/formatters/latex.py:460:    the Token.Escape type. Finally text that is not escaped is scanned
./.venv-build/lib/python3.12/site-packages/pygments/formatters/latex.py:470:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.12/site-packages/pygments/formatters/latex.py:471:        # find and remove all the escape tokens (replace with an empty string)
./.venv-build/lib/python3.12/site-packages/pygments/formatters/latex.py:472:        # this is very similar to DelegatingLexer.get_tokens_unprocessed.
./.venv-build/lib/python3.12/site-packages/pygments/formatters/latex.py:476:        for i, t, v in self._find_safe_escape_tokens(text):
./.venv-build/lib/python3.12/site-packages/pygments/formatters/latex.py:486:        return do_insertions(insertions, self.lang.get_tokens_unprocessed(buffered))
./.venv-build/lib/python3.12/site-packages/pygments/formatters/latex.py:488:    def _find_safe_escape_tokens(self, text):
./.venv-build/lib/python3.12/site-packages/pygments/formatters/latex.py:489:        """find escape tokens that are not in strings or comments"""
./.venv-build/lib/python3.12/site-packages/pygments/formatters/latex.py:491:            self.lang.get_tokens_unprocessed(text),
./.venv-build/lib/python3.12/site-packages/pygments/formatters/latex.py:492:            lambda t: t in Token.Comment or t in Token.String,
./.venv-build/lib/python3.12/site-packages/pygments/formatters/latex.py:495:                for i2, t2, v2 in self._find_escape_tokens(v):
./.venv-build/lib/python3.12/site-packages/pygments/formatters/latex.py:501:        """Keep only the tokens that match `pred`, merge the others together"""
./.venv-build/lib/python3.12/site-packages/pygments/formatters/latex.py:517:    def _find_escape_tokens(self, text):
./.venv-build/lib/python3.12/site-packages/pygments/formatters/latex.py:518:        """Find escape tokens within text, give token=None otherwise"""
./.venv-build/lib/python3.12/site-packages/pygments/formatters/latex.py:528:                    yield index + len(sep1), Token.Escape, b
./.venv-build/lib/python3.12/site-packages/pygments/formatters/latex.py:531:                    yield index, Token.Error, sep1
./.venv-build/lib/python3.12/site-packages/pygments/formatters/svg.py:12:from pygments.token import Comment
./.venv-build/lib/python3.12/site-packages/pygments/formatters/svg.py:34:    Format tokens as an SVG graphics file.  This formatter is still experimental.
./.venv-build/lib/python3.12/site-packages/pygments/formatters/svg.py:36:    coordinates containing ``<tspan>`` elements with the individual token styles.
./.venv-build/lib/python3.12/site-packages/pygments/formatters/svg.py:119:    def format_unencoded(self, tokensource, outfile):
./.venv-build/lib/python3.12/site-packages/pygments/formatters/svg.py:121:        Format ``tokensource``, an iterable of ``(tokentype, tokenstring)``
./.venv-build/lib/python3.12/site-packages/pygments/formatters/svg.py:155:        for ttype, value in tokensource:
./.venv-build/lib/python3.12/site-packages/pygments/formatters/svg.py:180:    def _get_style(self, tokentype):
./.venv-build/lib/python3.12/site-packages/pygments/formatters/svg.py:181:        if tokentype in self._stylecache:
./.venv-build/lib/python3.12/site-packages/pygments/formatters/svg.py:182:            return self._stylecache[tokentype]
./.venv-build/lib/python3.12/site-packages/pygments/formatters/svg.py:183:        otokentype = tokentype
./.venv-build/lib/python3.12/site-packages/pygments/formatters/svg.py:184:        while not self.style.styles_token(tokentype):
./.venv-build/lib/python3.12/site-packages/pygments/formatters/svg.py:185:            tokentype = tokentype.parent
./.venv-build/lib/python3.12/site-packages/pygments/formatters/svg.py:186:        value = self.style.style_for_token(tokentype)
./.venv-build/lib/python3.12/site-packages/pygments/formatters/svg.py:194:        self._stylecache[otokentype] = result
./.venv-build/lib/python3.12/site-packages/pygments/formatters/other.py:5:Other formatters: NullFormatter, RawTokenFormatter.
./.venv-build/lib/python3.12/site-packages/pygments/formatters/other.py:13:from pygments.token import Token
./.venv-build/lib/python3.12/site-packages/pygments/formatters/other.py:16:__all__ = ["NullFormatter", "RawTokenFormatter", "TestcaseFormatter"]
./.venv-build/lib/python3.12/site-packages/pygments/formatters/other.py:28:    def format(self, tokensource, outfile):
./.venv-build/lib/python3.12/site-packages/pygments/formatters/other.py:30:        for ttype, value in tokensource:
./.venv-build/lib/python3.12/site-packages/pygments/formatters/other.py:37:class RawTokenFormatter(Formatter):
./.venv-build/lib/python3.12/site-packages/pygments/formatters/other.py:39:    Format tokens as a raw representation for storing token streams.
./.venv-build/lib/python3.12/site-packages/pygments/formatters/other.py:41:    The format is ``tokentype<TAB>repr(tokenstring)\n``. The output can later
./.venv-build/lib/python3.12/site-packages/pygments/formatters/other.py:42:    be converted to a token stream with the `RawTokenLexer`, described in the
./.venv-build/lib/python3.12/site-packages/pygments/formatters/other.py:51:        If set to a color name, highlight error tokens using that color.  If
./.venv-build/lib/python3.12/site-packages/pygments/formatters/other.py:58:    name = "Raw tokens"
./.venv-build/lib/python3.12/site-packages/pygments/formatters/other.py:59:    aliases = ["raw", "tokens"]
./.venv-build/lib/python3.12/site-packages/pygments/formatters/other.py:68:        # The RawTokenFormatter outputs only ASCII. Override here.
./.venv-build/lib/python3.12/site-packages/pygments/formatters/other.py:80:    def format(self, tokensource, outfile):
./.venv-build/lib/python3.12/site-packages/pygments/formatters/other.py:84:            raise TypeError("The raw tokens formatter needs a binary " "output file")
./.venv-build/lib/python3.12/site-packages/pygments/formatters/other.py:109:            for ttype, value in tokensource:
./.venv-build/lib/python3.12/site-packages/pygments/formatters/other.py:111:                if ttype is Token.Error:
./.venv-build/lib/python3.12/site-packages/pygments/formatters/other.py:116:            for ttype, value in tokensource:
./.venv-build/lib/python3.12/site-packages/pygments/formatters/other.py:124:        tokens = [
./.venv-build/lib/python3.12/site-packages/pygments/formatters/other.py:128:        assert list(lexer.get_tokens(fragment)) == tokens
./.venv-build/lib/python3.12/site-packages/pygments/formatters/other.py:134:    Format tokens as appropriate for a new testcase.
./.venv-build/lib/python3.12/site-packages/pygments/formatters/other.py:147:    def format(self, tokensource, outfile):
./.venv-build/lib/python3.12/site-packages/pygments/formatters/other.py:151:        for ttype, value in tokensource:
./.venv-build/lib/python3.12/site-packages/pygments/formatters/terminal256.py:100:    Format tokens with ANSI color sequences, for output in a 256-color
./.venv-build/lib/python3.12/site-packages/pygments/formatters/terminal256.py:252:    def format(self, tokensource, outfile):
./.venv-build/lib/python3.12/site-packages/pygments/formatters/terminal256.py:253:        return Formatter.format(self, tokensource, outfile)
./.venv-build/lib/python3.12/site-packages/pygments/formatters/terminal256.py:255:    def format_unencoded(self, tokensource, outfile):
./.venv-build/lib/python3.12/site-packages/pygments/formatters/terminal256.py:259:        for ttype, value in tokensource:
./.venv-build/lib/python3.12/site-packages/pygments/formatters/terminal256.py:297:    Format tokens with ANSI color sequences, for output in a true-color
./.venv-build/lib/python3.12/site-packages/pygments/formatters/irc.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/formatters/irc.py:21:    Token,
./.venv-build/lib/python3.12/site-packages/pygments/formatters/irc.py:30:#: Map token types to a tuple of color values for light and dark
./.venv-build/lib/python3.12/site-packages/pygments/formatters/irc.py:33:    Token: ("", ""),
./.venv-build/lib/python3.12/site-packages/pygments/formatters/irc.py:109:    Format tokens with IRC color sequences
./.venv-build/lib/python3.12/site-packages/pygments/formatters/irc.py:121:        A dictionary mapping token types to (lightbg, darkbg) color names or
./.venv-build/lib/python3.12/site-packages/pygments/formatters/irc.py:145:    def format_unencoded(self, tokensource, outfile):
./.venv-build/lib/python3.12/site-packages/pygments/formatters/irc.py:148:        for ttype, value in tokensource:
./.venv-build/lib/python3.12/site-packages/pygments/formatters/html.py:18:from pygments.token import Token, Text, STANDARD_TYPES
./.venv-build/lib/python3.12/site-packages/pygments/formatters/html.py:126:    Format tokens as HTML 4 ``<span>`` tags. By default, the content is enclosed
./.venv-build/lib/python3.12/site-packages/pygments/formatters/html.py:180:    `get_style_defs()` method to request multiple prefixes for the tokens:
./.venv-build/lib/python3.12/site-packages/pygments/formatters/html.py:200:        around the tokens. This disables most other options (default: ``False``).
./.venv-build/lib/python3.12/site-packages/pygments/formatters/html.py:217:        If set to true, token ``<span>`` tags (as well as line number elements)
./.venv-build/lib/python3.12/site-packages/pygments/formatters/html.py:223:        Since the token types use relatively short class names, they may clash
./.venv-build/lib/python3.12/site-packages/pygments/formatters/html.py:226:        CSS class names for token types.
./.venv-build/lib/python3.12/site-packages/pygments/formatters/html.py:362:    `debug_token_types`
./.venv-build/lib/python3.12/site-packages/pygments/formatters/html.py:363:        Add ``title`` attributes to all token ``<span>`` tags that show the
./.venv-build/lib/python3.12/site-packages/pygments/formatters/html.py:364:        name of the token.
./.venv-build/lib/python3.12/site-packages/pygments/formatters/html.py:438:        self.debug_token_types = get_bool_opt(options, "debug_token_types", False)
./.venv-build/lib/python3.12/site-packages/pygments/formatters/html.py:474:        """Return the css class of this token type prefixed with
./.venv-build/lib/python3.12/site-packages/pygments/formatters/html.py:482:        """Return the CSS classes of this token type prefixed with the classprefix option."""
./.venv-build/lib/python3.12/site-packages/pygments/formatters/html.py:490:        """Return the inline CSS styles for this token type."""
./.venv-build/lib/python3.12/site-packages/pygments/formatters/html.py:498:        t2c = self.ttype2class = {Token: ""}
./.venv-build/lib/python3.12/site-packages/pygments/formatters/html.py:525:        insert before the token type classes.
./.venv-build/lib/python3.12/site-packages/pygments/formatters/html.py:531:        style_lines.extend(self.get_token_style_defs(arg))
./.venv-build/lib/python3.12/site-packages/pygments/formatters/html.py:535:    def get_token_style_defs(self, arg=None):
./.venv-build/lib/python3.12/site-packages/pygments/formatters/html.py:846:    def _format_lines(self, tokensource):
./.venv-build/lib/python3.12/site-packages/pygments/formatters/html.py:848:        Just format the tokens, without any wrapping tags.
./.venv-build/lib/python3.12/site-packages/pygments/formatters/html.py:857:        for ttype, value in tokensource:
./.venv-build/lib/python3.12/site-packages/pygments/formatters/html.py:861:                title = ' title="{}"'.format(".".join(ttype)) if self.debug_token_types else ""
./.venv-build/lib/python3.12/site-packages/pygments/formatters/html.py:879:            if tagsfile and ttype in Token.Name:
./.venv-build/lib/python3.12/site-packages/pygments/formatters/html.py:938:    def _lookup_ctag(self, token):
./.venv-build/lib/python3.12/site-packages/pygments/formatters/html.py:940:        if self._ctags.find(entry, token.encode(), 0):
./.venv-build/lib/python3.12/site-packages/pygments/formatters/html.py:945:    def _highlight_lines(self, tokensource):
./.venv-build/lib/python3.12/site-packages/pygments/formatters/html.py:948:        post-processing the token stream coming from `_format_lines`.
./.venv-build/lib/python3.12/site-packages/pygments/formatters/html.py:952:        for i, (t, value) in enumerate(tokensource):
./.venv-build/lib/python3.12/site-packages/pygments/formatters/html.py:981:    def format_unencoded(self, tokensource, outfile):
./.venv-build/lib/python3.12/site-packages/pygments/formatters/html.py:990:        is part of the original tokensource being highlighted, if it's
./.venv-build/lib/python3.12/site-packages/pygments/formatters/html.py:995:        source = self._format_lines(tokensource)
./.venv-build/lib/python3.12/site-packages/pygments/formatters/bbcode.py:19:    Format tokens with BBcodes. These formatting codes are used by many
./.venv-build/lib/python3.12/site-packages/pygments/formatters/bbcode.py:78:    def format_unencoded(self, tokensource, outfile):
./.venv-build/lib/python3.12/site-packages/pygments/formatters/bbcode.py:87:        for ttype, value in tokensource:
./.venv-build/lib/python3.12/site-packages/pygments/formatters/terminal.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/formatters/terminal.py:21:    Token,
./.venv-build/lib/python3.12/site-packages/pygments/formatters/terminal.py:31:#: Map token types to a tuple of color values for light and dark
./.venv-build/lib/python3.12/site-packages/pygments/formatters/terminal.py:34:    Token: ("", ""),
./.venv-build/lib/python3.12/site-packages/pygments/formatters/terminal.py:65:    Format tokens with ANSI color sequences, for output in a text console.
./.venv-build/lib/python3.12/site-packages/pygments/formatters/terminal.py:79:        A dictionary mapping token types to (lightbg, darkbg) color names or
./.venv-build/lib/python3.12/site-packages/pygments/formatters/terminal.py:98:    def format(self, tokensource, outfile):
./.venv-build/lib/python3.12/site-packages/pygments/formatters/terminal.py:99:        return Formatter.format(self, tokensource, outfile)
./.venv-build/lib/python3.12/site-packages/pygments/formatters/terminal.py:107:        # have to walk the tree of dots.  The base Token type must be a key,
./.venv-build/lib/python3.12/site-packages/pygments/formatters/terminal.py:115:    def format_unencoded(self, tokensource, outfile):
./.venv-build/lib/python3.12/site-packages/pygments/formatters/terminal.py:119:        for ttype, value in tokensource:
./.venv-build/lib/python3.12/site-packages/pygments/styles/sas.py:14:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/styles/lovelace.py:16:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/styles/default.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/styles/nord.py:13:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/styles/nord.py:25:    Token,
./.venv-build/lib/python3.12/site-packages/pygments/styles/nord.py:48:        Token: "#d8dee9",
./.venv-build/lib/python3.12/site-packages/pygments/styles/nord.py:110:        Token: "#d8dee9",
./.venv-build/lib/python3.12/site-packages/pygments/styles/native.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/styles/native.py:21:    Token,
./.venv-build/lib/python3.12/site-packages/pygments/styles/native.py:41:        Token: "#d0d0d0",
./.venv-build/lib/python3.12/site-packages/pygments/styles/manni.py:15:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/styles/dracula.py:15:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/styles/stata_dark.py:14:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/styles/stata_dark.py:15:    Token,
./.venv-build/lib/python3.12/site-packages/pygments/styles/stata_dark.py:38:        Token: "#cccccc",
./.venv-build/lib/python3.12/site-packages/pygments/styles/monokai.py:14:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/styles/monokai.py:20:    Token,
./.venv-build/lib/python3.12/site-packages/pygments/styles/monokai.py:46:        Token: "#f8f8f2",  # class:  ''
./.venv-build/lib/python3.12/site-packages/pygments/styles/perldoc.py:14:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/styles/gruvbox.py:13:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/styles/gruvbox.py:14:    Token,
./.venv-build/lib/python3.12/site-packages/pygments/styles/gruvbox.py:40:        Token: "#dddddd",
./.venv-build/lib/python3.12/site-packages/pygments/styles/lilypond.py:12:from pygments.token import Token
./.venv-build/lib/python3.12/site-packages/pygments/styles/lilypond.py:32:        Token.Text: "",
./.venv-build/lib/python3.12/site-packages/pygments/styles/lilypond.py:33:        Token.Keyword: "bold",
./.venv-build/lib/python3.12/site-packages/pygments/styles/lilypond.py:34:        Token.Comment: "italic #A3AAB2",
./.venv-build/lib/python3.12/site-packages/pygments/styles/lilypond.py:35:        Token.String: "#AB0909",
./.venv-build/lib/python3.12/site-packages/pygments/styles/lilypond.py:36:        Token.String.Escape: "#C46C6C",
./.venv-build/lib/python3.12/site-packages/pygments/styles/lilypond.py:37:        Token.String.Symbol: "noinherit",
./.venv-build/lib/python3.12/site-packages/pygments/styles/lilypond.py:38:        Token.Pitch: "",  # "#911520",
./.venv-build/lib/python3.12/site-packages/pygments/styles/lilypond.py:39:        Token.Number: "#976806",  # includes durations
./.venv-build/lib/python3.12/site-packages/pygments/styles/lilypond.py:42:        Token.ChordModifier: "#976806",
./.venv-build/lib/python3.12/site-packages/pygments/styles/lilypond.py:43:        Token.Name.Lvalue: "#08547A",
./.venv-build/lib/python3.12/site-packages/pygments/styles/lilypond.py:44:        Token.Name.BackslashReference: "#08547A",
./.venv-build/lib/python3.12/site-packages/pygments/styles/lilypond.py:45:        Token.Name.Builtin.MusicCommand: "bold #08547A",
./.venv-build/lib/python3.12/site-packages/pygments/styles/lilypond.py:46:        Token.Name.Builtin.PaperVariable: "bold #6C5A05",
./.venv-build/lib/python3.12/site-packages/pygments/styles/lilypond.py:47:        Token.Name.Builtin.HeaderVariable: "bold #6C5A05",
./.venv-build/lib/python3.12/site-packages/pygments/styles/lilypond.py:48:        Token.Name.Builtin.MusicFunction: "bold #08547A",
./.venv-build/lib/python3.12/site-packages/pygments/styles/lilypond.py:49:        Token.Name.Builtin.Clef: "bold #08547A",
./.venv-build/lib/python3.12/site-packages/pygments/styles/lilypond.py:50:        Token.Name.Builtin.Scale: "bold #08547A",
./.venv-build/lib/python3.12/site-packages/pygments/styles/lilypond.py:51:        Token.Name.Builtin.RepeatType: "#08547A",
./.venv-build/lib/python3.12/site-packages/pygments/styles/lilypond.py:52:        Token.Name.Builtin.Dynamic: "#68175A",
./.venv-build/lib/python3.12/site-packages/pygments/styles/lilypond.py:53:        Token.Name.Builtin.Articulation: "#68175A",
./.venv-build/lib/python3.12/site-packages/pygments/styles/lilypond.py:54:        Token.Name.Builtin.SchemeFunction: "bold #A83401",
./.venv-build/lib/python3.12/site-packages/pygments/styles/lilypond.py:55:        Token.Name.Builtin.SchemeBuiltin: "bold",
./.venv-build/lib/python3.12/site-packages/pygments/styles/lilypond.py:56:        Token.Name.Builtin.MarkupCommand: "bold #831E71",
./.venv-build/lib/python3.12/site-packages/pygments/styles/lilypond.py:57:        Token.Name.Builtin.Context: "bold #038B8B",
./.venv-build/lib/python3.12/site-packages/pygments/styles/lilypond.py:58:        Token.Name.Builtin.ContextProperty: "#038B8B",
./.venv-build/lib/python3.12/site-packages/pygments/styles/lilypond.py:59:        Token.Name.Builtin.Grob: "bold #0C7441",
./.venv-build/lib/python3.12/site-packages/pygments/styles/lilypond.py:60:        Token.Name.Builtin.GrobProperty: "#0C7441",
./.venv-build/lib/python3.12/site-packages/pygments/styles/lilypond.py:61:        Token.Name.Builtin.Translator: "bold #6200A4",
./.venv-build/lib/python3.12/site-packages/pygments/styles/onedark.py:15:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/styles/onedark.py:23:    Token,
./.venv-build/lib/python3.12/site-packages/pygments/styles/onedark.py:42:        Token: "#ABB2BF",
./.venv-build/lib/python3.12/site-packages/pygments/styles/colorful.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/styles/rainbow_dash.py:14:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/styles/friendly.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/styles/vs.py:12:from pygments.token import Keyword, Name, Comment, String, Error, Operator, Generic
./.venv-build/lib/python3.12/site-packages/pygments/styles/paraiso_dark.py:16:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/styles/rrt.py:12:from pygments.token import Token, Comment, Name, Keyword, String, Number, Operator
./.venv-build/lib/python3.12/site-packages/pygments/styles/rrt.py:29:        Token: "#dddddd",
./.venv-build/lib/python3.12/site-packages/pygments/styles/algol.py:33:from pygments.token import Keyword, Name, Comment, String, Error, Operator
./.venv-build/lib/python3.12/site-packages/pygments/styles/igor.py:12:from pygments.token import Keyword, Name, Comment, String
./.venv-build/lib/python3.12/site-packages/pygments/styles/borland.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/styles/staroffice.py:12:from pygments.token import Comment, Error, Literal, Name, Token
./.venv-build/lib/python3.12/site-packages/pygments/styles/staroffice.py:26:        Token: "#000080",  # Blue
./.venv-build/lib/python3.12/site-packages/pygments/styles/abap.py:12:from pygments.token import Keyword, Name, Comment, String, Error, Number, Operator
./.venv-build/lib/python3.12/site-packages/pygments/styles/algol_nu.py:33:from pygments.token import Keyword, Name, Comment, String, Error, Operator
./.venv-build/lib/python3.12/site-packages/pygments/styles/friendly_grayscale.py:15:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/styles/xcode.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/styles/xcode.py:46:        # In Obj-C code this token is used to colour Cocoa types
./.venv-build/lib/python3.12/site-packages/pygments/styles/gh_dark.py:13:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/styles/gh_dark.py:24:    Token,
./.venv-build/lib/python3.12/site-packages/pygments/styles/gh_dark.py:72:        Token: FG_DEFAULT,
./.venv-build/lib/python3.12/site-packages/pygments/styles/arduino.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/styles/tango.py:24:Token types, unlike most (if not all) of the styles included in the
./.venv-build/lib/python3.12/site-packages/pygments/styles/tango.py:40:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/styles/lightbulb.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/styles/lightbulb.py:23:    Token,
./.venv-build/lib/python3.12/site-packages/pygments/styles/lightbulb.py:109:        Token: COLORS["white"],
./.venv-build/lib/python3.12/site-packages/pygments/styles/coffee.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/styles/coffee.py:23:    Token,
./.venv-build/lib/python3.12/site-packages/pygments/styles/coffee.py:90:        Token: "#ddd0c0",
./.venv-build/lib/python3.12/site-packages/pygments/styles/material.py:14:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/styles/bw.py:12:from pygments.token import Keyword, Name, Comment, String, Error, Operator, Generic
./.venv-build/lib/python3.12/site-packages/pygments/styles/stata_light.py:13:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/styles/trac.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/styles/vim.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/styles/vim.py:22:    Token,
./.venv-build/lib/python3.12/site-packages/pygments/styles/vim.py:40:        Token: "#cccccc",
./.venv-build/lib/python3.12/site-packages/pygments/styles/fruity.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/styles/fruity.py:13:    Token,
./.venv-build/lib/python3.12/site-packages/pygments/styles/fruity.py:39:        Token: "#ffffff",
./.venv-build/lib/python3.12/site-packages/pygments/styles/murphy.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/styles/autumn.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/styles/inkpot.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/styles/pastie.py:14:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/styles/solarized.py:15:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/styles/solarized.py:24:    Token,
./.venv-build/lib/python3.12/site-packages/pygments/styles/solarized.py:33:        Token: colors["base0"],
./.venv-build/lib/python3.12/site-packages/pygments/styles/zenburn.py:15:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/styles/zenburn.py:16:    Token,
./.venv-build/lib/python3.12/site-packages/pygments/styles/zenburn.py:48:        Token: "#dcdccc",
./.venv-build/lib/python3.12/site-packages/pygments/styles/emacs.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/styles/paraiso_light.py:16:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/formatter.py:27:    Converts a token stream to text.
./.venv-build/lib/python3.12/site-packages/pygments/formatter.py:58:        convert the Unicode token strings to byte strings in the
./.venv-build/lib/python3.12/site-packages/pygments/formatter.py:114:    def format(self, tokensource, outfile):
./.venv-build/lib/python3.12/site-packages/pygments/formatter.py:116:        This method must format the tokens from the `tokensource` iterable and
./.venv-build/lib/python3.12/site-packages/pygments/formatter.py:119:        Formatter options can control how exactly the tokens are converted.
./.venv-build/lib/python3.12/site-packages/pygments/formatter.py:124:        return self.format_unencoded(tokensource, outfile)
./.venv-build/lib/python3.12/site-packages/pygments/token.py:2:pygments.token
./.venv-build/lib/python3.12/site-packages/pygments/token.py:5:Basic token types and the standard tokens.
./.venv-build/lib/python3.12/site-packages/pygments/token.py:12:class _TokenType(tuple):
./.venv-build/lib/python3.12/site-packages/pygments/token.py:34:        new = _TokenType(self + (val,))
./.venv-build/lib/python3.12/site-packages/pygments/token.py:41:        return "Token" + (self and "." or "") + ".".join(self)
./.venv-build/lib/python3.12/site-packages/pygments/token.py:52:Token = _TokenType()
./.venv-build/lib/python3.12/site-packages/pygments/token.py:54:# Special token types
./.venv-build/lib/python3.12/site-packages/pygments/token.py:55:Text = Token.Text
./.venv-build/lib/python3.12/site-packages/pygments/token.py:57:Escape = Token.Escape
./.venv-build/lib/python3.12/site-packages/pygments/token.py:58:Error = Token.Error
./.venv-build/lib/python3.12/site-packages/pygments/token.py:60:Other = Token.Other
./.venv-build/lib/python3.12/site-packages/pygments/token.py:62:# Common token types for source code
./.venv-build/lib/python3.12/site-packages/pygments/token.py:63:Keyword = Token.Keyword
./.venv-build/lib/python3.12/site-packages/pygments/token.py:64:Name = Token.Name
./.venv-build/lib/python3.12/site-packages/pygments/token.py:65:Literal = Token.Literal
./.venv-build/lib/python3.12/site-packages/pygments/token.py:68:Punctuation = Token.Punctuation
./.venv-build/lib/python3.12/site-packages/pygments/token.py:69:Operator = Token.Operator
./.venv-build/lib/python3.12/site-packages/pygments/token.py:70:Comment = Token.Comment
./.venv-build/lib/python3.12/site-packages/pygments/token.py:73:Generic = Token.Generic
./.venv-build/lib/python3.12/site-packages/pygments/token.py:75:# String and some others are not direct children of Token.
./.venv-build/lib/python3.12/site-packages/pygments/token.py:77:Token.Token = Token
./.venv-build/lib/python3.12/site-packages/pygments/token.py:78:Token.String = String
./.venv-build/lib/python3.12/site-packages/pygments/token.py:79:Token.Number = Number
./.venv-build/lib/python3.12/site-packages/pygments/token.py:82:def is_token_subtype(ttype, other):
./.venv-build/lib/python3.12/site-packages/pygments/token.py:91:def string_to_tokentype(s):
./.venv-build/lib/python3.12/site-packages/pygments/token.py:93:    Convert a string into a token type::
./.venv-build/lib/python3.12/site-packages/pygments/token.py:95:        >>> string_to_token('String.Double')
./.venv-build/lib/python3.12/site-packages/pygments/token.py:96:        Token.Literal.String.Double
./.venv-build/lib/python3.12/site-packages/pygments/token.py:97:        >>> string_to_token('Token.Literal.Number')
./.venv-build/lib/python3.12/site-packages/pygments/token.py:98:        Token.Literal.Number
./.venv-build/lib/python3.12/site-packages/pygments/token.py:99:        >>> string_to_token('')
./.venv-build/lib/python3.12/site-packages/pygments/token.py:100:        Token
./.venv-build/lib/python3.12/site-packages/pygments/token.py:102:    Tokens that are already tokens are returned unchanged:
./.venv-build/lib/python3.12/site-packages/pygments/token.py:104:        >>> string_to_token(String)
./.venv-build/lib/python3.12/site-packages/pygments/token.py:105:        Token.Literal.String
./.venv-build/lib/python3.12/site-packages/pygments/token.py:107:    if isinstance(s, _TokenType):
./.venv-build/lib/python3.12/site-packages/pygments/token.py:110:        return Token
./.venv-build/lib/python3.12/site-packages/pygments/token.py:111:    node = Token
./.venv-build/lib/python3.12/site-packages/pygments/token.py:117:# Map standard token types to short names, used in CSS class naming.
./.venv-build/lib/python3.12/site-packages/pygments/token.py:121:    Token: "",
./.venv-build/lib/python3.12/site-packages/pygments/style.py:11:from pygments.token import Token, STANDARD_TYPES
./.venv-build/lib/python3.12/site-packages/pygments/style.py:61:        for token in STANDARD_TYPES:
./.venv-build/lib/python3.12/site-packages/pygments/style.py:62:            if token not in obj.styles:
./.venv-build/lib/python3.12/site-packages/pygments/style.py:63:                obj.styles[token] = ""
./.venv-build/lib/python3.12/site-packages/pygments/style.py:83:            for token in ttype.split():
./.venv-build/lib/python3.12/site-packages/pygments/style.py:84:                if token in _styles:
./.venv-build/lib/python3.12/site-packages/pygments/style.py:86:                ndef = _styles.get(token.parent, None)
./.venv-build/lib/python3.12/site-packages/pygments/style.py:87:                styledefs = obj.styles.get(token, "").split()
./.venv-build/lib/python3.12/site-packages/pygments/style.py:88:                if not ndef or token is None:
./.venv-build/lib/python3.12/site-packages/pygments/style.py:90:                elif "noinherit" in styledefs and token is not Token:
./.venv-build/lib/python3.12/site-packages/pygments/style.py:91:                    ndef = _styles[Token][:]
./.venv-build/lib/python3.12/site-packages/pygments/style.py:94:                _styles[token] = ndef
./.venv-build/lib/python3.12/site-packages/pygments/style.py:95:                for styledef in obj.styles.get(token, "").split():
./.venv-build/lib/python3.12/site-packages/pygments/style.py:125:    def style_for_token(cls, token):
./.venv-build/lib/python3.12/site-packages/pygments/style.py:126:        t = cls._styles[token]
./.venv-build/lib/python3.12/site-packages/pygments/style.py:158:    def styles_token(cls, ttype):
./.venv-build/lib/python3.12/site-packages/pygments/style.py:162:        for token in cls._styles:
./.venv-build/lib/python3.12/site-packages/pygments/style.py:163:            yield token, cls.style_for_token(token)
./.venv-build/lib/python3.12/site-packages/pygments/style.py:188:    #: Style definitions for individual token types.
./.venv-build/lib/python3.12/site-packages/pygments/lexers/pointless.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/pointless.py:92:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/verification.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/verification.py:37:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/verification.py:102:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/hexdump.py:12:from pygments.token import Name, Number, String, Punctuation, Whitespace
./.venv-build/lib/python3.12/site-packages/pygments/lexers/hexdump.py:45:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/arturo.py:20:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/arturo.py:85:            yield from do_insertions([], lexer.get_tokens_unprocessed(code))
./.venv-build/lib/python3.12/site-packages/pygments/lexers/arturo.py:89:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/sas.py:13:from pygments.token import Comment, Keyword, Name, Number, String, Text, Other, Generic
./.venv-build/lib/python3.12/site-packages/pygments/lexers/sas.py:483:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/textfmts.py:15:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/textfmts.py:74:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/textfmts.py:132:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/textfmts.py:164:    def get_tokens_unprocessed(self, text, stack=("root",)):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/textfmts.py:167:        return RegexLexer.get_tokens_unprocessed(self, text, stack)
./.venv-build/lib/python3.12/site-packages/pygments/lexers/textfmts.py:207:                    for idx, token, value in lexer.get_tokens_unprocessed(content):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/textfmts.py:208:                        yield offset + idx, token, value
./.venv-build/lib/python3.12/site-packages/pygments/lexers/textfmts.py:212:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/textfmts.py:275:    # Aliases mapping standard token types of Todo.txt format concepts
./.venv-build/lib/python3.12/site-packages/pygments/lexers/textfmts.py:304:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/textfmts.py:338:            # Tokenize contexts and projects
./.venv-build/lib/python3.12/site-packages/pygments/lexers/textfmts.py:341:            # Tokenize non-whitespace text
./.venv-build/lib/python3.12/site-packages/pygments/lexers/textfmts.py:343:            # Tokenize whitespace not containing a newline
./.venv-build/lib/python3.12/site-packages/pygments/lexers/textfmts.py:350:            # Tokenize contexts and projects
./.venv-build/lib/python3.12/site-packages/pygments/lexers/textfmts.py:353:            # Tokenize non-whitespace text
./.venv-build/lib/python3.12/site-packages/pygments/lexers/textfmts.py:355:            # Tokenize whitespace not containing a newline
./.venv-build/lib/python3.12/site-packages/pygments/lexers/textfmts.py:388:        yield from lexer.get_tokens_unprocessed(code)
./.venv-build/lib/python3.12/site-packages/pygments/lexers/textfmts.py:390:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/textfmts.py:463:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/boa.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/boa.py:244:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/maple.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/maple.py:272:        yield from self.get_tokens_unprocessed(context=ctx)
./.venv-build/lib/python3.12/site-packages/pygments/lexers/maple.py:278:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/wren.py:14:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/wren.py:41:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/haskell.py:23:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/haskell.py:113:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/haskell.py:255:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/haskell.py:378:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/haskell.py:527:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/haskell.py:570:        "comment": HaskellLexer.tokens["comment"],
./.venv-build/lib/python3.12/site-packages/pygments/lexers/haskell.py:571:        "character": HaskellLexer.tokens["character"],
./.venv-build/lib/python3.12/site-packages/pygments/lexers/haskell.py:572:        "string": HaskellLexer.tokens["string"],
./.venv-build/lib/python3.12/site-packages/pygments/lexers/haskell.py:573:        "escape": HaskellLexer.tokens["escape"],
./.venv-build/lib/python3.12/site-packages/pygments/lexers/haskell.py:644:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/haskell.py:780:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/haskell.py:782:        for index, token, value in RegexLexer.get_tokens_unprocessed(self, text, stack):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/haskell.py:783:            if token is Name and value in self.EXTRA_KEYWORDS:
./.venv-build/lib/python3.12/site-packages/pygments/lexers/haskell.py:786:                yield index, token, value
./.venv-build/lib/python3.12/site-packages/pygments/lexers/haskell.py:808:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/haskell.py:843:                    insertions.append((len(code), list(lxlexer.get_tokens_unprocessed(latex))))
./.venv-build/lib/python3.12/site-packages/pygments/lexers/haskell.py:847:            insertions.append((len(code), list(lxlexer.get_tokens_unprocessed(latex))))
./.venv-build/lib/python3.12/site-packages/pygments/lexers/haskell.py:848:        yield from do_insertions(insertions, self.baselexer.get_tokens_unprocessed(code))
./.venv-build/lib/python3.12/site-packages/pygments/lexers/haskell.py:1040:    # koka token abstractions
./.venv-build/lib/python3.12/site-packages/pygments/lexers/haskell.py:1041:    tokenType = Name.Attribute
./.venv-build/lib/python3.12/site-packages/pygments/lexers/haskell.py:1042:    tokenTypeDef = Name.Class
./.venv-build/lib/python3.12/site-packages/pygments/lexers/haskell.py:1043:    tokenConstructor = Generic.Emph
./.venv-build/lib/python3.12/site-packages/pygments/lexers/haskell.py:1046:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/haskell.py:1050:            (r"::?" + sboundary, tokenType, "type"),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/haskell.py:1053:                bygroups(Keyword, Whitespace, tokenTypeDef),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/haskell.py:1058:                bygroups(Keyword, Whitespace, tokenTypeDef),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/haskell.py:1063:                bygroups(Keyword, Whitespace, tokenTypeDef),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/haskell.py:1066:            # special sequences of tokens (we use ?: for non-capturing group as
./.venv-build/lib/python3.12/site-packages/pygments/lexers/haskell.py:1112:            (r"((?:[a-z]\w*/)*)([A-Z]\w*)", bygroups(Name.Namespace, tokenConstructor)),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/haskell.py:1137:        "type": [(r"[(\[<]", tokenType, "type-nested"), include("type-content")],
./.venv-build/lib/python3.12/site-packages/pygments/lexers/haskell.py:1140:            (r"[)\]>]", tokenType, "#pop"),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/haskell.py:1141:            (r"[(\[<]", tokenType, "type-nested"),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/haskell.py:1142:            (r",", tokenType),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/haskell.py:1145:                bygroups(Name, Whitespace, tokenType),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/haskell.py:1160:            (r"[EPHVX]" + boundary, tokenType),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/haskell.py:1162:            (r"[a-z][0-9]*(?![\w/])", tokenType),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/haskell.py:1163:            (r"_\w*", tokenType.Variable),  # Generic.Emph
./.venv-build/lib/python3.12/site-packages/pygments/lexers/haskell.py:1164:            (r"((?:[a-z]\w*/)*)([A-Z]\w*)", bygroups(Name.Namespace, tokenType)),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/haskell.py:1165:            (r"((?:[a-z]\w*/)*)([a-z]\w+)", bygroups(Name.Namespace, tokenType)),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/haskell.py:1167:            (r"::|->|[.:|]", tokenType),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/hare.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/hare.py:41:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/apl.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/apl.py:45:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/apl.py:63:            # This token type is used for diamond and parenthesis
./.venv-build/lib/python3.12/site-packages/pygments/lexers/apl.py:69:            # Since this token type is very important in APL, it is not included in
./.venv-build/lib/python3.12/site-packages/pygments/lexers/apl.py:70:            # the punctuation token type but rather in the following one
./.venv-build/lib/python3.12/site-packages/pygments/lexers/apl.py:98:            (r"[\.\\\/âŒ¿â€Â¨â£â¨â â¤âˆ˜âŒ¸&âŒ¶@âŒºâ¥â›â¢]", Name.Attribute),  # closest token type
./.venv-build/lib/python3.12/site-packages/pygments/lexers/sgf.py:12:from pygments.token import Name, Literal, String, Punctuation, Whitespace
./.venv-build/lib/python3.12/site-packages/pygments/lexers/sgf.py:31:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/sgf.py:34:            # tokens:
./.venv-build/lib/python3.12/site-packages/pygments/lexers/elm.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/elm.py:97:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/tact.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/tact.py:35:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_mapping.py:2804:    "RawTokenLexer": (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_mapping.py:2806:        "Raw token data",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_mapping.py:2809:        ("application/x-pygments-tokens",),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_usd_builtins.py:99:    "token",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/dylan.py:14:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/dylan.py:300:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/dylan.py:301:        for index, token, value in RegexLexer.get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/dylan.py:302:            if token is Name:
./.venv-build/lib/python3.12/site-packages/pygments/lexers/dylan.py:316:            yield index, token, value
./.venv-build/lib/python3.12/site-packages/pygments/lexers/dylan.py:318:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/dylan.py:353:                r"(\?" + valid_name + ")(:)" r"(token|name|variable|expression|body|case-body|\*)",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/dylan.py:357:                r"(\?)(:)(token|name|variable|expression|body|case-body|\*)",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/dylan.py:416:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/dylan.py:448:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/dylan.py:462:                    yield from do_insertions(insertions, dylexer.get_tokens_unprocessed(curcode))
./.venv-build/lib/python3.12/site-packages/pygments/lexers/dylan.py:467:            yield from do_insertions(insertions, dylexer.get_tokens_unprocessed(curcode))
./.venv-build/lib/python3.12/site-packages/pygments/lexers/scripting.py:14:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/scripting.py:87:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/scripting.py:198:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/scripting.py:199:        for index, token, value in RegexLexer.get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/scripting.py:200:            if token is Name.Builtin and value not in self._functions:
./.venv-build/lib/python3.12/site-packages/pygments/lexers/scripting.py:209:            yield index, token, value
./.venv-build/lib/python3.12/site-packages/pygments/lexers/scripting.py:275:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/scripting.py:483:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/scripting.py:484:        for index, token, value in RegexLexer.get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/scripting.py:485:            if token is Name or token is Name.Other:
./.venv-build/lib/python3.12/site-packages/pygments/lexers/scripting.py:503:                        if token is Name:
./.venv-build/lib/python3.12/site-packages/pygments/lexers/scripting.py:514:            yield index, token, value
./.venv-build/lib/python3.12/site-packages/pygments/lexers/scripting.py:529:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/scripting.py:595:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/scripting.py:597:        for index, token, value in LuaLexer.get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/scripting.py:598:            if token == Punctuation and value == ".":
./.venv-build/lib/python3.12/site-packages/pygments/lexers/scripting.py:599:                token = Operator
./.venv-build/lib/python3.12/site-packages/pygments/lexers/scripting.py:600:            yield index, token, value
./.venv-build/lib/python3.12/site-packages/pygments/lexers/scripting.py:617:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/scripting.py:708:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/scripting.py:1533:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/scripting.py:1597:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/scripting.py:1782:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/scripting.py:1830:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/scripting.py:2321:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/scripting.py:2481:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/scripting.py:2570:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/ada.py:14:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/ada.py:43:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/nimrod.py:14:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/nimrod.py:148:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/objective.py:23:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/objective.py:65:        tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/objective.py:274:        def get_tokens_unprocessed(self, text, stack=("root",)):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/objective.py:281:            for index, token, value in baselexer.get_tokens_unprocessed(self, text, stack):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/objective.py:282:                if token is Name or token is Name.Class:
./.venv-build/lib/python3.12/site-packages/pygments/lexers/objective.py:288:                        token = Name.Builtin.Pseudo
./.venv-build/lib/python3.12/site-packages/pygments/lexers/objective.py:290:                yield index, token, value
./.venv-build/lib/python3.12/site-packages/pygments/lexers/objective.py:334:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/objective.py:409:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/objective.py:920:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/objective.py:927:        for index, token, value in RegexLexer.get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/objective.py:928:            if token is Name or token is Name.Class:
./.venv-build/lib/python3.12/site-packages/pygments/lexers/objective.py:934:                    token = Name.Builtin.Pseudo
./.venv-build/lib/python3.12/site-packages/pygments/lexers/objective.py:936:            yield index, token, value
./.venv-build/lib/python3.12/site-packages/pygments/lexers/numbair.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/numbair.py:40:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/php.py:25:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/php.py:61:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/php.py:144:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/php.py:158:                    yield from do_insertions(insertions, phplexer.get_tokens_unprocessed(curcode))
./.venv-build/lib/python3.12/site-packages/pygments/lexers/php.py:163:            yield from do_insertions(insertions, phplexer.get_tokens_unprocessed(curcode))
./.venv-build/lib/python3.12/site-packages/pygments/lexers/php.py:213:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/php.py:392:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/php.py:396:        for index, token, value in RegexLexer.get_tokens_unprocessed(self, text, stack):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/php.py:397:            if token is Name.Other:
./.venv-build/lib/python3.12/site-packages/pygments/lexers/php.py:401:            yield index, token, value
./.venv-build/lib/python3.12/site-packages/pygments/lexers/ruby.py:25:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/ruby.py:113:            yield from self.get_tokens_unprocessed(context=ctx)
./.venv-build/lib/python3.12/site-packages/pygments/lexers/ruby.py:146:            for i, t, v in self.get_tokens_unprocessed(context=nctx):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/ruby.py:154:            for i, t, v in self.get_tokens_unprocessed(context=nctx):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/ruby.py:248:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/ruby.py:624:    tokens.update(gen_rubystrings_rules())
./.venv-build/lib/python3.12/site-packages/pygments/lexers/ruby.py:644:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/ruby.py:658:                    yield from do_insertions(insertions, rblexer.get_tokens_unprocessed(curcode))
./.venv-build/lib/python3.12/site-packages/pygments/lexers/ruby.py:663:            yield from do_insertions(insertions, rblexer.get_tokens_unprocessed(curcode))
./.venv-build/lib/python3.12/site-packages/pygments/lexers/ruby.py:682:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/wgsl.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/wgsl.py:36:# https://www.w3.org/TR/WGSL/#syntax-ident_pattern_token
./.venv-build/lib/python3.12/site-packages/pygments/lexers/wgsl.py:37:ident_pattern_token = f"([{uni.xid_start}][{uni.xid_continue}]+)|[{uni.xid_start}]"
./.venv-build/lib/python3.12/site-packages/pygments/lexers/wgsl.py:370:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/wgsl.py:388:            (ident_pattern_token, Name.Decorator, "#pop"),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/wgsl.py:441:            (ident_pattern_token, Name),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/wgsl.py:442:            # TODO: templates start and end tokens.
./.venv-build/lib/python3.12/site-packages/pygments/lexers/roboconf.py:12:from pygments.token import Text, Operator, Keyword, Name, Comment
./.venv-build/lib/python3.12/site-packages/pygments/lexers/roboconf.py:29:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/roboconf.py:73:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/tcl.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/tcl.py:166:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/make.py:14:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/make.py:55:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/make.py:69:        yield from do_insertions(ins, lex.get_tokens_unprocessed(done))
./.venv-build/lib/python3.12/site-packages/pygments/lexers/make.py:89:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/make.py:156:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/verifpal.py:12:from pygments.token import Comment, Keyword, Name, String, Punctuation, Whitespace
./.venv-build/lib/python3.12/site-packages/pygments/lexers/verifpal.py:29:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/verifpal.py:92:            (words(("password",), suffix=r"\b"), Keyword.Constant),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/automation.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/automation.py:38:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/automation.py:329:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/json5.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/json5.py:52:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/fantom.py:14:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/fantom.py:55:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/teal.py:12:from pygments.token import Comment, Name, Number, String, Text, Keyword, Whitespace
./.venv-build/lib/python3.12/site-packages/pygments/lexers/teal.py:115:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/textedit.py:16:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/textedit.py:43:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/textedit.py:107:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/textedit.py:183:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/textedit.py:240:        we match `\b\w+\b` and then call is_in() on those tokens.  See
./.venv-build/lib/python3.12/site-packages/pygments/lexers/textedit.py:254:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/textedit.py:255:        # TODO: builtins are only subsequent tokens on lines
./.venv-build/lib/python3.12/site-packages/pygments/lexers/textedit.py:258:        for index, token, value in RegexLexer.get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/textedit.py:259:            if token is Name.Other:
./.venv-build/lib/python3.12/site-packages/pygments/lexers/textedit.py:267:                yield index, token, value
./.venv-build/lib/python3.12/site-packages/pygments/lexers/algebra.py:14:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/algebra.py:41:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/algebra.py:127:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/algebra.py:146:                    yield from do_insertions(insertions, gaplexer.get_tokens_unprocessed(curcode))
./.venv-build/lib/python3.12/site-packages/pygments/lexers/algebra.py:158:            yield from do_insertions(insertions, gaplexer.get_tokens_unprocessed(curcode))
./.venv-build/lib/python3.12/site-packages/pygments/lexers/algebra.py:230:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/algebra.py:260:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/algebra.py:347:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/go.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/go.py:39:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/go.py:180:            # Tokens
./.venv-build/lib/python3.12/site-packages/pygments/lexers/tnt.py:14:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/tnt.py:70:        """Tokenize whitespace."""
./.venv-build/lib/python3.12/site-packages/pygments/lexers/tnt.py:84:        """Tokenize a variable."""
./.venv-build/lib/python3.12/site-packages/pygments/lexers/tnt.py:94:        """Tokenize a term."""
./.venv-build/lib/python3.12/site-packages/pygments/lexers/tnt.py:120:        """Tokenize a formula."""
./.venv-build/lib/python3.12/site-packages/pygments/lexers/tnt.py:154:        """Tokenize a rule."""
./.venv-build/lib/python3.12/site-packages/pygments/lexers/tnt.py:171:        """Tokenize a line referral."""
./.venv-build/lib/python3.12/site-packages/pygments/lexers/tnt.py:200:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/tnt.py:201:        """Returns a list of TNT tokens."""
./.venv-build/lib/python3.12/site-packages/pygments/lexers/dsls.py:24:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/dsls.py:63:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/dsls.py:171:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/dsls.py:410:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/dsls.py:635:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/dsls.py:816:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/dsls.py:952:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/dsls.py:1009:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/dsls.py:1050:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/dsls.py:1118:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/dsls.py:1294:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/dsls.py:1494:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/dsls.py:1579:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/dsls.py:1680:    def get_tokens_unprocessed(self, text=None, context=None):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/dsls.py:1682:        return ExtendedRegexLexer.get_tokens_unprocessed(self, text, context)
./.venv-build/lib/python3.12/site-packages/pygments/lexers/tal.py:14:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/tal.py:76:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/srcinfo.py:15:from pygments.token import Text, Comment, Keyword, Name, Operator, Whitespace
./.venv-build/lib/python3.12/site-packages/pygments/lexers/srcinfo.py:65:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/carbon.py:14:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/carbon.py:43:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/carbon.py:114:            # tokens
./.venv-build/lib/python3.12/site-packages/pygments/lexers/ampl.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/ampl.py:38:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/iolang.py:12:from pygments.token import Comment, Operator, Keyword, Name, String, Number, Whitespace
./.venv-build/lib/python3.12/site-packages/pygments/lexers/iolang.py:28:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/special.py:14:from pygments.token import Token, Error, Text, Generic
./.venv-build/lib/python3.12/site-packages/pygments/lexers/special.py:18:__all__ = ["TextLexer", "OutputLexer", "RawTokenLexer"]
./.venv-build/lib/python3.12/site-packages/pygments/lexers/special.py:35:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/special.py:44:    Simple lexer that highlights everything as ``Token.Generic.Output``.
./.venv-build/lib/python3.12/site-packages/pygments/lexers/special.py:53:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/special.py:60:class RawTokenLexer(Lexer):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/special.py:62:    Recreate a token stream formatted with the `RawTokenFormatter`.
./.venv-build/lib/python3.12/site-packages/pygments/lexers/special.py:67:        If set to ``"gz"`` or ``"bz2"``, decompress the token stream with
./.venv-build/lib/python3.12/site-packages/pygments/lexers/special.py:71:    name = "Raw token data"
./.venv-build/lib/python3.12/site-packages/pygments/lexers/special.py:74:    mimetypes = ["application/x-pygments-tokens"]
./.venv-build/lib/python3.12/site-packages/pygments/lexers/special.py:75:    url = "https://pygments.org/docs/formatters/#RawTokenFormatter"
./.venv-build/lib/python3.12/site-packages/pygments/lexers/special.py:82:    def get_tokens(self, text):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/special.py:100:        # do not call Lexer.get_tokens() because stripping is not optional.
./.venv-build/lib/python3.12/site-packages/pygments/lexers/special.py:102:        for i, t, v in self.get_tokens_unprocessed(text):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/special.py:105:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/special.py:112:                    ttype = Token
./.venv-build/lib/python3.12/site-packages/pygments/lexers/special.py:116:                            raise ValueError("malformed token name")
./.venv-build/lib/python3.12/site-packages/pygments/lexers/modula2.py:15:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/modula2.py:103:    No whitespace is permitted between the tokens of a dialect tag.
./.venv-build/lib/python3.12/site-packages/pygments/lexers/modula2.py:178:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/modula2.py:419:    # Lexemes to Mark as Error Tokens for PIM Modula-2
./.venv-build/lib/python3.12/site-packages/pygments/lexers/modula2.py:482:    # Lexemes to Mark as Error Tokens for ISO Modula-2
./.venv-build/lib/python3.12/site-packages/pygments/lexers/modula2.py:633:    # Lexemes to Mark as Error Tokens for Modula-2 R10
./.venv-build/lib/python3.12/site-packages/pygments/lexers/modula2.py:782:    # Lexemes to Mark as Error Tokens for Objective Modula-2
./.venv-build/lib/python3.12/site-packages/pygments/lexers/modula2.py:1813:    # intercept the token stream, modify token attributes and return them
./.venv-build/lib/python3.12/site-packages/pygments/lexers/modula2.py:1814:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/modula2.py:1815:        for index, token, value in RegexLexer.get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/modula2.py:1818:            if not self.dialect_set_by_tag and token == Comment.Special:
./.venv-build/lib/python3.12/site-packages/pygments/lexers/modula2.py:1821:                    # token is a dialect indicator
./.venv-build/lib/python3.12/site-packages/pygments/lexers/modula2.py:1827:            if token is Name:
./.venv-build/lib/python3.12/site-packages/pygments/lexers/modula2.py:1829:                    token = Keyword.Reserved
./.venv-build/lib/python3.12/site-packages/pygments/lexers/modula2.py:1834:                    token = Name.Builtin
./.venv-build/lib/python3.12/site-packages/pygments/lexers/modula2.py:1839:                    token = Name.Builtin.Pseudo
./.venv-build/lib/python3.12/site-packages/pygments/lexers/modula2.py:1845:                        token = Name.Namespace
./.venv-build/lib/python3.12/site-packages/pygments/lexers/modula2.py:1847:                        token = Name.Builtin.Pseudo
./.venv-build/lib/python3.12/site-packages/pygments/lexers/modula2.py:1852:                    token = Name.Namespace
./.venv-build/lib/python3.12/site-packages/pygments/lexers/modula2.py:1855:                    token = Name.Class
./.venv-build/lib/python3.12/site-packages/pygments/lexers/modula2.py:1858:                    token = Name.Function
./.venv-build/lib/python3.12/site-packages/pygments/lexers/modula2.py:1861:                    token = Name.Variable
./.venv-build/lib/python3.12/site-packages/pygments/lexers/modula2.py:1864:                    token = Name.Constant
./.venv-build/lib/python3.12/site-packages/pygments/lexers/modula2.py:1866:            elif token in Number:
./.venv-build/lib/python3.12/site-packages/pygments/lexers/modula2.py:1871:                        token = Error
./.venv-build/lib/python3.12/site-packages/pygments/lexers/modula2.py:1875:                    if token is Number.Oct:
./.venv-build/lib/python3.12/site-packages/pygments/lexers/modula2.py:1876:                        token = Error
./.venv-build/lib/python3.12/site-packages/pygments/lexers/modula2.py:1878:                    elif token is Number.Hex and "H" in value:
./.venv-build/lib/python3.12/site-packages/pygments/lexers/modula2.py:1879:                        token = Error
./.venv-build/lib/python3.12/site-packages/pygments/lexers/modula2.py:1881:                    elif token is Number.Float and "E" in value:
./.venv-build/lib/python3.12/site-packages/pygments/lexers/modula2.py:1882:                        token = Error
./.venv-build/lib/python3.12/site-packages/pygments/lexers/modula2.py:1884:            elif token in Comment:
./.venv-build/lib/python3.12/site-packages/pygments/lexers/modula2.py:1887:                if token is Comment.Single:
./.venv-build/lib/python3.12/site-packages/pygments/lexers/modula2.py:1889:                        token = Error
./.venv-build/lib/python3.12/site-packages/pygments/lexers/modula2.py:1891:                if token is Comment.Preproc:
./.venv-build/lib/python3.12/site-packages/pygments/lexers/modula2.py:1894:                        token = Error
./.venv-build/lib/python3.12/site-packages/pygments/lexers/modula2.py:1901:                        token = Comment.Multiline
./.venv-build/lib/python3.12/site-packages/pygments/lexers/modula2.py:1903:            else:  # token is neither Name nor Comment
./.venv-build/lib/python3.12/site-packages/pygments/lexers/modula2.py:1905:                # mark lexemes matching the dialect's error token set as errors
./.venv-build/lib/python3.12/site-packages/pygments/lexers/modula2.py:1907:                    token = Error
./.venv-build/lib/python3.12/site-packages/pygments/lexers/modula2.py:1923:            yield index, token, value
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:14:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:72:    token_end = r"""
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:83:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:84:        for index, token, value in super().get_tokens_unprocessed(text):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:85:            if token is Name.Function or token is Name.Variable:
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:91:                    yield index, token, value
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:93:                yield index, token, value
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:186:          # Need to ensure we have a full token. 1+ is not a
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:189:          {token_end}
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:200:            token_type = Number.Float  # includes [+-](inf|nan).0
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:202:            token_type = Number.Integer
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:203:        yield match.start(), token_type, match.group()
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:218:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:285:            (rf"(?x).*?{token_end}", Comment, "#pop"),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:328:    # symbol token, reverse-engineered from hyperspec
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:352:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:354:        for index, token, value in RegexLexer.get_tokens_unprocessed(self, text, stack):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:355:            if token is Name.Variable:
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:377:            yield index, token, value
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:379:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:581:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:626:        "py-keywords": PythonLexer.tokens["keywords"],
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:627:        "py-builtins": PythonLexer.tokens["builtins"],
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:3087:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:3097:            # onto Pygments token types; some judgment calls here.
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:3616:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:3658:    An ELisp lexer, parsing a stream and outputting the tokens
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:3678:    # symbol token, reverse-engineered from hyperspec
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:5215:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:5217:        for index, token, value in RegexLexer.get_tokens_unprocessed(self, text, stack):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:5218:            if token is Name.Variable:
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:5237:            yield index, token, value
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:5239:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:5532:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:5557:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:5558:        tokens = RegexLexer.get_tokens_unprocessed(self, text)
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:5559:        tokens = self._process_symbols(tokens)
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:5560:        tokens = self._process_declarations(tokens)
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:5561:        return tokens
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:5563:    def _relevant(self, token):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:5564:        return token not in (Text, Whitespace, Comment.Single, Comment.Multiline)
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:5566:    def _process_declarations(self, tokens):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:5568:        for index, token, value in tokens:
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:5569:            yield index, token, value
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:5570:            if self._relevant(token):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:5571:                if opening_paren and token == Keyword and value in self.DECLARATIONS:
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:5573:                    yield from self._process_declaration(declaration, tokens)
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:5574:                opening_paren = value == "(" and token == Punctuation
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:5576:    def _process_symbols(self, tokens):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:5578:        for index, token, value in tokens:
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:5579:            if opening_paren and token in (Literal, Name.Variable):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:5580:                token = self.MAPPINGS.get(value, Name.Function)
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:5581:            elif token == Literal and value in self.BUILTINS_ANYWHERE:
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:5582:                token = Name.Builtin
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:5583:            opening_paren = value == "(" and token == Punctuation
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:5584:            yield index, token, value
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:5586:    def _process_declaration(self, declaration, tokens):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:5587:        for index, token, value in tokens:
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:5588:            if self._relevant(token):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:5590:            yield index, token, value
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:5594:            token = Keyword.Type if token == Literal else token
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:5595:            yield index, token, value
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:5596:            for index, token, value in tokens:
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:5597:                if prev_was_colon and token == Literal:
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:5598:                    token = Keyword.Type
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:5599:                yield index, token, value
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:5600:                if self._relevant(token):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:5601:                    prev_was_colon = token == Literal and value == ":"
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:5603:            token = Name.Namespace if token == Literal else token
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:5604:            yield index, token, value
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:5606:            token = Name.Function if token == Literal else token
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:5607:            yield index, token, value
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:5608:            for index, token, value in tokens:
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:5609:                if self._relevant(token):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:5611:                yield index, token, value
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:5612:            if value == "{" and token == Literal:
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:5614:                for index, token, value in self._process_signature(tokens):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:5615:                    yield index, token, value
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:5617:                yield index, token, value
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:5619:            token = Name.Function if token == Literal else token
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:5620:            yield index, token, value
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:5624:    def _process_signature(self, tokens):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:5625:        for index, token, value in tokens:
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:5626:            if token == Literal and value == "}":
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:5629:            elif token in (Literal, Name.Function):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:5630:                token = Name.Variable if value.istitle() else Keyword.Type
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:5631:            yield index, token, value
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:5689:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:6117:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:6331:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:7045:    # _token_end = r'''
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:7055:    _token_end = r"(?=\s|#|[)\]]|$)"
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:7077:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:7120:            (words(constants, suffix=_token_end), Keyword.Constants),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:7124:            (words(builtin_variables, suffix=_token_end), Name.Variable.Global),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:7126:                words(special_forms, prefix=r"(?<=\()", suffix=_token_end),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:7129:            (words(builtin_macros, prefix=r"(?<=\()", suffix=_token_end), Name.Builtin),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lisp.py:7131:                words(builtin_functions, prefix=r"(?<=\()", suffix=_token_end),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/sieve.py:21:from pygments.token import Comment, Name, Literal, String, Text, Punctuation, Keyword
./.venv-build/lib/python3.12/site-packages/pygments/lexers/sieve.py:37:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/sieve.py:52:            # tokens:
./.venv-build/lib/python3.12/site-packages/pygments/lexers/factor.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/factor.py:803:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/meson.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/meson.py:45:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_cocoa_builtins.py:93:    "ASAccountAuthenticationModificationReplacePasswordWithSignInWithAppleRequest",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_cocoa_builtins.py:95:    "ASAccountAuthenticationModificationUpgradePasswordToStrongPasswordRequest",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_cocoa_builtins.py:104:    "ASAuthorizationPasswordProvider",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_cocoa_builtins.py:105:    "ASAuthorizationPasswordRequest",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_cocoa_builtins.py:117:    "ASPasswordCredential",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_cocoa_builtins.py:118:    "ASPasswordCredentialIdentity",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_cocoa_builtins.py:409:    "CKFetchWebAuthTokenOperation",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_cocoa_builtins.py:434:    "CKServerChangeToken",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_cocoa_builtins.py:951:    "HMAccessoryOwnershipToken",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_cocoa_builtins.py:1840:    "NIDiscoveryToken",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_cocoa_builtins.py:1853:    "NLTokenizer",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_cocoa_builtins.py:2031:    "NSPersistentHistoryToken",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_cocoa_builtins.py:2053:    "NSQueryGenerationToken",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_cocoa_builtins.py:2264:    "PKPaymentToken",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_cocoa_builtins.py:2488:    "TKSmartCardToken",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_cocoa_builtins.py:2489:    "TKSmartCardTokenDriver",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_cocoa_builtins.py:2490:    "TKSmartCardTokenSession",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_cocoa_builtins.py:2496:    "TKToken",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_cocoa_builtins.py:2497:    "TKTokenAuthOperation",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_cocoa_builtins.py:2498:    "TKTokenConfiguration",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_cocoa_builtins.py:2499:    "TKTokenDriver",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_cocoa_builtins.py:2500:    "TKTokenDriverConfiguration",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_cocoa_builtins.py:2501:    "TKTokenKeyAlgorithm",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_cocoa_builtins.py:2502:    "TKTokenKeyExchangeParameters",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_cocoa_builtins.py:2503:    "TKTokenKeychainCertificate",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_cocoa_builtins.py:2504:    "TKTokenKeychainContents",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_cocoa_builtins.py:2505:    "TKTokenKeychainItem",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_cocoa_builtins.py:2506:    "TKTokenKeychainKey",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_cocoa_builtins.py:2507:    "TKTokenPasswordAuthOperation",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_cocoa_builtins.py:2508:    "TKTokenSession",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_cocoa_builtins.py:2509:    "TKTokenSmartCardPINAuthOperation",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_cocoa_builtins.py:2510:    "TKTokenWatcher",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_cocoa_builtins.py:2751:    "UISearchToken",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_cocoa_builtins.py:2796:    "UITextInputPasswordRules",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_cocoa_builtins.py:2797:    "UITextInputStringTokenizer",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_cocoa_builtins.py:3591:    "TKSmartCardTokenDriverDelegate",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_cocoa_builtins.py:3593:    "TKTokenDelegate",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_cocoa_builtins.py:3594:    "TKTokenDriverDelegate",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_cocoa_builtins.py:3595:    "TKTokenSessionDelegate",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_cocoa_builtins.py:3726:    "UITextInputTokenizer",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_cocoa_builtins.py:4124:    "opaqueCMBufferQueueTriggerToken",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/codeql.py:19:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/codeql.py:41:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/macaulay2.py:12:from pygments.token import Comment, Keyword, Name, String, Text
./.venv-build/lib/python3.12/site-packages/pygments/lexers/macaulay2.py:1786:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/fift.py:12:from pygments.token import Literal, Comment, Name, String, Number, Whitespace
./.venv-build/lib/python3.12/site-packages/pygments/lexers/fift.py:28:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/pony.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/pony.py:39:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/parsers.py:14:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/parsers.py:72:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/parsers.py:167:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/parsers.py:367:    _TOKEN_REF = r"[A-Z]\w*"
./.venv-build/lib/python3.12/site-packages/pygments/lexers/parsers.py:372:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/parsers.py:389:            # tokensSpec
./.venv-build/lib/python3.12/site-packages/pygments/lexers/parsers.py:390:            (r"tokens\b", Keyword, "tokens"),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/parsers.py:469:            # Tokens start with capital letter.
./.venv-build/lib/python3.12/site-packages/pygments/lexers/parsers.py:480:        "tokens": [
./.venv-build/lib/python3.12/site-packages/pygments/lexers/parsers.py:485:                r"(" + _TOKEN_REF + r")(\s*)(=)?(\s*)(" + _STRING_LITERAL + r")?(\s*)(;)",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/parsers.py:756:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/parsers.py:848:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/css.py:23:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/css.py:698:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/css.py:847:common_sass_tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/css.py:1151:def _starts_block(token, state):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/css.py:1153:        yield match.start(), token, match.group(0)
./.venv-build/lib/python3.12/site-packages/pygments/lexers/css.py:1180:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/css.py:1244:    for group, common in common_sass_tokens.items():
./.venv-build/lib/python3.12/site-packages/pygments/lexers/css.py:1245:        tokens[group] = copy.copy(common)
./.venv-build/lib/python3.12/site-packages/pygments/lexers/css.py:1246:    tokens["value"].append((r"\n", Whitespace, "root"))
./.venv-build/lib/python3.12/site-packages/pygments/lexers/css.py:1247:    tokens["selector"].append((r"\n", Whitespace, "root"))
./.venv-build/lib/python3.12/site-packages/pygments/lexers/css.py:1263:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/css.py:1294:    for group, common in common_sass_tokens.items():
./.venv-build/lib/python3.12/site-packages/pygments/lexers/css.py:1295:        tokens[group] = copy.copy(common)
./.venv-build/lib/python3.12/site-packages/pygments/lexers/css.py:1296:    tokens["value"].extend([(r"\n", Whitespace), (r"[;{}]", Punctuation, "#pop")])
./.venv-build/lib/python3.12/site-packages/pygments/lexers/css.py:1297:    tokens["selector"].extend([(r"\n", Whitespace), (r"[;{}]", Punctuation, "#pop")])
./.venv-build/lib/python3.12/site-packages/pygments/lexers/css.py:1312:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_sourcemod_builtins.py:403:    "SetAdminPassword",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_sourcemod_builtins.py:404:    "GetAdminPassword",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/stata.py:13:from pygments.token import Comment, Keyword, Name, Number, String, Text, Operator
./.venv-build/lib/python3.12/site-packages/pygments/lexers/stata.py:38:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/foxpro.py:14:from pygments.token import Punctuation, Text, Comment, Operator, Keyword, Name, String
./.venv-build/lib/python3.12/site-packages/pygments/lexers/foxpro.py:35:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/foxpro.py:228:                r"Parent|Partition|PasswordChar|PictureMargin|"
./.venv-build/lib/python3.12/site-packages/pygments/lexers/crystal.py:21:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/crystal.py:101:            yield from self.get_tokens_unprocessed(context=ctx)
./.venv-build/lib/python3.12/site-packages/pygments/lexers/crystal.py:217:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/crystal.py:491:    tokens.update(gen_crystalstrings_rules())
./.venv-build/lib/python3.12/site-packages/pygments/lexers/gdscript.py:17:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/gdscript.py:59:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/soong.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/soong.py:32:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/asc.py:14:from pygments.token import Comment, Generic, Name, Operator, String, Whitespace
./.venv-build/lib/python3.12/site-packages/pygments/lexers/asc.py:48:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/rust.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/rust.py:169:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/markup.py:30:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/markup.py:75:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/markup.py:109:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/markup.py:206:        yield from do_insertions(ins, lexer.get_tokens_unprocessed(code))
./.venv-build/lib/python3.12/site-packages/pygments/lexers/markup.py:215:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/markup.py:377:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/markup.py:437:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/markup.py:494:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/markup.py:555:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/markup.py:650:            yield from do_insertions([], lexer.get_tokens_unprocessed(code))
./.venv-build/lib/python3.12/site-packages/pygments/lexers/markup.py:654:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/markup.py:768:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/markup.py:855:            (_inline(r"=", r"="), String),  # TODO token
./.venv-build/lib/python3.12/site-packages/pygments/lexers/markup.py:925:        yield from do_insertions([], lexer.get_tokens_unprocessed(code))
./.venv-build/lib/python3.12/site-packages/pygments/lexers/markup.py:952:        yield from do_insertions([], lexer.get_tokens_unprocessed(code))
./.venv-build/lib/python3.12/site-packages/pygments/lexers/markup.py:956:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/markup.py:1106:    def text_rules(token):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/markup.py:1108:            (r"\w+", token),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/markup.py:1109:            (r"[^\S\n]+", token),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/markup.py:1110:            (r"(?s).", token),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/markup.py:1128:            yield from self.get_tokens_unprocessed(attr_content, stack=["root", "attr"])
./.venv-build/lib/python3.12/site-packages/pygments/lexers/markup.py:1131:        yield from self.get_tokens_unprocessed(attr, stack=["root", "attr"])
./.venv-build/lib/python3.12/site-packages/pygments/lexers/markup.py:1149:            yield from lexer.get_tokens_unprocessed(content)
./.venv-build/lib/python3.12/site-packages/pygments/lexers/markup.py:1164:            yield from self.get_tokens_unprocessed(attr_content, stack=["root", "attr"])
./.venv-build/lib/python3.12/site-packages/pygments/lexers/markup.py:1168:        yield from self.get_tokens_unprocessed(attr, stack=["root", "attr"])
./.venv-build/lib/python3.12/site-packages/pygments/lexers/markup.py:1176:            yield from LilyPondLexer().get_tokens_unprocessed(content)
./.venv-build/lib/python3.12/site-packages/pygments/lexers/markup.py:1577:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/rego.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/rego.py:61:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/prolog.py:14:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/prolog.py:40:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/prolog.py:122:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/forth.py:14:from pygments.token import Text, Comment, Keyword, Name, String, Number, Whitespace
./.venv-build/lib/python3.12/site-packages/pygments/lexers/forth.py:34:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/dalvik.py:14:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/dalvik.py:41:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/console.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/console.py:40:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/console.py:67:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/console.py:87:            (r"(None|descr|ConstClass|ConstPtr|TargetToken)", Name),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/console.py:100:                r"force_token|quasiimmut_field|same_as|virtual_ref_finish|"
./.venv-build/lib/python3.12/site-packages/pygments/lexers/urbi.py:14:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/urbi.py:43:    # - handle Experimental and deprecated tags with specific tokens
./.venv-build/lib/python3.12/site-packages/pygments/lexers/urbi.py:44:    # - handle Angles and Durations with specific tokens
./.venv-build/lib/python3.12/site-packages/pygments/lexers/urbi.py:67:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/urbi.py:155:            # deprecated keywords, use a meaningful token when available
./.venv-build/lib/python3.12/site-packages/pygments/lexers/urbi.py:157:            # ignored keywords, use a meaningful token when available
./.venv-build/lib/python3.12/site-packages/pygments/lexers/kusto.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/kusto.py:146:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/typoscript.py:23:from pygments.token import Text, Comment, Name, String, Number, Operator, Punctuation
./.venv-build/lib/python3.12/site-packages/pygments/lexers/typoscript.py:38:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/typoscript.py:85:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/typoscript.py:134:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/unicon.py:14:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/unicon.py:42:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/unicon.py:471:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/unicon.py:851:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/yang.py:12:from pygments.token import Text, Token, Name, String, Comment, Number
./.venv-build/lib/python3.12/site-packages/pygments/lexers/yang.py:142:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/yang.py:151:            (r"[{};]+", Token.Punctuation),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/yang.py:152:            (r"(?<![\-\w])(and|or|not|\+|\.)(?![\-\w])", Token.Operator),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/yang.py:160:                bygroups(Name.Namespace, Token.Punctuation, Name.Variable),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/yang.py:166:            (words(TOP_STMTS_KEYWORDS, suffix=suffix_re_pattern), Token.Keyword),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/yang.py:169:                Token.Keyword,
./.venv-build/lib/python3.12/site-packages/pygments/lexers/yang.py:171:            (words(META_STMT_KEYWORDS, suffix=suffix_re_pattern), Token.Keyword),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/yang.py:172:            (words(LINKAGE_STMTS_KEYWORDS, suffix=suffix_re_pattern), Token.Keyword),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/yang.py:173:            (words(BODY_STMT_KEYWORDS, suffix=suffix_re_pattern), Token.Keyword),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/yang.py:174:            (words(DATA_DEF_STMT_KEYWORDS, suffix=suffix_re_pattern), Token.Keyword),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/yang.py:175:            (words(TYPE_STMT_KEYWORDS, suffix=suffix_re_pattern), Token.Keyword),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/yang.py:176:            (words(LIST_STMT_KEYWORDS, suffix=suffix_re_pattern), Token.Keyword),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lilypond.py:38:from pygments.token import Token
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lilypond.py:42:# In LilyPond, (unquoted) name tokens only contain letters, hyphens,
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lilypond.py:44:# a name token.
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lilypond.py:93:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lilypond.py:95:        for index, token, value in super().get_tokens_unprocessed(text):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lilypond.py:96:            if token is Token.Name.Function or token is Token.Name.Variable:
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lilypond.py:98:                    token = Token.Name.Builtin.SchemeFunction
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lilypond.py:99:            elif token is Token.Name.Builtin:
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lilypond.py:100:                token = Token.Name.Builtin.SchemeBuiltin
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lilypond.py:101:            yield index, token, value
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lilypond.py:103:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lilypond.py:106:            (r"\s+", Token.Text.Whitespace),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lilypond.py:108:            (r"%\{.*?%\}", Token.Comment.Multiline),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lilypond.py:110:            (r"%.*?$", Token.Comment.Single),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lilypond.py:112:            (r"#\}", Token.Punctuation, "#pop"),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lilypond.py:116:            (r"[#$]@?", Token.Punctuation, "value"),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lilypond.py:136:                Token.Punctuation,
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lilypond.py:140:            (words(pitches, suffix=r"=?[',]*!?\??" + NAME_END_RE), Token.Pitch),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lilypond.py:142:            (r'[\-_^]?"', Token.String, "string"),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lilypond.py:144:            (r"-?\d+\.\d+", Token.Number.Float),  # 5. and .5 are not allowed
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lilypond.py:145:            (r"-?\d+/\d+", Token.Number.Fraction),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lilypond.py:160:                Token.Number,
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lilypond.py:163:            (r"\*", Token.Number),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lilypond.py:165:            (r"[~()[\]]", Token.Name.Builtin.Articulation),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lilypond.py:168:            (r"[\-_^][>^_!.\-+]", Token.Name.Builtin.Articulation),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lilypond.py:170:            (r"[\-_^]?\\?\d+", Token.Name.Builtin.Articulation),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lilypond.py:172:            (builtin_words(keywords, "mandatory"), Token.Keyword),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lilypond.py:175:                Token.Name.PitchLanguage,
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lilypond.py:177:            (builtin_words(clefs, "disallowed"), Token.Name.Builtin.Clef),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lilypond.py:178:            (builtin_words(scales, "mandatory"), Token.Name.Builtin.Scale),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lilypond.py:179:            (builtin_words(repeat_types, "disallowed"), Token.Name.Builtin.RepeatType),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lilypond.py:180:            (builtin_words(units, "mandatory"), Token.Number),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lilypond.py:181:            (builtin_words(chord_modifiers, "disallowed"), Token.ChordModifier),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lilypond.py:184:                Token.Name.Builtin.MusicFunction,
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lilypond.py:186:            (builtin_words(dynamics, "mandatory"), Token.Name.Builtin.Dynamic),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lilypond.py:190:                Token.Name.Builtin.Articulation,
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lilypond.py:194:                Token.Name.Builtin.MusicCommand,
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lilypond.py:198:                Token.Name.Builtin.MarkupCommand,
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lilypond.py:200:            (builtin_words(grobs, "disallowed"), Token.Name.Builtin.Grob),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lilypond.py:201:            (builtin_words(translators, "disallowed"), Token.Name.Builtin.Translator),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lilypond.py:203:            (builtin_words(contexts, "optional"), Token.Name.Builtin.Context),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lilypond.py:206:                Token.Name.Builtin.ContextProperty,
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lilypond.py:210:                Token.Name.Builtin.GrobProperty,
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lilypond.py:218:                Token.Name.Builtin.PaperVariable,
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lilypond.py:222:                Token.Name.Builtin.HeaderVariable,
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lilypond.py:226:            (r"[\-_^]?\\.+?" + NAME_END_RE, Token.Name.BackslashReference),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lilypond.py:234:                Token.Name.Lvalue,
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lilypond.py:239:            (r"([^\W\d]|-)+?" + NAME_END_RE, Token.Text),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lilypond.py:240:            (r".", Token.Text),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lilypond.py:243:            (r'"', Token.String, "#pop"),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lilypond.py:244:            (r"\\.", Token.String.Escape),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lilypond.py:245:            (r'[^\\"]+', Token.String),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lilypond.py:250:            (r"#\{", Token.Punctuation, ("#pop", "root")),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lilypond.py:258:            (r"\s+", Token.Text.Whitespace),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lilypond.py:261:                bygroups(Token.Punctuation, Token.Name.Builtin.GrobProperty),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/modeling.py:14:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/modeling.py:48:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/modeling.py:247:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/modeling.py:275:            # SLexer makes these tokens Operators.
./.venv-build/lib/python3.12/site-packages/pygments/lexers/modeling.py:399:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/modeling.py:469:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/vyper.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/vyper.py:36:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/clean.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/clean.py:77:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/gleam.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/gleam.py:66:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/jsonnet.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/jsonnet.py:26:jsonnet_token = r"[^\W\d]\w*"
./.venv-build/lib/python3.12/site-packages/pygments/lexers/jsonnet.py:27:jsonnet_function_token = jsonnet_token + r"(?=\()"
./.venv-build/lib/python3.12/site-packages/pygments/lexers/jsonnet.py:50:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/jsonnet.py:98:            (r"std\." + jsonnet_function_token, Name.Builtin, "function_args"),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/jsonnet.py:99:            (jsonnet_function_token, Name.Function, "function_args"),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/jsonnet.py:100:            (jsonnet_token, Name.Variable),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/jsonnet.py:111:            (jsonnet_function_token, Name.Function, "function_params"),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/jsonnet.py:112:            (jsonnet_token, Name.Variable),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/jsonnet.py:127:            (jsonnet_token, Name.Variable),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/jsonnet.py:146:            (rf"(?={jsonnet_token})", Text, "field_name"),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/jsonnet.py:154:                jsonnet_function_token,
./.venv-build/lib/python3.12/site-packages/pygments/lexers/jsonnet.py:158:            (jsonnet_token, Name.Variable, "field_separator"),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/jsonnet.py:186:            (jsonnet_token, Name.Variable, ("#pop", "object_local_value")),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/hdl.py:14:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/hdl.py:44:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/hdl.py:372:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/hdl.py:971:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/j.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/j.py:40:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/mips.py:12:from pygments.token import Whitespace, Comment, String, Keyword, Name, Text
./.venv-build/lib/python3.12/site-packages/pygments/lexers/mips.py:278:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/capnproto.py:12:from pygments.token import Text, Comment, Keyword, Name, Literal, Whitespace
./.venv-build/lib/python3.12/site-packages/pygments/lexers/capnproto.py:28:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/qvt.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/qvt.py:35:    Notable tokens assignments:
./.venv-build/lib/python3.12/site-packages/pygments/lexers/qvt.py:55:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/mime.py:15:from pygments.token import Text, Name, String, Operator, Comment, Other
./.venv-build/lib/python3.12/site-packages/pygments/lexers/mime.py:64:    def get_header_tokens(self, match):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/mime.py:73:            for i, t, v in self.get_tokens_unprocessed(body, ("root", field.lower())):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/mime.py:79:    def get_body_tokens(self, match):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/mime.py:91:            for i, t, v in self.get_bodypart_tokens(entire_body):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/mime.py:110:        # process tokens of each body part
./.venv-build/lib/python3.12/site-packages/pygments/lexers/mime.py:116:            for i, t, v in self.get_bodypart_tokens(part):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/mime.py:128:    def get_bodypart_tokens(self, text):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/mime.py:153:        return lexer.get_tokens_unprocessed(text)
./.venv-build/lib/python3.12/site-packages/pygments/lexers/mime.py:164:    def get_content_type_subtokens(self, match):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/mime.py:183:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/mime.py:185:            (r"^([\w-]+):( *)([\s\S]*?\n)(?![ \t])", get_header_tokens),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/mime.py:186:            (r"^$[\s\S]+", get_body_tokens),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/mime.py:202:                get_content_type_subtokens,
./.venv-build/lib/python3.12/site-packages/pygments/lexers/asm.py:24:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/asm.py:74:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/asm.py:146:def _objdump_lexer_tokens(asm_lexer):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/asm.py:148:    Common objdump lexer tokens to wrap an ASM lexer.
./.venv-build/lib/python3.12/site-packages/pygments/lexers/asm.py:256:    tokens = _objdump_lexer_tokens(GasLexer)
./.venv-build/lib/python3.12/site-packages/pygments/lexers/asm.py:349:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/asm.py:668:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/asm.py:1012:                        "token",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/asm.py:1106:                        "token",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/asm.py:1130:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/asm.py:1329:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/asm.py:1337:            # Consume everything else in one token for efficiency
./.venv-build/lib/python3.12/site-packages/pygments/lexers/asm.py:1455:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/asm.py:1521:    tokens = _objdump_lexer_tokens(NasmLexer)
./.venv-build/lib/python3.12/site-packages/pygments/lexers/asm.py:1562:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/asm.py:1634:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/asm.py:1746:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/xorg.py:12:from pygments.token import Comment, String, Name, Text
./.venv-build/lib/python3.12/site-packages/pygments/lexers/xorg.py:27:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/elpi.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/elpi.py:53:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/q.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/q.py:38:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/q.py:228:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:28:from pygments.token import Token
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:33:HEADING = Token.Generic.Heading
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:34:SETTING = Token.Keyword.Namespace
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:35:IMPORT = Token.Name.Namespace
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:36:TC_KW_NAME = Token.Generic.Subheading
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:37:KEYWORD = Token.Name.Function
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:38:ARGUMENT = Token.String
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:39:VARIABLE = Token.Name.Variable
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:40:COMMENT = Token.Comment
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:41:SEPARATOR = Token.Punctuation
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:42:SYNTAX = Token.Punctuation
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:43:GHERKIN = Token.Generic.Emph
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:44:ERROR = Token.Error
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:74:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:75:        row_tokenizer = RowTokenizer()
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:76:        var_tokenizer = VariableTokenizer()
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:79:            for value, token in row_tokenizer.tokenize(row):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:80:                for value, token in var_tokenizer.tokenize(value, token):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:82:                        yield index, token, str(value)
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:86:class VariableTokenizer:
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:87:    def tokenize(self, string, token):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:89:        if var.start < 0 or token in (COMMENT, ERROR):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:90:            yield string, token
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:92:        for value, token in self._tokenize(var, string, token):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:94:                yield value, token
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:96:    def _tokenize(self, var, string, orig_token):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:98:        yield before, orig_token
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:100:        yield from self.tokenize(var.base, VARIABLE)
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:104:            yield from self.tokenize(var.index, VARIABLE)
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:106:        yield from self.tokenize(string[var.end :], orig_token)
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:109:class RowTokenizer:
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:133:    def tokenize(self, row):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:144:            yield from self._tokenize(value, index, commented, separator, heading)
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:151:    def _tokenize(self, value, index, commented, separator, heading):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:159:            yield from self._table.tokenize(value, index)
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:185:class Tokenizer:
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:186:    _tokens = None
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:191:    def tokenize(self, value):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:192:        values_and_tokens = self._tokenize(value, self._index)
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:194:        if isinstance(values_and_tokens, type(Token)):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:195:            values_and_tokens = [(value, values_and_tokens)]
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:196:        return values_and_tokens
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:198:    def _tokenize(self, value, index):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:199:        index = min(index, len(self._tokens) - 1)
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:200:        return self._tokens[index]
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:209:class Comment(Tokenizer):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:210:    _tokens = (COMMENT,)
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:213:class Setting(Tokenizer):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:214:    _tokens = (SETTING, ARGUMENT)
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:238:    _custom_tokenizer = None
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:241:        Tokenizer.__init__(self)
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:244:    def _tokenize(self, value, index):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:250:                self._custom_tokenizer = KeywordCall(support_assign=False)
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:252:                self._custom_tokenizer = ImportSetting()
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:255:        elif self._custom_tokenizer:
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:256:            return self._custom_tokenizer.tokenize(value)
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:257:        return Tokenizer._tokenize(self, value, index)
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:260:class ImportSetting(Tokenizer):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:261:    _tokens = (IMPORT, ARGUMENT)
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:275:    def _tokenize(self, value, index):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:277:            type = Setting._tokenize(self, value[1:-1], index)
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:279:        return Setting._tokenize(self, value, index)
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:287:class Variable(Tokenizer):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:288:    _tokens = (SYNTAX, ARGUMENT)
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:290:    def _tokenize(self, value, index):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:293:        return Tokenizer._tokenize(self, value, index)
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:296:class KeywordCall(Tokenizer):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:297:    _tokens = (KEYWORD, ARGUMENT)
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:300:        Tokenizer.__init__(self)
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:304:    def _tokenize(self, value, index):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:307:            return SYNTAX  # VariableTokenizer tokenizes this later.
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:309:            return Tokenizer._tokenize(self, value, index - self._assigns)
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:311:        return GherkinTokenizer().tokenize(value, KEYWORD)
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:314:class GherkinTokenizer:
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:317:    def tokenize(self, value, token):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:320:            return [(value, token)]
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:322:        return [(value[:end], GHERKIN), (value[end:], token)]
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:325:class TemplatedKeywordCall(Tokenizer):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:326:    _tokens = (ARGUMENT,)
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:329:class ForLoop(Tokenizer):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:331:        Tokenizer.__init__(self)
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:334:    def _tokenize(self, value, index):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:335:        token = self._in_arguments and ARGUMENT or SYNTAX
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:338:        return token
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:342:    _tokenizer_class = None
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:344:    def __init__(self, prev_tokenizer=None):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:345:        self._tokenizer = self._tokenizer_class()
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:346:        self._prev_tokenizer = prev_tokenizer
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:349:    def tokenize(self, value, index):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:351:            self._tokenizer = self._prev_tokenizer
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:354:            yield from self._tokenize(value, index)
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:363:    def _tokenize(self, value, index):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:364:        return self._tokenizer.tokenize(value)
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:367:        self.__init__(prev_tokenizer=self._tokenizer)
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:371:    _tokenizer_class = Comment
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:378:    _tokenizer_class = Variable
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:382:    _tokenizer_class = Setting
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:384:    def __init__(self, template_setter, prev_tokenizer=None):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:385:        _Table.__init__(self, prev_tokenizer)
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:388:    def _tokenize(self, value, index):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:390:            self._tokenizer = Setting(self._template_setter)
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:391:        return _Table._tokenize(self, value, index)
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:394:        self.__init__(self._template_setter, prev_tokenizer=self._tokenizer)
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:403:    def _tokenizer_class(self):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:411:    def _tokenize(self, value, index):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:415:            return GherkinTokenizer().tokenize(value, TC_KW_NAME)
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:419:                self._tokenizer = self._setting_class(self.set_test_template)
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:421:                self._tokenizer = self._setting_class()
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:423:            self._tokenizer = ForLoop()
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:426:        return _Table._tokenize(self, value, index)
./.venv-build/lib/python3.12/site-packages/pygments/lexers/robotframework.py:448:    _tokenizer_class = KeywordCall
./.venv-build/lib/python3.12/site-packages/pygments/lexers/prql.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/prql.py:81:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/apdlexer.py:14:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/apdlexer.py:2161:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/ptx.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/ptx.py:45:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:14:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:53:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:614:                        "tokens",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:711:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:713:        # If the token two tokens after 'in' is ')', 'in' is a keyword:
./.venv-build/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:718:        objectloop_token_count = -1
./.venv-build/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:719:        previous_token = None
./.venv-build/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:720:        for index, token, value in RegexLexer.get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:721:            if previous_token is Name.Variable and value == "in":
./.venv-build/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:722:                objectloop_queue = [[index, token, value]]
./.venv-build/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:723:                objectloop_token_count = 2
./.venv-build/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:724:            elif objectloop_token_count > 0:
./.venv-build/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:725:                if token not in Comment and token not in Text:
./.venv-build/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:726:                    objectloop_token_count -= 1
./.venv-build/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:727:                objectloop_queue.append((index, token, value))
./.venv-build/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:729:                if objectloop_token_count == 0:
./.venv-build/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:734:                    objectloop_token_count = -1
./.venv-build/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:735:                yield index, token, value
./.venv-build/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:736:            if token not in Comment and token not in Text:
./.venv-build/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:737:                previous_token = token
./.venv-build/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:772:    # and use options, tokens in braces are treated as I7. Use options
./.venv-build/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:774:    tokens = {}
./.venv-build/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:775:    token_variants = ["+i6t-not-inline", "+i6t-inline", "+i6t-use-option"]
./.venv-build/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:777:    for level in token_variants:
./.venv-build/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:778:        tokens[level] = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:779:            "+i6-root": list(Inform6Lexer.tokens["root"]),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:968:        for token in Inform6Lexer.tokens:
./.venv-build/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:969:            if token == "root":
./.venv-build/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:971:            tokens[level][token] = list(Inform6Lexer.tokens[token])
./.venv-build/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:972:            if not token.startswith("_"):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:973:                tokens[level][token][:0] = [include("+i6t"), include(level)]
./.venv-build/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:977:        if level not in self._all_tokens:
./.venv-build/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:978:            self._tokens = self.__class__.process_tokendef(level)
./.venv-build/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:980:            self._tokens = self._all_tokens[level]
./.venv-build/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:994:    def get_tokens_unprocessed(self, text, stack=("+i6t-root",)):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:995:        return Inform7Lexer.get_tokens_unprocessed(self, text, stack)
./.venv-build/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:1026:        token = String.Double if double else String.Single
./.venv-build/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:1033:                (rf"{char}{{3,}}", token, "#pop"),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:1035:                (char, token),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:1038:            state.append((char, token, "#pop"))
./.venv-build/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:1039:        state += [include("s/verbatim"), (rf"[^\\<&{{}}{char}]+", token)]
./.venv-build/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:1079:            (r"[\\&{}<]", token),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:1087:        token = String.Double if double else String.Single
./.venv-build/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:1090:            (rf"{char}{quantifier}", token, "#pop:2"),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:1111:        token = (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:1118:        host_token = String.Double if host_double else String.Single
./.venv-build/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:1121:            (rf"{host_char}{host_quantifier}", host_token, "#pop:3"),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:1123:                r"{}{}".format(r"" if token is String.Other else r"\\?", terminator),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:1124:                token,
./.venv-build/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:1133:            (r'([^\s"\'<%s{}\\&])+' % (r">" if token is String.Other else r""), token),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:1135:            (r'["\'\s&{<}\\]', token),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:1138:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:1268:            # Two-token keywords
./.venv-build/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:1441:            (r"token\b", Keyword, ("#pop", "constants")),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:1626:    def get_tokens_unprocessed(self, text, **kwargs):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:1629:        for index, token, value in RegexLexer.get_tokens_unprocessed(self, text, **kwargs):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:1631:                if token is Comment.Preproc and re.match(
./.venv-build/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:1636:                if token is Comment.Preproc:
./.venv-build/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:1644:                    token = Comment
./.venv-build/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:1645:            yield index, token, value
./.venv-build/lib/python3.12/site-packages/pygments/lexers/diff.py:14:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/diff.py:40:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/diff.py:88:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/diff.py:183:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/webmisc.py:21:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/webmisc.py:56:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/webmisc.py:79:    An XQuery lexer, parsing a stream and outputting the tokens needed to
./.venv-build/lib/python3.12/site-packages/pygments/lexers/webmisc.py:339:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/webmisc.py:977:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/webmisc.py:1060:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/webmisc.py:1109:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_php_builtins.py:1743:        "oci_password_change",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_php_builtins.py:1896:        "openssl_x509_check_private_key",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_php_builtins.py:2137:    "Password Hashing": (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_php_builtins.py:2138:        "password_algos",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_php_builtins.py:2139:        "password_get_info",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_php_builtins.py:2140:        "password_hash",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_php_builtins.py:2141:        "password_needs_rehash",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_php_builtins.py:2142:        "password_verify",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_php_builtins.py:2312:        "radius_server_secret",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_php_builtins.py:2409:        "ssh2_auth_password",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_php_builtins.py:2615:        "sodium_crypto_box_keypair_from_secretkey_and_publickey",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_php_builtins.py:2618:        "sodium_crypto_box_publickey_from_secretkey",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_php_builtins.py:2622:        "sodium_crypto_box_secretkey",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_php_builtins.py:2635:        "sodium_crypto_kx_secretkey",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_php_builtins.py:2647:        "sodium_crypto_secretbox_keygen",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_php_builtins.py:2648:        "sodium_crypto_secretbox_open",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_php_builtins.py:2649:        "sodium_crypto_secretbox",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_php_builtins.py:2650:        "sodium_crypto_secretstream_xchacha20poly1305_init_pull",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_php_builtins.py:2651:        "sodium_crypto_secretstream_xchacha20poly1305_init_push",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_php_builtins.py:2652:        "sodium_crypto_secretstream_xchacha20poly1305_keygen",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_php_builtins.py:2653:        "sodium_crypto_secretstream_xchacha20poly1305_pull",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_php_builtins.py:2654:        "sodium_crypto_secretstream_xchacha20poly1305_push",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_php_builtins.py:2655:        "sodium_crypto_secretstream_xchacha20poly1305_rekey",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_php_builtins.py:2661:        "sodium_crypto_sign_keypair_from_secretkey_and_publickey",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_php_builtins.py:2664:        "sodium_crypto_sign_publickey_from_secretkey",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_php_builtins.py:2666:        "sodium_crypto_sign_secretkey",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_php_builtins.py:2873:    "Tokenizer": ("token_get_all", "token_name"),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/maxima.py:16:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/maxima.py:96:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/graphviz.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/graphviz.py:38:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/tls.py:14:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/tls.py:42:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/tls.py:61:            # tokens
./.venv-build/lib/python3.12/site-packages/pygments/lexers/graph.py:14:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/graph.py:44:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/haxe.py:14:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/haxe.py:97:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/haxe.py:270:        # same as 'ident' but set token as Name.Decorator instead of Name
./.venv-build/lib/python3.12/site-packages/pygments/lexers/haxe.py:912:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/gsql.py:14:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/gsql.py:41:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/igor.py:14:from pygments.token import Text, Comment, Keyword, Name, String, Whitespace
./.venv-build/lib/python3.12/site-packages/pygments/lexers/igor.py:1618:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/floscript.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/floscript.py:53:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/rebol.py:14:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/rebol.py:136:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/rebol.py:349:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/ncl.py:14:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/ncl.py:42:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/templates.py:33:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/templates.py:45:    Token,
./.venv-build/lib/python3.12/site-packages/pygments/lexers/templates.py:141:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/templates.py:147:        tokens = self._block_re.split(text)
./.venv-build/lib/python3.12/site-packages/pygments/lexers/templates.py:148:        tokens.reverse()
./.venv-build/lib/python3.12/site-packages/pygments/lexers/templates.py:154:                    val = tokens.pop()
./.venv-build/lib/python3.12/site-packages/pygments/lexers/templates.py:160:                    tag = tokens.pop()
./.venv-build/lib/python3.12/site-packages/pygments/lexers/templates.py:169:                        val = tokens.pop()
./.venv-build/lib/python3.12/site-packages/pygments/lexers/templates.py:177:                        data = tokens.pop()
./.venv-build/lib/python3.12/site-packages/pygments/lexers/templates.py:181:                            r_token,
./.venv-build/lib/python3.12/site-packages/pygments/lexers/templates.py:183:                        ) in self.ruby_lexer.get_tokens_unprocessed(data):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/templates.py:184:                            yield r_idx + idx, r_token, r_value
./.venv-build/lib/python3.12/site-packages/pygments/lexers/templates.py:197:                            r_token,
./.venv-build/lib/python3.12/site-packages/pygments/lexers/templates.py:199:                        ) in self.ruby_lexer.get_tokens_unprocessed(tag[1:]):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/templates.py:200:                            yield idx + 1 + r_idx, r_token, r_value
./.venv-build/lib/python3.12/site-packages/pygments/lexers/templates.py:205:                    tag = tokens.pop()
./.venv-build/lib/python3.12/site-packages/pygments/lexers/templates.py:237:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/templates.py:301:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/templates.py:432:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/templates.py:535:    markup is yielded as `Token.Other`.
./.venv-build/lib/python3.12/site-packages/pygments/lexers/templates.py:545:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/templates.py:669:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/templates.py:724:    markup is yielded as `Token.Other`.
./.venv-build/lib/python3.12/site-packages/pygments/lexers/templates.py:734:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/templates.py:873:    Lexer for handling Cheetah's special $ tokens in Python syntax.
./.venv-build/lib/python3.12/site-packages/pygments/lexers/templates.py:876:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/templates.py:878:        for pos, type_, value in pylexer.get_tokens_unprocessed(text):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/templates.py:879:            if type_ == Token.Error and value == "$":
./.venv-build/lib/python3.12/site-packages/pygments/lexers/templates.py:887:    markup is yielded as `Token.Other`.  This also works for
./.venv-build/lib/python3.12/site-packages/pygments/lexers/templates.py:900:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/templates.py:1001:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/templates.py:1038:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/templates.py:1573:    Base for the `JspLexer`. Yields `Token.Other` for area outside of
./.venv-build/lib/python3.12/site-packages/pygments/lexers/templates.py:1579:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/templates.py:1636:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/templates.py:1761:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/templates.py:1814:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/templates.py:1917:    Base for the `TeaTemplateLexer`. Yields `Token.Other` for area outside of
./.venv-build/lib/python3.12/site-packages/pygments/lexers/templates.py:1923:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/templates.py:2087:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/templates.py:2195:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/templates.py:2427:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/templates.py:2550:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/eiffel.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/eiffel.py:38:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/ldap.py:15:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/ldap.py:43:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/ldap.py:186:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_mysql_builtins.py:479:    "validate_password_strength",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_mysql_builtins.py:840:    "master_password",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_mysql_builtins.py:937:    "password",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_mysql_builtins.py:938:    "password_lock_time",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_mysql_builtins.py:1082:    "source_password",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/pddl.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/pddl.py:40:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/typst.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/typst.py:93:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/typst.py:274:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/typst.py:279:        yield from RegexLexer.get_tokens_unprocessed(self, text, stack)
./.venv-build/lib/python3.12/site-packages/pygments/lexers/csound.py:14:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/csound.py:43:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/csound.py:154:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/csound.py:213:        type_annotation_token = Keyword.Type
./.venv-build/lib/python3.12/site-packages/pygments/lexers/csound.py:221:            type_annotation_token = Name
./.venv-build/lib/python3.12/site-packages/pygments/lexers/csound.py:231:            yield match.start(3), type_annotation_token, match.group(3)
./.venv-build/lib/python3.12/site-packages/pygments/lexers/csound.py:233:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/csound.py:431:    # These tokens are based on those in XmlLexer in pygments/lexers/html.py. Making
./.venv-build/lib/python3.12/site-packages/pygments/lexers/csound.py:438:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/dotnet.py:23:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/dotnet.py:108:    tokens = {}
./.venv-build/lib/python3.12/site-packages/pygments/lexers/dotnet.py:109:    token_variants = True
./.venv-build/lib/python3.12/site-packages/pygments/lexers/dotnet.py:112:        tokens[levelname] = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/dotnet.py:347:        level = get_choice_opt(options, "unicodelevel", list(self.tokens), "basic")
./.venv-build/lib/python3.12/site-packages/pygments/lexers/dotnet.py:348:        if level not in self._all_tokens:
./.venv-build/lib/python3.12/site-packages/pygments/lexers/dotnet.py:350:            self._tokens = self.__class__.process_tokendef(level)
./.venv-build/lib/python3.12/site-packages/pygments/lexers/dotnet.py:352:            self._tokens = self._all_tokens[level]
./.venv-build/lib/python3.12/site-packages/pygments/lexers/dotnet.py:410:    tokens = {}
./.venv-build/lib/python3.12/site-packages/pygments/lexers/dotnet.py:411:    token_variants = True
./.venv-build/lib/python3.12/site-packages/pygments/lexers/dotnet.py:414:        tokens[levelname] = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/dotnet.py:535:        level = get_choice_opt(options, "unicodelevel", list(self.tokens), "basic")
./.venv-build/lib/python3.12/site-packages/pygments/lexers/dotnet.py:536:        if level not in self._all_tokens:
./.venv-build/lib/python3.12/site-packages/pygments/lexers/dotnet.py:538:            self._tokens = self.__class__.process_tokendef(level)
./.venv-build/lib/python3.12/site-packages/pygments/lexers/dotnet.py:540:            self._tokens = self._all_tokens[level]
./.venv-build/lib/python3.12/site-packages/pygments/lexers/dotnet.py:567:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/dotnet.py:655:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/dotnet.py:888:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/dotnet.py:1137:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/dotnet.py:1621:    tokens = {}
./.venv-build/lib/python3.12/site-packages/pygments/lexers/dotnet.py:1623:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/futhark.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/futhark.py:106:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/comal.py:14:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/comal.py:47:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_openedge_builtins.py:76:    "APPSERVER-PASSWORD",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_openedge_builtins.py:1780:    "PASSWORD-FIELD",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_openedge_builtins.py:1892:    "PROXY-PASSWORD",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_openedge_builtins.py:2434:    "URL-PASSWORD",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/archetype.py:17:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/archetype.py:41:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/archetype.py:177:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/archetype.py:226:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/archetype.py:300:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/ride.py:12:from pygments.token import Comment, Keyword, Name, Number, Punctuation, String, Text
./.venv-build/lib/python3.12/site-packages/pygments/lexers/ride.py:225:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/cplint.py:13:from pygments.token import Operator, Keyword, Name, String, Punctuation
./.venv-build/lib/python3.12/site-packages/pygments/lexers/cplint.py:31:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/bare.py:12:from pygments.token import Text, Comment, Keyword, Name, Literal, Whitespace
./.venv-build/lib/python3.12/site-packages/pygments/lexers/bare.py:51:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/usd.py:20:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/usd.py:55:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/usd.py:60:                    Keyword.Token,
./.venv-build/lib/python3.12/site-packages/pygments/lexers/usd.py:62:                    Keyword.Token,
./.venv-build/lib/python3.12/site-packages/pygments/lexers/usd.py:68:                    Name.Keyword.Tokens,
./.venv-build/lib/python3.12/site-packages/pygments/lexers/usd.py:76:                    Keyword.Token,
./.venv-build/lib/python3.12/site-packages/pygments/lexers/usd.py:82:                    Name.Keyword.Tokens,
./.venv-build/lib/python3.12/site-packages/pygments/lexers/usd.py:90:                    Keyword.Token,
./.venv-build/lib/python3.12/site-packages/pygments/lexers/usd.py:96:                    Name.Keyword.Tokens,
./.venv-build/lib/python3.12/site-packages/pygments/lexers/usd.py:108:                    Name.Keyword.Tokens,
./.venv-build/lib/python3.12/site-packages/pygments/lexers/usd.py:114:        + _keywords(KEYWORDS, Keyword.Tokens)
./.venv-build/lib/python3.12/site-packages/pygments/lexers/rnc.py:12:from pygments.token import Text, Comment, Operator, Keyword, Name, String, Punctuation
./.venv-build/lib/python3.12/site-packages/pygments/lexers/rnc.py:28:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/vip.py:14:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/vip.py:54:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/vip.py:187:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/vip.py:230:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/berry.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/berry.py:40:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/snobol.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/snobol.py:41:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/webassembly.py:16:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/webassembly.py:239:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/inferno.py:14:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/inferno.py:44:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/shell.py:25:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/shell.py:84:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/shell.py:188:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/shell.py:189:        for index, token, value in BashLexer.get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/shell.py:190:            if token is Text and value in self.EXTRA_KEYWORDS:
./.venv-build/lib/python3.12/site-packages/pygments/lexers/shell.py:192:            elif token is Comment.Single and "SBATCH" in value:
./.venv-build/lib/python3.12/site-packages/pygments/lexers/shell.py:195:                yield index, token, value
./.venv-build/lib/python3.12/site-packages/pygments/lexers/shell.py:208:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/shell.py:251:                    toks = innerlexer.get_tokens_unprocessed(curcode)
./.venv-build/lib/python3.12/site-packages/pygments/lexers/shell.py:258:            for i, t, v in do_insertions(insertions, innerlexer.get_tokens_unprocessed(curcode)):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/shell.py:304:    _token_terminator = rf"(?=\^?[{_ws}]|[{_punct}{_nl}])"
./.venv-build/lib/python3.12/site-packages/pygments/lexers/shell.py:308:    _number = rf"(?:-?(?:0[0-7]+|0x[\da-f]+|\d+){_token_terminator})"
./.venv-build/lib/python3.12/site-packages/pygments/lexers/shell.py:318:    _core_token = rf'(?:(?:(?:\^[{_nl}]?)?[^"{_nlws}{_punct}])+)'
./.venv-build/lib/python3.12/site-packages/pygments/lexers/shell.py:319:    _core_token_compound = rf'(?:(?:(?:\^[{_nl}]?)?[^"{_nlws}{_punct})])+)'
./.venv-build/lib/python3.12/site-packages/pygments/lexers/shell.py:320:    _token = rf"(?:[{_punct}]+|{_core_token})"
./.venv-build/lib/python3.12/site-packages/pygments/lexers/shell.py:321:    _token_compound = rf"(?:[{_punct}]+|{_core_token_compound})"
./.venv-build/lib/python3.12/site-packages/pygments/lexers/shell.py:322:    _stoken = rf"(?:[{_punct}]+|(?:{_string}|{_variable}|{_core_token})+)"
./.venv-build/lib/python3.12/site-packages/pygments/lexers/shell.py:326:        _core_token=_core_token,
./.venv-build/lib/python3.12/site-packages/pygments/lexers/shell.py:327:        _core_token_compound=_core_token_compound,
./.venv-build/lib/python3.12/site-packages/pygments/lexers/shell.py:334:        _stoken=_stoken,
./.venv-build/lib/python3.12/site-packages/pygments/lexers/shell.py:335:        _token_terminator=_token_terminator,
./.venv-build/lib/python3.12/site-packages/pygments/lexers/shell.py:348:            _token_terminator = rf"(?:(?=\))|{_token_terminator})"
./.venv-build/lib/python3.12/site-packages/pygments/lexers/shell.py:354:                else (rf"\)((?=\()|{_token_terminator}){rest_of_line}", Comment.Single)
./.venv-build/lib/python3.12/site-packages/pygments/lexers/shell.py:364:                rf"(?<=m))(?:(?=\()|{_token_terminator})))({_space}?{_core_token_compound if compound else _core_token}?(?:\^[{_nl}]?)?/(?:\^[{_nl}]?)?\?)",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/shell.py:428:                rf"(for{_token_terminator}(?!\^))({_space})(/f{_token_terminator})",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/shell.py:433:                rf"(for{_token_terminator}(?!\^))({_space})(/l{_token_terminator})",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/shell.py:437:            (rf"for{_token_terminator}(?!\^)", Keyword, ("for2", "for")),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/shell.py:444:                rf"(if(?:(?=\()|{_token_terminator})(?!\^))({_space}?)((?:/i{_token_terminator})?)({_space}?)((?:not{_token_terminator})?)({_space}?)",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/shell.py:456:                rf"rem(((?=\()|{_token_terminator}){_space}?{_stoken}?.*|{_keyword_terminator}{rest_of_line_compound if compound else rest_of_line})",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/shell.py:498:        _token=_token,
./.venv-build/lib/python3.12/site-packages/pygments/lexers/shell.py:499:        _token_compound=_token_compound,
./.venv-build/lib/python3.12/site-packages/pygments/lexers/shell.py:600:        _core_token_compound=_core_token_compound,
./.venv-build/lib/python3.12/site-packages/pygments/lexers/shell.py:603:        _stoken=_stoken,
./.venv-build/lib/python3.12/site-packages/pygments/lexers/shell.py:609:        stoken_compound = rf"(?:[{_punct}]+|(?:{_string}|{_variable}|{_core_token_compound})+)"
./.venv-build/lib/python3.12/site-packages/pygments/lexers/shell.py:616:                rf"((?:(?<=[{_nlws}])(?<!\^[{_nl}])\d)?)(>>?|<)({_space}?{stoken_compound if compound else _stoken})",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/shell.py:621:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/shell.py:673:                rf"({_space})(do{_token_terminator})",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/shell.py:699:                rf"((?:cmdextversion|errorlevel){_token_terminator})({_space})(\d+)",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/shell.py:704:                rf"(defined{_token_terminator})({_space})({_stoken})",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/shell.py:709:                rf"(exist{_token_terminator})({_space}{_stoken})",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/shell.py:722:            (_stoken, using(this, state="text"), ("#pop", "if2")),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/shell.py:726:                rf"({_space}?)(==)({_space}?{_stoken})",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/shell.py:731:                rf"({_space})({_opword})({_space}{_stoken})",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/shell.py:743:            (rf"else{_token_terminator}", Keyword, "#pop"),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/shell.py:780:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/shell.py:925:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/shell.py:1012:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/shell.py:1088:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/arrow.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/arrow.py:41:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/supercollider.py:14:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/supercollider.py:41:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/qlik.py:14:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/qlik.py:47:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/praat.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/praat.py:421:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/sophia.py:14:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/sophia.py:90:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lean.py:14:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lean.py:47:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/lean.py:406:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/tablegen.py:13:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/tablegen.py:127:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/tablegen.py:149:            # numbers, and we want to parse 1X as one name token as opposed to
./.venv-build/lib/python3.12/site-packages/pygments/lexers/erlang.py:23:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/erlang.py:235:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/erlang.py:315:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/erlang.py:329:                    yield from do_insertions(insertions, erlexer.get_tokens_unprocessed(curcode))
./.venv-build/lib/python3.12/site-packages/pygments/lexers/erlang.py:337:            yield from do_insertions(insertions, erlexer.get_tokens_unprocessed(curcode))
./.venv-build/lib/python3.12/site-packages/pygments/lexers/erlang.py:340:def gen_elixir_string_rules(name, symbol, token):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/erlang.py:343:        (rf"[^#{symbol}\\]+", token),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/erlang.py:345:        (r"\\.", token),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/erlang.py:346:        (rf"({symbol})", bygroups(token), "#pop"),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/erlang.py:352:def gen_elixir_sigstr_rules(term, term_class, token, interpol=True):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/erlang.py:355:            (rf"[^#{term_class}\\]+", token),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/erlang.py:357:            (r"\\.", token),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/erlang.py:358:            (rf"{term}[a-zA-Z]*", token, "#pop"),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/erlang.py:363:            (rf"[^{term_class}\\]+", token),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/erlang.py:364:            (r"\\.", token),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/erlang.py:365:            (rf"{term}[a-zA-Z]*", token, "#pop"),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/erlang.py:467:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/erlang.py:468:        for index, token, value in RegexLexer.get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/erlang.py:469:            if token is Name:
./.venv-build/lib/python3.12/site-packages/pygments/lexers/erlang.py:485:                    yield index, token, value
./.venv-build/lib/python3.12/site-packages/pygments/lexers/erlang.py:487:                yield index, token, value
./.venv-build/lib/python3.12/site-packages/pygments/lexers/erlang.py:505:        token = String.Other
./.venv-build/lib/python3.12/site-packages/pygments/lexers/erlang.py:512:                    bygroups(token, String.Heredoc),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/erlang.py:517:                    bygroups(token, String.Heredoc),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/erlang.py:523:                (r"[a-zA-Z]+", token, "#pop"),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/erlang.py:537:                (r"~[a-z]" + lterm, token, name + "-intp"),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/erlang.py:538:                (r"~[A-Z]" + lterm, token, name + "-no-intp"),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/erlang.py:540:            states[name + "-intp"] = gen_elixir_sigstr_rules(rterm, rterm_class, token)
./.venv-build/lib/python3.12/site-packages/pygments/lexers/erlang.py:542:                rterm, rterm_class, token, interpol=False
./.venv-build/lib/python3.12/site-packages/pygments/lexers/erlang.py:562:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/erlang.py:658:    tokens.update(gen_elixir_string_rules("double", '"', String.Double))
./.venv-build/lib/python3.12/site-packages/pygments/lexers/erlang.py:659:    tokens.update(gen_elixir_string_rules("single", "'", String.Single))
./.venv-build/lib/python3.12/site-packages/pygments/lexers/erlang.py:660:    tokens.update(gen_elixir_string_rules("double_atom", '"', String.Symbol))
./.venv-build/lib/python3.12/site-packages/pygments/lexers/erlang.py:661:    tokens.update(gen_elixir_string_rules("single_atom", "'", String.Symbol))
./.venv-build/lib/python3.12/site-packages/pygments/lexers/erlang.py:662:    tokens.update(gen_elixir_sigil_rules())
./.venv-build/lib/python3.12/site-packages/pygments/lexers/erlang.py:691:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/erlang.py:713:                            insertions, exlexer.get_tokens_unprocessed(curcode)
./.venv-build/lib/python3.12/site-packages/pygments/lexers/erlang.py:717:                    token = Generic.Error if in_error else Generic.Output
./.venv-build/lib/python3.12/site-packages/pygments/lexers/erlang.py:718:                    yield match.start(), token, line
./.venv-build/lib/python3.12/site-packages/pygments/lexers/erlang.py:720:            yield from do_insertions(insertions, exlexer.get_tokens_unprocessed(curcode))
./.venv-build/lib/python3.12/site-packages/pygments/lexers/jvm.py:24:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/jvm.py:72:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/jvm.py:220:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/jvm.py:221:        for index, token, value in JavaLexer.get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/jvm.py:222:            if token is Name and value in self.aj_keywords:
./.venv-build/lib/python3.12/site-packages/pygments/lexers/jvm.py:224:            elif token is Name.Label and value in self.aj_inter_type:
./.venv-build/lib/python3.12/site-packages/pygments/lexers/jvm.py:227:            elif token is Name.Decorator and value in self.aj_inter_type_annotation:
./.venv-build/lib/python3.12/site-packages/pygments/lexers/jvm.py:230:                yield index, token, value
./.venv-build/lib/python3.12/site-packages/pygments/lexers/jvm.py:330:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/jvm.py:606:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/jvm.py:698:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/jvm.py:701:        yield from lexer.get_tokens_unprocessed(text, stack)
./.venv-build/lib/python3.12/site-packages/pygments/lexers/jvm.py:718:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/jvm.py:801:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/jvm.py:1327:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/jvm.py:1395:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/jvm.py:1446:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/jvm.py:1556:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/jvm.py:1752:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/jvm.py:1822:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/jvm.py:1861:                r"TOKENIZE)\b",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/jvm.py:1894:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/jvm.py:1999:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/jvm.py:2493:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/freefem.py:11:from pygments.token import Comment, Operator, Keyword, Name
./.venv-build/lib/python3.12/site-packages/pygments/lexers/freefem.py:936:    def get_tokens_unprocessed(self, text, stack=("root",)):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/freefem.py:937:        for index, token, value in CppLexer.get_tokens_unprocessed(self, text, stack):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/freefem.py:955:                yield index, token, value
./.venv-build/lib/python3.12/site-packages/pygments/lexers/ml.py:14:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/ml.py:123:    # Callbacks for distinguishing tokens and reserved words
./.venv-build/lib/python3.12/site-packages/pygments/lexers/ml.py:126:            token = Error
./.venv-build/lib/python3.12/site-packages/pygments/lexers/ml.py:128:            token = Name.Namespace
./.venv-build/lib/python3.12/site-packages/pygments/lexers/ml.py:129:        yield match.start(1), token, match.group(1)
./.venv-build/lib/python3.12/site-packages/pygments/lexers/ml.py:134:            token = Error
./.venv-build/lib/python3.12/site-packages/pygments/lexers/ml.py:136:            token = Error
./.venv-build/lib/python3.12/site-packages/pygments/lexers/ml.py:138:            token = Name
./.venv-build/lib/python3.12/site-packages/pygments/lexers/ml.py:139:        yield match.start(1), token, match.group(1)
./.venv-build/lib/python3.12/site-packages/pygments/lexers/ml.py:144:            token = Keyword.Reserved
./.venv-build/lib/python3.12/site-packages/pygments/lexers/ml.py:146:            token = Punctuation
./.venv-build/lib/python3.12/site-packages/pygments/lexers/ml.py:148:            token = Name
./.venv-build/lib/python3.12/site-packages/pygments/lexers/ml.py:149:        yield match.start(1), token, str
./.venv-build/lib/python3.12/site-packages/pygments/lexers/ml.py:151:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/ml.py:481:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/ml.py:586:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/ml.py:965:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/ml.py:1142:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/minecraft.py:23:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/minecraft.py:49:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/minecraft.py:119:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/minecraft.py:318:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/smalltalk.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/smalltalk.py:40:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/smalltalk.py:194:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/jsx.py:15:from pygments.token import Name, Operator, Punctuation, String, Text, Whitespace
./.venv-build/lib/python3.12/site-packages/pygments/lexers/jsx.py:74:    # Use same tokens as `JavascriptLexer`, but with tags and attributes support
./.venv-build/lib/python3.12/site-packages/pygments/lexers/jsx.py:75:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/jsx.py:96:    # Use same tokens as `TypescriptLexer`, but with tags and attributes support
./.venv-build/lib/python3.12/site-packages/pygments/lexers/jsx.py:97:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/esoteric.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/esoteric.py:46:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/esoteric.py:104:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/esoteric.py:133:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/esoteric.py:269:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/esoteric.py:413:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/esoteric.py:444:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/smv.py:12:from pygments.token import Comment, Keyword, Name, Number, Operator, Punctuation, Text
./.venv-build/lib/python3.12/site-packages/pygments/lexers/smv.py:29:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/x10.py:12:from pygments.token import Text, Comment, Keyword, String
./.venv-build/lib/python3.12/site-packages/pygments/lexers/x10.py:95:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/ambient.py:14:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/ambient.py:64:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/thingsdb.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/thingsdb.py:38:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/thingsdb.py:110:                r"collections_info|del_collection|del_expired|del_node|del_token|"
./.venv-build/lib/python3.12/site-packages/pygments/lexers/thingsdb.py:111:                r"del_user|grant|has_collection|has_node|has_token|has_user|"
./.venv-build/lib/python3.12/site-packages/pygments/lexers/thingsdb.py:112:                r"new_collection|new_node|new_token|new_user|rename_collection|"
./.venv-build/lib/python3.12/site-packages/pygments/lexers/thingsdb.py:113:                r"rename_user|restore|revoke|set_password|set_time_zone|"
./.venv-build/lib/python3.12/site-packages/pygments/lexers/oberon.py:14:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/oberon.py:42:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/rdf.py:14:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/rdf.py:116:    # Lexer token definitions ::
./.venv-build/lib/python3.12/site-packages/pygments/lexers/rdf.py:118:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/rdf.py:286:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/rdf.py:463:    # Lexer token definitions ::
./.venv-build/lib/python3.12/site-packages/pygments/lexers/rdf.py:465:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/phix.py:14:from pygments.token import Text, Comment, Operator, Keyword, Name, String, Whitespace
./.venv-build/lib/python3.12/site-packages/pygments/lexers/phix.py:1160:        "CURLOPT_PASSWORD",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/phix.py:1395:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/d.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/d.py:37:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/d.py:246:            # -- TokenString
./.venv-build/lib/python3.12/site-packages/pygments/lexers/d.py:247:            (r"q\{", String, "token_string"),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/d.py:250:            # Tokens
./.venv-build/lib/python3.12/site-packages/pygments/lexers/d.py:271:        "token_string": [
./.venv-build/lib/python3.12/site-packages/pygments/lexers/d.py:272:            (r"\{", Punctuation, "token_string_nest"),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/d.py:276:        "token_string_nest": [
./.venv-build/lib/python3.12/site-packages/pygments/lexers/d.py:336:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/d.py:411:            # Tokens
./.venv-build/lib/python3.12/site-packages/pygments/lexers/varnish.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/varnish.py:51:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/varnish.py:289:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/asn1.py:13:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/asn1.py:121:def word_sequences(tokens):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/asn1.py:122:    return "(" + "|".join(token.replace(" ", r"\s+") for token in tokens) + r")\b"
./.venv-build/lib/python3.12/site-packages/pygments/lexers/asn1.py:138:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/pascal.py:15:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/pascal.py:48:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/pascal.py:49:        return self.lexer.get_tokens_unprocessed(text)
./.venv-build/lib/python3.12/site-packages/pygments/lexers/pascal.py:1298:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/pascal.py:1304:        next_token_is_function = False
./.venv-build/lib/python3.12/site-packages/pygments/lexers/pascal.py:1305:        next_token_is_property = False
./.venv-build/lib/python3.12/site-packages/pygments/lexers/pascal.py:1311:            token = Error
./.venv-build/lib/python3.12/site-packages/pygments/lexers/pascal.py:1315:                    token = Whitespace
./.venv-build/lib/python3.12/site-packages/pygments/lexers/pascal.py:1318:                        token = Comment.Preproc
./.venv-build/lib/python3.12/site-packages/pygments/lexers/pascal.py:1320:                        token = Comment.Multiline
./.venv-build/lib/python3.12/site-packages/pygments/lexers/pascal.py:1322:                    token = Comment.Single
./.venv-build/lib/python3.12/site-packages/pygments/lexers/pascal.py:1326:                    token = Operator
./.venv-build/lib/python3.12/site-packages/pygments/lexers/pascal.py:1328:                    token = Operator
./.venv-build/lib/python3.12/site-packages/pygments/lexers/pascal.py:1333:                    token = Punctuation
./.venv-build/lib/python3.12/site-packages/pygments/lexers/pascal.py:1335:                    next_token_is_function = False
./.venv-build/lib/python3.12/site-packages/pygments/lexers/pascal.py:1351:                        token = Name.Builtin.Pseudo
./.venv-build/lib/python3.12/site-packages/pygments/lexers/pascal.py:1353:                        token = Keyword
./.venv-build/lib/python3.12/site-packages/pygments/lexers/pascal.py:1360:                                next_token_is_function = True
./.venv-build/lib/python3.12/site-packages/pygments/lexers/pascal.py:1378:                                next_token_is_property = True
./.venv-build/lib/python3.12/site-packages/pygments/lexers/pascal.py:1387:                                next_token_is_function = True
./.venv-build/lib/python3.12/site-packages/pygments/lexers/pascal.py:1396:                        token = Keyword.Pseudo
./.venv-build/lib/python3.12/site-packages/pygments/lexers/pascal.py:1404:                        token = Keyword.Pseudo
./.venv-build/lib/python3.12/site-packages/pygments/lexers/pascal.py:1405:                        next_token_is_function = True
./.venv-build/lib/python3.12/site-packages/pygments/lexers/pascal.py:1406:                    # if the last iteration set next_token_is_function
./.venv-build/lib/python3.12/site-packages/pygments/lexers/pascal.py:1409:                    elif next_token_is_function:
./.venv-build/lib/python3.12/site-packages/pygments/lexers/pascal.py:1410:                        # Look if the next token is a dot. If yes it's
./.venv-build/lib/python3.12/site-packages/pygments/lexers/pascal.py:1414:                            token = Name.Class
./.venv-build/lib/python3.12/site-packages/pygments/lexers/pascal.py:1417:                            token = Name.Function
./.venv-build/lib/python3.12/site-packages/pygments/lexers/pascal.py:1418:                            next_token_is_function = False
./.venv-build/lib/python3.12/site-packages/pygments/lexers/pascal.py:1424:                    elif not self.is_portugol and next_token_is_property:
./.venv-build/lib/python3.12/site-packages/pygments/lexers/pascal.py:1425:                        token = Name.Property
./.venv-build/lib/python3.12/site-packages/pygments/lexers/pascal.py:1426:                        next_token_is_property = False
./.venv-build/lib/python3.12/site-packages/pygments/lexers/pascal.py:1427:                    # Highlight this token as label and add it
./.venv-build/lib/python3.12/site-packages/pygments/lexers/pascal.py:1430:                        token = Name.Label
./.venv-build/lib/python3.12/site-packages/pygments/lexers/pascal.py:1434:                        token = Name.Label
./.venv-build/lib/python3.12/site-packages/pygments/lexers/pascal.py:1436:                        token = Keyword.Type
./.venv-build/lib/python3.12/site-packages/pygments/lexers/pascal.py:1438:                        token = Keyword.Type
./.venv-build/lib/python3.12/site-packages/pygments/lexers/pascal.py:1440:                        token = Keyword.Pseudo
./.venv-build/lib/python3.12/site-packages/pygments/lexers/pascal.py:1441:                    # builtins are just builtins if the token
./.venv-build/lib/python3.12/site-packages/pygments/lexers/pascal.py:1444:                        token = Name.Builtin
./.venv-build/lib/python3.12/site-packages/pygments/lexers/pascal.py:1446:                        token = Name
./.venv-build/lib/python3.12/site-packages/pygments/lexers/pascal.py:1448:                    token = String
./.venv-build/lib/python3.12/site-packages/pygments/lexers/pascal.py:1451:                    token = String
./.venv-build/lib/python3.12/site-packages/pygments/lexers/pascal.py:1454:                    token = String.Char
./.venv-build/lib/python3.12/site-packages/pygments/lexers/pascal.py:1456:                    token = Number.Hex
./.venv-build/lib/python3.12/site-packages/pygments/lexers/pascal.py:1458:                    token = Number.Integer
./.venv-build/lib/python3.12/site-packages/pygments/lexers/pascal.py:1460:                    token = Number.Float
./.venv-build/lib/python3.12/site-packages/pygments/lexers/pascal.py:1470:                        token = String.Escape
./.venv-build/lib/python3.12/site-packages/pygments/lexers/pascal.py:1472:                        token = String
./.venv-build/lib/python3.12/site-packages/pygments/lexers/pascal.py:1475:                        token = String
./.venv-build/lib/python3.12/site-packages/pygments/lexers/pascal.py:1481:                        token = String.Escape
./.venv-build/lib/python3.12/site-packages/pygments/lexers/pascal.py:1483:                        token = String
./.venv-build/lib/python3.12/site-packages/pygments/lexers/pascal.py:1486:                        token = String
./.venv-build/lib/python3.12/site-packages/pygments/lexers/pascal.py:1492:                    token = Whitespace
./.venv-build/lib/python3.12/site-packages/pygments/lexers/pascal.py:1494:                    token = Keyword
./.venv-build/lib/python3.12/site-packages/pygments/lexers/pascal.py:1498:                        token = Comment.Preproc
./.venv-build/lib/python3.12/site-packages/pygments/lexers/pascal.py:1500:                        token = Comment.Multiline
./.venv-build/lib/python3.12/site-packages/pygments/lexers/pascal.py:1502:                    token = Comment.Single
./.venv-build/lib/python3.12/site-packages/pygments/lexers/pascal.py:1504:                    token = String
./.venv-build/lib/python3.12/site-packages/pygments/lexers/pascal.py:1507:                    token = Name.Label
./.venv-build/lib/python3.12/site-packages/pygments/lexers/pascal.py:1511:                        token = Keyword
./.venv-build/lib/python3.12/site-packages/pygments/lexers/pascal.py:1513:                        token = Name.Builtin
./.venv-build/lib/python3.12/site-packages/pygments/lexers/pascal.py:1515:                        token = Name
./.venv-build/lib/python3.12/site-packages/pygments/lexers/pascal.py:1517:                    token = Operator
./.venv-build/lib/python3.12/site-packages/pygments/lexers/pascal.py:1519:                    token = Punctuation
./.venv-build/lib/python3.12/site-packages/pygments/lexers/pascal.py:1521:                    token = Number.Hex
./.venv-build/lib/python3.12/site-packages/pygments/lexers/pascal.py:1523:                    token = Number.Integer
./.venv-build/lib/python3.12/site-packages/pygments/lexers/pascal.py:1525:                    token = Number.Float
./.venv-build/lib/python3.12/site-packages/pygments/lexers/pascal.py:1534:            yield scanner.start_pos, token, scanner.match or ""
./.venv-build/lib/python3.12/site-packages/pygments/lexers/graphics.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/graphics.py:46:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/graphics.py:343:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/graphics.py:835:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/graphics.py:1018:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/graphics.py:1121:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/graphics.py:1124:        for index, token, value in RegexLexer.get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/graphics.py:1125:            if token is Name and value in ASYFUNCNAME:
./.venv-build/lib/python3.12/site-packages/pygments/lexers/graphics.py:1126:                token = Name.Function
./.venv-build/lib/python3.12/site-packages/pygments/lexers/graphics.py:1127:            elif token is Name and value in ASYVARNAME:
./.venv-build/lib/python3.12/site-packages/pygments/lexers/graphics.py:1128:                token = Name.Variable
./.venv-build/lib/python3.12/site-packages/pygments/lexers/graphics.py:1129:            yield index, token, value
./.venv-build/lib/python3.12/site-packages/pygments/lexers/graphics.py:1153:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/graphics.py:1219:            # don't add the newline to the Comment token
./.venv-build/lib/python3.12/site-packages/pygments/lexers/graphics.py:1486:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/savi.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/savi.py:56:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/rita.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/rita.py:37:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/bibtex.py:14:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/bibtex.py:64:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/bibtex.py:140:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/mojo.py:24:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/mojo.py:99:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/ul4.py:14:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/ul4.py:51:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/nix.py:14:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/nix.py:88:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_lasso_builtins.py:29:        "curltoken",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_lasso_builtins.py:433:        "client_password",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_lasso_builtins.py:502:        "curle_bad_password_entered",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_lasso_builtins.py:520:        "curle_ftp_user_password_incorrect",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_lasso_builtins.py:760:        "email_token",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_lasso_builtins.py:1093:        "json_consume_token",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_lasso_builtins.py:1604:        "token_value",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_lasso_builtins.py:2079:        "client_password",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_lasso_builtins.py:2232:        "email_token",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_lasso_builtins.py:2298:        "error_code_invalidpassword",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_lasso_builtins.py:2339:        "error_invalidpassword",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_lasso_builtins.py:2380:        "error_msg_invalidpassword",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_lasso_builtins.py:2981:        "token_value",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_lasso_builtins.py:3149:        "addpasswordfield",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_lasso_builtins.py:3568:        "encodepassword",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_lasso_builtins.py:3949:        "hostpassword",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_lasso_builtins.py:4363:        "pop_token",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_lasso_builtins.py:4780:        "token",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_lasso_builtins.py:4891:        "addpasswordfield",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/resource.py:14:from pygments.token import Comment, String, Number, Operator, Text, Keyword, Name
./.venv-build/lib/python3.12/site-packages/pygments/lexers/resource.py:40:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/func.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/func.py:44:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_googlesql_builtins.py:914:    "TOKENLIST",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/yara.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/yara.py:39:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/graphql.py:16:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/graphql.py:75:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/graphql.py:76:        "ignored_tokens": [
./.venv-build/lib/python3.12/site-packages/pygments/lexers/graphql.py:82:            include("ignored_tokens"),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/graphql.py:97:            include("ignored_tokens"),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/graphql.py:102:            include("ignored_tokens"),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/graphql.py:113:            include("ignored_tokens"),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/graphql.py:120:            include("ignored_tokens"),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/graphql.py:126:            include("ignored_tokens"),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/graphql.py:134:            include("ignored_tokens"),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/graphql.py:140:            include("ignored_tokens"),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/graphql.py:155:            include("ignored_tokens"),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/graphql.py:159:            include("ignored_tokens"),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/graphql.py:166:            include("ignored_tokens"),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/graphql.py:174:            include("ignored_tokens"),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/graphql.py:179:            include("ignored_tokens"),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/chapel.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/chapel.py:141:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/chapel.py:182:            # tokens
./.venv-build/lib/python3.12/site-packages/pygments/lexers/trafficscript.py:12:from pygments.token import String, Number, Name, Keyword, Operator, Text, Comment
./.venv-build/lib/python3.12/site-packages/pygments/lexers/trafficscript.py:28:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/blueprint.py:14:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/blueprint.py:41:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_scilab_builtins.py:953:    "tokens",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_scilab_builtins.py:2891:    "tokenpos",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/bqn.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/bqn.py:42:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/bqn.py:65:            # This token type is used for diamond, commas
./.venv-build/lib/python3.12/site-packages/pygments/lexers/bqn.py:71:            # Since this token type is important in BQN, it is not included in
./.venv-build/lib/python3.12/site-packages/pygments/lexers/bqn.py:72:            # the punctuation token type but rather in the following one
./.venv-build/lib/python3.12/site-packages/pygments/lexers/openscad.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/openscad.py:38:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/pawn.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/pawn.py:44:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/pawn.py:190:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/pawn.py:191:        for index, token, value in RegexLexer.get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/pawn.py:192:            if token is Name:
./.venv-build/lib/python3.12/site-packages/pygments/lexers/pawn.py:195:                        token = Keyword.Type
./.venv-build/lib/python3.12/site-packages/pygments/lexers/pawn.py:197:                        token = Name.Builtin
./.venv-build/lib/python3.12/site-packages/pygments/lexers/pawn.py:198:            yield index, token, value
./.venv-build/lib/python3.12/site-packages/pygments/lexers/pawn.py:218:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/basic.py:14:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/basic.py:63:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/basic.py:258:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/basic.py:441:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/basic.py:579:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/basic.py:887:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/basic.py:1013:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/basic.py:1299:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/html.py:23:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/html.py:67:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/html.py:153:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/html.py:190:                r"CDATA|IDREFS|IDREF|ID|NMTOKENS|NMTOKEN|ENTITIES|ENTITY|NOTATION",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/html.py:239:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/html.py:319:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/html.py:320:        for index, token, value in XmlLexer.get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/html.py:323:            if token is Name.Tag and m and m.group(1) in self.EXTRA_KEYWORDS:
./.venv-build/lib/python3.12/site-packages/pygments/lexers/html.py:326:                yield index, token, value
./.venv-build/lib/python3.12/site-packages/pygments/lexers/html.py:354:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/html.py:472:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/html.py:593:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/html.py:711:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/html.py:734:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/perl.py:23:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/perl.py:53:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/perl.py:560:        "token",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/perl.py:1864:    def brackets_callback(token_class):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/perl.py:1918:            yield match.start(), token_class, text[match.start() : end_pos + n_chars]
./.venv-build/lib/python3.12/site-packages/pygments/lexers/perl.py:1930:        # below a token state, it means we need to increment
./.venv-build/lib/python3.12/site-packages/pygments/lexers/perl.py:1932:        # we should return to the token rules.
./.venv-build/lib/python3.12/site-packages/pygments/lexers/perl.py:1933:        if len(stack) > 2 and stack[-2] == "token":
./.venv-build/lib/python3.12/site-packages/pygments/lexers/perl.py:1934:            context.perl6_token_nesting_level += 1
./.venv-build/lib/python3.12/site-packages/pygments/lexers/perl.py:1943:        # below a token state, it means we need to check the nesting
./.venv-build/lib/python3.12/site-packages/pygments/lexers/perl.py:1944:        # level to see if we need to return to the token state.
./.venv-build/lib/python3.12/site-packages/pygments/lexers/perl.py:1945:        if len(stack) > 2 and stack[-2] == "token":
./.venv-build/lib/python3.12/site-packages/pygments/lexers/perl.py:1946:            context.perl6_token_nesting_level -= 1
./.venv-build/lib/python3.12/site-packages/pygments/lexers/perl.py:1947:            if context.perl6_token_nesting_level == 0:
./.venv-build/lib/python3.12/site-packages/pygments/lexers/perl.py:1951:        context.perl6_token_nesting_level = 1
./.venv-build/lib/python3.12/site-packages/pygments/lexers/perl.py:1960:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/perl.py:1973:                r"(regex|token|rule)(\s*" + PERL6_IDENTIFIER_RANGE + "+:sym)",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/perl.py:1975:                "token-sym-brackets",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/perl.py:1978:                r"(regex|token|rule)(?!"
./.venv-build/lib/python3.12/site-packages/pygments/lexers/perl.py:1984:                "pre-token",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/perl.py:2047:        "pre-token": [
./.venv-build/lib/python3.12/site-packages/pygments/lexers/perl.py:2049:            (r"\{", Text, ("#pop", "token")),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/perl.py:2052:        "token-sym-brackets": [
./.venv-build/lib/python3.12/site-packages/pygments/lexers/perl.py:2056:                ("#pop", "pre-token"),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/perl.py:2058:            default(("#pop", "pre-token")),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/perl.py:2060:        "token": [
./.venv-build/lib/python3.12/site-packages/pygments/lexers/theorem.py:14:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/theorem.py:354:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/theorem.py:414:            # Consume comments like ***** as one token
./.venv-build/lib/python3.12/site-packages/pygments/lexers/theorem.py:842:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/sql.py:53:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/sql.py:126:        yield from lx.get_tokens_unprocessed(match.group(4))
./.venv-build/lib/python3.12/site-packages/pygments/lexers/sql.py:140:    had, _tokens could be created on this ancestor and not updated for the
./.venv-build/lib/python3.12/site-packages/pygments/lexers/sql.py:145:    def get_tokens_unprocessed(self, text, *args):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/sql.py:148:        yield from super().get_tokens_unprocessed(text, *args)
./.venv-build/lib/python3.12/site-packages/pygments/lexers/sql.py:185:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/sql.py:246:    tokens = {name: state[:] for (name, state) in PostgresLexer.tokens.items()}
./.venv-build/lib/python3.12/site-packages/pygments/lexers/sql.py:249:    for i, pattern in enumerate(tokens["root"]):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/sql.py:251:            tokens["root"][i] = (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/sql.py:264:    tokens["root"][:0] = [
./.venv-build/lib/python3.12/site-packages/pygments/lexers/sql.py:284:    tokens = {name: state[:] for (name, state) in PostgresLexer.tokens.items()}
./.venv-build/lib/python3.12/site-packages/pygments/lexers/sql.py:286:    tokens["root"].append((r"\\[^\s]+", Keyword.Pseudo, "psql-command"))
./.venv-build/lib/python3.12/site-packages/pygments/lexers/sql.py:287:    tokens["psql-command"] = [
./.venv-build/lib/python3.12/site-packages/pygments/lexers/sql.py:344:    def get_tokens_unprocessed(self, data):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/sql.py:359:                    yield from lexer.get_tokens_unprocessed(line)
./.venv-build/lib/python3.12/site-packages/pygments/lexers/sql.py:377:            yield from do_insertions(insertions, sql.get_tokens_unprocessed(curcode))
./.venv-build/lib/python3.12/site-packages/pygments/lexers/sql.py:380:            out_token = Generic.Output
./.venv-build/lib/python3.12/site-packages/pygments/lexers/sql.py:391:                        out_token = Generic.Error
./.venv-build/lib/python3.12/site-packages/pygments/lexers/sql.py:393:                    yield (mmsg.start(2), out_token, mmsg.group(2))
./.venv-build/lib/python3.12/site-packages/pygments/lexers/sql.py:395:                    yield (0, out_token, line)
./.venv-build/lib/python3.12/site-packages/pygments/lexers/sql.py:412:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/sql.py:680:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/sql.py:724:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/sql.py:748:            # tokens starting with a digit have already been recognized
./.venv-build/lib/python3.12/site-packages/pygments/lexers/sql.py:809:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/sql.py:876:            # Exceptions; these words tokenize differently in different contexts.
./.venv-build/lib/python3.12/site-packages/pygments/lexers/sql.py:879:            # In all other known cases, "SET" is tokenized by MYSQL_DATATYPES.
./.venv-build/lib/python3.12/site-packages/pygments/lexers/sql.py:960:        # additional styles based on the token name. This gives users
./.venv-build/lib/python3.12/site-packages/pygments/lexers/sql.py:1004:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/sql.py:1065:            # Exceptions; these words tokenize differently in different contexts.
./.venv-build/lib/python3.12/site-packages/pygments/lexers/sql.py:1144:        # additional styles based on the token name. This gives users
./.venv-build/lib/python3.12/site-packages/pygments/lexers/sql.py:1155:        tokens = collections.Counter(text.split())
./.venv-build/lib/python3.12/site-packages/pygments/lexers/sql.py:1156:        return 0.001 * sum(count for t, count in tokens.items() if t in googlesql_identifiers)
./.venv-build/lib/python3.12/site-packages/pygments/lexers/sql.py:1172:    def get_tokens_unprocessed(self, data):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/sql.py:1186:                    yield from do_insertions(insertions, sql.get_tokens_unprocessed(curcode))
./.venv-build/lib/python3.12/site-packages/pygments/lexers/sql.py:1194:            yield from do_insertions(insertions, sql.get_tokens_unprocessed(curcode))
./.venv-build/lib/python3.12/site-packages/pygments/lexers/sql.py:1210:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/zig.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/zig.py:151:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/procfile.py:12:from pygments.token import Name, Number, String, Text, Punctuation
./.venv-build/lib/python3.12/site-packages/pygments/lexers/procfile.py:31:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/felix.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/felix.py:242:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/kuin.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/kuin.py:38:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/nit.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/nit.py:36:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/testing.py:12:from pygments.token import Comment, Keyword, Name, String, Number, Generic, Text
./.venv-build/lib/python3.12/site-packages/pygments/lexers/testing.py:34:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/testing.py:152:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/tlb.py:12:from pygments.token import Operator, Name, Number, Whitespace, Punctuation, Comment
./.venv-build/lib/python3.12/site-packages/pygments/lexers/tlb.py:28:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/parasail.py:14:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/parasail.py:43:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/mosel.py:13:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/mosel.py:407:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/grammar_notation.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/grammar_notation.py:61:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/grammar_notation.py:106:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/grammar_notation.py:155:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/grammar_notation.py:251:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_postgres_builtins.py:302:    "PASSWORD",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/webidl.py:12:from pygments.token import Comment, Keyword, Name, Number, Punctuation, String, Text
./.venv-build/lib/python3.12/site-packages/pygments/lexers/webidl.py:65:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/installers.py:14:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/installers.py:50:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/installers.py:193:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/installers.py:267:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/installers.py:298:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/installers.py:344:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/cddl.py:16:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/cddl.py:111:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/cddl.py:138:            # Token type is String as barewords are always interpreted as such.
./.venv-build/lib/python3.12/site-packages/pygments/lexers/cddl.py:164:            # (r";.+$", Token.Other),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/smithy.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/smithy.py:70:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/c_like.py:14:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/c_like.py:56:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/c_like.py:157:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/c_like.py:225:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/c_like.py:281:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/c_like.py:369:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/c_like.py:624:    def get_tokens_unprocessed(self, text, stack=("root",)):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/c_like.py:625:        for index, token, value in CLexer.get_tokens_unprocessed(self, text, stack):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/c_like.py:626:            if token is Name:
./.venv-build/lib/python3.12/site-packages/pygments/lexers/c_like.py:628:                    token = Keyword.Type
./.venv-build/lib/python3.12/site-packages/pygments/lexers/c_like.py:630:                    token = Keyword.Type
./.venv-build/lib/python3.12/site-packages/pygments/lexers/c_like.py:632:                    token = Name.Builtin
./.venv-build/lib/python3.12/site-packages/pygments/lexers/c_like.py:634:                    token = Keyword.Pseudo
./.venv-build/lib/python3.12/site-packages/pygments/lexers/c_like.py:636:                    token = Keyword.Reserved
./.venv-build/lib/python3.12/site-packages/pygments/lexers/c_like.py:638:                    token = Name.Function
./.venv-build/lib/python3.12/site-packages/pygments/lexers/c_like.py:639:            yield index, token, value
./.venv-build/lib/python3.12/site-packages/pygments/lexers/c_like.py:654:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/c_like.py:786:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/c_like.py:1296:    def get_tokens_unprocessed(self, text, stack=("root",)):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/c_like.py:1297:        for index, token, value in CppLexer.get_tokens_unprocessed(self, text, stack):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/c_like.py:1309:                yield index, token, value
./.venv-build/lib/python3.12/site-packages/pygments/lexers/c_like.py:1324:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/c_like.py:1399:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/c_like.py:1595:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/data.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/data.py:51:    def something(token_class):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/data.py:52:        """Do not produce empty tokens."""
./.venv-build/lib/python3.12/site-packages/pygments/lexers/data.py:58:            yield match.start(), token_class, text
./.venv-build/lib/python3.12/site-packages/pygments/lexers/data.py:63:    def reset_indent(token_class):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/data.py:72:            yield match.start(), token_class, text
./.venv-build/lib/python3.12/site-packages/pygments/lexers/data.py:77:    def save_indent(token_class, start=False):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/data.py:94:                yield match.start(), token_class, text
./.venv-build/lib/python3.12/site-packages/pygments/lexers/data.py:96:                yield match.start() + len(text), token_class.Error, extra
./.venv-build/lib/python3.12/site-packages/pygments/lexers/data.py:101:    def set_indent(token_class, implicit=False):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/data.py:111:            yield match.start(), token_class, text
./.venv-build/lib/python3.12/site-packages/pygments/lexers/data.py:116:    def set_block_scalar_indent(token_class):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/data.py:130:                yield match.start(), token_class, text
./.venv-build/lib/python3.12/site-packages/pygments/lexers/data.py:135:    def parse_block_scalar_empty_line(indent_token_class, content_token_class):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/data.py:142:                    yield match.start(), indent_token_class, text
./.venv-build/lib/python3.12/site-packages/pygments/lexers/data.py:146:                yield match.start(), indent_token_class, indentation
./.venv-build/lib/python3.12/site-packages/pygments/lexers/data.py:149:                    content_token_class,
./.venv-build/lib/python3.12/site-packages/pygments/lexers/data.py:156:    def parse_block_scalar_indent(token_class):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/data.py:173:                yield match.start(), token_class, text
./.venv-build/lib/python3.12/site-packages/pygments/lexers/data.py:178:    def parse_plain_scalar_indent(token_class):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/data.py:188:                yield match.start(), token_class, text
./.venv-build/lib/python3.12/site-packages/pygments/lexers/data.py:193:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/data.py:253:            # whitespaces separating tokens
./.venv-build/lib/python3.12/site-packages/pygments/lexers/data.py:447:    def get_tokens_unprocessed(self, text=None, context=None):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/data.py:450:        return super().get_tokens_unprocessed(text, context)
./.venv-build/lib/python3.12/site-packages/pygments/lexers/data.py:479:    # sets, the token will be considered valid. For example,
./.venv-build/lib/python3.12/site-packages/pygments/lexers/data.py:492:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/data.py:510:        # The queue is used to store data that may need to be tokenized
./.venv-build/lib/python3.12/site-packages/pygments/lexers/data.py:512:        # keys are tokenized differently than string values, but cannot
./.venv-build/lib/python3.12/site-packages/pygments/lexers/data.py:522:        #     (start_index, token_type, text)
./.venv-build/lib/python3.12/site-packages/pygments/lexers/data.py:524:        # By default the token type of text in double quotes is
./.venv-build/lib/python3.12/site-packages/pygments/lexers/data.py:525:        # String.Double. The token type will be replaced if a colon
./.venv-build/lib/python3.12/site-packages/pygments/lexers/data.py:636:                # Exhaust the queue. Accept the existing token types.
./.venv-build/lib/python3.12/site-packages/pygments/lexers/data.py:652:                # Exhaust the queue. Accept the existing token types.
./.venv-build/lib/python3.12/site-packages/pygments/lexers/data.py:659:                # Exhaust the queue. Accept the existing token types.
./.venv-build/lib/python3.12/site-packages/pygments/lexers/data.py:666:                # Yield from the queue. Replace string token types.
./.venv-build/lib/python3.12/site-packages/pygments/lexers/data.py:667:                for _start, _token, _text in queue:
./.venv-build/lib/python3.12/site-packages/pygments/lexers/data.py:668:                    # There can be only three types of tokens before a ':':
./.venv-build/lib/python3.12/site-packages/pygments/lexers/data.py:672:                    # Otherwise, we yield the original token.
./.venv-build/lib/python3.12/site-packages/pygments/lexers/data.py:676:                    if _token is String.Double:
./.venv-build/lib/python3.12/site-packages/pygments/lexers/data.py:679:                        yield _start, _token, _text
./.venv-build/lib/python3.12/site-packages/pygments/lexers/data.py:685:                # Exhaust the queue. Accept the existing token types.
./.venv-build/lib/python3.12/site-packages/pygments/lexers/data.py:696:                # Exhaust the queue. Accept the existing token types.
./.venv-build/lib/python3.12/site-packages/pygments/lexers/data.py:781:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/data.py:782:        for start, token, value in super().get_tokens_unprocessed(text):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/data.py:783:            if token is Name.Tag and value in self.json_ld_keywords:
./.venv-build/lib/python3.12/site-packages/pygments/lexers/data.py:786:                yield start, token, value
./.venv-build/lib/python3.12/site-packages/pygments/lexers/javascript.py:27:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/javascript.py:88:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/javascript.py:218:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/javascript.py:262:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/javascript.py:476:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/javascript.py:604:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/javascript.py:731:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/javascript.py:822:                r"Error_InvalidDatabase|Error_InvalidPassword|"
./.venv-build/lib/python3.12/site-packages/pygments/lexers/javascript.py:994:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/javascript.py:998:        for index, token, value in RegexLexer.get_tokens_unprocessed(self, text, stack):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/javascript.py:1000:                token is Name.Other
./.venv-build/lib/python3.12/site-packages/pygments/lexers/javascript.py:1002:                or token is Name.Other.Member
./.venv-build/lib/python3.12/site-packages/pygments/lexers/javascript.py:1007:            yield index, token, value
./.venv-build/lib/python3.12/site-packages/pygments/lexers/javascript.py:1037:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/javascript.py:1288:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/javascript.py:1407:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/javascript.py:1512:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/javascript.py:1850:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/javascript.py:1948:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/javascript.py:1975:                    yield from do_insertions(insertions, jslexer.get_tokens_unprocessed(curcode))
./.venv-build/lib/python3.12/site-packages/pygments/lexers/javascript.py:1980:                yield from do_insertions([], jslexer.get_tokens_unprocessed(line))
./.venv-build/lib/python3.12/site-packages/pygments/lexers/javascript.py:1983:            yield from do_insertions(insertions, jslexer.get_tokens_unprocessed(curcode))
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_scheme_builtins.py:1408:    "string-tokenize",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/jslt.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/jslt.py:42:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/business.py:14:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/business.py:62:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/business.py:675:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/business.py:696:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/business.py:923:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/business.py:986:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/business.py:1030:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/idl.py:14:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/idl.py:946:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/ecl.py:14:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/ecl.py:42:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/ecl.py:75:                r"QSTRING|REAL|RECORD|RULE|SET OF|STRING|TOKEN|UDECIMAL|UNICODE|"
./.venv-build/lib/python3.12/site-packages/pygments/lexers/ecl.py:209:                        "TOKEN",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/promql.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/promql.py:136:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/scdoc.py:14:from pygments.token import Text, Comment, Keyword, String, Generic
./.venv-build/lib/python3.12/site-packages/pygments/lexers/scdoc.py:31:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/slash.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/slash.py:40:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/spice.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/spice.py:39:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/spice.py:126:            # tokens
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_stata_builtins.py:468:    "gettoken",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_stata_builtins.py:1489:    "token",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_stata_builtins.py:1490:    "tokeni",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_stata_builtins.py:1491:    "tokeniz",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/_stata_builtins.py:1492:    "tokenize",
./.venv-build/lib/python3.12/site-packages/pygments/lexers/matlab.py:22:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/matlab.py:54:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/matlab.py:2805:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/matlab.py:2828:                token = (0, Generic.Traceback, line)
./.venv-build/lib/python3.12/site-packages/pygments/lexers/matlab.py:2829:                insertions.append((idx, [token]))
./.venv-build/lib/python3.12/site-packages/pygments/lexers/matlab.py:2842:                    yield from do_insertions(insertions, mlexer.get_tokens_unprocessed(curcode))
./.venv-build/lib/python3.12/site-packages/pygments/lexers/matlab.py:2856:            yield from do_insertions(insertions, mlexer.get_tokens_unprocessed(curcode))
./.venv-build/lib/python3.12/site-packages/pygments/lexers/matlab.py:4081:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/matlab.py:4220:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/monte.py:11:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/monte.py:202:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/c_cpp.py:24:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/c_cpp.py:73:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/c_cpp.py:508:    def get_tokens_unprocessed(self, text, stack=("root",)):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/c_cpp.py:509:        for index, token, value in RegexLexer.get_tokens_unprocessed(self, text, stack):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/c_cpp.py:510:            if token is Name:
./.venv-build/lib/python3.12/site-packages/pygments/lexers/c_cpp.py:512:                    token = Keyword.Type
./.venv-build/lib/python3.12/site-packages/pygments/lexers/c_cpp.py:514:                    token = Keyword.Type
./.venv-build/lib/python3.12/site-packages/pygments/lexers/c_cpp.py:516:                    token = Keyword.Type
./.venv-build/lib/python3.12/site-packages/pygments/lexers/c_cpp.py:518:                    token = Keyword.Type
./.venv-build/lib/python3.12/site-packages/pygments/lexers/c_cpp.py:519:            yield index, token, value
./.venv-build/lib/python3.12/site-packages/pygments/lexers/c_cpp.py:554:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/c_cpp.py:635:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/solidity.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/solidity.py:45:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/actionscript.py:14:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/actionscript.py:46:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/actionscript.py:389:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/actionscript.py:513:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/amdgpu.py:12:from pygments.token import Name, Text, Keyword, Whitespace, Number, Comment
./.venv-build/lib/python3.12/site-packages/pygments/lexers/amdgpu.py:32:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/dax.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/dax.py:39:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/python.py:25:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/python.py:135:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/python.py:800:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/python.py:1278:    Code tokens are output as ``Token.Other.Code``, traceback tokens as
./.venv-build/lib/python3.12/site-packages/pygments/lexers/python.py:1279:    ``Token.Other.Traceback``.
./.venv-build/lib/python3.12/site-packages/pygments/lexers/python.py:1281:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/python.py:1352:        # different tokens.  TODO: DelegatingLexer should support this
./.venv-build/lib/python3.12/site-packages/pygments/lexers/python.py:1354:        # distinguishing tokens. Then we wouldn't need this intermediary
./.venv-build/lib/python3.12/site-packages/pygments/lexers/python.py:1379:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/python.py:1457:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/python.py:1511:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/python.py:1852:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/python.py:2423:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/python.py:2424:        for index, token, value in PythonLexer.get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/python.py:2425:            if token is Name and value in self.EXTRA_KEYWORDS:
./.venv-build/lib/python3.12/site-packages/pygments/lexers/python.py:2428:                yield index, token, value
./.venv-build/lib/python3.12/site-packages/pygments/lexers/devicetree.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/devicetree.py:42:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/whiley.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/whiley.py:41:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/wowtoc.py:16:from pygments.token import Comment, Name, Text, Punctuation, String, Keyword
./.venv-build/lib/python3.12/site-packages/pygments/lexers/wowtoc.py:30:def _create_tag_line_token(inner_pattern, inner_token, ignore_case=False):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/wowtoc.py:32:    # have a different pattern and different token. otherwise, everything about a tag
./.venv-build/lib/python3.12/site-packages/pygments/lexers/wowtoc.py:39:            inner_token,
./.venv-build/lib/python3.12/site-packages/pygments/lexers/wowtoc.py:60:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/wowtoc.py:64:            _create_tag_line_token(
./.venv-build/lib/python3.12/site-packages/pygments/lexers/wowtoc.py:70:            _create_tag_line_token(
./.venv-build/lib/python3.12/site-packages/pygments/lexers/wowtoc.py:78:            _create_tag_line_token(
./.venv-build/lib/python3.12/site-packages/pygments/lexers/wowtoc.py:84:            _create_tag_line_token(
./.venv-build/lib/python3.12/site-packages/pygments/lexers/teraterm.py:14:from pygments.token import Text, Comment, Operator, Name, String, Number, Keyword, Error
./.venv-build/lib/python3.12/site-packages/pygments/lexers/teraterm.py:31:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/teraterm.py:86:                r"delpassword|"
./.venv-build/lib/python3.12/site-packages/pygments/lexers/teraterm.py:141:                r"getpassword|"
./.venv-build/lib/python3.12/site-packages/pygments/lexers/teraterm.py:154:                r"ispassword|"
./.venv-build/lib/python3.12/site-packages/pygments/lexers/teraterm.py:174:                r"passwordbox|"
./.venv-build/lib/python3.12/site-packages/pygments/lexers/teraterm.py:208:                r"setpassword|"
./.venv-build/lib/python3.12/site-packages/pygments/lexers/teraterm.py:326:        if re.search(TeraTermLexer.tokens["commands"][0][0], text):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/fortran.py:14:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/fortran.py:49:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/fortran.py:600:        for index, token, value in lexer.get_tokens_unprocessed(text):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/fortran.py:603:                yield index, token, value
./.venv-build/lib/python3.12/site-packages/pygments/lexers/fortran.py:605:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/jmespath.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/jmespath.py:37:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/email.py:13:from pygments.token import Text, Keyword, Name, String, Number, Comment
./.venv-build/lib/python3.12/site-packages/pygments/lexers/email.py:30:    def get_x_header_tokens(self, match):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/email.py:36:            default_actions = self.get_tokens_unprocessed(match.group(2), stack=("root", "header"))
./.venv-build/lib/python3.12/site-packages/pygments/lexers/email.py:43:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/email.py:46:            (r"^(X-(?:\w[\w\-]*:))([\s\S]*?\n)(?![ \t])", get_x_header_tokens),
./.venv-build/lib/python3.12/site-packages/pygments/lexers/ooc.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/ooc.py:38:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/ezhil.py:14:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/ezhil.py:42:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/r.py:14:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/r.py:45:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/r.py:64:                        insertions, slexer.get_tokens_unprocessed(current_code_block)
./.venv-build/lib/python3.12/site-packages/pygments/lexers/r.py:76:            yield from do_insertions(insertions, slexer.get_tokens_unprocessed(current_code_block))
./.venv-build/lib/python3.12/site-packages/pygments/lexers/r.py:100:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/r.py:197:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/julia.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/julia.py:53:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/julia.py:315:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.12/site-packages/pygments/lexers/julia.py:339:                    yield from do_insertions(insertions, jllexer.get_tokens_unprocessed(curcode))
./.venv-build/lib/python3.12/site-packages/pygments/lexers/julia.py:351:            yield from do_insertions(insertions, jllexer.get_tokens_unprocessed(curcode))
./.venv-build/lib/python3.12/site-packages/pygments/lexers/gcodelexer.py:12:from pygments.token import Comment, Name, Text, Keyword, Number
./.venv-build/lib/python3.12/site-packages/pygments/lexers/gcodelexer.py:28:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/bdd.py:12:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/bdd.py:43:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/dns.py:13:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/dns.py:53:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/configs.py:23:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/lexers/configs.py:83:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/configs.py:150:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/configs.py:201:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/configs.py:255:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/configs.py:308:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/configs.py:397:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/configs.py:442:            # Skip blank lines after help token, if any
./.venv-build/lib/python3.12/site-packages/pygments/lexers/configs.py:479:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/configs.py:560:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/configs.py:877:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/configs.py:912:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/configs.py:958:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/configs.py:991:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/configs.py:1238:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/configs.py:1344:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/configs.py:1392:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/configs.py:1434:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/configs.py:1475:    but it yield error token. It is because pacman.conf has
./.venv-build/lib/python3.12/site-packages/pygments/lexers/configs.py:1494:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/configs.py:1536:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/configs.py:1611:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/configs.py:1747:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/configs.py:1837:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexers/configs.py:1880:    tokens = {
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:17:from pygments.token import Error, Text, Other, Whitespace, _TokenType
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:274:    def get_tokens(self, text, unfiltered=False):
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:278:        iterable of ``(tokentype, value)`` pairs from `text`.
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:282:        (`stripnl`, `stripall` and so on), and then yields all tokens
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:283:        from `get_tokens_unprocessed()`, with the ``index`` dropped.
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:291:            for _, t, v in self.get_tokens_unprocessed(text):
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:299:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:302:        ``(index, tokentype, value)`` tuples where ``index`` is the starting
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:303:        position of the token within the input text.
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:315:    lexer, afterwards all ``Other`` tokens are lexed using the root
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:327:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:331:        for i, t, v in self.language_lexer.get_tokens_unprocessed(text):
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:341:        return do_insertions(insertions, self.root_lexer.get_tokens_unprocessed(buffered))
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:418:            elif type(action) is _TokenType:
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:479:            for i, t, v in lx.get_tokens_unprocessed(match.group(), **gt_kwargs):
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:492:            for i, t, v in lx.get_tokens_unprocessed(match.group(), **gt_kwargs):
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:503:    For example default('#pop') is equivalent to ('', Token, '#pop')
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:532:    Metaclass for RegexLexer, creates the self._tokens attribute from
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:533:    self.tokens on the first instantiation.
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:537:        """Preprocess the regular expression component of a token definition."""
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:542:    def _process_token(cls, token):
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:543:        """Preprocess the token component of a token definition."""
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:544:        assert type(token) is _TokenType or callable(
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:545:            token
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:546:        ), f"token type must be simple type or callable, not {token!r}"
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:547:        return token
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:550:        """Preprocess the state transition action of a token definition."""
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:567:            itokens = []
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:570:                itokens.extend(cls._process_state(unprocessed, processed, istate))
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:571:            processed[tmp_state] = itokens
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:589:        tokens = processed[state] = []
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:595:                tokens.extend(cls._process_state(unprocessed, processed, str(tdef)))
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:604:                tokens.append((re.compile("").match, None, new_state))
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:616:            token = cls._process_token(tdef[1])
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:623:            tokens.append((rex, token, new_state))
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:624:        return tokens
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:626:    def process_tokendef(cls, name, tokendefs=None):
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:627:        """Preprocess a dictionary of token definitions."""
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:628:        processed = cls._all_tokens[name] = {}
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:629:        tokendefs = tokendefs or cls.tokens[name]
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:630:        for state in list(tokendefs):
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:631:            cls._process_state(tokendefs, processed, state)
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:634:    def get_tokendefs(cls):
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:636:        Merge tokens from superclasses in MRO order, returning a single tokendef
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:646:        tokens = {}
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:649:            toks = c.__dict__.get("tokens", {})
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:652:                curitems = tokens.get(state)
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:658:                    tokens[state] = items
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:681:        return tokens
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:684:        """Instantiate cls after preprocessing its token definitions."""
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:685:        if "_tokens" not in cls.__dict__:
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:686:            cls._all_tokens = {}
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:688:            if hasattr(cls, "token_variants") and cls.token_variants:
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:692:                cls._tokens = cls.process_tokendef("", cls.get_tokendefs())
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:711:    #: Dict of ``{'state': [(regex, tokentype, new_state), ...], ...}``
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:730:    tokens = {}
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:732:    def get_tokens_unprocessed(self, text, stack=("root",)):
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:734:        Split ``text`` into (tokentype, text) pairs.
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:739:        tokendefs = self._tokens
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:741:        statetokens = tokendefs[statestack[-1]]
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:743:            for rexmatch, action, new_state in statetokens:
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:747:                        if type(action) is _TokenType:
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:775:                        statetokens = tokendefs[statestack[-1]]
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:778:                # We are here only if all state tokens have been considered
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:784:                        statetokens = tokendefs["root"]
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:814:    def get_tokens_unprocessed(self, text=None, context=None):
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:816:        Split ``text`` into (tokentype, text) pairs.
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:819:        tokendefs = self._tokens
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:822:            statetokens = tokendefs["root"]
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:825:            statetokens = tokendefs[ctx.stack[-1]]
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:828:            for rexmatch, action, new_state in statetokens:
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:832:                        if type(action) is _TokenType:
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:839:                                statetokens = tokendefs[ctx.stack[-1]]
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:862:                        statetokens = tokendefs[ctx.stack[-1]]
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:871:                        statetokens = tokendefs["root"]
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:881:def do_insertions(insertions, tokens):
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:886:    ``insertions`` is a list of ``(index, itokens)`` pairs.
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:887:    Each ``itokens`` iterable should be inserted at position
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:888:    ``index`` into the token stream given by the ``tokens``
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:891:    The result is a combined token stream.
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:897:        index, itokens = next(insertions)
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:900:        yield from tokens
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:906:    # iterate over the token stream where we want to insert
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:907:    # the tokens from the insertion list.
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:908:    for i, t, v in tokens:
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:918:            for it_index, it_token, it_value in itokens:
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:919:                yield realpos, it_token, it_value
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:923:                index, itokens = next(insertions)
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:931:    # leftover tokens
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:933:        # no normal tokens, set realpos to zero
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:935:        for p, t, v in itokens:
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:939:            index, itokens = next(insertions)
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:973:    def get_tokens_unprocessed(self, text, stack=("root",)):
./.venv-build/lib/python3.12/site-packages/pygments/lexer.py:976:        yield from RegexLexer.get_tokens_unprocessed(self, text, stack)
./.venv-build/lib/python3.12/site-packages/pygments/cmdline.py:583:        help="Add a filter to the token stream.  (Query names with -L.) "
./.venv-build/lib/python3.12/site-packages/pygments/__init__.py:39:    and return an iterable of tokens. Currently, this only calls
./.venv-build/lib/python3.12/site-packages/pygments/__init__.py:40:    `lexer.get_tokens()`.
./.venv-build/lib/python3.12/site-packages/pygments/__init__.py:43:        return lexer.get_tokens(code)
./.venv-build/lib/python3.12/site-packages/pygments/__init__.py:53:def format(tokens, formatter, outfile=None):  # pylint: disable=redefined-builtin
./.venv-build/lib/python3.12/site-packages/pygments/__init__.py:55:    Format ``tokens`` (an iterable of tokens) with the formatter ``formatter``
./.venv-build/lib/python3.12/site-packages/pygments/__init__.py:65:            formatter.format(tokens, realoutfile)
./.venv-build/lib/python3.12/site-packages/pygments/__init__.py:68:            formatter.format(tokens, outfile)
./.venv-build/lib/python3.12/site-packages/pygments/filters/__init__.py:14:from pygments.token import (
./.venv-build/lib/python3.12/site-packages/pygments/filters/__init__.py:21:    string_to_tokentype,
./.venv-build/lib/python3.12/site-packages/pygments/filters/__init__.py:723:    """Highlight a normal Name (and Name.*) token with a different token type.
./.venv-build/lib/python3.12/site-packages/pygments/filters/__init__.py:729:            tokentype=Name.Function,
./.venv-build/lib/python3.12/site-packages/pygments/filters/__init__.py:733:    as functions. `Name.Function` is the default token type.
./.venv-build/lib/python3.12/site-packages/pygments/filters/__init__.py:738:      A list of names that should be given the different token type.
./.venv-build/lib/python3.12/site-packages/pygments/filters/__init__.py:740:    `tokentype` : TokenType or string
./.venv-build/lib/python3.12/site-packages/pygments/filters/__init__.py:741:      A token type or a string containing a token type name that is
./.venv-build/lib/python3.12/site-packages/pygments/filters/__init__.py:749:        tokentype = options.get("tokentype")
./.venv-build/lib/python3.12/site-packages/pygments/filters/__init__.py:750:        if tokentype:
./.venv-build/lib/python3.12/site-packages/pygments/filters/__init__.py:751:            self.tokentype = string_to_tokentype(tokentype)
./.venv-build/lib/python3.12/site-packages/pygments/filters/__init__.py:753:            self.tokentype = Name.Function
./.venv-build/lib/python3.12/site-packages/pygments/filters/__init__.py:758:                yield self.tokentype, value
./.venv-build/lib/python3.12/site-packages/pygments/filters/__init__.py:763:class ErrorToken(Exception):
./.venv-build/lib/python3.12/site-packages/pygments/filters/__init__.py:767:class RaiseOnErrorTokenFilter(Filter):
./.venv-build/lib/python3.12/site-packages/pygments/filters/__init__.py:768:    """Raise an exception when the lexer generates an error token.
./.venv-build/lib/python3.12/site-packages/pygments/filters/__init__.py:774:      The default is `pygments.filters.ErrorToken`.
./.venv-build/lib/python3.12/site-packages/pygments/filters/__init__.py:781:        self.exception = options.get("excclass", ErrorToken)
./.venv-build/lib/python3.12/site-packages/pygments/filters/__init__.py:818:    `wstokentype` : bool
./.venv-build/lib/python3.12/site-packages/pygments/filters/__init__.py:819:      If true, give whitespace the special `Whitespace` token type.  This allows
./.venv-build/lib/python3.12/site-packages/pygments/filters/__init__.py:839:        self.wstt = get_bool_opt(options, "wstokentype", True)
./.venv-build/lib/python3.12/site-packages/pygments/filters/__init__.py:901:            # Remove ``left`` tokens from first line, ``n`` from all others.
./.venv-build/lib/python3.12/site-packages/pygments/filters/__init__.py:912:class TokenMergeFilter(Filter):
./.venv-build/lib/python3.12/site-packages/pygments/filters/__init__.py:913:    """Merges consecutive tokens with the same token type in the output
./.venv-build/lib/python3.12/site-packages/pygments/filters/__init__.py:941:    "raiseonerror": RaiseOnErrorTokenFilter,
./.venv-build/lib/python3.12/site-packages/pygments/filters/__init__.py:944:    "tokenmerge": TokenMergeFilter,
./.venv-build/lib/python3.12/site-packages/markdown_it/tree.py:1:"""A tree representation of a linear markdown-it token stream.
./.venv-build/lib/python3.12/site-packages/markdown_it/tree.py:12:from .token import Token
./.venv-build/lib/python3.12/site-packages/markdown_it/tree.py:15:class _NesterTokens(NamedTuple):
./.venv-build/lib/python3.12/site-packages/markdown_it/tree.py:16:    opening: Token
./.venv-build/lib/python3.12/site-packages/markdown_it/tree.py:17:    closing: Token
./.venv-build/lib/python3.12/site-packages/markdown_it/tree.py:27:    `markdown-it-py` token stream.
./.venv-build/lib/python3.12/site-packages/markdown_it/tree.py:31:      - a single unnested `Token`
./.venv-build/lib/python3.12/site-packages/markdown_it/tree.py:32:      - a `Token` "_open" and "_close" token pair, and the tokens nested in
./.venv-build/lib/python3.12/site-packages/markdown_it/tree.py:36:    def __init__(self, tokens: Sequence[Token] = (), *, create_root: bool = True) -> None:
./.venv-build/lib/python3.12/site-packages/markdown_it/tree.py:37:        """Initialize a `SyntaxTreeNode` from a token stream.
./.venv-build/lib/python3.12/site-packages/markdown_it/tree.py:41:        # Only nodes representing an unnested token have self.token
./.venv-build/lib/python3.12/site-packages/markdown_it/tree.py:42:        self.token: Token | None = None
./.venv-build/lib/python3.12/site-packages/markdown_it/tree.py:44:        # Only containers have nester tokens
./.venv-build/lib/python3.12/site-packages/markdown_it/tree.py:45:        self.nester_tokens: _NesterTokens | None = None
./.venv-build/lib/python3.12/site-packages/markdown_it/tree.py:50:        # Empty list unless a non-empty container, or unnested token that has
./.venv-build/lib/python3.12/site-packages/markdown_it/tree.py:55:            self._set_children_from_tokens(tokens)
./.venv-build/lib/python3.12/site-packages/markdown_it/tree.py:58:        if not tokens:
./.venv-build/lib/python3.12/site-packages/markdown_it/tree.py:60:                "Can only create root from empty token sequence." " Set `create_root=True`."
./.venv-build/lib/python3.12/site-packages/markdown_it/tree.py:62:        elif len(tokens) == 1:
./.venv-build/lib/python3.12/site-packages/markdown_it/tree.py:63:            inline_token = tokens[0]
./.venv-build/lib/python3.12/site-packages/markdown_it/tree.py:64:            if inline_token.nesting:
./.venv-build/lib/python3.12/site-packages/markdown_it/tree.py:65:                raise ValueError("Unequal nesting level at the start and end of token stream.")
./.venv-build/lib/python3.12/site-packages/markdown_it/tree.py:66:            self.token = inline_token
./.venv-build/lib/python3.12/site-packages/markdown_it/tree.py:67:            if inline_token.children:
./.venv-build/lib/python3.12/site-packages/markdown_it/tree.py:68:                self._set_children_from_tokens(inline_token.children)
./.venv-build/lib/python3.12/site-packages/markdown_it/tree.py:70:            self.nester_tokens = _NesterTokens(tokens[0], tokens[-1])
./.venv-build/lib/python3.12/site-packages/markdown_it/tree.py:71:            self._set_children_from_tokens(tokens[1:-1])
./.venv-build/lib/python3.12/site-packages/markdown_it/tree.py:85:    def to_tokens(self: _NodeType) -> list[Token]:
./.venv-build/lib/python3.12/site-packages/markdown_it/tree.py:86:        """Recover the linear token stream."""
./.venv-build/lib/python3.12/site-packages/markdown_it/tree.py:88:        def recursive_collect_tokens(node: _NodeType, token_list: list[Token]) -> None:
./.venv-build/lib/python3.12/site-packages/markdown_it/tree.py:91:                    recursive_collect_tokens(child, token_list)
./.venv-build/lib/python3.12/site-packages/markdown_it/tree.py:92:            elif node.token:
./.venv-build/lib/python3.12/site-packages/markdown_it/tree.py:93:                token_list.append(node.token)
./.venv-build/lib/python3.12/site-packages/markdown_it/tree.py:95:                assert node.nester_tokens
./.venv-build/lib/python3.12/site-packages/markdown_it/tree.py:96:                token_list.append(node.nester_tokens.opening)
./.venv-build/lib/python3.12/site-packages/markdown_it/tree.py:98:                    recursive_collect_tokens(child, token_list)
./.venv-build/lib/python3.12/site-packages/markdown_it/tree.py:99:                token_list.append(node.nester_tokens.closing)
./.venv-build/lib/python3.12/site-packages/markdown_it/tree.py:101:        tokens: list[Token] = []
./.venv-build/lib/python3.12/site-packages/markdown_it/tree.py:102:        recursive_collect_tokens(self, tokens)
./.venv-build/lib/python3.12/site-packages/markdown_it/tree.py:103:        return tokens
./.venv-build/lib/python3.12/site-packages/markdown_it/tree.py:124:        return not (self.token or self.nester_tokens)
./.venv-build/lib/python3.12/site-packages/markdown_it/tree.py:130:        Returns `True` if the node represents a `Token` pair and tokens in the
./.venv-build/lib/python3.12/site-packages/markdown_it/tree.py:131:        sequence between them, where `Token.nesting` of the first `Token` in
./.venv-build/lib/python3.12/site-packages/markdown_it/tree.py:132:        the pair is 1 and nesting of the other `Token` is -1.
./.venv-build/lib/python3.12/site-packages/markdown_it/tree.py:134:        return bool(self.nester_tokens)
./.venv-build/lib/python3.12/site-packages/markdown_it/tree.py:151:        - `Token.type` if the node represents an unnested token
./.venv-build/lib/python3.12/site-packages/markdown_it/tree.py:152:        - `Token.type` of the opening token, with "_open" suffix stripped, if
./.venv-build/lib/python3.12/site-packages/markdown_it/tree.py:153:            the node represents a nester token pair
./.venv-build/lib/python3.12/site-packages/markdown_it/tree.py:157:        if self.token:
./.venv-build/lib/python3.12/site-packages/markdown_it/tree.py:158:            return self.token.type
./.venv-build/lib/python3.12/site-packages/markdown_it/tree.py:159:        assert self.nester_tokens
./.venv-build/lib/python3.12/site-packages/markdown_it/tree.py:160:        return self.nester_tokens.opening.type.removesuffix("_open")
./.venv-build/lib/python3.12/site-packages/markdown_it/tree.py:186:        tokens: Sequence[Token],
./.venv-build/lib/python3.12/site-packages/markdown_it/tree.py:189:        child = type(self)(tokens, create_root=False)
./.venv-build/lib/python3.12/site-packages/markdown_it/tree.py:193:    def _set_children_from_tokens(self, tokens: Sequence[Token]) -> None:
./.venv-build/lib/python3.12/site-packages/markdown_it/tree.py:194:        """Convert the token stream to a tree structure and set the resulting
./.venv-build/lib/python3.12/site-packages/markdown_it/tree.py:196:        reversed_tokens = list(reversed(tokens))
./.venv-build/lib/python3.12/site-packages/markdown_it/tree.py:197:        while reversed_tokens:
./.venv-build/lib/python3.12/site-packages/markdown_it/tree.py:198:            token = reversed_tokens.pop()
./.venv-build/lib/python3.12/site-packages/markdown_it/tree.py:200:            if not token.nesting:
./.venv-build/lib/python3.12/site-packages/markdown_it/tree.py:201:                self._add_child([token])
./.venv-build/lib/python3.12/site-packages/markdown_it/tree.py:203:            if token.nesting != 1:
./.venv-build/lib/python3.12/site-packages/markdown_it/tree.py:204:                raise ValueError("Invalid token nesting")
./.venv-build/lib/python3.12/site-packages/markdown_it/tree.py:206:            nested_tokens = [token]
./.venv-build/lib/python3.12/site-packages/markdown_it/tree.py:208:            while reversed_tokens and nesting:
./.venv-build/lib/python3.12/site-packages/markdown_it/tree.py:209:                token = reversed_tokens.pop()
./.venv-build/lib/python3.12/site-packages/markdown_it/tree.py:210:                nested_tokens.append(token)
./.venv-build/lib/python3.12/site-packages/markdown_it/tree.py:211:                nesting += token.nesting
./.venv-build/lib/python3.12/site-packages/markdown_it/tree.py:213:                raise ValueError(f"unclosed tokens starting {nested_tokens[0]}")
./.venv-build/lib/python3.12/site-packages/markdown_it/tree.py:215:            self._add_child(nested_tokens)
./.venv-build/lib/python3.12/site-packages/markdown_it/tree.py:240:        The order mimics the order of the underlying linear token
./.venv-build/lib/python3.12/site-packages/markdown_it/tree.py:250:    # of the underlying `Token`s. A root node does not translate to a `Token`
./.venv-build/lib/python3.12/site-packages/markdown_it/tree.py:254:    # There is no mapping for `Token.nesting` because the `is_nested` property
./.venv-build/lib/python3.12/site-packages/markdown_it/tree.py:257:    def _attribute_token(self) -> Token:
./.venv-build/lib/python3.12/site-packages/markdown_it/tree.py:258:        """Return the `Token` that is used as the data source for the
./.venv-build/lib/python3.12/site-packages/markdown_it/tree.py:260:        if self.token:
./.venv-build/lib/python3.12/site-packages/markdown_it/tree.py:261:            return self.token
./.venv-build/lib/python3.12/site-packages/markdown_it/tree.py:262:        if self.nester_tokens:
./.venv-build/lib/python3.12/site-packages/markdown_it/tree.py:263:            return self.nester_tokens.opening
./.venv-build/lib/python3.12/site-packages/markdown_it/tree.py:269:        return self._attribute_token().tag
./.venv-build/lib/python3.12/site-packages/markdown_it/tree.py:274:        return self._attribute_token().attrs
./.venv-build/lib/python3.12/site-packages/markdown_it/tree.py:278:        return self._attribute_token().attrGet(name)
./.venv-build/lib/python3.12/site-packages/markdown_it/tree.py:283:        map_ = self._attribute_token().map
./.venv-build/lib/python3.12/site-packages/markdown_it/tree.py:285:            # Type ignore because `Token`s attribute types are not perfect
./.venv-build/lib/python3.12/site-packages/markdown_it/tree.py:292:        return self._attribute_token().level
./.venv-build/lib/python3.12/site-packages/markdown_it/tree.py:298:        return self._attribute_token().content
./.venv-build/lib/python3.12/site-packages/markdown_it/tree.py:303:        return self._attribute_token().markup
./.venv-build/lib/python3.12/site-packages/markdown_it/tree.py:308:        return self._attribute_token().info
./.venv-build/lib/python3.12/site-packages/markdown_it/tree.py:313:        return self._attribute_token().meta
./.venv-build/lib/python3.12/site-packages/markdown_it/tree.py:317:        """True for block-level tokens, false for inline tokens."""
./.venv-build/lib/python3.12/site-packages/markdown_it/tree.py:318:        return self._attribute_token().block
./.venv-build/lib/python3.12/site-packages/markdown_it/tree.py:324:        return self._attribute_token().hidden
./.venv-build/lib/python3.12/site-packages/markdown_it/utils.py:40:    """Store link label in link/image token's metadata (under Token.meta['label']).
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/paragraph.py:52:    token = state.push("paragraph_open", "p", 1)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/paragraph.py:53:    token.map = [startLine, state.line]
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/paragraph.py:55:    token = state.push("inline", "", 0)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/paragraph.py:56:    token.content = content
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/paragraph.py:57:    token.map = [startLine, state.line]
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/paragraph.py:58:    token.children = []
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/paragraph.py:60:    token = state.push("paragraph_close", "p", -1)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/fence.py:95:    token = state.push("fence", "code", 0)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/fence.py:96:    token.info = params
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/fence.py:97:    token.content = state.getLines(startLine + 1, nextLine, length, True)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/fence.py:98:    token.markup = markup
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/fence.py:99:    token.map = [startLine, state.line]
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/lheading.py:72:    token = state.push("heading_open", "h" + str(level), 1)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/lheading.py:73:    token.markup = marker
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/lheading.py:74:    token.map = [startLine, state.line]
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/lheading.py:76:    token = state.push("inline", "", 0)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/lheading.py:77:    token.content = content
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/lheading.py:78:    token.map = [startLine, state.line - 1]
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/lheading.py:79:    token.children = []
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/lheading.py:81:    token = state.push("heading_close", "h" + str(level), -1)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/lheading.py:82:    token.markup = marker
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/table.py:156:    token = state.push("table_open", "table", 1)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/table.py:157:    token.map = tableLines = [startLine, 0]
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/table.py:159:    token = state.push("thead_open", "thead", 1)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/table.py:160:    token.map = [startLine, startLine + 1]
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/table.py:162:    token = state.push("tr_open", "tr", 1)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/table.py:163:    token.map = [startLine, startLine + 1]
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/table.py:166:        token = state.push("th_open", "th", 1)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/table.py:168:            token.attrs = {"style": "text-align:" + aligns[i]}
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/table.py:170:        token = state.push("inline", "", 0)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/table.py:172:        # since it is helpful to propagate to children tokens
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/table.py:173:        token.map = [startLine, startLine + 1]
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/table.py:174:        token.content = columns[i].strip()
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/table.py:175:        token.children = []
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/table.py:177:        token = state.push("th_close", "th", -1)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/table.py:179:    token = state.push("tr_close", "tr", -1)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/table.py:180:    token = state.push("thead_close", "thead", -1)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/table.py:214:            token = state.push("tbody_open", "tbody", 1)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/table.py:215:            token.map = tbodyLines = [startLine + 2, 0]
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/table.py:217:        token = state.push("tr_open", "tr", 1)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/table.py:218:        token.map = [nextLine, nextLine + 1]
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/table.py:221:            token = state.push("td_open", "td", 1)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/table.py:223:                token.attrs = {"style": "text-align:" + aligns[i]}
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/table.py:225:            token = state.push("inline", "", 0)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/table.py:227:            # since it is helpful to propagate to children tokens
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/table.py:228:            token.map = [nextLine, nextLine + 1]
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/table.py:230:                token.content = columns[i].strip() if columns[i] else ""
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/table.py:232:                token.content = ""
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/table.py:233:            token.children = []
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/table.py:235:            token = state.push("td_close", "td", -1)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/table.py:237:        token = state.push("tr_close", "tr", -1)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/table.py:242:        token = state.push("tbody_close", "tbody", -1)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/table.py:245:    token = state.push("table_close", "table", -1)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/state_block.py:7:from ..token import Token
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/state_block.py:15:    def __init__(self, src: str, md: MarkdownIt, env: EnvType, tokens: list[Token]) -> None:
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/state_block.py:27:        self.tokens = tokens
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/state_block.py:114:            f"(line={self.line},level={self.level},tokens={len(self.tokens)})"
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/state_block.py:117:    def push(self, ttype: str, tag: str, nesting: Literal[-1, 0, 1]) -> Token:
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/state_block.py:118:        """Push new token to "stream"."""
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/state_block.py:119:        token = Token(ttype, tag, nesting)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/state_block.py:120:        token.block = True
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/state_block.py:123:        token.level = self.level
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/state_block.py:126:        self.tokens.append(token)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/state_block.py:127:        return token
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/hr.py:52:    token = state.push("hr", "hr", 0)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/hr.py:53:    token.map = [startLine, state.line]
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/hr.py:54:    token.markup = marker * (cnt + 1)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/blockquote.py:230:            # normally if you call `tokenize(state, startLine, nextLine)`,
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/blockquote.py:261:    token = state.push("blockquote_open", "blockquote", 1)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/blockquote.py:262:    token.markup = ">"
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/blockquote.py:263:    token.map = lines = [startLine, 0]
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/blockquote.py:265:    state.md.block.tokenize(state, startLine, nextLine)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/blockquote.py:267:    token = state.push("blockquote_close", "blockquote", -1)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/blockquote.py:268:    token.markup = ">"
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/code.py:32:    token = state.push("code_block", "code", 0)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/code.py:33:    token.content = state.getLines(startLine, last, 4 + state.blkIndent, False) + "\n"
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/code.py:34:    token.map = [startLine, state.line]
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/reference.py:163:        token = state.push("definition", "", 0)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/reference.py:164:        token.meta = {
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/reference.py:170:        token.map = [startLine, state.line]
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/list.py:92:    length = len(state.tokens) - 2
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/list.py:94:        if state.tokens[i].level == level and state.tokens[i].type == "paragraph_open":
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/list.py:95:            state.tokens[i + 2].hidden = True
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/list.py:96:            state.tokens[i].hidden = True
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/list.py:164:    listTokIdx = len(state.tokens)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/list.py:167:        token = state.push("ordered_list_open", "ol", 1)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/list.py:169:            token.attrs = {"start": markerValue}
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/list.py:172:        token = state.push("bullet_list_open", "ul", 1)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/list.py:174:    token.map = listLines = [startLine, 0]
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/list.py:175:    token.markup = markerChar
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/list.py:224:        # Run subparser & write tokens
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/list.py:225:        token = state.push("list_item_open", "li", 1)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/list.py:226:        token.markup = markerChar
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/list.py:227:        token.map = itemLines = [startLine, 0]
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/list.py:229:            token.info = state.src[start : posAfterMarker - 1]
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/list.py:259:            # state.md.block.tokenize(state, startLine, endLine, True)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/list.py:260:            # but  tokeniz does not take the final parameter
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/list.py:261:            state.md.block.tokenize(state, startLine, endLine)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/list.py:277:        token = state.push("list_item_close", "li", -1)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/list.py:278:        token.markup = markerChar
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/list.py:323:        token = state.push("ordered_list_close", "ol", -1)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/list.py:325:        token = state.push("bullet_list_close", "ul", -1)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/list.py:327:    token.markup = markerChar
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/html_block.py:84:    token = state.push("html_block", "", 0)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/html_block.py:85:    token.map = [startLine, nextLine]
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/html_block.py:86:    token.content = state.getLines(startLine, nextLine, state.blkIndent, True)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/heading.py:57:    token = state.push("heading_open", "h" + str(level), 1)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/heading.py:58:    token.markup = "########"[:level]
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/heading.py:59:    token.map = [startLine, state.line]
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/heading.py:61:    token = state.push("inline", "", 0)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/heading.py:62:    token.content = state.src[pos:maximum].strip()
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/heading.py:63:    token.map = [startLine, state.line]
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/heading.py:64:    token.children = []
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/heading.py:66:    token = state.push("heading_close", "h" + str(level), -1)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_block/heading.py:67:    token.markup = "########"[:level]
./.venv-build/lib/python3.12/site-packages/markdown_it/token.py:10:    """Convert Token.attrs set as ``None`` or ``[[key, value], ...]`` to a dict.
./.venv-build/lib/python3.12/site-packages/markdown_it/token.py:22:class Token:
./.venv-build/lib/python3.12/site-packages/markdown_it/token.py:24:    """Type of the token (string, e.g. "paragraph_open")"""
./.venv-build/lib/python3.12/site-packages/markdown_it/token.py:48:    children: list[Token] | None = None
./.venv-build/lib/python3.12/site-packages/markdown_it/token.py:49:    """Array of child nodes (inline and img tokens)."""
./.venv-build/lib/python3.12/site-packages/markdown_it/token.py:59:    - Info string for "fence" tokens
./.venv-build/lib/python3.12/site-packages/markdown_it/token.py:60:    - The value "auto" for autolink "link_open" and "link_close" tokens
./.venv-build/lib/python3.12/site-packages/markdown_it/token.py:61:    - The string value of the item marker for ordered-list "list_item_open" tokens
./.venv-build/lib/python3.12/site-packages/markdown_it/token.py:68:    """True for block-level tokens, false for inline tokens.
./.venv-build/lib/python3.12/site-packages/markdown_it/token.py:82:            "Token.attrIndex should not be used, since Token.attrs is a dictionary",
./.venv-build/lib/python3.12/site-packages/markdown_it/token.py:109:        Useful to operate with token classes.
./.venv-build/lib/python3.12/site-packages/markdown_it/token.py:119:    def copy(self, **changes: Any) -> Token:
./.venv-build/lib/python3.12/site-packages/markdown_it/token.py:132:        """Return the token as a dictionary.
./.venv-build/lib/python3.12/site-packages/markdown_it/token.py:137:        :param meta_serializer: hook for serializing ``Token.meta``
./.venv-build/lib/python3.12/site-packages/markdown_it/token.py:169:    def from_dict(cls, dct: MutableMapping[str, Any]) -> Token:
./.venv-build/lib/python3.12/site-packages/markdown_it/token.py:170:        """Convert a dict to a Token."""
./.venv-build/lib/python3.12/site-packages/markdown_it/token.py:171:        token = cls(**dct)
./.venv-build/lib/python3.12/site-packages/markdown_it/token.py:172:        if token.children:
./.venv-build/lib/python3.12/site-packages/markdown_it/token.py:173:            token.children = [cls.from_dict(c) for c in token.children]  # type: ignore[arg-type]
./.venv-build/lib/python3.12/site-packages/markdown_it/token.py:174:        return token
./.venv-build/lib/python3.12/site-packages/markdown_it/parser_inline.py:1:"""Tokenizes paragraph content."""
./.venv-build/lib/python3.12/site-packages/markdown_it/parser_inline.py:11:from .token import Token
./.venv-build/lib/python3.12/site-packages/markdown_it/parser_inline.py:22:`silent` disables token generation, useful for lookahead.
./.venv-build/lib/python3.12/site-packages/markdown_it/parser_inline.py:30:    ("strikethrough", rules_inline.strikethrough.tokenize),
./.venv-build/lib/python3.12/site-packages/markdown_it/parser_inline.py:31:    ("emphasis", rules_inline.emphasis.tokenize),
./.venv-build/lib/python3.12/site-packages/markdown_it/parser_inline.py:49:    # rules for pairs separate '**' into its own text tokens, which may be left unused,
./.venv-build/lib/python3.12/site-packages/markdown_it/parser_inline.py:65:    def skipToken(self, state: StateInline) -> None:
./.venv-build/lib/python3.12/site-packages/markdown_it/parser_inline.py:66:        """Skip single token by running all rules in validation mode;
./.venv-build/lib/python3.12/site-packages/markdown_it/parser_inline.py:82:                # It's harmless to do here, because no tokens are created.
./.venv-build/lib/python3.12/site-packages/markdown_it/parser_inline.py:107:    def tokenize(self, state: StateInline) -> None:
./.venv-build/lib/python3.12/site-packages/markdown_it/parser_inline.py:108:        """Generate tokens for input range."""
./.venv-build/lib/python3.12/site-packages/markdown_it/parser_inline.py:119:            # - update `state.tokens`
./.venv-build/lib/python3.12/site-packages/markdown_it/parser_inline.py:139:    def parse(self, src: str, md: MarkdownIt, env: EnvType, tokens: list[Token]) -> list[Token]:
./.venv-build/lib/python3.12/site-packages/markdown_it/parser_inline.py:140:        """Process input string and push inline tokens into `tokens`"""
./.venv-build/lib/python3.12/site-packages/markdown_it/parser_inline.py:141:        state = StateInline(src, md, env, tokens)
./.venv-build/lib/python3.12/site-packages/markdown_it/parser_inline.py:142:        self.tokenize(state)
./.venv-build/lib/python3.12/site-packages/markdown_it/parser_inline.py:146:        return state.tokens
./.venv-build/lib/python3.12/site-packages/markdown_it/main.py:14:from .token import Token
./.venv-build/lib/python3.12/site-packages/markdown_it/main.py:217:        """Add a rule for rendering a particular Token type.
./.venv-build/lib/python3.12/site-packages/markdown_it/main.py:231:            def func(tokens, idx):
./.venv-build/lib/python3.12/site-packages/markdown_it/main.py:232:                tokens[idx].content = tokens[idx].content.replace('foo', 'bar')
./.venv-build/lib/python3.12/site-packages/markdown_it/main.py:239:    def parse(self, src: str, env: EnvType | None = None) -> list[Token]:
./.venv-build/lib/python3.12/site-packages/markdown_it/main.py:240:        """Parse the source string to a token stream
./.venv-build/lib/python3.12/site-packages/markdown_it/main.py:245:        Parse input string and return list of block tokens (special token type
./.venv-build/lib/python3.12/site-packages/markdown_it/main.py:246:        "inline" will contain list of inline tokens).
./.venv-build/lib/python3.12/site-packages/markdown_it/main.py:260:        return state.tokens
./.venv-build/lib/python3.12/site-packages/markdown_it/main.py:276:    def parseInline(self, src: str, env: EnvType | None = None) -> list[Token]:
./.venv-build/lib/python3.12/site-packages/markdown_it/main.py:283:        block tokens list with the single `inline` element, containing parsed inline
./.venv-build/lib/python3.12/site-packages/markdown_it/main.py:284:        tokens in `children` property. Also updates `env` object.
./.venv-build/lib/python3.12/site-packages/markdown_it/main.py:294:        return state.tokens
./.venv-build/lib/python3.12/site-packages/markdown_it/port.yaml:19:      `Token.attrs` is a dictionary, instead of a list of lists.
./.venv-build/lib/python3.12/site-packages/markdown_it/port.yaml:23:      to manipulate `Token.attrs`, which have an identical signature to those upstream.
./.venv-build/lib/python3.12/site-packages/markdown_it/port.yaml:40:      `func(tokens, idx, options, env, slf)` to
./.venv-build/lib/python3.12/site-packages/markdown_it/port.yaml:41:      `func(self, tokens, idx, options, env)`
./.venv-build/lib/python3.12/site-packages/markdown_it/port.yaml:48:    - inline tokens in tables are assigned a map (this is helpful for propagation to children)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/escape.py:51:        token = state.push("text_special", "", 0)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/escape.py:52:        token.content = escapedStr if ch1 in _ESCAPED else origStr
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/escape.py:53:        token.markup = origStr
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/escape.py:54:        token.info = "escape"
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/entity.py:28:                token = state.push("text_special", "", 0)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/entity.py:29:                token.content = (
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/entity.py:32:                token.markup = match.group(0)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/entity.py:33:                token.info = "entity"
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/entity.py:41:                token = state.push("text_special", "", 0)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/entity.py:42:                token.content = entities[match.group(1)]
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/entity.py:43:                token.markup = match.group(0)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/entity.py:44:                token.info = "entity"
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/link.py:125:    # so all that's left to do is to call tokenizer.
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/link.py:131:        token = state.push("link_open", "a", 1)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/link.py:132:        token.attrs = {"href": href}
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/link.py:135:            token.attrSet("title", title)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/link.py:139:            token.meta["label"] = label
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/link.py:142:        state.md.inline.tokenize(state)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/link.py:145:        token = state.push("link_close", "a", -1)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/linkify.py:49:        token = state.push("link_open", "a", 1)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/linkify.py:50:        token.attrs = {"href": full_url}
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/linkify.py:51:        token.markup = "linkify"
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/linkify.py:52:        token.info = "auto"
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/linkify.py:54:        token = state.push("text", "", 0)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/linkify.py:55:        token.content = state.md.normalizeLinkText(url)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/linkify.py:57:        token = state.push("link_close", "a", -1)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/linkify.py:58:        token.markup = "linkify"
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/linkify.py:59:        token.info = "auto"
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/image.py:5:from ..token import Token
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/image.py:126:    # so all that's left to do is to call tokenizer.
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/image.py:131:        tokens: list[Token] = []
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/image.py:132:        state.md.inline.parse(content, state.md, state.env, tokens)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/image.py:134:        token = state.push("image", "img", 0)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/image.py:135:        token.attrs = {"src": href, "alt": ""}
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/image.py:136:        token.children = tokens or None
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/image.py:137:        token.content = content
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/image.py:140:            token.attrSet("title", title)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/image.py:144:            token.meta["label"] = label
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/emphasis.py:8:def tokenize(state: StateInline, silent: bool) -> bool:
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/emphasis.py:9:    """Insert each marker as a separate text token, and add it to delimiter list"""
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/emphasis.py:22:        token = state.push("text", "", 0)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/emphasis.py:23:        token.content = marker
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/emphasis.py:28:                token=len(state.tokens) - 1,
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/emphasis.py:67:            and delimiters[i - 1].token == startDelim.token - 1
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/emphasis.py:69:            and delimiters[startDelim.end + 1].token == endDelim.token + 1
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/emphasis.py:74:        token = state.tokens[startDelim.token]
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/emphasis.py:75:        token.type = "strong_open" if isStrong else "em_open"
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/emphasis.py:76:        token.tag = "strong" if isStrong else "em"
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/emphasis.py:77:        token.nesting = 1
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/emphasis.py:78:        token.markup = ch + ch if isStrong else ch
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/emphasis.py:79:        token.content = ""
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/emphasis.py:81:        token = state.tokens[endDelim.token]
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/emphasis.py:82:        token.type = "strong_close" if isStrong else "em_close"
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/emphasis.py:83:        token.tag = "strong" if isStrong else "em"
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/emphasis.py:84:        token.nesting = -1
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/emphasis.py:85:        token.markup = ch + ch if isStrong else ch
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/emphasis.py:86:        token.content = ""
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/emphasis.py:89:            state.tokens[delimiters[i - 1].token].content = ""
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/emphasis.py:90:            state.tokens[delimiters[startDelim.end + 1].token].content = ""
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/emphasis.py:97:    """Walk through delimiter list and replace text tokens with tags."""
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/emphasis.py:100:    for token in state.tokens_meta:
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/emphasis.py:101:        if token and "delimiters" in token:
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/emphasis.py:102:            _postProcess(state, token["delimiters"])
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/autolink.py:41:            token = state.push("link_open", "a", 1)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/autolink.py:42:            token.attrs = {"href": fullUrl}
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/autolink.py:43:            token.markup = "autolink"
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/autolink.py:44:            token.info = "auto"
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/autolink.py:46:            token = state.push("text", "", 0)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/autolink.py:47:            token.content = state.md.normalizeLinkText(url)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/autolink.py:49:            token = state.push("link_close", "a", -1)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/autolink.py:50:            token.markup = "autolink"
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/autolink.py:51:            token.info = "auto"
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/autolink.py:62:            token = state.push("link_open", "a", 1)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/autolink.py:63:            token.attrs = {"href": fullUrl}
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/autolink.py:64:            token.markup = "autolink"
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/autolink.py:65:            token.info = "auto"
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/autolink.py:67:            token = state.push("text", "", 0)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/autolink.py:68:            token.content = state.md.normalizeLinkText(url)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/autolink.py:70:            token = state.push("link_close", "a", -1)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/autolink.py:71:            token.markup = "autolink"
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/autolink.py:72:            token.info = "auto"
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/fragments_join.py:6:    Clean up tokens after emphasis and strikethrough postprocessing:
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/fragments_join.py:7:    merge adjacent text nodes into one and re-calculate all token levels
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/fragments_join.py:10:    are treated as their own separate text tokens. Then emphasis rule either
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/fragments_join.py:15:    maximum = len(state.tokens)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/fragments_join.py:21:        if state.tokens[curr].nesting < 0:
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/fragments_join.py:23:        state.tokens[curr].level = level
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/fragments_join.py:24:        if state.tokens[curr].nesting > 0:
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/fragments_join.py:28:            state.tokens[curr].type == "text"
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/fragments_join.py:30:            and state.tokens[curr + 1].type == "text"
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/fragments_join.py:33:            state.tokens[curr + 1].content = (
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/fragments_join.py:34:                state.tokens[curr].content + state.tokens[curr + 1].content
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/fragments_join.py:38:                state.tokens[last] = state.tokens[curr]
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/fragments_join.py:43:        del state.tokens[last:]
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/state_inline.py:9:from ..token import Token
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/state_inline.py:24:    # A position of the token this delimiter corresponds to.
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/state_inline.py:25:    token: int
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/state_inline.py:43:    def __init__(self, src: str, md: MarkdownIt, env: EnvType, outTokens: list[Token]) -> None:
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/state_inline.py:47:        self.tokens = outTokens
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/state_inline.py:48:        self.tokens_meta: list[dict[str, Any] | None] = [None] * len(outTokens)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/state_inline.py:77:            f"(pos=[{self.pos} of {self.posMax}], token={len(self.tokens)})"
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/state_inline.py:80:    def pushPending(self) -> Token:
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/state_inline.py:81:        token = Token("text", "", 0)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/state_inline.py:82:        token.content = self.pending
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/state_inline.py:83:        token.level = self.pendingLevel
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/state_inline.py:84:        self.tokens.append(token)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/state_inline.py:86:        return token
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/state_inline.py:88:    def push(self, ttype: str, tag: str, nesting: Literal[-1, 0, 1]) -> Token:
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/state_inline.py:89:        """Push new token to "stream".
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/state_inline.py:90:        If pending text exists - flush it as text token
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/state_inline.py:95:        token = Token(ttype, tag, nesting)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/state_inline.py:96:        token_meta = None
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/state_inline.py:103:        token.level = self.level
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/state_inline.py:110:            token_meta = {"delimiters": self.delimiters}
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/state_inline.py:113:        self.tokens.append(token)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/state_inline.py:114:        self.tokens_meta.append(token_meta)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/state_inline.py:115:        return token
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/html_inline.py:34:        token = state.push("html_inline", "", 0)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/html_inline.py:35:        token.content = state.src[pos : pos + len(match.group(0))]
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/html_inline.py:37:        if isLinkOpen(token.content):
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/html_inline.py:39:        if isLinkClose(token.content):
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/backticks.py:51:                token = state.push("code_inline", "code", 0)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/backticks.py:52:                token.markup = marker
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/backticks.py:53:                token.content = state.src[pos:matchStart].replace("\n", " ")
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/backticks.py:55:                    token.content.startswith(" ")
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/backticks.py:56:                    and token.content.endswith(" ")
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/backticks.py:57:                    and len(token.content.strip()) > 0
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/backticks.py:59:                    token.content = token.content[1:-1]
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/text.py:4:# Skip text characters for text token, place those to pending buffer
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/balance_pairs.py:1:"""Balance paired characters (*, _, etc) in inline tokens."""
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/balance_pairs.py:18:    lastTokenIdx = -2  # needs any value lower than -1
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/balance_pairs.py:27:        #  - they have adjacent tokens
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/balance_pairs.py:30:        if delimiters[headerIdx].marker != closer.marker or lastTokenIdx != closer.token - 1:
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/balance_pairs.py:32:        lastTokenIdx = closer.token
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/balance_pairs.py:99:                    # treat next token as start of run,
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/balance_pairs.py:101:                    lastTokenIdx = -2
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/balance_pairs.py:123:    tokens_meta = state.tokens_meta
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/balance_pairs.py:124:    maximum = len(state.tokens_meta)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/balance_pairs.py:130:        curr_meta = tokens_meta[curr]
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/strikethrough.py:7:def tokenize(state: StateInline, silent: bool) -> bool:
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/strikethrough.py:8:    """Insert each marker as a separate text token, and add it to delimiter list"""
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/strikethrough.py:25:        token = state.push("text", "", 0)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/strikethrough.py:26:        token.content = ch
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/strikethrough.py:31:        token = state.push("text", "", 0)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/strikethrough.py:32:        token.content = ch + ch
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/strikethrough.py:37:                token=len(state.tokens) - 1,
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/strikethrough.py:69:        token = state.tokens[startDelim.token]
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/strikethrough.py:70:        token.type = "s_open"
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/strikethrough.py:71:        token.tag = "s"
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/strikethrough.py:72:        token.nesting = 1
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/strikethrough.py:73:        token.markup = "~~"
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/strikethrough.py:74:        token.content = ""
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/strikethrough.py:76:        token = state.tokens[endDelim.token]
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/strikethrough.py:77:        token.type = "s_close"
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/strikethrough.py:78:        token.tag = "s"
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/strikethrough.py:79:        token.nesting = -1
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/strikethrough.py:80:        token.markup = "~~"
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/strikethrough.py:81:        token.content = ""
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/strikethrough.py:84:            state.tokens[endDelim.token - 1].type == "text"
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/strikethrough.py:85:            and state.tokens[endDelim.token - 1].content == "~"
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/strikethrough.py:87:            loneMarkers.append(endDelim.token - 1)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/strikethrough.py:101:        while (j < len(state.tokens)) and (state.tokens[j].type == "s_close"):
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/strikethrough.py:107:            token = state.tokens[j]
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/strikethrough.py:108:            state.tokens[j] = state.tokens[i]
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/strikethrough.py:109:            state.tokens[i] = token
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/strikethrough.py:113:    """Walk through delimiter list and replace text tokens with tags."""
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/strikethrough.py:114:    tokens_meta = state.tokens_meta
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/strikethrough.py:115:    maximum = len(state.tokens_meta)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_inline/strikethrough.py:121:            curr_meta = tokens_meta[curr]
./.venv-build/lib/python3.12/site-packages/markdown_it/renderer.py:4:Generates HTML from parsed token stream. Each instance has independent
./.venv-build/lib/python3.12/site-packages/markdown_it/renderer.py:6:rules if you create plugin and adds new token types.
./.venv-build/lib/python3.12/site-packages/markdown_it/renderer.py:16:from .token import Token
./.venv-build/lib/python3.12/site-packages/markdown_it/renderer.py:23:    def render(self, tokens: Sequence[Token], options: OptionsDict, env: EnvType) -> Any: ...
./.venv-build/lib/python3.12/site-packages/markdown_it/renderer.py:27:    """Contains render rules for tokens. Can be updated and extended.
./.venv-build/lib/python3.12/site-packages/markdown_it/renderer.py:36:            def token_type_name(self, tokens, idx, options, env) {
./.venv-build/lib/python3.12/site-packages/markdown_it/renderer.py:43:            def strong_open(self, tokens, idx, options, env):
./.venv-build/lib/python3.12/site-packages/markdown_it/renderer.py:45:            def strong_close(self, tokens, idx, options, env):
./.venv-build/lib/python3.12/site-packages/markdown_it/renderer.py:65:    def render(self, tokens: Sequence[Token], options: OptionsDict, env: EnvType) -> str:
./.venv-build/lib/python3.12/site-packages/markdown_it/renderer.py:66:        """Takes token stream and generates HTML.
./.venv-build/lib/python3.12/site-packages/markdown_it/renderer.py:68:        :param tokens: list on block tokens to render
./.venv-build/lib/python3.12/site-packages/markdown_it/renderer.py:75:        for i, token in enumerate(tokens):
./.venv-build/lib/python3.12/site-packages/markdown_it/renderer.py:76:            if token.type == "inline":
./.venv-build/lib/python3.12/site-packages/markdown_it/renderer.py:77:                if token.children:
./.venv-build/lib/python3.12/site-packages/markdown_it/renderer.py:78:                    result += self.renderInline(token.children, options, env)
./.venv-build/lib/python3.12/site-packages/markdown_it/renderer.py:79:            elif token.type in self.rules:
./.venv-build/lib/python3.12/site-packages/markdown_it/renderer.py:80:                result += self.rules[token.type](tokens, i, options, env)
./.venv-build/lib/python3.12/site-packages/markdown_it/renderer.py:82:                result += self.renderToken(tokens, i, options, env)
./.venv-build/lib/python3.12/site-packages/markdown_it/renderer.py:86:    def renderInline(self, tokens: Sequence[Token], options: OptionsDict, env: EnvType) -> str:
./.venv-build/lib/python3.12/site-packages/markdown_it/renderer.py:87:        """The same as ``render``, but for single token of `inline` type.
./.venv-build/lib/python3.12/site-packages/markdown_it/renderer.py:89:        :param tokens: list on block tokens to render
./.venv-build/lib/python3.12/site-packages/markdown_it/renderer.py:95:        for i, token in enumerate(tokens):
./.venv-build/lib/python3.12/site-packages/markdown_it/renderer.py:96:            if token.type in self.rules:
./.venv-build/lib/python3.12/site-packages/markdown_it/renderer.py:97:                result += self.rules[token.type](tokens, i, options, env)
./.venv-build/lib/python3.12/site-packages/markdown_it/renderer.py:99:                result += self.renderToken(tokens, i, options, env)
./.venv-build/lib/python3.12/site-packages/markdown_it/renderer.py:103:    def renderToken(
./.venv-build/lib/python3.12/site-packages/markdown_it/renderer.py:105:        tokens: Sequence[Token],
./.venv-build/lib/python3.12/site-packages/markdown_it/renderer.py:110:        """Default token renderer.
./.venv-build/lib/python3.12/site-packages/markdown_it/renderer.py:114:        :param idx: token index to render
./.venv-build/lib/python3.12/site-packages/markdown_it/renderer.py:119:        token = tokens[idx]
./.venv-build/lib/python3.12/site-packages/markdown_it/renderer.py:122:        if token.hidden:
./.venv-build/lib/python3.12/site-packages/markdown_it/renderer.py:132:        if token.block and token.nesting != -1 and idx and tokens[idx - 1].hidden:
./.venv-build/lib/python3.12/site-packages/markdown_it/renderer.py:135:        # Add token name, e.g. `<img`
./.venv-build/lib/python3.12/site-packages/markdown_it/renderer.py:136:        result += ("</" if token.nesting == -1 else "<") + token.tag
./.venv-build/lib/python3.12/site-packages/markdown_it/renderer.py:139:        result += self.renderAttrs(token)
./.venv-build/lib/python3.12/site-packages/markdown_it/renderer.py:142:        if token.nesting == 0 and options["xhtmlOut"]:
./.venv-build/lib/python3.12/site-packages/markdown_it/renderer.py:146:        if token.block:
./.venv-build/lib/python3.12/site-packages/markdown_it/renderer.py:149:            if token.nesting == 1 and (idx + 1 < len(tokens)):
./.venv-build/lib/python3.12/site-packages/markdown_it/renderer.py:150:                nextToken = tokens[idx + 1]
./.venv-build/lib/python3.12/site-packages/markdown_it/renderer.py:152:                if nextToken.type == "inline" or nextToken.hidden:
./.venv-build/lib/python3.12/site-packages/markdown_it/renderer.py:157:                elif nextToken.nesting == -1 and nextToken.tag == token.tag:
./.venv-build/lib/python3.12/site-packages/markdown_it/renderer.py:167:    def renderAttrs(token: Token) -> str:
./.venv-build/lib/python3.12/site-packages/markdown_it/renderer.py:168:        """Render token attributes to string."""
./.venv-build/lib/python3.12/site-packages/markdown_it/renderer.py:171:        for key, value in token.attrItems():
./.venv-build/lib/python3.12/site-packages/markdown_it/renderer.py:178:        tokens: Sequence[Token] | None,
./.venv-build/lib/python3.12/site-packages/markdown_it/renderer.py:187:        :param tokens: list on block tokens to render
./.venv-build/lib/python3.12/site-packages/markdown_it/renderer.py:193:        for token in tokens or []:
./.venv-build/lib/python3.12/site-packages/markdown_it/renderer.py:194:            if token.type == "text":
./.venv-build/lib/python3.12/site-packages/markdown_it/renderer.py:195:                result += token.content
./.venv-build/lib/python3.12/site-packages/markdown_it/renderer.py:196:            elif token.type == "image":
./.venv-build/lib/python3.12/site-packages/markdown_it/renderer.py:197:                if token.children:
./.venv-build/lib/python3.12/site-packages/markdown_it/renderer.py:198:                    result += self.renderInlineAsText(token.children, options, env)
./.venv-build/lib/python3.12/site-packages/markdown_it/renderer.py:199:            elif token.type == "softbreak":
./.venv-build/lib/python3.12/site-packages/markdown_it/renderer.py:207:        self, tokens: Sequence[Token], idx: int, options: OptionsDict, env: EnvType
./.venv-build/lib/python3.12/site-packages/markdown_it/renderer.py:209:        token = tokens[idx]
./.venv-build/lib/python3.12/site-packages/markdown_it/renderer.py:210:        return "<code" + self.renderAttrs(token) + ">" + escapeHtml(tokens[idx].content) + "</code>"
./.venv-build/lib/python3.12/site-packages/markdown_it/renderer.py:214:        tokens: Sequence[Token],
./.venv-build/lib/python3.12/site-packages/markdown_it/renderer.py:219:        token = tokens[idx]
./.venv-build/lib/python3.12/site-packages/markdown_it/renderer.py:223:            + self.renderAttrs(token)
./.venv-build/lib/python3.12/site-packages/markdown_it/renderer.py:225:            + escapeHtml(tokens[idx].content)
./.venv-build/lib/python3.12/site-packages/markdown_it/renderer.py:231:        tokens: Sequence[Token],
./.venv-build/lib/python3.12/site-packages/markdown_it/renderer.py:236:        token = tokens[idx]
./.venv-build/lib/python3.12/site-packages/markdown_it/renderer.py:237:        info = unescapeAll(token.info).strip() if token.info else ""
./.venv-build/lib/python3.12/site-packages/markdown_it/renderer.py:248:            highlighted = options.highlight(token.content, langName, langAttrs) or escapeHtml(
./.venv-build/lib/python3.12/site-packages/markdown_it/renderer.py:249:                token.content
./.venv-build/lib/python3.12/site-packages/markdown_it/renderer.py:252:            highlighted = escapeHtml(token.content)
./.venv-build/lib/python3.12/site-packages/markdown_it/renderer.py:257:        # If language exists, inject class gently, without modifying original token.
./.venv-build/lib/python3.12/site-packages/markdown_it/renderer.py:258:        # May be, one day we will add .deepClone() for token and simplify this part, but
./.venv-build/lib/python3.12/site-packages/markdown_it/renderer.py:261:            # Fake token just to render attributes
./.venv-build/lib/python3.12/site-packages/markdown_it/renderer.py:262:            tmpToken = Token(type="", tag="", nesting=0, attrs=token.attrs.copy())
./.venv-build/lib/python3.12/site-packages/markdown_it/renderer.py:263:            tmpToken.attrJoin("class", options.langPrefix + langName)
./.venv-build/lib/python3.12/site-packages/markdown_it/renderer.py:265:            return "<pre><code" + self.renderAttrs(tmpToken) + ">" + highlighted + "</code></pre>\n"
./.venv-build/lib/python3.12/site-packages/markdown_it/renderer.py:267:        return "<pre><code" + self.renderAttrs(token) + ">" + highlighted + "</code></pre>\n"
./.venv-build/lib/python3.12/site-packages/markdown_it/renderer.py:271:        tokens: Sequence[Token],
./.venv-build/lib/python3.12/site-packages/markdown_it/renderer.py:276:        token = tokens[idx]
./.venv-build/lib/python3.12/site-packages/markdown_it/renderer.py:280:        if token.children:
./.venv-build/lib/python3.12/site-packages/markdown_it/renderer.py:281:            token.attrSet("alt", self.renderInlineAsText(token.children, options, env))
./.venv-build/lib/python3.12/site-packages/markdown_it/renderer.py:283:            token.attrSet("alt", "")
./.venv-build/lib/python3.12/site-packages/markdown_it/renderer.py:285:        return self.renderToken(tokens, idx, options, env)
./.venv-build/lib/python3.12/site-packages/markdown_it/renderer.py:288:        self, tokens: Sequence[Token], idx: int, options: OptionsDict, env: EnvType
./.venv-build/lib/python3.12/site-packages/markdown_it/renderer.py:293:        self, tokens: Sequence[Token], idx: int, options: OptionsDict, env: EnvType
./.venv-build/lib/python3.12/site-packages/markdown_it/renderer.py:297:    def text(self, tokens: Sequence[Token], idx: int, options: OptionsDict, env: EnvType) -> str:
./.venv-build/lib/python3.12/site-packages/markdown_it/renderer.py:298:        return escapeHtml(tokens[idx].content)
./.venv-build/lib/python3.12/site-packages/markdown_it/renderer.py:301:        self, tokens: Sequence[Token], idx: int, options: OptionsDict, env: EnvType
./.venv-build/lib/python3.12/site-packages/markdown_it/renderer.py:303:        return tokens[idx].content
./.venv-build/lib/python3.12/site-packages/markdown_it/renderer.py:306:        self, tokens: Sequence[Token], idx: int, options: OptionsDict, env: EnvType
./.venv-build/lib/python3.12/site-packages/markdown_it/renderer.py:308:        return tokens[idx].content
./.venv-build/lib/python3.12/site-packages/markdown_it/parser_block.py:1:"""Block-level tokenizer."""
./.venv-build/lib/python3.12/site-packages/markdown_it/parser_block.py:12:from .token import Token
./.venv-build/lib/python3.12/site-packages/markdown_it/parser_block.py:24:`silent` disables token generation, useful for lookahead.
./.venv-build/lib/python3.12/site-packages/markdown_it/parser_block.py:60:    def tokenize(self, state: StateBlock, startLine: int, endLine: int) -> None:
./.venv-build/lib/python3.12/site-packages/markdown_it/parser_block.py:61:        """Generate tokens for input range."""
./.venv-build/lib/python3.12/site-packages/markdown_it/parser_block.py:84:            # - update `state.tokens`
./.venv-build/lib/python3.12/site-packages/markdown_it/parser_block.py:106:        self, src: str, md: MarkdownIt, env: EnvType, outTokens: list[Token]
./.venv-build/lib/python3.12/site-packages/markdown_it/parser_block.py:107:    ) -> list[Token] | None:
./.venv-build/lib/python3.12/site-packages/markdown_it/parser_block.py:108:        """Process input string and push block tokens into `outTokens`."""
./.venv-build/lib/python3.12/site-packages/markdown_it/parser_block.py:111:        state = StateBlock(src, md, env, outTokens)
./.venv-build/lib/python3.12/site-packages/markdown_it/parser_block.py:112:        self.tokenize(state, state.line, state.lineMax)
./.venv-build/lib/python3.12/site-packages/markdown_it/parser_block.py:113:        return state.tokens
./.venv-build/lib/python3.12/site-packages/markdown_it/helpers/parse_link_label.py:29:        state.md.inline.skipToken(state)
./.venv-build/lib/python3.12/site-packages/markdown_it/helpers/parse_link_label.py:33:                # which is not a part of any token
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/block.py:1:from ..token import Token
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/block.py:7:        token = Token("inline", "", 0)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/block.py:8:        token.content = state.src
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/block.py:9:        token.map = [0, 1]
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/block.py:10:        token.children = []
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/block.py:11:        state.tokens.append(token)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/block.py:13:        state.md.block.parse(state.src, state.md, state.env, state.tokens)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/text_join.py:1:"""Join raw text tokens with the rest of the text
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/text_join.py:11:from ..token import Token
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/text_join.py:16:    """Join raw text for escape sequences (`text_special`) tokens with the rest of the text"""
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/text_join.py:18:    for inline_token in state.tokens[:]:
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/text_join.py:19:        if inline_token.type != "inline":
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/text_join.py:23:        new_tokens: list[Token] = []
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/text_join.py:24:        for child_token in inline_token.children or []:
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/text_join.py:25:            if child_token.type == "text_special":
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/text_join.py:26:                child_token.type = "text"
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/text_join.py:27:            if child_token.type == "text" and new_tokens and new_tokens[-1].type == "text":
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/text_join.py:28:                new_tokens[-1].content += child_token.content
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/text_join.py:30:                new_tokens.append(child_token)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/text_join.py:31:        inline_token.children = new_tokens
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/linkify.py:7:from ..token import Token
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/linkify.py:23:    for inline_token in state.tokens:
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/linkify.py:24:        if inline_token.type != "inline" or not state.md.linkify.pretest(inline_token.content):
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/linkify.py:27:        tokens = inline_token.children
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/linkify.py:33:        assert tokens is not None
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/linkify.py:34:        i = len(tokens)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/linkify.py:37:            assert isinstance(tokens, list)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/linkify.py:38:            currentToken = tokens[i]
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/linkify.py:41:            if currentToken.type == "link_close":
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/linkify.py:43:                while tokens[i].level != currentToken.level and tokens[i].type != "link_open":
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/linkify.py:48:            if currentToken.type == "html_inline":
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/linkify.py:49:                if isLinkOpen(currentToken.content) and htmlLinkLevel > 0:
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/linkify.py:51:                if isLinkClose(currentToken.content):
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/linkify.py:56:            if currentToken.type == "text" and state.md.linkify.test(currentToken.content):
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/linkify.py:57:                text = currentToken.content
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/linkify.py:62:                level = currentToken.level
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/linkify.py:68:                if links and links[0].index == 0 and i > 0 and tokens[i - 1].type == "text_special":
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/linkify.py:92:                        token = Token("text", "", 0)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/linkify.py:93:                        token.content = text[lastPos:pos]
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/linkify.py:94:                        token.level = level
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/linkify.py:95:                        nodes.append(token)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/linkify.py:97:                    token = Token("link_open", "a", 1)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/linkify.py:98:                    token.attrs = {"href": fullUrl}
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/linkify.py:99:                    token.level = level
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/linkify.py:101:                    token.markup = "linkify"
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/linkify.py:102:                    token.info = "auto"
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/linkify.py:103:                    nodes.append(token)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/linkify.py:105:                    token = Token("text", "", 0)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/linkify.py:106:                    token.content = urlText
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/linkify.py:107:                    token.level = level
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/linkify.py:108:                    nodes.append(token)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/linkify.py:110:                    token = Token("link_close", "a", -1)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/linkify.py:112:                    token.level = level
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/linkify.py:113:                    token.markup = "linkify"
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/linkify.py:114:                    token.info = "auto"
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/linkify.py:115:                    nodes.append(token)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/linkify.py:120:                    token = Token("text", "", 0)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/linkify.py:121:                    token.content = text[lastPos:]
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/linkify.py:122:                    token.level = level
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/linkify.py:123:                    nodes.append(token)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/linkify.py:125:                inline_token.children = tokens = arrayReplaceAt(tokens, i, nodes)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/replacements.py:22:from ..token import Token
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/replacements.py:63:def replace_scoped(inlineTokens: list[Token]) -> None:
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/replacements.py:66:    for token in inlineTokens:
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/replacements.py:67:        if token.type == "text" and not inside_autolink:
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/replacements.py:68:            token.content = SCOPED_ABBR_RE.sub(replaceFn, token.content)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/replacements.py:70:        if token.type == "link_open" and token.info == "auto":
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/replacements.py:73:        if token.type == "link_close" and token.info == "auto":
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/replacements.py:77:def replace_rare(inlineTokens: list[Token]) -> None:
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/replacements.py:80:    for token in inlineTokens:
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/replacements.py:81:        if token.type == "text" and (not inside_autolink) and RARE_RE.search(token.content):
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/replacements.py:83:            token.content = PLUS_MINUS_RE.sub("Â±", token.content)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/replacements.py:86:            token.content = ELLIPSIS_RE.sub("â€¦", token.content)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/replacements.py:89:            token.content = ELLIPSIS_QUESTION_EXCLAMATION_RE.sub("\\1..", token.content)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/replacements.py:90:            token.content = QUESTION_EXCLAMATION_RE.sub("\\1\\1\\1", token.content)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/replacements.py:93:            token.content = COMMA_RE.sub(",", token.content)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/replacements.py:96:            token.content = EM_DASH_RE.sub("\\1\u2014", token.content)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/replacements.py:99:            token.content = EN_DASH_RE.sub("\\1\u2013", token.content)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/replacements.py:100:            token.content = EN_DASH_INDENT_RE.sub("\\1\u2013", token.content)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/replacements.py:102:        if token.type == "link_open" and token.info == "auto":
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/replacements.py:105:        if token.type == "link_close" and token.info == "auto":
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/replacements.py:113:    for token in state.tokens:
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/replacements.py:114:        if token.type != "inline":
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/replacements.py:116:        if token.children is None:
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/replacements.py:119:        if SCOPED_ABBR_RE.search(token.content):
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/replacements.py:120:            replace_scoped(token.children)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/replacements.py:122:        if RARE_RE.search(token.content):
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/replacements.py:123:            replace_rare(token.children)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/inline.py:6:    for token in state.tokens:
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/inline.py:7:        if token.type == "inline":
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/inline.py:8:            if token.children is None:
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/inline.py:9:                token.children = []
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/inline.py:10:            state.md.inline.parse(token.content, state.md, state.env, token.children)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/smartquotes.py:9:from ..token import Token
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/smartquotes.py:24:def process_inlines(tokens: list[Token], state: StateCore) -> None:
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/smartquotes.py:27:    for i, token in enumerate(tokens):
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/smartquotes.py:28:        thisLevel = token.level
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/smartquotes.py:41:        if token.type != "text":
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/smartquotes.py:44:        text = token.content
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/smartquotes.py:67:                    if tokens[j].type == "softbreak" or tokens[j].type == "hardbreak":
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/smartquotes.py:69:                    # should skip all tokens except 'text', 'html_inline' or 'code_inline'
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/smartquotes.py:70:                    if not tokens[j].content:
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/smartquotes.py:73:                    lastChar = charCodeAt(tokens[j].content, len(tokens[j].content) - 1)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/smartquotes.py:83:                for j in range(i + 1, len(tokens)):
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/smartquotes.py:85:                    if tokens[j].type == "softbreak" or tokens[j].type == "hardbreak":
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/smartquotes.py:87:                    # should skip all tokens except 'text', 'html_inline' or 'code_inline'
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/smartquotes.py:88:                    if not tokens[j].content:
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/smartquotes.py:91:                    nextChar = charCodeAt(tokens[j].content, 0)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/smartquotes.py:134:                    token.content = replaceAt(token.content, t.start(0) + lastIndex, APOSTROPHE)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/smartquotes.py:153:                        # replace token.content *before* tokens[item.token].content,
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/smartquotes.py:154:                        # because, if they are pointing at the same token, replaceAt
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/smartquotes.py:156:                        token.content = replaceAt(token.content, t.start(0) + lastIndex, closeQuote)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/smartquotes.py:157:                        tokens[item["token"]].content = replaceAt(
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/smartquotes.py:158:                            tokens[item["token"]].content, item["pos"], openQuote
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/smartquotes.py:162:                        if item["token"] == i:
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/smartquotes.py:165:                        text = token.content
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/smartquotes.py:178:                        "token": i,
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/smartquotes.py:185:                token.content = replaceAt(token.content, t.start(0) + lastIndex, APOSTROPHE)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/smartquotes.py:192:    for token in state.tokens:
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/smartquotes.py:193:        if token.type != "inline" or not QUOTE_RE.search(token.content):
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/smartquotes.py:195:        if token.children is not None:
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/smartquotes.py:196:            process_inlines(token.children, state)
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/state_core.py:6:from ..token import Token
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/state_core.py:19:        tokens: list[Token] | None = None,
./.venv-build/lib/python3.12/site-packages/markdown_it/rules_core/state_core.py:24:        self.tokens: list[Token] = tokens or []
./.venv-build/lib/python3.12/site-packages/markdown_it/common/utils.py:49:    Useful for some operations with tokens
./.venv-build/lib/python3.12/site-packages/cryptography/fernet.py:21:class InvalidToken(Exception):
./.venv-build/lib/python3.12/site-packages/cryptography/fernet.py:73:    def decrypt(self, token: bytes | str, ttl: int | None = None) -> bytes:
./.venv-build/lib/python3.12/site-packages/cryptography/fernet.py:74:        timestamp, data = Fernet._get_unverified_token_data(token)
./.venv-build/lib/python3.12/site-packages/cryptography/fernet.py:81:    def decrypt_at_time(self, token: bytes | str, ttl: int, current_time: int) -> bytes:
./.venv-build/lib/python3.12/site-packages/cryptography/fernet.py:84:        timestamp, data = Fernet._get_unverified_token_data(token)
./.venv-build/lib/python3.12/site-packages/cryptography/fernet.py:87:    def extract_timestamp(self, token: bytes | str) -> int:
./.venv-build/lib/python3.12/site-packages/cryptography/fernet.py:88:        timestamp, data = Fernet._get_unverified_token_data(token)
./.venv-build/lib/python3.12/site-packages/cryptography/fernet.py:89:        # Verify the token was not tampered with.
./.venv-build/lib/python3.12/site-packages/cryptography/fernet.py:94:    def _get_unverified_token_data(token: bytes | str) -> tuple[int, bytes]:
./.venv-build/lib/python3.12/site-packages/cryptography/fernet.py:95:        if not isinstance(token, (str, bytes)):
./.venv-build/lib/python3.12/site-packages/cryptography/fernet.py:96:            raise TypeError("token must be bytes or str")
./.venv-build/lib/python3.12/site-packages/cryptography/fernet.py:99:            data = base64.urlsafe_b64decode(token)
./.venv-build/lib/python3.12/site-packages/cryptography/fernet.py:101:            raise InvalidToken
./.venv-build/lib/python3.12/site-packages/cryptography/fernet.py:104:            raise InvalidToken
./.venv-build/lib/python3.12/site-packages/cryptography/fernet.py:107:            raise InvalidToken
./.venv-build/lib/python3.12/site-packages/cryptography/fernet.py:118:            raise InvalidToken
./.venv-build/lib/python3.12/site-packages/cryptography/fernet.py:129:                raise InvalidToken
./.venv-build/lib/python3.12/site-packages/cryptography/fernet.py:132:                raise InvalidToken
./.venv-build/lib/python3.12/site-packages/cryptography/fernet.py:143:            raise InvalidToken
./.venv-build/lib/python3.12/site-packages/cryptography/fernet.py:150:            raise InvalidToken
./.venv-build/lib/python3.12/site-packages/cryptography/fernet.py:168:        timestamp, data = Fernet._get_unverified_token_data(msg)
./.venv-build/lib/python3.12/site-packages/cryptography/fernet.py:173:            except InvalidToken:
./.venv-build/lib/python3.12/site-packages/cryptography/fernet.py:176:            raise InvalidToken
./.venv-build/lib/python3.12/site-packages/cryptography/fernet.py:185:            except InvalidToken:
./.venv-build/lib/python3.12/site-packages/cryptography/fernet.py:187:        raise InvalidToken
./.venv-build/lib/python3.12/site-packages/cryptography/fernet.py:193:            except InvalidToken:
./.venv-build/lib/python3.12/site-packages/cryptography/fernet.py:195:        raise InvalidToken
./.venv-build/lib/python3.12/site-packages/cryptography/fernet.py:201:            except InvalidToken:
./.venv-build/lib/python3.12/site-packages/cryptography/fernet.py:203:        raise InvalidToken
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/asymmetric/ec.py:52:        Bit size of a secret scalar for the curve.
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/asymmetric/ec.py:99:        Bit size of a secret scalar for the curve.
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/asymmetric/ec.py:152:        Bit size of a secret scalar for the curve.
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/asymmetric/ec.py:387:generate_private_key = rust_openssl.ec.generate_private_key
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/asymmetric/ec.py:390:def derive_private_key(
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/asymmetric/ec.py:401:    return rust_openssl.ec.derive_private_key(private_value, curve)
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/asymmetric/rsa.py:150:def generate_private_key(
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/asymmetric/rsa.py:156:    return rust_openssl.rsa.generate_private_key(public_exponent, key_size)
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/asymmetric/dsa.py:18:    def generate_private_key(self) -> DSAPrivateKey:
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/asymmetric/dsa.py:161:def generate_private_key(key_size: int, backend: typing.Any = None) -> DSAPrivateKey:
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/asymmetric/dsa.py:163:    return parameters.generate_private_key()
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/asymmetric/types.py:51:PRIVATE_KEY_TYPES = PrivateKeyTypes
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/asymmetric/types.py:53:    PRIVATE_KEY_TYPES,
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/asymmetric/types.py:57:    name="PRIVATE_KEY_TYPES",
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/asymmetric/types.py:68:CERTIFICATE_PRIVATE_KEY_TYPES = CertificateIssuerPrivateKeyTypes
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/asymmetric/types.py:70:    CERTIFICATE_PRIVATE_KEY_TYPES,
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/asymmetric/types.py:74:    name="CERTIFICATE_PRIVATE_KEY_TYPES",
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/asymmetric/dh.py:22:    def generate_private_key(self) -> DHPrivateKey:
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/twofactor/hotp.py:13:from cryptography.hazmat.primitives.twofactor import InvalidToken
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/twofactor/hotp.py:28:        ("secret", base64.b32encode(hotp._key)),
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/twofactor/hotp.py:76:            raise InvalidToken("Supplied HOTP value does not match.")
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/twofactor/__init__.py:8:class InvalidToken(Exception):
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/twofactor/totp.py:10:from cryptography.hazmat.primitives.twofactor import InvalidToken
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/twofactor/totp.py:41:            raise InvalidToken("Supplied TOTP value does not match.")
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/_serialization.py:64:    def __init__(self, password: bytes):
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/_serialization.py:65:        if not isinstance(password, bytes) or len(password) == 0:
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/_serialization.py:66:            raise ValueError("Password must be 1 or more bytes.")
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/_serialization.py:68:        self.password = password
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/_serialization.py:132:    def build(self, password: bytes) -> KeySerializationEncryption:
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/_serialization.py:133:        if not isinstance(password, bytes) or len(password) == 0:
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/_serialization.py:134:            raise ValueError("Password must be 1 or more bytes.")
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/_serialization.py:138:            password,
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/_serialization.py:149:        password: bytes,
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/_serialization.py:156:        self.password = password
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/serialization/ssh.py:50:        password: bytes,
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/serialization/ssh.py:78:_SK_START = b"-----BEGIN OPENSSH PRIVATE KEY-----"
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/serialization/ssh.py:189:    password: bytes | None,
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/serialization/ssh.py:194:    if not password:
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/serialization/ssh.py:195:        raise TypeError("Key is password-protected, but password was not provided.")
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/serialization/ssh.py:198:    seed = _bcrypt_kdf(password, salt, ciph.key_len + ciph.iv_len, rounds, True)
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/serialization/ssh.py:338:        private_key = private_numbers.private_key(
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/serialization/ssh.py:341:        return private_key, data
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/serialization/ssh.py:349:    def encode_private(self, private_key: rsa.RSAPrivateKey, f_priv: _FragList) -> None:
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/serialization/ssh.py:351:        private_numbers = private_key.private_numbers()
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/serialization/ssh.py:402:        private_key = private_numbers.private_key()
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/serialization/ssh.py:403:        return private_key, data
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/serialization/ssh.py:416:    def encode_private(self, private_key: dsa.DSAPrivateKey, f_priv: _FragList) -> None:
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/serialization/ssh.py:418:        self.encode_public(private_key.public_key(), f_priv)
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/serialization/ssh.py:419:        f_priv.put_mpint(private_key.private_numbers().x)
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/serialization/ssh.py:436:        mpint secret
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/serialization/ssh.py:464:        secret, data = _get_mpint(data)
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/serialization/ssh.py:468:        private_key = ec.derive_private_key(secret, self.curve)
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/serialization/ssh.py:469:        return private_key, data
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/serialization/ssh.py:477:    def encode_private(self, private_key: ec.EllipticCurvePrivateKey, f_priv: _FragList) -> None:
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/serialization/ssh.py:479:        public_key = private_key.public_key()
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/serialization/ssh.py:480:        private_numbers = private_key.private_numbers()
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/serialization/ssh.py:493:        bytes secret_and_point
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/serialization/ssh.py:514:        secret = keypair[:32]
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/serialization/ssh.py:518:        private_key = ed25519.Ed25519PrivateKey.from_private_bytes(secret)
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/serialization/ssh.py:519:        return private_key, data
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/serialization/ssh.py:526:    def encode_private(self, private_key: ed25519.Ed25519PrivateKey, f_priv: _FragList) -> None:
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/serialization/ssh.py:528:        public_key = private_key.public_key()
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/serialization/ssh.py:529:        raw_private_key = private_key.private_bytes(Encoding.Raw, PrivateFormat.Raw, NoEncryption())
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/serialization/ssh.py:531:        f_keypair = _FragList([raw_private_key, raw_public_key])
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/serialization/ssh.py:619:def load_ssh_private_key(
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/serialization/ssh.py:621:    password: bytes | None,
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/serialization/ssh.py:628:    if password is not None:
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/serialization/ssh.py:629:        utils._check_bytes("password", password)
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/serialization/ssh.py:664:        # load secret data
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/serialization/ssh.py:678:        ciph = _init_cipher(ciphername_bytes, password, salt.tobytes(), rounds)
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/serialization/ssh.py:689:        if password:
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/serialization/ssh.py:690:            raise TypeError("Password was given but private key is not encrypted.")
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/serialization/ssh.py:691:        # load secret data
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/serialization/ssh.py:705:    private_key, edata = kformat.load_private(
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/serialization/ssh.py:718:    if isinstance(private_key, dsa.DSAPrivateKey):
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/serialization/ssh.py:725:    return private_key
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/serialization/ssh.py:728:def _serialize_ssh_private_key(
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/serialization/ssh.py:729:    private_key: SSHPrivateKeyTypes,
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/serialization/ssh.py:730:    password: bytes,
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/serialization/ssh.py:734:    utils._check_bytes("password", password)
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/serialization/ssh.py:735:    if isinstance(private_key, dsa.DSAPrivateKey):
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/serialization/ssh.py:742:    key_type = _get_ssh_key_type(private_key)
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/serialization/ssh.py:747:    if password:
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/serialization/ssh.py:760:        ciph = _init_cipher(ciphername, password, salt, rounds)
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/serialization/ssh.py:772:    kformat.encode_public(private_key.public_key(), f_public_key)
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/serialization/ssh.py:774:    f_secrets = _FragList([checkval, checkval])
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/serialization/ssh.py:775:    f_secrets.put_sshstr(key_type)
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/serialization/ssh.py:776:    kformat.encode_private(private_key, f_secrets)
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/serialization/ssh.py:777:    f_secrets.put_sshstr(comment)
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/serialization/ssh.py:778:    f_secrets.put_raw(_PADDING[: blklen - (f_secrets.size() % blklen)])
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/serialization/ssh.py:788:    f_main.put_sshstr(f_secrets)
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/serialization/ssh.py:791:    slen = f_secrets.size()
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/serialization/ssh.py:1383:    def sign(self, private_key: SSHCertPrivateKeyTypes) -> SSHCertificate:
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/serialization/ssh.py:1385:            private_key,
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/serialization/ssh.py:1467:        ca_type = _get_ssh_key_type(private_key)
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/serialization/ssh.py:1471:        caformat.encode_public(private_key.public_key(), caf)
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/serialization/ssh.py:1476:        if isinstance(private_key, ed25519.Ed25519PrivateKey):
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/serialization/ssh.py:1477:            signature = private_key.sign(f.tobytes())
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/serialization/ssh.py:1482:        elif isinstance(private_key, ec.EllipticCurvePrivateKey):
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/serialization/ssh.py:1483:            hash_alg = _get_ec_hash_alg(private_key.curve)
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/serialization/ssh.py:1484:            signature = private_key.sign(f.tobytes(), ec.ECDSA(hash_alg))
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/serialization/ssh.py:1495:            assert isinstance(private_key, rsa.RSAPrivateKey)
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/serialization/ssh.py:1502:            signature = private_key.sign(f.tobytes(), padding.PKCS1v15(), hashes.SHA512())
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/serialization/pkcs7.py:82:        private_key: PKCS7PrivateKeyTypes,
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/serialization/pkcs7.py:102:        if not isinstance(private_key, (rsa.RSAPrivateKey, ec.EllipticCurvePrivateKey)):
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/serialization/pkcs7.py:108:            if not isinstance(private_key, rsa.RSAPrivateKey):
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/serialization/pkcs7.py:115:                (certificate, private_key, hash_algorithm, rsa_padding),
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/serialization/base.py:7:load_pem_private_key = rust_openssl.keys.load_pem_private_key
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/serialization/base.py:8:load_der_private_key = rust_openssl.keys.load_der_private_key
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/serialization/__init__.py:19:    load_der_private_key,
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/serialization/__init__.py:22:    load_pem_private_key,
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/serialization/__init__.py:33:    load_ssh_private_key,
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/serialization/__init__.py:56:    "load_der_private_key",
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/serialization/__init__.py:59:    "load_pem_private_key",
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/primitives/serialization/__init__.py:61:    "load_ssh_private_key",
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/_oid.py:17:    PRIVATE_KEY_USAGE_PERIOD = ObjectIdentifier("2.5.29.16")
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/_oid.py:223:    CHALLENGE_PASSWORD = ObjectIdentifier("1.2.840.113549.1.9.7")
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/_oid.py:309:    ExtensionOID.PRIVATE_KEY_USAGE_PERIOD: "privateKeyUsagePeriod",
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/_oid.py:343:    AttributeOID.CHALLENGE_PASSWORD: "challengePassword",
./.venv-build/lib/python3.12/site-packages/cryptography/hazmat/bindings/openssl/_conditional.py:103:        "ENGINE_load_private_key",
./.venv-build/lib/python3.12/site-packages/cryptography/x509/extensions.py:1173:    oid = ExtensionOID.PRIVATE_KEY_USAGE_PERIOD
./.venv-build/lib/python3.12/site-packages/cryptography/x509/base.py:323:        private_key: CertificateIssuerPrivateKeyTypes,
./.venv-build/lib/python3.12/site-packages/cryptography/x509/base.py:339:            if not isinstance(private_key, rsa.RSAPrivateKey):
./.venv-build/lib/python3.12/site-packages/cryptography/x509/base.py:343:            if not isinstance(private_key, ec.EllipticCurvePrivateKey):
./.venv-build/lib/python3.12/site-packages/cryptography/x509/base.py:348:            private_key,
./.venv-build/lib/python3.12/site-packages/cryptography/x509/base.py:545:        private_key: CertificateIssuerPrivateKeyTypes,
./.venv-build/lib/python3.12/site-packages/cryptography/x509/base.py:576:            if not isinstance(private_key, rsa.RSAPrivateKey):
./.venv-build/lib/python3.12/site-packages/cryptography/x509/base.py:580:            if not isinstance(private_key, ec.EllipticCurvePrivateKey):
./.venv-build/lib/python3.12/site-packages/cryptography/x509/base.py:585:            private_key,
./.venv-build/lib/python3.12/site-packages/cryptography/x509/base.py:697:        private_key: CertificateIssuerPrivateKeyTypes,
./.venv-build/lib/python3.12/site-packages/cryptography/x509/base.py:716:            if not isinstance(private_key, rsa.RSAPrivateKey):
./.venv-build/lib/python3.12/site-packages/cryptography/x509/base.py:720:            if not isinstance(private_key, ec.EllipticCurvePrivateKey):
./.venv-build/lib/python3.12/site-packages/cryptography/x509/base.py:725:            private_key,
./.venv-build/lib/python3.12/site-packages/cryptography/x509/__init__.py:119:OID_PRIVATE_KEY_USAGE_PERIOD = ExtensionOID.PRIVATE_KEY_USAGE_PERIOD
./.venv-build/lib/python3.12/site-packages/cryptography/x509/ocsp.py:322:        private_key: CertificateIssuerPrivateKeyTypes,
./.venv-build/lib/python3.12/site-packages/cryptography/x509/ocsp.py:331:            OCSPResponseStatus.SUCCESSFUL, self, private_key, algorithm
./.venv-build/lib/python3.12/site-packages/docutils/transforms/universal.py:266:    def get_tokens(self, txtnodes):
./.venv-build/lib/python3.12/site-packages/docutils/transforms/universal.py:268:        # of "Text" nodes (interface to ``smartquotes.educate_tokens()``).
./.venv-build/lib/python3.12/site-packages/docutils/transforms/universal.py:335:            teacher = smartquotes.educate_tokens(
./.venv-build/lib/python3.12/site-packages/docutils/transforms/universal.py:336:                self.get_tokens(txtnodes), attr=self.smartquotes_action, language=lang
./.venv-build/lib/python3.12/site-packages/docutils/writers/odf_odt/pygmentsformatter.py:24:    def format(self, tokensource, outfile) -> None:
./.venv-build/lib/python3.12/site-packages/docutils/writers/odf_odt/pygmentsformatter.py:25:        tokenclass = pygments.token.Token
./.venv-build/lib/python3.12/site-packages/docutils/writers/odf_odt/pygmentsformatter.py:26:        for ttype, value in tokensource:
./.venv-build/lib/python3.12/site-packages/docutils/writers/odf_odt/pygmentsformatter.py:28:            if ttype == tokenclass.Keyword:
./.venv-build/lib/python3.12/site-packages/docutils/writers/odf_odt/pygmentsformatter.py:34:            elif ttype == tokenclass.Literal.String:
./.venv-build/lib/python3.12/site-packages/docutils/writers/odf_odt/pygmentsformatter.py:41:                tokenclass.Literal.Number.Integer,
./.venv-build/lib/python3.12/site-packages/docutils/writers/odf_odt/pygmentsformatter.py:42:                tokenclass.Literal.Number.Integer.Long,
./.venv-build/lib/python3.12/site-packages/docutils/writers/odf_odt/pygmentsformatter.py:43:                tokenclass.Literal.Number.Float,
./.venv-build/lib/python3.12/site-packages/docutils/writers/odf_odt/pygmentsformatter.py:44:                tokenclass.Literal.Number.Hex,
./.venv-build/lib/python3.12/site-packages/docutils/writers/odf_odt/pygmentsformatter.py:45:                tokenclass.Literal.Number.Oct,
./.venv-build/lib/python3.12/site-packages/docutils/writers/odf_odt/pygmentsformatter.py:46:                tokenclass.Literal.Number,
./.venv-build/lib/python3.12/site-packages/docutils/writers/odf_odt/pygmentsformatter.py:53:            elif ttype == tokenclass.Operator:
./.venv-build/lib/python3.12/site-packages/docutils/writers/odf_odt/pygmentsformatter.py:59:            elif ttype == tokenclass.Comment:
./.venv-build/lib/python3.12/site-packages/docutils/writers/odf_odt/pygmentsformatter.py:65:            elif ttype == tokenclass.Name.Class:
./.venv-build/lib/python3.12/site-packages/docutils/writers/odf_odt/pygmentsformatter.py:71:            elif ttype == tokenclass.Name.Function:
./.venv-build/lib/python3.12/site-packages/docutils/writers/odf_odt/pygmentsformatter.py:77:            elif ttype == tokenclass.Name:
./.venv-build/lib/python3.12/site-packages/docutils/writers/odf_odt/pygmentsformatter.py:89:    def format(self, tokensource, outfile) -> None:
./.venv-build/lib/python3.12/site-packages/docutils/writers/odf_odt/pygmentsformatter.py:90:        tokenclass = pygments.token.Token
./.venv-build/lib/python3.12/site-packages/docutils/writers/odf_odt/pygmentsformatter.py:91:        for ttype, value in tokensource:
./.venv-build/lib/python3.12/site-packages/docutils/writers/odf_odt/pygmentsformatter.py:93:            if ttype == tokenclass.Keyword:
./.venv-build/lib/python3.12/site-packages/docutils/writers/odf_odt/pygmentsformatter.py:100:                tokenclass.Literal.String,
./.venv-build/lib/python3.12/site-packages/docutils/writers/odf_odt/pygmentsformatter.py:101:                tokenclass.Literal.String.Backtick,
./.venv-build/lib/python3.12/site-packages/docutils/writers/odf_odt/pygmentsformatter.py:108:            elif ttype == tokenclass.Name.Attribute:
./.venv-build/lib/python3.12/site-packages/docutils/writers/odf_odt/pygmentsformatter.py:114:            elif ttype == tokenclass.Comment:
./.venv-build/lib/python3.12/site-packages/docutils/writers/odf_odt/pygmentsformatter.py:127:            elif ttype == tokenclass.Name.Builtin:
./.venv-build/lib/python3.12/site-packages/docutils/writers/html5_polyglot/__init__.py:348:        for token in self.words_and_spaces.findall(text):
./.venv-build/lib/python3.12/site-packages/docutils/writers/html5_polyglot/__init__.py:349:            if token.strip() and self.in_word_wrap_point.search(token):
./.venv-build/lib/python3.12/site-packages/docutils/writers/html5_polyglot/__init__.py:350:                self.body.append(f'<span class="pre">{self.encode(token)}</span>')
./.venv-build/lib/python3.12/site-packages/docutils/writers/html5_polyglot/__init__.py:352:                self.body.append(self.encode(token))
./.venv-build/lib/python3.12/site-packages/docutils/writers/_html_base.py:1398:        for token in self.words_and_spaces.findall(text):
./.venv-build/lib/python3.12/site-packages/docutils/writers/_html_base.py:1399:            if token.strip() and self.in_word_wrap_point.search(token):
./.venv-build/lib/python3.12/site-packages/docutils/writers/_html_base.py:1400:                self.body.append('<span class="pre">%s</span>' % self.encode(token))
./.venv-build/lib/python3.12/site-packages/docutils/writers/_html_base.py:1402:                self.body.append(self.encode(token))
./.venv-build/lib/python3.12/site-packages/docutils/writers/html4css1/__init__.py:691:        for token in self.words_and_spaces.findall(text):
./.venv-build/lib/python3.12/site-packages/docutils/writers/html4css1/__init__.py:692:            if token.strip():
./.venv-build/lib/python3.12/site-packages/docutils/writers/html4css1/__init__.py:695:                if self.in_word_wrap_point.search(token):
./.venv-build/lib/python3.12/site-packages/docutils/writers/html4css1/__init__.py:696:                    self.body.append('<span class="pre">%s</span>' % self.encode(token))
./.venv-build/lib/python3.12/site-packages/docutils/writers/html4css1/__init__.py:698:                    self.body.append(self.encode(token))
./.venv-build/lib/python3.12/site-packages/docutils/writers/html4css1/__init__.py:699:            elif token in ("\n", " "):
./.venv-build/lib/python3.12/site-packages/docutils/writers/html4css1/__init__.py:701:                self.body.append(token)
./.venv-build/lib/python3.12/site-packages/docutils/writers/html4css1/__init__.py:704:                self.body.append("&nbsp;" * (len(token) - 1) + " ")
./.venv-build/lib/python3.12/site-packages/docutils/nodes.py:3187:    - The `HTML 4.01 spec`_ defines identifiers based on SGML tokens:
./.venv-build/lib/python3.12/site-packages/docutils/nodes.py:3189:          ID and NAME tokens must begin with a letter ([A-Za-z]) and may be
./.venv-build/lib/python3.12/site-packages/docutils/nodes.py:3193:    - However the `CSS1 spec`_ defines identifiers based on the "name" token,
./.venv-build/lib/python3.12/site-packages/docutils/nodes.py:3194:      a tighter interpretation ("flex" tokenizer notation; "latin1" and
./.venv-build/lib/python3.12/site-packages/docutils/nodes.py:3377:    Used in `idref.type`__ and for the tokens in `validate_identifier_list()`.
./.venv-build/lib/python3.12/site-packages/docutils/nodes.py:3405:    for token in value:
./.venv-build/lib/python3.12/site-packages/docutils/nodes.py:3406:        validate_identifier(token)
./.venv-build/lib/python3.12/site-packages/docutils/nodes.py:3452:def validate_NMTOKEN(value: str) -> str:
./.venv-build/lib/python3.12/site-packages/docutils/nodes.py:3454:    Validate a "name token": a `str` of ASCII letters, digits, and [-._].
./.venv-build/lib/python3.12/site-packages/docutils/nodes.py:3459:        raise ValueError(f'"{value}" is no NMTOKEN.')
./.venv-build/lib/python3.12/site-packages/docutils/nodes.py:3463:def validate_NMTOKENS(value: str | list[str]) -> list[str]:
./.venv-build/lib/python3.12/site-packages/docutils/nodes.py:3465:    Validate a list of "name tokens".
./.venv-build/lib/python3.12/site-packages/docutils/nodes.py:3471:    for token in value:
./.venv-build/lib/python3.12/site-packages/docutils/nodes.py:3472:        validate_NMTOKEN(token)
./.venv-build/lib/python3.12/site-packages/docutils/nodes.py:3520:    "charoff": validate_NMTOKEN,  # from CALS, currently ignored
./.venv-build/lib/python3.12/site-packages/docutils/nodes.py:3521:    "colname": validate_NMTOKEN,  # from CALS, currently ignored
./.venv-build/lib/python3.12/site-packages/docutils/nodes.py:3523:    "cols": int,  # from CALS: "NMTOKEN, [â€¦] must be an integer > 0".
./.venv-build/lib/python3.12/site-packages/docutils/nodes.py:3549:    # 'name': node_attributes.validate_NMTOKEN,  # in <meta>
./.venv-build/lib/python3.12/site-packages/docutils/nodes.py:3551:    "namest": validate_NMTOKEN,  # start of span, from CALS, currently ignored
./.venv-build/lib/python3.12/site-packages/docutils/nodes.py:3552:    "nameend": validate_NMTOKEN,  # end of span, from CALS, currently ignored
./.venv-build/lib/python3.12/site-packages/docutils/nodes.py:3567:    "type": validate_NMTOKEN,
./.venv-build/lib/python3.12/site-packages/docutils/parsers/rst/directives/body.py:161:            tokens = Lexer(
./.venv-build/lib/python3.12/site-packages/docutils/parsers/rst/directives/body.py:169:                tokens = Lexer("\n".join(self.content), language, "none")
./.venv-build/lib/python3.12/site-packages/docutils/parsers/rst/directives/body.py:181:            tokens = NumberLines(tokens, startline, endline)
./.venv-build/lib/python3.12/site-packages/docutils/parsers/rst/directives/body.py:188:        # analyze content and add nodes for every token
./.venv-build/lib/python3.12/site-packages/docutils/parsers/rst/directives/body.py:189:        for classes, value in tokens:
./.venv-build/lib/python3.12/site-packages/docutils/parsers/rst/directives/misc.py:185:            tokens = NumberLines([([], text)], firstline, lastline)
./.venv-build/lib/python3.12/site-packages/docutils/parsers/rst/directives/misc.py:186:            for classes, value in tokens:
./.venv-build/lib/python3.12/site-packages/docutils/parsers/rst/directives/misc.py:626:        tokens = name.split()
./.venv-build/lib/python3.12/site-packages/docutils/parsers/rst/directives/misc.py:628:            attname, val = utils.extract_name_value(tokens[0])[0]
./.venv-build/lib/python3.12/site-packages/docutils/parsers/rst/directives/misc.py:631:            node["name"] = tokens[0]
./.venv-build/lib/python3.12/site-packages/docutils/parsers/rst/directives/misc.py:632:        for token in tokens[1:]:
./.venv-build/lib/python3.12/site-packages/docutils/parsers/rst/directives/misc.py:634:                attname, val = utils.extract_name_value(token)[0]
./.venv-build/lib/python3.12/site-packages/docutils/parsers/rst/directives/misc.py:639:                    'Error parsing meta tag attribute "%s": %s.' % (token, detail),
./.venv-build/lib/python3.12/site-packages/docutils/parsers/rst/states.py:1776:            tokens = optionstring.split()
./.venv-build/lib/python3.12/site-packages/docutils/parsers/rst/states.py:1778:            firstopt = tokens[0].split("=", 1)
./.venv-build/lib/python3.12/site-packages/docutils/parsers/rst/states.py:1781:                tokens[:1] = firstopt
./.venv-build/lib/python3.12/site-packages/docutils/parsers/rst/states.py:1783:            elif len(tokens[0]) > 2 and (
./.venv-build/lib/python3.12/site-packages/docutils/parsers/rst/states.py:1784:                (tokens[0].startswith("-") and not tokens[0].startswith("--"))
./.venv-build/lib/python3.12/site-packages/docutils/parsers/rst/states.py:1785:                or tokens[0].startswith("+")
./.venv-build/lib/python3.12/site-packages/docutils/parsers/rst/states.py:1788:                tokens[:1] = [tokens[0][:2], tokens[0][2:]]
./.venv-build/lib/python3.12/site-packages/docutils/parsers/rst/states.py:1790:            if len(tokens) > 1 and (tokens[1].startswith("<") and tokens[-1].endswith(">")):
./.venv-build/lib/python3.12/site-packages/docutils/parsers/rst/states.py:1791:                # "-o <value1 value2>" form; join all values into one token
./.venv-build/lib/python3.12/site-packages/docutils/parsers/rst/states.py:1792:                tokens[1:] = [" ".join(tokens[1:])]
./.venv-build/lib/python3.12/site-packages/docutils/parsers/rst/states.py:1793:            if 0 < len(tokens) <= 2:
./.venv-build/lib/python3.12/site-packages/docutils/parsers/rst/states.py:1795:                option += nodes.option_string(tokens[0], tokens[0])
./.venv-build/lib/python3.12/site-packages/docutils/parsers/rst/states.py:1796:                if len(tokens) > 1:
./.venv-build/lib/python3.12/site-packages/docutils/parsers/rst/states.py:1797:                    option += nodes.option_argument(tokens[1], tokens[1], delimiter=delimiter)
./.venv-build/lib/python3.12/site-packages/docutils/parsers/rst/states.py:1801:                    "wrong number of option tokens (=%s), should be 1 or 2: "
./.venv-build/lib/python3.12/site-packages/docutils/parsers/rst/states.py:1802:                    '"%s"' % (len(tokens), optionstring)
./.venv-build/lib/python3.12/site-packages/docutils/parsers/rst/__init__.py:145:                "Token name set for parsing code with Pygments: one of "
./.venv-build/lib/python3.12/site-packages/docutils/parsers/rst/roles.py:367:        tokens = Lexer(
./.venv-build/lib/python3.12/site-packages/docutils/parsers/rst/roles.py:379:    # analyse content and add nodes for every token
./.venv-build/lib/python3.12/site-packages/docutils/parsers/rst/roles.py:380:    for classes, value in tokens:
./.venv-build/lib/python3.12/site-packages/docutils/utils/math/latex2mathml.py:479:def tex_token(string):
./.venv-build/lib/python3.12/site-packages/docutils/utils/math/latex2mathml.py:480:    """Return first simple TeX token and remainder of `string`.
./.venv-build/lib/python3.12/site-packages/docutils/utils/math/latex2mathml.py:482:    >>> tex_token('\\command{without argument}')
./.venv-build/lib/python3.12/site-packages/docutils/utils/math/latex2mathml.py:484:    >>> tex_token('or first character')
./.venv-build/lib/python3.12/site-packages/docutils/utils/math/latex2mathml.py:503:# >>> tex_token('{opening bracket of group}')
./.venv-build/lib/python3.12/site-packages/docutils/utils/math/latex2mathml.py:505:# >>> tex_token('\\skip whitespace after macro name')
./.venv-build/lib/python3.12/site-packages/docutils/utils/math/latex2mathml.py:507:# >>> tex_token('. but not after single char')
./.venv-build/lib/python3.12/site-packages/docutils/utils/math/latex2mathml.py:509:# >>> tex_token('') # empty string.
./.venv-build/lib/python3.12/site-packages/docutils/utils/math/latex2mathml.py:511:# >>> tex_token('\{escaped bracket')
./.venv-build/lib/python3.12/site-packages/docutils/utils/math/latex2mathml.py:516:    """Return first TeX group or token and remainder of `string`.
./.venv-build/lib/python3.12/site-packages/docutils/utils/math/latex2mathml.py:527:        # special case: there is no group, return first token and remainder
./.venv-build/lib/python3.12/site-packages/docutils/utils/math/latex2mathml.py:564:def tex_token_or_group(string):
./.venv-build/lib/python3.12/site-packages/docutils/utils/math/latex2mathml.py:565:    """Return first TeX group or token and remainder of `string`.
./.venv-build/lib/python3.12/site-packages/docutils/utils/math/latex2mathml.py:567:    >>> tex_token_or_group('\\command{without argument}')
./.venv-build/lib/python3.12/site-packages/docutils/utils/math/latex2mathml.py:569:    >>> tex_token_or_group('first character')
./.venv-build/lib/python3.12/site-packages/docutils/utils/math/latex2mathml.py:571:    >>> tex_token_or_group(' also whitespace')
./.venv-build/lib/python3.12/site-packages/docutils/utils/math/latex2mathml.py:573:    >>> tex_token_or_group('{first group} keep rest')
./.venv-build/lib/python3.12/site-packages/docutils/utils/math/latex2mathml.py:577:    arg, remainder = tex_token(string)
./.venv-build/lib/python3.12/site-packages/docutils/utils/math/latex2mathml.py:583:# >>> tex_token_or_group('\{no group but left bracket')
./.venv-build/lib/python3.12/site-packages/docutils/utils/math/latex2mathml.py:752:    # Token elements
./.venv-build/lib/python3.12/site-packages/docutils/utils/math/latex2mathml.py:780:            arg, string = tex_token_or_group(string)
./.venv-build/lib/python3.12/site-packages/docutils/utils/math/latex2mathml.py:815:        arg, string = tex_token_or_group(string)
./.venv-build/lib/python3.12/site-packages/docutils/utils/math/latex2mathml.py:861:        delimiter, string = tex_token_or_group(string)
./.venv-build/lib/python3.12/site-packages/docutils/utils/math/latex2mathml.py:883:        arg, string = tex_token(string)
./.venv-build/lib/python3.12/site-packages/docutils/utils/math/latex2mathml.py:897:        arg, string = tex_token_or_group(string)
./.venv-build/lib/python3.12/site-packages/docutils/utils/math/latex2mathml.py:1026:        arg, remainder = tex_token(string)
./.venv-build/lib/python3.12/site-packages/docutils/utils/math/latex2mathml.py:1088:    arg, string = tex_token_or_group(string)
./.venv-build/lib/python3.12/site-packages/docutils/utils/math/mathml_elements.py:209:            if not isinstance(e, MathToken) and e.text:
./.venv-build/lib/python3.12/site-packages/docutils/utils/math/mathml_elements.py:274:# Token elements represent the smallest units of mathematical notation which
./.venv-build/lib/python3.12/site-packages/docutils/utils/math/mathml_elements.py:278:class MathToken(MathElement):
./.venv-build/lib/python3.12/site-packages/docutils/utils/math/mathml_elements.py:279:    """Token Element: contains textual data instead of children.
./.venv-build/lib/python3.12/site-packages/docutils/utils/math/mathml_elements.py:289:            raise ValueError("MathToken element expects `str` or number," f' not "{text}".')
./.venv-build/lib/python3.12/site-packages/docutils/utils/math/mathml_elements.py:301:# Token elements
./.venv-build/lib/python3.12/site-packages/docutils/utils/math/mathml_elements.py:305:class mtext(MathToken):
./.venv-build/lib/python3.12/site-packages/docutils/utils/math/mathml_elements.py:309:class mi(MathToken):
./.venv-build/lib/python3.12/site-packages/docutils/utils/math/mathml_elements.py:313:class mn(MathToken):
./.venv-build/lib/python3.12/site-packages/docutils/utils/math/mathml_elements.py:324:class mo(MathToken):
./.venv-build/lib/python3.12/site-packages/docutils/utils/code_analyzer.py:22:# Filter the following token types from the list of class arguments:
./.venv-build/lib/python3.12/site-packages/docutils/utils/code_analyzer.py:23:unstyled_tokens = [
./.venv-build/lib/python3.12/site-packages/docutils/utils/code_analyzer.py:24:    "token",  # Token (base token type)
./.venv-build/lib/python3.12/site-packages/docutils/utils/code_analyzer.py:25:    "text",  # Token.Text
./.venv-build/lib/python3.12/site-packages/docutils/utils/code_analyzer.py:27:]  # short name for Token and Text
./.venv-build/lib/python3.12/site-packages/docutils/utils/code_analyzer.py:28:# (Add, e.g., Token.Punctuation with ``unstyled_tokens += 'punctuation'``.)
./.venv-build/lib/python3.12/site-packages/docutils/utils/code_analyzer.py:36:    """Parse `code` lines and yield "classified" tokens.
./.venv-build/lib/python3.12/site-packages/docutils/utils/code_analyzer.py:42:      tokennames -- either 'long', 'short', or 'none' (see below).
./.venv-build/lib/python3.12/site-packages/docutils/utils/code_analyzer.py:44:    Merge subsequent tokens of the same token-type.
./.venv-build/lib/python3.12/site-packages/docutils/utils/code_analyzer.py:46:    Iterating over an instance yields the tokens as ``(tokentype, value)``
./.venv-build/lib/python3.12/site-packages/docutils/utils/code_analyzer.py:47:    tuples. The value of `tokennames` configures the naming of the tokentype:
./.venv-build/lib/python3.12/site-packages/docutils/utils/code_analyzer.py:49:      'long':  downcased full token type name,
./.venv-build/lib/python3.12/site-packages/docutils/utils/code_analyzer.py:50:      'short': short name defined by pygments.token.STANDARD_TYPES
./.venv-build/lib/python3.12/site-packages/docutils/utils/code_analyzer.py:55:    def __init__(self, code, language, tokennames="short") -> None:
./.venv-build/lib/python3.12/site-packages/docutils/utils/code_analyzer.py:61:        self.tokennames = tokennames
./.venv-build/lib/python3.12/site-packages/docutils/utils/code_analyzer.py:64:        if language in ("", "text") or tokennames == "none":
./.venv-build/lib/python3.12/site-packages/docutils/utils/code_analyzer.py:72:        # self.lexer.add_filter('tokenmerge')
./.venv-build/lib/python3.12/site-packages/docutils/utils/code_analyzer.py:74:        # TokenMergeFilter. # ``self.merge(tokens)`` in __iter__ could
./.venv-build/lib/python3.12/site-packages/docutils/utils/code_analyzer.py:75:        # be replaced by ``self.lexer.add_filter('tokenmerge')`` in __init__.
./.venv-build/lib/python3.12/site-packages/docutils/utils/code_analyzer.py:78:        # self.lexer.add_filter('tokenmerge')
./.venv-build/lib/python3.12/site-packages/docutils/utils/code_analyzer.py:80:    def merge(self, tokens):
./.venv-build/lib/python3.12/site-packages/docutils/utils/code_analyzer.py:81:        """Merge subsequent tokens of same token-type.
./.venv-build/lib/python3.12/site-packages/docutils/utils/code_analyzer.py:85:        tokens = iter(tokens)
./.venv-build/lib/python3.12/site-packages/docutils/utils/code_analyzer.py:86:        (lasttype, lastval) = next(tokens)
./.venv-build/lib/python3.12/site-packages/docutils/utils/code_analyzer.py:87:        for ttype, value in tokens:
./.venv-build/lib/python3.12/site-packages/docutils/utils/code_analyzer.py:98:        """Parse self.code and yield "classified" tokens."""
./.venv-build/lib/python3.12/site-packages/docutils/utils/code_analyzer.py:102:        tokens = pygments.lex(self.code, self.lexer)
./.venv-build/lib/python3.12/site-packages/docutils/utils/code_analyzer.py:103:        for tokentype, value in self.merge(tokens):
./.venv-build/lib/python3.12/site-packages/docutils/utils/code_analyzer.py:104:            if self.tokennames == "long":  # long CSS class args
./.venv-build/lib/python3.12/site-packages/docutils/utils/code_analyzer.py:105:                classes = str(tokentype).lower().split(".")
./.venv-build/lib/python3.12/site-packages/docutils/utils/code_analyzer.py:107:                classes = [_get_ttype_class(tokentype)]
./.venv-build/lib/python3.12/site-packages/docutils/utils/code_analyzer.py:108:            classes = [cls for cls in classes if cls not in unstyled_tokens]
./.venv-build/lib/python3.12/site-packages/docutils/utils/code_analyzer.py:113:    """Insert linenumber-tokens at the start of every code line.
./.venv-build/lib/python3.12/site-packages/docutils/utils/code_analyzer.py:117:       tokens    -- iterable of ``(classes, value)`` tuples
./.venv-build/lib/python3.12/site-packages/docutils/utils/code_analyzer.py:121:    Iterating over an instance yields the tokens with a
./.venv-build/lib/python3.12/site-packages/docutils/utils/code_analyzer.py:122:    ``(['ln'], '<the line number>')`` token added for every code line.
./.venv-build/lib/python3.12/site-packages/docutils/utils/code_analyzer.py:123:    Multi-line tokens are split."""
./.venv-build/lib/python3.12/site-packages/docutils/utils/code_analyzer.py:125:    def __init__(self, tokens, startline, endline) -> None:
./.venv-build/lib/python3.12/site-packages/docutils/utils/code_analyzer.py:126:        self.tokens = tokens
./.venv-build/lib/python3.12/site-packages/docutils/utils/code_analyzer.py:134:        for ttype, value in self.tokens:
./.venv-build/lib/python3.12/site-packages/docutils/utils/smartquotes.py:279:        - `educate_tokens()` generator as interface for Docutils.
./.venv-build/lib/python3.12/site-packages/docutils/utils/smartquotes.py:576:    return "".join(t for t in educate_tokens(tokenize(text), attr, language))
./.venv-build/lib/python3.12/site-packages/docutils/utils/smartquotes.py:579:def educate_tokens(text_tokens, attr=default_smartypants_attr, language="en"):
./.venv-build/lib/python3.12/site-packages/docutils/utils/smartquotes.py:580:    """Return iterator that "educates" the items of `text_tokens`."""
./.venv-build/lib/python3.12/site-packages/docutils/utils/smartquotes.py:603:    # if attr == "0":  # pass tokens unchanged (see below).
./.venv-build/lib/python3.12/site-packages/docutils/utils/smartquotes.py:641:    prev_token_last_char = " "
./.venv-build/lib/python3.12/site-packages/docutils/utils/smartquotes.py:642:    # Last character of the previous text token. Used as
./.venv-build/lib/python3.12/site-packages/docutils/utils/smartquotes.py:645:    for ttype, text in text_tokens:
./.venv-build/lib/python3.12/site-packages/docutils/utils/smartquotes.py:646:        # skip HTML and/or XML tags as well as empty text tokens
./.venv-build/lib/python3.12/site-packages/docutils/utils/smartquotes.py:654:            prev_token_last_char = text[-1:]
./.venv-build/lib/python3.12/site-packages/docutils/utils/smartquotes.py:685:            context = prev_token_last_char.replace('"', ";").replace("'", ";")
./.venv-build/lib/python3.12/site-packages/docutils/utils/smartquotes.py:691:        # Remember last char as context for the next token
./.venv-build/lib/python3.12/site-packages/docutils/utils/smartquotes.py:692:        prev_token_last_char = last_char
./.venv-build/lib/python3.12/site-packages/docutils/utils/smartquotes.py:905:def tokenize(text):
./.venv-build/lib/python3.12/site-packages/docutils/utils/smartquotes.py:908:    Returns:    An iterator that yields the tokens comprising the input
./.venv-build/lib/python3.12/site-packages/docutils/utils/smartquotes.py:909:                string. Each token is either a tag (possibly with nested,
./.venv-build/lib/python3.12/site-packages/docutils/utils/smartquotes.py:915:    Based on the _tokenize() subroutine from Brad Choate's MTRegex plugin.
./.venv-build/lib/python3.12/site-packages/docutils/utils/smartquotes.py:918:    token_match = tag_soup.search(text)
./.venv-build/lib/python3.12/site-packages/docutils/utils/smartquotes.py:921:    while token_match is not None:
./.venv-build/lib/python3.12/site-packages/docutils/utils/smartquotes.py:922:        if token_match.group(1):
./.venv-build/lib/python3.12/site-packages/docutils/utils/smartquotes.py:923:            yield "text", token_match.group(1)
./.venv-build/lib/python3.12/site-packages/docutils/utils/smartquotes.py:924:        yield "tag", token_match.group(2)
./.venv-build/lib/python3.12/site-packages/docutils/utils/smartquotes.py:925:        previous_end = token_match.end()
./.venv-build/lib/python3.12/site-packages/docutils/utils/smartquotes.py:926:        token_match = tag_soup.search(text, token_match.end())
./.venv-build/lib/python3.12/site-packages/docutils/utils/urischemes.py:79:    "opaquelocktoken": "RFC 2518",
./.venv-build/lib/python3.12/site-packages/twine/auth.py:33:TOKEN_USERNAME: t.Final[str] = "__token__"
./.venv-build/lib/python3.12/site-packages/twine/auth.py:34:#: Tokens expire after 15 minutes, let's start allowing renewal/replacement
./.venv-build/lib/python3.12/site-packages/twine/auth.py:38:#: the token.
./.venv-build/lib/python3.12/site-packages/twine/auth.py:39:TOKEN_RENEWAL_THRESHOLD: t.Final[datetime.timedelta] = datetime.timedelta(
./.venv-build/lib/python3.12/site-packages/twine/auth.py:45:    def __init__(self, username: t.Optional[str] = None, password: t.Optional[str] = None) -> None:
./.venv-build/lib/python3.12/site-packages/twine/auth.py:47:        self.password = password
./.venv-build/lib/python3.12/site-packages/twine/auth.py:50:class TrustedPublishingTokenRetrievalError(t.TypedDict):
./.venv-build/lib/python3.12/site-packages/twine/auth.py:55:class TrustedPublishingToken(t.TypedDict, total=False):
./.venv-build/lib/python3.12/site-packages/twine/auth.py:57:    errors: t.Optional[list[TrustedPublishingTokenRetrievalError]]
./.venv-build/lib/python3.12/site-packages/twine/auth.py:58:    token: t.Optional[str]
./.venv-build/lib/python3.12/site-packages/twine/auth.py:74:        token = self.resolver.make_trusted_publishing_token()
./.venv-build/lib/python3.12/site-packages/twine/auth.py:75:        if token is None:
./.venv-build/lib/python3.12/site-packages/twine/auth.py:77:                "Expected a trusted publishing token but got None"
./.venv-build/lib/python3.12/site-packages/twine/auth.py:83:            username=TOKEN_USERNAME,
./.venv-build/lib/python3.12/site-packages/twine/auth.py:84:            password=token,
./.venv-build/lib/python3.12/site-packages/twine/auth.py:90:    _tp_token: t.Optional[TrustedPublishingToken] = None
./.venv-build/lib/python3.12/site-packages/twine/auth.py:105:        password = self.password
./.venv-build/lib/python3.12/site-packages/twine/auth.py:106:        if self._tp_token:
./.venv-build/lib/python3.12/site-packages/twine/auth.py:107:            # If `self.password` ended up getting a Trusted Publishing token,
./.venv-build/lib/python3.12/site-packages/twine/auth.py:110:            # `make_trusted_publishing_token` which if the token is 10 minutes
./.venv-build/lib/python3.12/site-packages/twine/auth.py:113:        if username and password:
./.venv-build/lib/python3.12/site-packages/twine/auth.py:116:                password=password,
./.venv-build/lib/python3.12/site-packages/twine/auth.py:131:            self.input.username = TOKEN_USERNAME
./.venv-build/lib/python3.12/site-packages/twine/auth.py:142:    def password(self) -> t.Optional[str]:
./.venv-build/lib/python3.12/site-packages/twine/auth.py:144:            self.input.password,
./.venv-build/lib/python3.12/site-packages/twine/auth.py:146:            key="password",
./.venv-build/lib/python3.12/site-packages/twine/auth.py:147:            prompt_strategy=self.password_from_keyring_or_trusted_publishing_or_prompt,
./.venv-build/lib/python3.12/site-packages/twine/auth.py:150:    def _has_valid_cached_tp_token(self) -> bool:
./.venv-build/lib/python3.12/site-packages/twine/auth.py:151:        return self._tp_token is not None and (
./.venv-build/lib/python3.12/site-packages/twine/auth.py:152:            int(time.time()) + TOKEN_RENEWAL_THRESHOLD.seconds
./.venv-build/lib/python3.12/site-packages/twine/auth.py:153:            < cast(int, self._tp_token.get("expires", self._expires))
./.venv-build/lib/python3.12/site-packages/twine/auth.py:156:    def _make_trusted_publishing_token(self) -> t.Optional[TrustedPublishingToken]:
./.venv-build/lib/python3.12/site-packages/twine/auth.py:157:        if self._has_valid_cached_tp_token():
./.venv-build/lib/python3.12/site-packages/twine/auth.py:158:            return self._tp_token
./.venv-build/lib/python3.12/site-packages/twine/auth.py:159:        # Trusted publishing (OpenID Connect): get one token from the CI
./.venv-build/lib/python3.12/site-packages/twine/auth.py:160:        # system, and exchange that for a PyPI token.
./.venv-build/lib/python3.12/site-packages/twine/auth.py:172:            oidc_token = detect_credential(audience)
./.venv-build/lib/python3.12/site-packages/twine/auth.py:175:            # publishing, and we have not been given any token, so we can error.
./.venv-build/lib/python3.12/site-packages/twine/auth.py:177:                "Unable to retrieve an OIDC token from the CI platform for "
./.venv-build/lib/python3.12/site-packages/twine/auth.py:181:        if oidc_token is None:
./.venv-build/lib/python3.12/site-packages/twine/auth.py:183:            if self._tp_token and int(time.time()) > cast(
./.venv-build/lib/python3.12/site-packages/twine/auth.py:184:                int, self._tp_token.get("expires", self._expires)
./.venv-build/lib/python3.12/site-packages/twine/auth.py:186:                return None  # Fall back to prompting for a token (if possible)
./.venv-build/lib/python3.12/site-packages/twine/auth.py:187:            # The cached trusted publishing token may still be valid for a
./.venv-build/lib/python3.12/site-packages/twine/auth.py:189:            return self._tp_token
./.venv-build/lib/python3.12/site-packages/twine/auth.py:191:        logger.warning("Got OIDC token for audience %s", audience)
./.venv-build/lib/python3.12/site-packages/twine/auth.py:193:        token_exchange_url = f"https://{repository_domain}/_/oidc/mint-token"
./.venv-build/lib/python3.12/site-packages/twine/auth.py:195:        mint_token_resp = session.post(
./.venv-build/lib/python3.12/site-packages/twine/auth.py:196:            token_exchange_url,
./.venv-build/lib/python3.12/site-packages/twine/auth.py:197:            json={"token": oidc_token},
./.venv-build/lib/python3.12/site-packages/twine/auth.py:201:            mint_token_payload = mint_token_resp.json()
./.venv-build/lib/python3.12/site-packages/twine/auth.py:204:                "The token-minting request returned invalid JSON"
./.venv-build/lib/python3.12/site-packages/twine/auth.py:207:        if not mint_token_resp.ok:
./.venv-build/lib/python3.12/site-packages/twine/auth.py:210:                for error in mint_token_payload["errors"]
./.venv-build/lib/python3.12/site-packages/twine/auth.py:213:                "The token request failed; the index server gave the following"
./.venv-build/lib/python3.12/site-packages/twine/auth.py:217:        logger.warning("Minted upload token for trusted publishing")
./.venv-build/lib/python3.12/site-packages/twine/auth.py:218:        self._tp_token = cast(TrustedPublishingToken, mint_token_payload)
./.venv-build/lib/python3.12/site-packages/twine/auth.py:220:        return self._tp_token
./.venv-build/lib/python3.12/site-packages/twine/auth.py:222:    def make_trusted_publishing_token(self) -> t.Optional[str]:
./.venv-build/lib/python3.12/site-packages/twine/auth.py:223:        mint_token_payload = self._make_trusted_publishing_token()
./.venv-build/lib/python3.12/site-packages/twine/auth.py:224:        if not mint_token_payload:
./.venv-build/lib/python3.12/site-packages/twine/auth.py:226:        return cast(str, mint_token_payload["token"])
./.venv-build/lib/python3.12/site-packages/twine/auth.py:249:    def get_password_from_keyring(self) -> t.Optional[str]:
./.venv-build/lib/python3.12/site-packages/twine/auth.py:256:            logger.info("Querying keyring for password")
./.venv-build/lib/python3.12/site-packages/twine/auth.py:257:            return cast(str, keyring.get_password(system, username))
./.venv-build/lib/python3.12/site-packages/twine/auth.py:261:            logger.warning("Error getting password from keyring", exc_info=exc)
./.venv-build/lib/python3.12/site-packages/twine/auth.py:272:    def password_from_keyring_or_trusted_publishing_or_prompt(self) -> str:
./.venv-build/lib/python3.12/site-packages/twine/auth.py:273:        password = self.get_password_from_keyring()
./.venv-build/lib/python3.12/site-packages/twine/auth.py:274:        if password:
./.venv-build/lib/python3.12/site-packages/twine/auth.py:275:            logger.info("password set from keyring")
./.venv-build/lib/python3.12/site-packages/twine/auth.py:276:            return password
./.venv-build/lib/python3.12/site-packages/twine/auth.py:278:        if self.is_pypi() and self.username == TOKEN_USERNAME:
./.venv-build/lib/python3.12/site-packages/twine/auth.py:279:            logger.info("Trying to use trusted publishing (no token was explicitly provided)")
./.venv-build/lib/python3.12/site-packages/twine/auth.py:280:            if (token := self.make_trusted_publishing_token()) is not None:
./.venv-build/lib/python3.12/site-packages/twine/auth.py:281:                return token
./.venv-build/lib/python3.12/site-packages/twine/auth.py:283:        # Prompt for API token when required.
./.venv-build/lib/python3.12/site-packages/twine/auth.py:284:        what = "API token" if self.is_pypi() else "password"
./.venv-build/lib/python3.12/site-packages/twine/auth.py:292:        """As of 2024-01-01, PyPI requires API tokens for uploads."""
./.venv-build/lib/python3.12/site-packages/twine/utils.py:44:# instances of this type aren't None, except for username and password.
./.venv-build/lib/python3.12/site-packages/twine/utils.py:46:# requires reworking the username/password handling, probably starting with
./.venv-build/lib/python3.12/site-packages/twine/utils.py:76:        "password": parser.get("server-login", "password", fallback=None),
./.venv-build/lib/python3.12/site-packages/twine/utils.py:94:            "password",
./.venv-build/lib/python3.12/site-packages/twine/utils.py:108:    Sanitize URLs, removing any user:password combinations and replacing them with
./.venv-build/lib/python3.12/site-packages/twine/utils.py:160:        # since it could leak tokens and other sensitive values.
./.venv-build/lib/python3.12/site-packages/twine/utils.py:180:    config = {"repository": url, "username": None, "password": None}
./.venv-build/lib/python3.12/site-packages/twine/utils.py:183:        config["password"] = parsed.password
./.venv-build/lib/python3.12/site-packages/twine/utils.py:250:    """Get a credential (e.g. a username or password) from the configuration.
./.venv-build/lib/python3.12/site-packages/twine/utils.py:264:        The credential to look up in ``config``, e.g. ``"username"`` or ``"password"``.
./.venv-build/lib/python3.12/site-packages/twine/utils.py:270:        The credential value, i.e. the username or password.
./.venv-build/lib/python3.12/site-packages/twine/repository.py:40:        password: Optional[str],
./.venv-build/lib/python3.12/site-packages/twine/repository.py:47:        # But username or password could be None
./.venv-build/lib/python3.12/site-packages/twine/repository.py:49:        self.session.auth = (username or "", password or "") if username or password else None
./.venv-build/lib/python3.12/site-packages/twine/repository.py:51:        logger.info(f"password: <{'hidden' if password else 'empty'}>")
./.venv-build/lib/python3.12/site-packages/twine/settings.py:53:        password: Optional[str] = None,
./.venv-build/lib/python3.12/site-packages/twine/settings.py:79:        :param password:
./.venv-build/lib/python3.12/site-packages/twine/settings.py:80:            The password used to authenticate to the repository (package
./.venv-build/lib/python3.12/site-packages/twine/settings.py:83:            Do not interactively prompt for username/password if the required
./.venv-build/lib/python3.12/site-packages/twine/settings.py:130:            auth.CredentialInput(username, password),
./.venv-build/lib/python3.12/site-packages/twine/settings.py:138:    def password(self) -> Optional[str]:
./.venv-build/lib/python3.12/site-packages/twine/settings.py:140:            return self.auth.password
./.venv-build/lib/python3.12/site-packages/twine/settings.py:217:            "--password",
./.venv-build/lib/python3.12/site-packages/twine/settings.py:219:            env="TWINE_PASSWORD",
./.venv-build/lib/python3.12/site-packages/twine/settings.py:221:            help="The password to authenticate to the repository "
./.venv-build/lib/python3.12/site-packages/twine/settings.py:229:            help="Do not interactively prompt for username/password if the "
./.venv-build/lib/python3.12/site-packages/twine/settings.py:347:            self.password,
./.venv-build/lib/python3.11/site-packages/black/lines.py:29:from blib2to3.pgen2 import token
./.venv-build/lib/python3.11/site-packages/black/lines.py:66:            or leaf.type == token.FSTRING_MIDDLE
./.venv-build/lib/python3.11/site-packages/black/lines.py:72:        if leaf.type == token.COLON and self.is_class_paren_empty:
./.venv-build/lib/python3.11/site-packages/black/lines.py:115:        return bool(self) and self.leaves[0].type == token.AT
./.venv-build/lib/python3.11/site-packages/black/lines.py:130:        return bool(self) and self.leaves[0].type == token.NAME and self.leaves[0].value == "class"
./.venv-build/lib/python3.11/site-packages/black/lines.py:135:        return self.is_class and self.leaves[-3:] == [Leaf(token.DOT, ".") for _ in range(3)]
./.venv-build/lib/python3.11/site-packages/black/lines.py:149:        return (first_leaf.type == token.NAME and first_leaf.value == "def") or (
./.venv-build/lib/python3.11/site-packages/black/lines.py:150:            first_leaf.type == token.ASYNC
./.venv-build/lib/python3.11/site-packages/black/lines.py:152:            and second_leaf.type == token.NAME
./.venv-build/lib/python3.11/site-packages/black/lines.py:159:        return self.is_def and self.leaves[-4:] == [Leaf(token.COLON, ":")] + [
./.venv-build/lib/python3.11/site-packages/black/lines.py:160:            Leaf(token.DOT, ".") for _ in range(3)
./.venv-build/lib/python3.11/site-packages/black/lines.py:173:            and self.leaves[2].type == token.LPAR
./.venv-build/lib/python3.11/site-packages/black/lines.py:175:            and self.leaves[3].type == token.RPAR
./.venv-build/lib/python3.11/site-packages/black/lines.py:182:        if not self or self.leaves[0].type != token.STRING:
./.venv-build/lib/python3.11/site-packages/black/lines.py:199:        return [leaf.type for leaf in self.leaves].count(token.EQUAL) > 1
./.venv-build/lib/python3.11/site-packages/black/lines.py:206:        return self.leaves[-1].type == token.COLON
./.venv-build/lib/python3.11/site-packages/black/lines.py:236:            if leaf_type != token.STRING:
./.venv-build/lib/python3.11/site-packages/black/lines.py:251:            if last_leaf.type == token.COMMA or (
./.venv-build/lib/python3.11/site-packages/black/lines.py:252:                last_leaf.type == token.RPAR and not last_leaf.value
./.venv-build/lib/python3.11/site-packages/black/lines.py:322:            closing.type in CLOSING_BRACKETS and self.leaves and self.leaves[-1].type == token.COMMA
./.venv-build/lib/python3.11/site-packages/black/lines.py:326:        if closing.type == token.RBRACE:
./.venv-build/lib/python3.11/site-packages/black/lines.py:329:        if closing.type == token.RSQB:
./.venv-build/lib/python3.11/site-packages/black/lines.py:338:                    brackets=(token.LSQB, token.RSQB),
./.venv-build/lib/python3.11/site-packages/black/lines.py:363:        if comment.type != token.COMMENT:
./.venv-build/lib/python3.11/site-packages/black/lines.py:373:            last_leaf.type == token.RPAR
./.venv-build/lib/python3.11/site-packages/black/lines.py:620:                    and current_line.leaves[-1].type == token.COLON
./.venv-build/lib/python3.11/site-packages/black/lines.py:828:        if leaf.bracket_depth <= max_level_to_update and leaf.type == token.COMMA:
./.venv-build/lib/python3.11/site-packages/black/lines.py:878:    if leaves[0].type == token.STRING and leaves[1].type == token.DOT:
./.venv-build/lib/python3.11/site-packages/black/lines.py:888:            elif leaf.type == token.DOT:
./.venv-build/lib/python3.11/site-packages/black/lines.py:890:            elif leaf.type == token.NAME:
./.venv-build/lib/python3.11/site-packages/black/lines.py:891:                if not (next.type == token.DOT or next.type in OPENING_BRACKETS):
./.venv-build/lib/python3.11/site-packages/black/lines.py:975:        last.type == token.RPAR
./.venv-build/lib/python3.11/site-packages/black/lines.py:976:        or last.type == token.RBRACE
./.venv-build/lib/python3.11/site-packages/black/lines.py:980:            last.type == token.RSQB
./.venv-build/lib/python3.11/site-packages/black/ranges.py:19:from blib2to3.pgen2.token import ASYNC, NEWLINE
./.venv-build/lib/python3.11/site-packages/black/ranges.py:156:    the unformatted code as its value. `STANDALONE_COMMENT` is a "fake" token
./.venv-build/lib/python3.11/site-packages/black/ranges.py:165:       line. "unwrapped lines" are divided by the `NEWLINE` token. e.g. a
./.venv-build/lib/python3.11/site-packages/black/ranges.py:168:       tokenizer only sees the last '\n' as the `NEWLINE` token.
./.venv-build/lib/python3.11/site-packages/black/ranges.py:230:        # `async_funcdef`, the ASYNC token is defined on a separate level by the
./.venv-build/lib/python3.11/site-packages/black/ranges.py:250:            # token.
./.venv-build/lib/python3.11/site-packages/black/ranges.py:275:            # token is on the grandparent node.
./.venv-build/lib/python3.11/site-packages/black/brackets.py:19:from blib2to3.pgen2 import token
./.venv-build/lib/python3.11/site-packages/black/brackets.py:37:    token.VBAR: 9,
./.venv-build/lib/python3.11/site-packages/black/brackets.py:38:    token.CIRCUMFLEX: 8,
./.venv-build/lib/python3.11/site-packages/black/brackets.py:39:    token.AMPER: 7,
./.venv-build/lib/python3.11/site-packages/black/brackets.py:40:    token.LEFTSHIFT: 6,
./.venv-build/lib/python3.11/site-packages/black/brackets.py:41:    token.RIGHTSHIFT: 6,
./.venv-build/lib/python3.11/site-packages/black/brackets.py:42:    token.PLUS: 5,
./.venv-build/lib/python3.11/site-packages/black/brackets.py:43:    token.MINUS: 5,
./.venv-build/lib/python3.11/site-packages/black/brackets.py:44:    token.STAR: 4,
./.venv-build/lib/python3.11/site-packages/black/brackets.py:45:    token.SLASH: 4,
./.venv-build/lib/python3.11/site-packages/black/brackets.py:46:    token.DOUBLESLASH: 4,
./.venv-build/lib/python3.11/site-packages/black/brackets.py:47:    token.PERCENT: 4,
./.venv-build/lib/python3.11/site-packages/black/brackets.py:48:    token.AT: 4,
./.venv-build/lib/python3.11/site-packages/black/brackets.py:49:    token.TILDE: 3,
./.venv-build/lib/python3.11/site-packages/black/brackets.py:50:    token.DOUBLESTAR: 2,
./.venv-build/lib/python3.11/site-packages/black/brackets.py:85:        If a leaf is a delimiter (a token on which Black can split the line if
./.venv-build/lib/python3.11/site-packages/black/brackets.py:89:        if leaf.type == token.COMMENT:
./.venv-build/lib/python3.11/site-packages/black/brackets.py:164:        tokens between `for` and `in`.
./.venv-build/lib/python3.11/site-packages/black/brackets.py:166:        if leaf.type == token.NAME and leaf.value == "for":
./.venv-build/lib/python3.11/site-packages/black/brackets.py:178:            and leaf.type == token.NAME
./.venv-build/lib/python3.11/site-packages/black/brackets.py:191:        tokens between `lambda` and `:`.
./.venv-build/lib/python3.11/site-packages/black/brackets.py:193:        if leaf.type == token.NAME and leaf.value == "lambda":
./.venv-build/lib/python3.11/site-packages/black/brackets.py:205:            and leaf.type == token.COLON
./.venv-build/lib/python3.11/site-packages/black/brackets.py:215:        return self.bracket_match.get((self.depth - 1, token.RSQB))
./.venv-build/lib/python3.11/site-packages/black/brackets.py:226:    if leaf.type == token.COMMA:
./.venv-build/lib/python3.11/site-packages/black/brackets.py:246:        leaf.type == token.DOT
./.venv-build/lib/python3.11/site-packages/black/brackets.py:263:    if leaf.type == token.STRING and previous is not None and previous.type == token.STRING:
./.venv-build/lib/python3.11/site-packages/black/brackets.py:266:    if leaf.type not in {token.NAME, token.ASYNC}:
./.venv-build/lib/python3.11/site-packages/black/brackets.py:273:        or leaf.type == token.ASYNC
./.venv-build/lib/python3.11/site-packages/black/brackets.py:291:        and not (previous is not None and previous.type == token.NAME and previous.value == "not")
./.venv-build/lib/python3.11/site-packages/black/brackets.py:299:        and not (previous is not None and previous.type == token.NAME and previous.value == "is")
./.venv-build/lib/python3.11/site-packages/black/brackets.py:320:    if not (first.type == token.LPAR and last.type == token.RPAR):
./.venv-build/lib/python3.11/site-packages/black/comments.py:18:from blib2to3.pgen2 import token
./.venv-build/lib/python3.11/site-packages/black/comments.py:45:    type: int  # token.COMMENT or STANDALONE_COMMENT
./.venv-build/lib/python3.11/site-packages/black/comments.py:57:    in `pgen2/driver.py:Driver.parse_tokens()`.  This was a brilliant implementation
./.venv-build/lib/python3.11/site-packages/black/comments.py:69:    Inline comments are emitted as regular token.COMMENT leaves.  Standalone
./.venv-build/lib/python3.11/site-packages/black/comments.py:70:    are emitted with a fake STANDALONE_COMMENT token identifier.
./.venv-build/lib/python3.11/site-packages/black/comments.py:73:    for pc in list_comments(leaf.prefix, is_endmarker=leaf.type == token.ENDMARKER):
./.venv-build/lib/python3.11/site-packages/black/comments.py:109:            comment_type = token.COMMENT  # simple trailing comment
./.venv-build/lib/python3.11/site-packages/black/comments.py:263:    while container is not None and container.type != token.ENDMARKER:
./.venv-build/lib/python3.11/site-packages/black/comments.py:279:                    child.type == token.INDENT
./.venv-build/lib/python3.11/site-packages/black/comments.py:284:                    # level, and we shouldn't swallow the previous INDENT token.
./.venv-build/lib/python3.11/site-packages/black/comments.py:290:            if container.type == token.DEDENT and container.next_sibling is None:
./.venv-build/lib/python3.11/site-packages/black/comments.py:348:            if current_node.type in (token.NEWLINE, token.INDENT):
./.venv-build/lib/python3.11/site-packages/black/comments.py:357:    elif parent is not None and parent.type == syms.suite and leaf.type == token.NEWLINE:
./.venv-build/lib/python3.11/site-packages/black/comments.py:366:        # Special case for `async_stmt` where the ASYNC token is on the
./.venv-build/lib/python3.11/site-packages/black/comments.py:372:            and grandparent.prev_sibling.type == token.ASYNC
./.venv-build/lib/python3.11/site-packages/black/linegen.py:50:    is_lpar_token,
./.venv-build/lib/python3.11/site-packages/black/linegen.py:52:    is_name_token,
./.venv-build/lib/python3.11/site-packages/black/linegen.py:57:    is_rpar_token,
./.venv-build/lib/python3.11/site-packages/black/linegen.py:87:from blib2to3.pgen2 import token
./.venv-build/lib/python3.11/site-packages/black/linegen.py:147:                elif comment.type == token.COMMENT:
./.venv-build/lib/python3.11/site-packages/black/linegen.py:168:        already_parenthesized = node.prev_sibling and node.prev_sibling.type == token.LPAR
./.venv-build/lib/python3.11/site-packages/black/linegen.py:172:            lpar = Leaf(token.LPAR, "")
./.venv-build/lib/python3.11/site-packages/black/linegen.py:173:            rpar = Leaf(token.RPAR, "")
./.venv-build/lib/python3.11/site-packages/black/linegen.py:218:            if is_name_token(child) and child.value in keywords:
./.venv-build/lib/python3.11/site-packages/black/linegen.py:240:                if node.children[i - 1].type == token.COLON:
./.venv-build/lib/python3.11/site-packages/black/linegen.py:264:            if child.type == token.RARROW:
./.venv-build/lib/python3.11/site-packages/black/linegen.py:267:                if child.type == syms.atom and child.children[0].type == token.LPAR:
./.venv-build/lib/python3.11/site-packages/black/linegen.py:302:            if (prev_type is None or prev_type == token.SEMI) and is_arith_like(child):
./.venv-build/lib/python3.11/site-packages/black/linegen.py:330:            if child.type == token.ASYNC or child.type == STANDALONE_COMMENT:
./.venv-build/lib/python3.11/site-packages/black/linegen.py:353:                leaf.type == token.NUMBER
./.venv-build/lib/python3.11/site-packages/black/linegen.py:356:                and next_leaf.children[0].type == token.DOT
./.venv-build/lib/python3.11/site-packages/black/linegen.py:391:            and operand.children[1].type == token.DOUBLESTAR
./.venv-build/lib/python3.11/site-packages/black/linegen.py:393:            lpar = Leaf(token.LPAR, "(")
./.venv-build/lib/python3.11/site-packages/black/linegen.py:394:            rpar = Leaf(token.RPAR, ")")
./.venv-build/lib/python3.11/site-packages/black/linegen.py:501:        if self.mode.string_normalization and leaf.type == token.STRING:
./.venv-build/lib/python3.11/site-packages/black/linegen.py:515:            if (first.type == token.LSQB and last.type == token.RSQB) or (
./.venv-build/lib/python3.11/site-packages/black/linegen.py:516:                first.type == token.LBRACE and last.type == token.RBRACE
./.venv-build/lib/python3.11/site-packages/black/linegen.py:565:        #     if leaf.type == token.FSTRING_MIDDLE
./.venv-build/lib/python3.11/site-packages/black/linegen.py:753:        if leaf.type == token.COLON:
./.venv-build/lib/python3.11/site-packages/black/linegen.py:757:        if leaf.type == token.RARROW:
./.venv-build/lib/python3.11/site-packages/black/linegen.py:789:    for leaf_type in [token.LPAR, token.LSQB]:
./.venv-build/lib/python3.11/site-packages/black/linegen.py:797:            if index == 2 and leaf.type == token.LSQB:
./.venv-build/lib/python3.11/site-packages/black/linegen.py:802:                if leaf.type == token.LSQB:
./.venv-build/lib/python3.11/site-packages/black/linegen.py:804:                elif leaf.type == token.RSQB:
./.venv-build/lib/python3.11/site-packages/black/linegen.py:814:                and not (leaf_type == token.LPAR and depth > 0)
./.venv-build/lib/python3.11/site-packages/black/linegen.py:909:        is_unpacking = body_leaves[0].type in [token.STAR, token.DOUBLESTAR]
./.venv-build/lib/python3.11/site-packages/black/linegen.py:929:            if line.mode.magic_trailing_comma and inner_body_leaves[-1].type == token.COMMA:
./.venv-build/lib/python3.11/site-packages/black/linegen.py:972:        and rhs.opening_bracket.type == token.LPAR
./.venv-build/lib/python3.11/site-packages/black/linegen.py:975:        and rhs.closing_bracket.type == token.RPAR
./.venv-build/lib/python3.11/site-packages/black/linegen.py:1049:        return any(leaf.type == token.COLON for leaf in rhs_oop.tail.leaves)
./.venv-build/lib/python3.11/site-packages/black/linegen.py:1052:    if not (len(rhs.head.leaves) >= 2 and rhs.head.leaves[-2].type == token.EQUAL):
./.venv-build/lib/python3.11/site-packages/black/linegen.py:1070:    rhs_head_equal_count = [leaf.type for leaf in rhs.head.leaves].count(token.EQUAL)
./.venv-build/lib/python3.11/site-packages/black/linegen.py:1071:    rhs_oop_head_equal_count = [leaf.type for leaf in rhs_oop.head.leaves].count(token.EQUAL)
./.venv-build/lib/python3.11/site-packages/black/linegen.py:1077:        if leaf.type == token.EQUAL:
./.venv-build/lib/python3.11/site-packages/black/linegen.py:1089:            any(leaf.type == token.EQUAL for leaf in rhs_oop.head.leaves)
./.venv-build/lib/python3.11/site-packages/black/linegen.py:1134:    if any(leaf.type == token.COMMA and not is_part_of_annotation(leaf) for leaf in leaves):
./.venv-build/lib/python3.11/site-packages/black/linegen.py:1148:        and leaf_with_parent.parent.next_sibling.type == token.VBAR
./.venv-build/lib/python3.11/site-packages/black/linegen.py:1179:                if leaves[i].type != token.COMMA:
./.venv-build/lib/python3.11/site-packages/black/linegen.py:1180:                    new_comma = Leaf(token.COMMA, ",")
./.venv-build/lib/python3.11/site-packages/black/linegen.py:1235:        and line.leaves[-1].type != token.COMMA
./.venv-build/lib/python3.11/site-packages/black/linegen.py:1238:        new_comma = Leaf(token.COMMA, ",")
./.venv-build/lib/python3.11/site-packages/black/linegen.py:1410:                and child.prev_sibling.type == token.NAME
./.venv-build/lib/python3.11/site-packages/black/linegen.py:1426:                and is_lpar_token(child.children[0])
./.venv-build/lib/python3.11/site-packages/black/linegen.py:1427:                and is_rpar_token(child.children[-1])
./.venv-build/lib/python3.11/site-packages/black/linegen.py:1439:            elif index == 1 and child.type == token.STAR and node.type == syms.except_clause:
./.venv-build/lib/python3.11/site-packages/black/linegen.py:1448:                and child.next_sibling.type == token.COLON
./.venv-build/lib/python3.11/site-packages/black/linegen.py:1458:        comma_check = child.type == token.COMMA
./.venv-build/lib/python3.11/site-packages/black/linegen.py:1466:    if is_lpar_token(child):
./.venv-build/lib/python3.11/site-packages/black/linegen.py:1467:        assert is_rpar_token(parent.children[-1])
./.venv-build/lib/python3.11/site-packages/black/linegen.py:1471:    elif child.type != token.STAR:
./.venv-build/lib/python3.11/site-packages/black/linegen.py:1473:        parent.insert_child(index, Leaf(token.LPAR, ""))
./.venv-build/lib/python3.11/site-packages/black/linegen.py:1474:        parent.append_child(Leaf(token.RPAR, ""))
./.venv-build/lib/python3.11/site-packages/black/linegen.py:1478:    if node.children[0].type == token.AWAIT and len(node.children) > 1:
./.venv-build/lib/python3.11/site-packages/black/linegen.py:1479:        if node.children[1].type == syms.atom and node.children[1].children[0].type == token.LPAR:
./.venv-build/lib/python3.11/site-packages/black/linegen.py:1501:                or bracket_contents.children[0].type == token.AWAIT
./.venv-build/lib/python3.11/site-packages/black/linegen.py:1503:                    isinstance(child, Leaf) and child.type == token.DOUBLESTAR
./.venv-build/lib/python3.11/site-packages/black/linegen.py:1525:        if node.children[i].type == token.COLON:
./.venv-build/lib/python3.11/site-packages/black/linegen.py:1529:        lpar = Leaf(token.LPAR, "")
./.venv-build/lib/python3.11/site-packages/black/linegen.py:1530:        rpar = Leaf(token.RPAR, "")
./.venv-build/lib/python3.11/site-packages/black/linegen.py:1583:        leaf.type == token.COLONEQUAL for leaf in node.leaves()
./.venv-build/lib/python3.11/site-packages/black/linegen.py:1616:            and has_sibling_with_type(node, token.COMMA)
./.venv-build/lib/python3.11/site-packages/black/linegen.py:1640:                    and is_name_token(node.next_sibling)
./.venv-build/lib/python3.11/site-packages/black/linegen.py:1671:    if is_lpar_token(first) and is_rpar_token(last):
./.venv-build/lib/python3.11/site-packages/black/linegen.py:1720:        if last_leaf.type == token.COMMA:
./.venv-build/lib/python3.11/site-packages/black/linegen.py:1769:                    and prev.type == token.COMMA
./.venv-build/lib/python3.11/site-packages/black/linegen.py:1795:                and prev.type == token.COMMA
./.venv-build/lib/python3.11/site-packages/black/trans.py:38:from blib2to3.pgen2 import token
./.venv-build/lib/python3.11/site-packages/black/trans.py:74:        if leaf.type == token.DOUBLESTAR:
./.venv-build/lib/python3.11/site-packages/black/trans.py:77:        raise CannotTransform("No doublestar token was found in the line.")
./.venv-build/lib/python3.11/site-packages/black/trans.py:84:            return handle_is_simple_look_up_prev(line, index, {token.RPAR, token.RSQB})
./.venv-build/lib/python3.11/site-packages/black/trans.py:86:            return handle_is_simple_lookup_forward(line, index, {token.LPAR, token.LSQB})
./.venv-build/lib/python3.11/site-packages/black/trans.py:92:        if start.type in {token.NAME, token.NUMBER}:
./.venv-build/lib/python3.11/site-packages/black/trans.py:95:        if start.type in {token.PLUS, token.MINUS, token.TILDE}:
./.venv-build/lib/python3.11/site-packages/black/trans.py:96:            if line.leaves[index + 1].type in {token.NAME, token.NUMBER}:
./.venv-build/lib/python3.11/site-packages/black/trans.py:98:                # for simplicity starting from the next token (so it'll hit the check
./.venv-build/lib/python3.11/site-packages/black/trans.py:114:            and leaf.type == token.DOUBLESTAR
./.venv-build/lib/python3.11/site-packages/black/trans.py:135:    token. This is required because of the need to isolate the chained expression
./.venv-build/lib/python3.11/site-packages/black/trans.py:156:    Handling decision is_simple_lookup for the lines behind the doublestar token.
./.venv-build/lib/python3.11/site-packages/black/trans.py:164:        if current.type not in {token.NAME, token.DOT} or (
./.venv-build/lib/python3.11/site-packages/black/trans.py:165:            current.type == token.NAME and current.value == "for"
./.venv-build/lib/python3.11/site-packages/black/trans.py:167:            # If the current token isn't disallowed, we'll assume this is simple as
./.venv-build/lib/python3.11/site-packages/black/trans.py:168:            # only the disallowed tokens are semantically attached to this lookup
./.venv-build/lib/python3.11/site-packages/black/trans.py:189:    if past_leaf.type == token.NAME:
./.venv-build/lib/python3.11/site-packages/black/trans.py:190:        return current_leaf.type in {token.DOT}
./.venv-build/lib/python3.11/site-packages/black/trans.py:191:    elif past_leaf.type in {token.RPAR, token.RSQB}:
./.venv-build/lib/python3.11/site-packages/black/trans.py:192:        return current_leaf.type in {token.RSQB, token.RPAR}
./.venv-build/lib/python3.11/site-packages/black/trans.py:193:    elif past_leaf.type in {token.LPAR, token.LSQB}:
./.venv-build/lib/python3.11/site-packages/black/trans.py:194:        return current_leaf.type in {token.NAME, token.LPAR, token.LSQB}
./.venv-build/lib/python3.11/site-packages/black/trans.py:277:        if not any(leaf.type == token.STRING for leaf in line.leaves):
./.venv-build/lib/python3.11/site-packages/black/trans.py:419:                leaf.type == token.STRING
./.venv-build/lib/python3.11/site-packages/black/trans.py:421:                and LL[idx + 1].type == token.STRING
./.venv-build/lib/python3.11/site-packages/black/trans.py:428:                    if LL[i].type != token.STRING:
./.venv-build/lib/python3.11/site-packages/black/trans.py:440:                while is_valid_index(idx) and LL[idx].type == token.STRING:
./.venv-build/lib/python3.11/site-packages/black/trans.py:443:            elif leaf.type == token.STRING and "\\\n" in leaf.value:
./.venv-build/lib/python3.11/site-packages/black/trans.py:447:                while is_valid_index(idx) and LL[idx].type == token.STRING:
./.venv-build/lib/python3.11/site-packages/black/trans.py:504:                string_leaf.type == token.STRING
./.venv-build/lib/python3.11/site-packages/black/trans.py:651:        while not prefix and is_valid_index(next_str_idx) and LL[next_str_idx].type == token.STRING:
./.venv-build/lib/python3.11/site-packages/black/trans.py:668:        while is_valid_index(next_str_idx) and LL[next_str_idx].type == token.STRING:
./.venv-build/lib/python3.11/site-packages/black/trans.py:693:        S_leaf = Leaf(token.STRING, S)
./.venv-build/lib/python3.11/site-packages/black/trans.py:707:        string_leaf = Leaf(token.STRING, S_leaf.value.replace(BREAK_MARK, ""))
./.venv-build/lib/python3.11/site-packages/black/trans.py:759:                token.STRING,
./.venv-build/lib/python3.11/site-packages/black/trans.py:778:            if leaf.type != token.STRING:
./.venv-build/lib/python3.11/site-packages/black/trans.py:782:                if leaf.type == token.COMMA and id(leaf) in line.comments:
./.venv-build/lib/python3.11/site-packages/black/trans.py:862:            if leaf.type != token.STRING:
./.venv-build/lib/python3.11/site-packages/black/trans.py:872:                or LL[idx - 1].type != token.LPAR
./.venv-build/lib/python3.11/site-packages/black/trans.py:882:                LL[idx - 2].type == token.COLON
./.venv-build/lib/python3.11/site-packages/black/trans.py:883:                or LL[idx - 2].type == token.NAME
./.venv-build/lib/python3.11/site-packages/black/trans.py:900:                if token.PERCENT in {leaf.type for leaf in LL[idx - 1 : next_idx]} and (
./.venv-build/lib/python3.11/site-packages/black/trans.py:904:                            token.STAR,
./.venv-build/lib/python3.11/site-packages/black/trans.py:905:                            token.AT,
./.venv-build/lib/python3.11/site-packages/black/trans.py:906:                            token.SLASH,
./.venv-build/lib/python3.11/site-packages/black/trans.py:907:                            token.DOUBLESLASH,
./.venv-build/lib/python3.11/site-packages/black/trans.py:908:                            token.PERCENT,
./.venv-build/lib/python3.11/site-packages/black/trans.py:909:                            token.TILDE,
./.venv-build/lib/python3.11/site-packages/black/trans.py:910:                            token.DOUBLESTAR,
./.venv-build/lib/python3.11/site-packages/black/trans.py:911:                            token.AWAIT,
./.venv-build/lib/python3.11/site-packages/black/trans.py:912:                            token.LSQB,
./.venv-build/lib/python3.11/site-packages/black/trans.py:913:                            token.LPAR,
./.venv-build/lib/python3.11/site-packages/black/trans.py:920:                        and (before_lpar.type in {token.PLUS, token.MINUS})
./.venv-build/lib/python3.11/site-packages/black/trans.py:928:                and LL[next_idx].type == token.RPAR
./.venv-build/lib/python3.11/site-packages/black/trans.py:934:                    token.DOUBLESTAR,
./.venv-build/lib/python3.11/site-packages/black/trans.py:935:                    token.LSQB,
./.venv-build/lib/python3.11/site-packages/black/trans.py:936:                    token.LPAR,
./.venv-build/lib/python3.11/site-packages/black/trans.py:937:                    token.DOT,
./.venv-build/lib/python3.11/site-packages/black/trans.py:943:                while idx < len(LL) - 1 and LL[idx + 1].type == token.STRING:
./.venv-build/lib/python3.11/site-packages/black/trans.py:986:            lpar_or_rpar_idx = idx - 1 if leaf.type == token.STRING else idx
./.venv-build/lib/python3.11/site-packages/black/trans.py:988:            if leaf.type == token.STRING:
./.venv-build/lib/python3.11/site-packages/black/trans.py:989:                string_leaf = Leaf(token.STRING, LL[idx].value)
./.venv-build/lib/python3.11/site-packages/black/trans.py:1033:        token.EQEQUAL,
./.venv-build/lib/python3.11/site-packages/black/trans.py:1034:        token.GREATER,
./.venv-build/lib/python3.11/site-packages/black/trans.py:1035:        token.GREATEREQUAL,
./.venv-build/lib/python3.11/site-packages/black/trans.py:1036:        token.LESS,
./.venv-build/lib/python3.11/site-packages/black/trans.py:1037:        token.LESSEQUAL,
./.venv-build/lib/python3.11/site-packages/black/trans.py:1038:        token.NOTEQUAL,
./.venv-build/lib/python3.11/site-packages/black/trans.py:1039:        token.PERCENT,
./.venv-build/lib/python3.11/site-packages/black/trans.py:1040:        token.PLUS,
./.venv-build/lib/python3.11/site-packages/black/trans.py:1041:        token.STAR,
./.venv-build/lib/python3.11/site-packages/black/trans.py:1092:            token.STRING,
./.venv-build/lib/python3.11/site-packages/black/trans.py:1093:            token.NEWLINE,
./.venv-build/lib/python3.11/site-packages/black/trans.py:1148:                LL[string_idx - 1].type == token.LPAR
./.venv-build/lib/python3.11/site-packages/black/trans.py:1160:            if P.type == token.COMMA:
./.venv-build/lib/python3.11/site-packages/black/trans.py:1164:            if P.type in [token.COLON, token.EQUAL, token.PLUSEQUAL, token.NAME]:
./.venv-build/lib/python3.11/site-packages/black/trans.py:1181:            if N.type == token.RPAR and N.value == "" and len(LL) > string_idx + 2:
./.venv-build/lib/python3.11/site-packages/black/trans.py:1185:            if N.type == token.COMMA:
./.venv-build/lib/python3.11/site-packages/black/trans.py:1192:                if N.type == token.DOT and NN.type == token.NAME:
./.venv-build/lib/python3.11/site-packages/black/trans.py:1199:                    if is_valid_index(string_idx + 3) and LL[string_idx + 3].type == token.LPAR:
./.venv-build/lib/python3.11/site-packages/black/trans.py:1231:        if LL[0].type != token.STRING:
./.venv-build/lib/python3.11/site-packages/black/trans.py:1250:            if (not prev_sibling or prev_sibling.type == token.COMMA) and (
./.venv-build/lib/python3.11/site-packages/black/trans.py:1251:                not next_sibling or next_sibling.type == token.COMMA
./.venv-build/lib/python3.11/site-packages/black/trans.py:1387:            and [LL[idx].type, LL[idx + 1].type] == [token.NAME, token.NAME]
./.venv-build/lib/python3.11/site-packages/black/trans.py:1394:            or LL[idx].type == token.NAME
./.venv-build/lib/python3.11/site-packages/black/trans.py:1404:        if not is_valid_index(idx) or LL[idx].type != token.STRING:
./.venv-build/lib/python3.11/site-packages/black/trans.py:1418:        if is_valid_index(idx) and LL[idx].type == token.COMMA:
./.venv-build/lib/python3.11/site-packages/black/trans.py:1470:        ends_with_comma = is_valid_index(string_idx + 1) and LL[string_idx + 1].type == token.COMMA
./.venv-build/lib/python3.11/site-packages/black/trans.py:1588:            next_leaf = Leaf(token.STRING, next_value)
./.venv-build/lib/python3.11/site-packages/black/trans.py:1606:        rest_leaf = Leaf(token.STRING, rest_value)
./.venv-build/lib/python3.11/site-packages/black/trans.py:1625:                if leaf.type == token.LPAR:
./.venv-build/lib/python3.11/site-packages/black/trans.py:1631:                or LL[string_idx + 1].type == token.COMMA
./.venv-build/lib/python3.11/site-packages/black/trans.py:1824:        while LL[i].type in self.STRING_OPERATORS + [token.NAME]:
./.venv-build/lib/python3.11/site-packages/black/trans.py:1944:            if is_valid_index(idx) and LL[idx].type == token.STRING:
./.venv-build/lib/python3.11/site-packages/black/trans.py:1962:        if parent_type(LL[0]) == syms.test and LL[0].type == token.NAME and LL[0].value == "else":
./.venv-build/lib/python3.11/site-packages/black/trans.py:1967:            if is_valid_index(idx) and LL[idx].type == token.STRING:
./.venv-build/lib/python3.11/site-packages/black/trans.py:1990:                if leaf.type == token.COMMA:
./.venv-build/lib/python3.11/site-packages/black/trans.py:1994:                    if is_valid_index(idx) and LL[idx].type == token.STRING:
./.venv-build/lib/python3.11/site-packages/black/trans.py:2022:            and LL[0].type == token.NAME
./.venv-build/lib/python3.11/site-packages/black/trans.py:2028:                if leaf.type in [token.EQUAL, token.PLUSEQUAL]:
./.venv-build/lib/python3.11/site-packages/black/trans.py:2032:                    if is_valid_index(idx) and LL[idx].type == token.STRING:
./.venv-build/lib/python3.11/site-packages/black/trans.py:2044:                            and LL[idx].type == token.COMMA
./.venv-build/lib/python3.11/site-packages/black/trans.py:2072:                if leaf.type == token.COLON and i < len(LL) - 1:
./.venv-build/lib/python3.11/site-packages/black/trans.py:2076:                    if is_valid_index(idx) and LL[idx].type == token.STRING:
./.venv-build/lib/python3.11/site-packages/black/trans.py:2084:                        if is_valid_index(idx) and LL[idx].type == token.COMMA:
./.venv-build/lib/python3.11/site-packages/black/trans.py:2106:        if LL[comma_idx].type == token.COMMA:
./.venv-build/lib/python3.11/site-packages/black/trans.py:2121:        if left_leaves and left_leaves[-1].type == token.LPAR:
./.venv-build/lib/python3.11/site-packages/black/trans.py:2128:        lpar_leaf = Leaf(token.LPAR, "(")
./.venv-build/lib/python3.11/site-packages/black/trans.py:2155:        string_leaf = Leaf(token.STRING, string_value)
./.venv-build/lib/python3.11/site-packages/black/trans.py:2166:                assert right_leaves and right_leaves[-1].type == token.RPAR, (
./.venv-build/lib/python3.11/site-packages/black/trans.py:2171:            elif right_leaves and right_leaves[-1].type == token.RPAR:
./.venv-build/lib/python3.11/site-packages/black/trans.py:2185:                        and left_leaves[index - 1].type == token.COLON
./.venv-build/lib/python3.11/site-packages/black/trans.py:2198:        new_rpar_leaf = Leaf(token.RPAR, ")")
./.venv-build/lib/python3.11/site-packages/black/trans.py:2208:            comma_leaf = Leaf(token.COMMA, ",")
./.venv-build/lib/python3.11/site-packages/black/trans.py:2241:        assert line.leaves[idx].type == token.PLUS
./.venv-build/lib/python3.11/site-packages/black/trans.py:2245:    DEFAULT_TOKEN: Final = 20210605
./.venv-build/lib/python3.11/site-packages/black/trans.py:2260:        (START, token.DOT): DOT,
./.venv-build/lib/python3.11/site-packages/black/trans.py:2261:        (START, token.PERCENT): PERCENT,
./.venv-build/lib/python3.11/site-packages/black/trans.py:2262:        (START, DEFAULT_TOKEN): DONE,
./.venv-build/lib/python3.11/site-packages/black/trans.py:2264:        (DOT, token.NAME): NAME,
./.venv-build/lib/python3.11/site-packages/black/trans.py:2267:        (NAME, token.LPAR): LPAR,
./.venv-build/lib/python3.11/site-packages/black/trans.py:2268:        (NAME, DEFAULT_TOKEN): DONE,
./.venv-build/lib/python3.11/site-packages/black/trans.py:2271:        (PERCENT, token.LPAR): LPAR,
./.venv-build/lib/python3.11/site-packages/black/trans.py:2272:        (PERCENT, DEFAULT_TOKEN): SINGLE_FMT_ARG,
./.venv-build/lib/python3.11/site-packages/black/trans.py:2275:        (SINGLE_FMT_ARG, DEFAULT_TOKEN): DONE,
./.venv-build/lib/python3.11/site-packages/black/trans.py:2280:        (RPAR, DEFAULT_TOKEN): DONE,
./.venv-build/lib/python3.11/site-packages/black/trans.py:2290:            * @leaves[@string_idx].type == token.STRING
./.venv-build/lib/python3.11/site-packages/black/trans.py:2298:        assert leaves[string_idx].type == token.STRING
./.venv-build/lib/python3.11/site-packages/black/trans.py:2322:        next_token = leaf.type
./.venv-build/lib/python3.11/site-packages/black/trans.py:2323:        if next_token == token.LPAR:
./.venv-build/lib/python3.11/site-packages/black/trans.py:2329:        # find the matching RPAR token.
./.venv-build/lib/python3.11/site-packages/black/trans.py:2331:            if next_token == token.RPAR:
./.venv-build/lib/python3.11/site-packages/black/trans.py:2338:            # token, we use the lookup table.
./.venv-build/lib/python3.11/site-packages/black/trans.py:2339:            if (current_state, next_token) in self._goto:
./.venv-build/lib/python3.11/site-packages/black/trans.py:2340:                self._state = self._goto[current_state, next_token]
./.venv-build/lib/python3.11/site-packages/black/trans.py:2344:                if (current_state, self.DEFAULT_TOKEN) in self._goto:
./.venv-build/lib/python3.11/site-packages/black/trans.py:2345:                    self._state = self._goto[current_state, self.DEFAULT_TOKEN]
./.venv-build/lib/python3.11/site-packages/black/trans.py:2364:        Let `string_leaf = Leaf(token.STRING, '"foo"')` and `N =
./.venv-build/lib/python3.11/site-packages/black/trans.py:2380:        lpar = Leaf(token.LPAR, '(')
./.venv-build/lib/python3.11/site-packages/black/trans.py:2383:        bar = Leaf(token.STRING, '"bar"')
./.venv-build/lib/python3.11/site-packages/black/trans.py:2386:        rpar = Leaf(token.RPAR, ')')
./.venv-build/lib/python3.11/site-packages/black/nodes.py:20:from blib2to3.pgen2 import token
./.venv-build/lib/python3.11/site-packages/black/nodes.py:34:WHITESPACE: Final = {token.DEDENT, token.INDENT, token.NEWLINE}
./.venv-build/lib/python3.11/site-packages/black/nodes.py:48:token.tok_name[STANDALONE_COMMENT] = "STANDALONE_COMMENT"
./.venv-build/lib/python3.11/site-packages/black/nodes.py:51:    token.LESS,
./.venv-build/lib/python3.11/site-packages/black/nodes.py:52:    token.GREATER,
./.venv-build/lib/python3.11/site-packages/black/nodes.py:53:    token.EQEQUAL,
./.venv-build/lib/python3.11/site-packages/black/nodes.py:54:    token.NOTEQUAL,
./.venv-build/lib/python3.11/site-packages/black/nodes.py:55:    token.LESSEQUAL,
./.venv-build/lib/python3.11/site-packages/black/nodes.py:56:    token.GREATEREQUAL,
./.venv-build/lib/python3.11/site-packages/black/nodes.py:59:    token.VBAR,
./.venv-build/lib/python3.11/site-packages/black/nodes.py:60:    token.CIRCUMFLEX,
./.venv-build/lib/python3.11/site-packages/black/nodes.py:61:    token.AMPER,
./.venv-build/lib/python3.11/site-packages/black/nodes.py:62:    token.LEFTSHIFT,
./.venv-build/lib/python3.11/site-packages/black/nodes.py:63:    token.RIGHTSHIFT,
./.venv-build/lib/python3.11/site-packages/black/nodes.py:64:    token.PLUS,
./.venv-build/lib/python3.11/site-packages/black/nodes.py:65:    token.MINUS,
./.venv-build/lib/python3.11/site-packages/black/nodes.py:66:    token.STAR,
./.venv-build/lib/python3.11/site-packages/black/nodes.py:67:    token.SLASH,
./.venv-build/lib/python3.11/site-packages/black/nodes.py:68:    token.DOUBLESLASH,
./.venv-build/lib/python3.11/site-packages/black/nodes.py:69:    token.PERCENT,
./.venv-build/lib/python3.11/site-packages/black/nodes.py:70:    token.AT,
./.venv-build/lib/python3.11/site-packages/black/nodes.py:71:    token.TILDE,
./.venv-build/lib/python3.11/site-packages/black/nodes.py:72:    token.DOUBLESTAR,
./.venv-build/lib/python3.11/site-packages/black/nodes.py:74:STARS: Final = {token.STAR, token.DOUBLESTAR}
./.venv-build/lib/python3.11/site-packages/black/nodes.py:75:VARARGS_SPECIALS: Final = STARS | {token.SLASH}
./.venv-build/lib/python3.11/site-packages/black/nodes.py:131:    token.LPAR: token.RPAR,
./.venv-build/lib/python3.11/site-packages/black/nodes.py:132:    token.LSQB: token.RSQB,
./.venv-build/lib/python3.11/site-packages/black/nodes.py:133:    token.LBRACE: token.RBRACE,
./.venv-build/lib/python3.11/site-packages/black/nodes.py:139:    token.COMMA,
./.venv-build/lib/python3.11/site-packages/black/nodes.py:141:    token.FSTRING_MIDDLE,
./.venv-build/lib/python3.11/site-packages/black/nodes.py:142:    token.FSTRING_END,
./.venv-build/lib/python3.11/site-packages/black/nodes.py:143:    token.BANG,
./.venv-build/lib/python3.11/site-packages/black/nodes.py:164:            name = token.tok_name[node.type]
./.venv-build/lib/python3.11/site-packages/black/nodes.py:199:    if t == token.COMMENT:
./.venv-build/lib/python3.11/site-packages/black/nodes.py:203:    if t == token.COLON and p.type not in {
./.venv-build/lib/python3.11/site-packages/black/nodes.py:210:    if t == token.LBRACE and p.type == syms.fstring_replacement_field:
./.venv-build/lib/python3.11/site-packages/black/nodes.py:219:        if t == token.COLON:
./.venv-build/lib/python3.11/site-packages/black/nodes.py:220:            if prevp.type == token.COLON:
./.venv-build/lib/python3.11/site-packages/black/nodes.py:223:            elif prevp.type != token.COMMA and not complex_subscript:
./.venv-build/lib/python3.11/site-packages/black/nodes.py:228:        if prevp.type == token.EQUAL:
./.venv-build/lib/python3.11/site-packages/black/nodes.py:245:            prevp.type == token.STAR
./.venv-build/lib/python3.11/site-packages/black/nodes.py:256:        elif prevp.type == token.COLON:
./.venv-build/lib/python3.11/site-packages/black/nodes.py:263:        elif prevp.type == token.AT and p.parent and p.parent.type == syms.decorator:
./.venv-build/lib/python3.11/site-packages/black/nodes.py:270:    elif prev.type == token.BANG:
./.venv-build/lib/python3.11/site-packages/black/nodes.py:275:        if not prev or prev.type != token.COMMA:
./.venv-build/lib/python3.11/site-packages/black/nodes.py:280:        if prev and prev.type != token.COMMA:
./.venv-build/lib/python3.11/site-packages/black/nodes.py:288:        if t == token.EQUAL:
./.venv-build/lib/python3.11/site-packages/black/nodes.py:292:        elif prev.type == token.EQUAL:
./.venv-build/lib/python3.11/site-packages/black/nodes.py:297:        elif prev.type != token.COMMA:
./.venv-build/lib/python3.11/site-packages/black/nodes.py:304:            if not prevp or prevp.type != token.COMMA:
./.venv-build/lib/python3.11/site-packages/black/nodes.py:309:        if t == token.LPAR or t == token.RPAR:
./.venv-build/lib/python3.11/site-packages/black/nodes.py:313:            if t == token.DOT or t == token.LSQB:
./.venv-build/lib/python3.11/site-packages/black/nodes.py:316:        elif prev.type != token.COMMA:
./.venv-build/lib/python3.11/site-packages/black/nodes.py:321:        if t == token.EQUAL:
./.venv-build/lib/python3.11/site-packages/black/nodes.py:326:            if not prevp or prevp.type == token.LPAR:
./.venv-build/lib/python3.11/site-packages/black/nodes.py:329:        elif prev.type in {token.EQUAL} | VARARGS_SPECIALS:
./.venv-build/lib/python3.11/site-packages/black/nodes.py:341:        if not prevp or prevp.type == token.AT or prevp.type == token.DOT:
./.venv-build/lib/python3.11/site-packages/black/nodes.py:345:        if t == token.LPAR:
./.venv-build/lib/python3.11/site-packages/black/nodes.py:348:        if prev and prev.type == token.LPAR:
./.venv-build/lib/python3.11/site-packages/black/nodes.py:360:        elif t == token.COLONEQUAL or prev.type == token.COLONEQUAL:
./.venv-build/lib/python3.11/site-packages/black/nodes.py:367:        if prev and t == token.DOT:
./.venv-build/lib/python3.11/site-packages/black/nodes.py:373:        if prev and prev.type == token.DOUBLESTAR:
./.venv-build/lib/python3.11/site-packages/black/nodes.py:385:            if prevp.type == token.COLON and prevp_parent.type in {
./.venv-build/lib/python3.11/site-packages/black/nodes.py:391:            elif prevp.type == token.EQUAL and prevp_parent.type == syms.argument:
./.venv-build/lib/python3.11/site-packages/black/nodes.py:395:        elif t in {token.NAME, token.NUMBER, token.STRING}:
./.venv-build/lib/python3.11/site-packages/black/nodes.py:399:        if t == token.DOT:
./.venv-build/lib/python3.11/site-packages/black/nodes.py:400:            if prev and prev.type == token.DOT:
./.venv-build/lib/python3.11/site-packages/black/nodes.py:403:        elif t == token.NAME:
./.venv-build/lib/python3.11/site-packages/black/nodes.py:407:            if prev and prev.type == token.DOT:
./.venv-build/lib/python3.11/site-packages/black/nodes.py:414:        if t == token.STAR:
./.venv-build/lib/python3.11/site-packages/black/nodes.py:445:def prev_siblings_are(node: Optional[LN], tokens: list[Optional[NodeType]]) -> bool:
./.venv-build/lib/python3.11/site-packages/black/nodes.py:447:    list of tokens; the provided `node`has its type matched against the last element in
./.venv-build/lib/python3.11/site-packages/black/nodes.py:450:    if not tokens:
./.venv-build/lib/python3.11/site-packages/black/nodes.py:452:    if tokens[-1] is None:
./.venv-build/lib/python3.11/site-packages/black/nodes.py:456:    if node.type != tokens[-1]:
./.venv-build/lib/python3.11/site-packages/black/nodes.py:458:    return prev_siblings_are(node.prev_sibling, tokens[:-1])
./.venv-build/lib/python3.11/site-packages/black/nodes.py:546:        if node.type != token.STRING:
./.venv-build/lib/python3.11/site-packages/black/nodes.py:562:    if prev_siblings_are(node.parent, [None, token.NEWLINE, token.INDENT, syms.simple_stmt]):
./.venv-build/lib/python3.11/site-packages/black/nodes.py:566:    if prev_siblings_are(node.parent, [syms.parameters, token.COLON, syms.simple_stmt]):
./.venv-build/lib/python3.11/site-packages/black/nodes.py:579:        and node.children[0].type == token.LPAR
./.venv-build/lib/python3.11/site-packages/black/nodes.py:580:        and node.children[1].type == token.RPAR
./.venv-build/lib/python3.11/site-packages/black/nodes.py:591:        return len(gexp.children) == 2 and gexp.children[1].type == token.COMMA
./.venv-build/lib/python3.11/site-packages/black/nodes.py:596:        and node.children[1].type == token.COMMA
./.venv-build/lib/python3.11/site-packages/black/nodes.py:648:    brackets: tuple[int, int] = (token.LPAR, token.RPAR),
./.venv-build/lib/python3.11/site-packages/black/nodes.py:669:        if bracket_depth == depth and leaf.type == token.COMMA:
./.venv-build/lib/python3.11/site-packages/black/nodes.py:692:            and node.children[0].type == token.DOT
./.venv-build/lib/python3.11/site-packages/black/nodes.py:693:            and node.children[1].type == token.NAME
./.venv-build/lib/python3.11/site-packages/black/nodes.py:699:            and node.children[0].type == token.LPAR
./.venv-build/lib/python3.11/site-packages/black/nodes.py:700:            and node.children[1].type == token.RPAR
./.venv-build/lib/python3.11/site-packages/black/nodes.py:706:            and node.children[0].type == token.LPAR
./.venv-build/lib/python3.11/site-packages/black/nodes.py:708:            and node.children[2].type == token.RPAR
./.venv-build/lib/python3.11/site-packages/black/nodes.py:722:    if node.type == token.NAME:
./.venv-build/lib/python3.11/site-packages/black/nodes.py:727:                node.children[0].type == token.NAME
./.venv-build/lib/python3.11/site-packages/black/nodes.py:742:    if is_name_token(node) and node.value == "yield":
./.venv-build/lib/python3.11/site-packages/black/nodes.py:752:    if lpar.type == token.LPAR and rpar.type == token.RPAR:
./.venv-build/lib/python3.11/site-packages/black/nodes.py:789:    string_leaf = Leaf(token.STRING, string_without_prefix, prefix=node.prefix)
./.venv-build/lib/python3.11/site-packages/black/nodes.py:828:        or node.children[0].type != token.NEWLINE
./.venv-build/lib/python3.11/site-packages/black/nodes.py:829:        or node.children[1].type != token.INDENT
./.venv-build/lib/python3.11/site-packages/black/nodes.py:830:        or node.children[3].type != token.DEDENT
./.venv-build/lib/python3.11/site-packages/black/nodes.py:853:        and all(leaf == Leaf(token.DOT, ".") for leaf in child.children)
./.venv-build/lib/python3.11/site-packages/black/nodes.py:867:        and first.type == token.LPAR
./.venv-build/lib/python3.11/site-packages/black/nodes.py:870:        and last.type == token.RPAR
./.venv-build/lib/python3.11/site-packages/black/nodes.py:880:    return leaf.type == token.LPAR and leaf.value == ""
./.venv-build/lib/python3.11/site-packages/black/nodes.py:884:    return leaf.type == token.RPAR and leaf.value == ""
./.venv-build/lib/python3.11/site-packages/black/nodes.py:893:        t == token.NAME
./.venv-build/lib/python3.11/site-packages/black/nodes.py:904:        leaf.type == token.NAME
./.venv-build/lib/python3.11/site-packages/black/nodes.py:909:        leaf.type == token.ASYNC and leaf.next_sibling and leaf.next_sibling.type == syms.with_stmt
./.venv-build/lib/python3.11/site-packages/black/nodes.py:920:        leaf.type == token.ASYNC
./.venv-build/lib/python3.11/site-packages/black/nodes.py:933:    return t in {token.COMMENT, STANDALONE_COMMENT} and v.startswith("# type:")
./.venv-build/lib/python3.11/site-packages/black/nodes.py:940:    return t in {token.COMMENT, STANDALONE_COMMENT} and is_type_ignore_comment_string(v)
./.venv-build/lib/python3.11/site-packages/black/nodes.py:957:    lpar = Leaf(token.LPAR, "(" if visible else "")
./.venv-build/lib/python3.11/site-packages/black/nodes.py:958:    rpar = Leaf(token.RPAR, ")" if visible else "")
./.venv-build/lib/python3.11/site-packages/black/nodes.py:975:    if not (lpar.type == token.LPAR and rpar.type == token.RPAR):
./.venv-build/lib/python3.11/site-packages/black/nodes.py:987:    if leaf.type == token.LPAR:
./.venv-build/lib/python3.11/site-packages/black/nodes.py:989:    elif leaf.type == token.RPAR:
./.venv-build/lib/python3.11/site-packages/black/nodes.py:993:def is_name_token(nl: NL) -> TypeGuard[Leaf]:
./.venv-build/lib/python3.11/site-packages/black/nodes.py:994:    return nl.type == token.NAME
./.venv-build/lib/python3.11/site-packages/black/nodes.py:997:def is_lpar_token(nl: NL) -> TypeGuard[Leaf]:
./.venv-build/lib/python3.11/site-packages/black/nodes.py:998:    return nl.type == token.LPAR
./.venv-build/lib/python3.11/site-packages/black/nodes.py:1001:def is_rpar_token(nl: NL) -> TypeGuard[Leaf]:
./.venv-build/lib/python3.11/site-packages/black/nodes.py:1002:    return nl.type == token.RPAR
./.venv-build/lib/python3.11/site-packages/black/nodes.py:1005:def is_number_token(nl: NL) -> TypeGuard[Leaf]:
./.venv-build/lib/python3.11/site-packages/black/nodes.py:1006:    return nl.type == token.NUMBER
./.venv-build/lib/python3.11/site-packages/black/nodes.py:1013:        if ancestor.prev_sibling and ancestor.prev_sibling.type == token.RARROW:
./.venv-build/lib/python3.11/site-packages/black/debug.py:8:from blib2to3.pgen2 import token
./.venv-build/lib/python3.11/site-packages/black/debug.py:38:            _type = token.tok_name.get(node.type, str(node.type))
./.venv-build/lib/python3.11/site-packages/black/strings.py:110:    token.STRING`. A more precise description of the pre-conditions that are
./.venv-build/lib/python3.11/site-packages/black/__init__.py:6:import tokenize
./.venv-build/lib/python3.11/site-packages/black/__init__.py:66:from black.nodes import STARS, is_number_token, is_simple_decorator_expression, syms
./.venv-build/lib/python3.11/site-packages/black/__init__.py:82:from blib2to3.pgen2 import token
./.venv-build/lib/python3.11/site-packages/black/__init__.py:1260:    encoding, lines = tokenize.detect_encoding(srcbuf.readline)
./.venv-build/lib/python3.11/site-packages/black/__init__.py:1317:        if n.type == token.FSTRING_START:
./.venv-build/lib/python3.11/site-packages/black/__init__.py:1320:            n.type == token.RBRACE
./.venv-build/lib/python3.11/site-packages/black/__init__.py:1322:            and any(child.type == token.EQUAL for child in n.parent.children)
./.venv-build/lib/python3.11/site-packages/black/__init__.py:1326:        elif is_number_token(n):
./.venv-build/lib/python3.11/site-packages/black/__init__.py:1330:        elif n.type == token.SLASH:
./.venv-build/lib/python3.11/site-packages/black/__init__.py:1338:        elif n.type == token.COLONEQUAL:
./.venv-build/lib/python3.11/site-packages/black/__init__.py:1348:            and n.children[-1].type == token.COMMA
./.venv-build/lib/python3.11/site-packages/black/__init__.py:1383:                and atom_children[0].type == token.LPAR
./.venv-build/lib/python3.11/site-packages/black/__init__.py:1385:                and atom_children[2].type == token.RPAR
./.venv-build/lib/python3.11/site-packages/black/__init__.py:1409:            and n.children[-2].type == token.EQUAL
./.venv-build/lib/python3.11/site-packages/black/__init__.py:1416:            and (n.children[1].type == token.STAR or n.children[1].type == syms.testlist)
./.venv-build/lib/python3.11/site-packages/black/__init__.py:1418:            is_star_except = n.children[1].type == token.STAR
./.venv-build/lib/python3.11/site-packages/black/__init__.py:1426:                and n.children[is_star_except + 2].type == token.NAME
./.venv-build/lib/python3.11/site-packages/black/__init__.py:1447:            and node.children[0].type == token.LPAR
./.venv-build/lib/python3.11/site-packages/black/__init__.py:1448:            and node.children[2].type == token.RPAR
./.venv-build/lib/python3.11/site-packages/black/__init__.py:1471:                if child.type == token.NAME:
./.venv-build/lib/python3.11/site-packages/black/__init__.py:1477:                assert orig_name.type == token.NAME, "Invalid syntax parsing imports"
./.venv-build/lib/python3.11/site-packages/black/__init__.py:1495:                and first_child.type == token.STRING
./.venv-build/lib/python3.11/site-packages/black/__init__.py:1496:                and child.children[1].type == token.NEWLINE
./.venv-build/lib/python3.11/site-packages/black/parsing.py:16:from blib2to3.pgen2.tokenize import TokenError
./.venv-build/lib/python3.11/site-packages/black/parsing.py:85:        except TokenError as te:
./.venv-build/lib/python3.11/site-packages/black/parsing.py:107:    except (ParseError, TokenError, IndentationError):
./.venv-build/lib/python3.11/site-packages/black/handle_ipynb_magics.py:7:import secrets
./.venv-build/lib/python3.11/site-packages/black/handle_ipynb_magics.py:30:TOKENS_TO_IGNORE = frozenset(
./.venv-build/lib/python3.11/site-packages/black/handle_ipynb_magics.py:62:    installed = find_spec("tokenize_rt") is not None and find_spec("IPython") is not None
./.venv-build/lib/python3.11/site-packages/black/handle_ipynb_magics.py:74:    or non-Python cell magics, which might cause tokenizer_rt to break because of
./.venv-build/lib/python3.11/site-packages/black/handle_ipynb_magics.py:113:    ``tokenize_rt`` so that round-tripping works fine.
./.venv-build/lib/python3.11/site-packages/black/handle_ipynb_magics.py:115:    from tokenize_rt import reversed_enumerate, src_to_tokens, tokens_to_src
./.venv-build/lib/python3.11/site-packages/black/handle_ipynb_magics.py:117:    tokens = src_to_tokens(src)
./.venv-build/lib/python3.11/site-packages/black/handle_ipynb_magics.py:119:    for idx, token in reversed_enumerate(tokens):
./.venv-build/lib/python3.11/site-packages/black/handle_ipynb_magics.py:120:        if token.name in TOKENS_TO_IGNORE:
./.venv-build/lib/python3.11/site-packages/black/handle_ipynb_magics.py:122:        if token.name == "OP" and token.src == ";":
./.venv-build/lib/python3.11/site-packages/black/handle_ipynb_magics.py:123:            del tokens[idx]
./.venv-build/lib/python3.11/site-packages/black/handle_ipynb_magics.py:128:    return tokens_to_src(tokens), True
./.venv-build/lib/python3.11/site-packages/black/handle_ipynb_magics.py:135:    ``tokenize_rt`` so that round-tripping works fine.
./.venv-build/lib/python3.11/site-packages/black/handle_ipynb_magics.py:139:    from tokenize_rt import reversed_enumerate, src_to_tokens, tokens_to_src
./.venv-build/lib/python3.11/site-packages/black/handle_ipynb_magics.py:141:    tokens = src_to_tokens(src)
./.venv-build/lib/python3.11/site-packages/black/handle_ipynb_magics.py:142:    for idx, token in reversed_enumerate(tokens):
./.venv-build/lib/python3.11/site-packages/black/handle_ipynb_magics.py:143:        if token.name in TOKENS_TO_IGNORE:
./.venv-build/lib/python3.11/site-packages/black/handle_ipynb_magics.py:145:        tokens[idx] = token._replace(src=token.src + ";")
./.venv-build/lib/python3.11/site-packages/black/handle_ipynb_magics.py:152:    return str(tokens_to_src(tokens))
./.venv-build/lib/python3.11/site-packages/black/handle_ipynb_magics.py:197:def create_token(n_chars: int) -> str:
./.venv-build/lib/python3.11/site-packages/black/handle_ipynb_magics.py:198:    """Create a randomly generated token that is n_chars characters long."""
./.venv-build/lib/python3.11/site-packages/black/handle_ipynb_magics.py:201:    token = secrets.token_hex(n_bytes)
./.venv-build/lib/python3.11/site-packages/black/handle_ipynb_magics.py:202:    if len(token) + 3 > n_chars:
./.venv-build/lib/python3.11/site-packages/black/handle_ipynb_magics.py:203:        token = token[:-1]
./.venv-build/lib/python3.11/site-packages/black/handle_ipynb_magics.py:206:    return f'b"{token}"'
./.venv-build/lib/python3.11/site-packages/black/handle_ipynb_magics.py:209:def get_token(src: str, magic: str) -> str:
./.venv-build/lib/python3.11/site-packages/black/handle_ipynb_magics.py:210:    """Return randomly generated token to mask IPython magic with.
./.venv-build/lib/python3.11/site-packages/black/handle_ipynb_magics.py:213:    token to mask it with would be `"43fdd17f7e5ddc83"`. The token
./.venv-build/lib/python3.11/site-packages/black/handle_ipynb_magics.py:219:    token = create_token(n_chars)
./.venv-build/lib/python3.11/site-packages/black/handle_ipynb_magics.py:221:    while token in src:
./.venv-build/lib/python3.11/site-packages/black/handle_ipynb_magics.py:222:        token = create_token(n_chars)
./.venv-build/lib/python3.11/site-packages/black/handle_ipynb_magics.py:230:    return token
./.venv-build/lib/python3.11/site-packages/black/handle_ipynb_magics.py:234:    r"""Replace cell magic with token.
./.venv-build/lib/python3.11/site-packages/black/handle_ipynb_magics.py:259:    mask = get_token(src, header)
./.venv-build/lib/python3.11/site-packages/black/handle_ipynb_magics.py:298:            mask = get_token(src, magic)
./.venv-build/lib/python3.11/site-packages/six.py:429:    MovedAttribute("HTTPPasswordMgr", "urllib2", "urllib.request"),
./.venv-build/lib/python3.11/site-packages/six.py:430:    MovedAttribute("HTTPPasswordMgrWithDefaultRealm", "urllib2", "urllib.request"),
./.venv-build/lib/python3.11/site-packages/urllib3/connectionpool.py:966:    ``ca_cert_dir``, ``ssl_version``, ``key_password`` are only used if :mod:`ssl`
./.venv-build/lib/python3.11/site-packages/urllib3/connectionpool.py:988:        key_password: str | None = None,
./.venv-build/lib/python3.11/site-packages/urllib3/connectionpool.py:1014:        self.key_password = key_password
./.venv-build/lib/python3.11/site-packages/urllib3/connectionpool.py:1065:            key_password=self.key_password,
./.venv-build/lib/python3.11/site-packages/urllib3/poolmanager.py:48:    "key_password",
./.venv-build/lib/python3.11/site-packages/urllib3/poolmanager.py:72:    key_key_password: str | None
./.venv-build/lib/python3.11/site-packages/urllib3/connection.py:389:                f"Method cannot contain non-token characters {method!r} (found at least {match.group()!r})"
./.venv-build/lib/python3.11/site-packages/urllib3/connection.py:629:        key_password: str | None = None,
./.venv-build/lib/python3.11/site-packages/urllib3/connection.py:644:        self.key_password = key_password
./.venv-build/lib/python3.11/site-packages/urllib3/connection.py:670:        key_password: str | None = None,
./.venv-build/lib/python3.11/site-packages/urllib3/connection.py:699:        self.key_password = key_password
./.venv-build/lib/python3.11/site-packages/urllib3/connection.py:789:                key_password=self.key_password,
./.venv-build/lib/python3.11/site-packages/urllib3/connection.py:866:            key_password=None,
./.venv-build/lib/python3.11/site-packages/urllib3/connection.py:892:    key_password: str | None,
./.venv-build/lib/python3.11/site-packages/urllib3/connection.py:961:        key_password=key_password,
./.venv-build/lib/python3.11/site-packages/urllib3/util/timeout.py:17:    token = -1
./.venv-build/lib/python3.11/site-packages/urllib3/util/timeout.py:20:_DEFAULT_TIMEOUT: Final[_TYPE_DEFAULT] = _TYPE_DEFAULT.token
./.venv-build/lib/python3.11/site-packages/urllib3/util/request.py:48:    token = 0
./.venv-build/lib/python3.11/site-packages/urllib3/util/request.py:51:_FAILEDTELL: Final[_TYPE_FAILEDTELL] = _TYPE_FAILEDTELL.token
./.venv-build/lib/python3.11/site-packages/urllib3/util/request.py:91:        Colon-separated username:password string for 'authorization: basic ...'
./.venv-build/lib/python3.11/site-packages/urllib3/util/request.py:95:        Colon-separated username:password string for 'proxy-authorization: basic ...'
./.venv-build/lib/python3.11/site-packages/urllib3/util/ssl_.py:386:    key_password: str | None = ...,
./.venv-build/lib/python3.11/site-packages/urllib3/util/ssl_.py:404:    key_password: str | None = ...,
./.venv-build/lib/python3.11/site-packages/urllib3/util/ssl_.py:421:    key_password: str | None = None,
./.venv-build/lib/python3.11/site-packages/urllib3/util/ssl_.py:442:    :param key_password:
./.venv-build/lib/python3.11/site-packages/urllib3/util/ssl_.py:443:        Optional password if the keyfile is encrypted.
./.venv-build/lib/python3.11/site-packages/urllib3/util/ssl_.py:469:    if keyfile and key_password is None and _is_key_file_encrypted(keyfile):
./.venv-build/lib/python3.11/site-packages/urllib3/util/ssl_.py:470:        raise SSLError("Client private key is encrypted, password is required")
./.venv-build/lib/python3.11/site-packages/urllib3/util/ssl_.py:473:        if key_password is None:
./.venv-build/lib/python3.11/site-packages/urllib3/util/ssl_.py:476:            context.load_cert_chain(certfile, keyfile, key_password)
./.venv-build/lib/python3.11/site-packages/urllib3/util/url.py:180:            print( urllib3.util.Url("https", "username:password",
./.venv-build/lib/python3.11/site-packages/urllib3/util/url.py:184:            # "https://username:password@host.com:80/path?query#fragment"
./.venv-build/lib/python3.11/site-packages/urllib3/_base_connection.py:136:        key_password: str | None
./.venv-build/lib/python3.11/site-packages/urllib3/_base_connection.py:162:            key_password: str | None = None,
./.venv-build/lib/python3.11/site-packages/urllib3/contrib/pyopenssl.py:472:        password: str | None = None,
./.venv-build/lib/python3.11/site-packages/urllib3/contrib/pyopenssl.py:476:            if password is not None:
./.venv-build/lib/python3.11/site-packages/urllib3/contrib/pyopenssl.py:477:                if not isinstance(password, bytes):
./.venv-build/lib/python3.11/site-packages/urllib3/contrib/pyopenssl.py:478:                    password = password.encode("utf-8")  # type: ignore[assignment]
./.venv-build/lib/python3.11/site-packages/urllib3/contrib/pyopenssl.py:479:                self._ctx.set_passwd_cb(lambda *_: password)
./.venv-build/lib/python3.11/site-packages/urllib3/contrib/emscripten/connection.py:169:    key_password: str | None
./.venv-build/lib/python3.11/site-packages/urllib3/contrib/emscripten/connection.py:201:        key_password: str | None = None,
./.venv-build/lib/python3.11/site-packages/urllib3/contrib/emscripten/connection.py:217:        self.key_password = key_password
./.venv-build/lib/python3.11/site-packages/urllib3/contrib/emscripten/connection.py:240:        key_password: str | None = None,
./.venv-build/lib/python3.11/site-packages/urllib3/contrib/socks.py:14:- Usernames and passwords for the SOCKS proxy
./.venv-build/lib/python3.11/site-packages/urllib3/contrib/socks.py:31:When connecting to a SOCKS5 proxy the ``username`` and ``password`` portion
./.venv-build/lib/python3.11/site-packages/urllib3/contrib/socks.py:32:of the ``proxy_url`` will be sent as the username/password to authenticate
./.venv-build/lib/python3.11/site-packages/urllib3/contrib/socks.py:37:    proxy_url="socks5h://<username>:<password>@proxy-host"
./.venv-build/lib/python3.11/site-packages/urllib3/contrib/socks.py:80:    password: str | None
./.venv-build/lib/python3.11/site-packages/urllib3/contrib/socks.py:116:                proxy_password=self._socks_options["password"],
./.venv-build/lib/python3.11/site-packages/urllib3/contrib/socks.py:182:        password: str | None = None,
./.venv-build/lib/python3.11/site-packages/urllib3/contrib/socks.py:189:        if username is None and password is None and parsed.auth is not None:
./.venv-build/lib/python3.11/site-packages/urllib3/contrib/socks.py:192:                username, password = split
./.venv-build/lib/python3.11/site-packages/urllib3/contrib/socks.py:215:            "password": password,
./.venv-build/lib/python3.11/site-packages/packaging/markers.py:15:from ._tokenizer import ParserSyntaxError
./.venv-build/lib/python3.11/site-packages/packaging/_tokenizer.py:12:class Token:
./.venv-build/lib/python3.11/site-packages/packaging/_tokenizer.py:91:class Tokenizer:
./.venv-build/lib/python3.11/site-packages/packaging/_tokenizer.py:92:    """Context-sensitive token parsing.
./.venv-build/lib/python3.11/site-packages/packaging/_tokenizer.py:94:    Provides methods to examine the input stream to check whether the next token
./.venv-build/lib/python3.11/site-packages/packaging/_tokenizer.py:108:        self.next_token: Token | None = None
./.venv-build/lib/python3.11/site-packages/packaging/_tokenizer.py:112:        """Move beyond provided token name, if at current position."""
./.venv-build/lib/python3.11/site-packages/packaging/_tokenizer.py:117:        """Check whether the next token has the provided name.
./.venv-build/lib/python3.11/site-packages/packaging/_tokenizer.py:119:        By default, if the check succeeds, the token *must* be read before
./.venv-build/lib/python3.11/site-packages/packaging/_tokenizer.py:120:        another check. If `peek` is set to `True`, the token is not loaded and
./.venv-build/lib/python3.11/site-packages/packaging/_tokenizer.py:124:            self.next_token is None
./.venv-build/lib/python3.11/site-packages/packaging/_tokenizer.py:125:        ), f"Cannot check for {name!r}, already have {self.next_token!r}"
./.venv-build/lib/python3.11/site-packages/packaging/_tokenizer.py:126:        assert name in self.rules, f"Unknown token name: {name!r}"
./.venv-build/lib/python3.11/site-packages/packaging/_tokenizer.py:134:            self.next_token = Token(name, match[0], self.position)
./.venv-build/lib/python3.11/site-packages/packaging/_tokenizer.py:137:    def expect(self, name: str, *, expected: str) -> Token:
./.venv-build/lib/python3.11/site-packages/packaging/_tokenizer.py:138:        """Expect a certain token name next, failing with a syntax error otherwise.
./.venv-build/lib/python3.11/site-packages/packaging/_tokenizer.py:140:        The token is *not* read.
./.venv-build/lib/python3.11/site-packages/packaging/_tokenizer.py:146:    def read(self) -> Token:
./.venv-build/lib/python3.11/site-packages/packaging/_tokenizer.py:147:        """Consume the next token and return it."""
./.venv-build/lib/python3.11/site-packages/packaging/_tokenizer.py:148:        token = self.next_token
./.venv-build/lib/python3.11/site-packages/packaging/_tokenizer.py:149:        assert token is not None
./.venv-build/lib/python3.11/site-packages/packaging/_tokenizer.py:151:        self.position += len(token.text)
./.venv-build/lib/python3.11/site-packages/packaging/_tokenizer.py:152:        self.next_token = None
./.venv-build/lib/python3.11/site-packages/packaging/_tokenizer.py:154:        return token
./.venv-build/lib/python3.11/site-packages/packaging/_tokenizer.py:175:    def enclosing_tokens(self, open_token: str, close_token: str, *, around: str) -> Iterator[None]:
./.venv-build/lib/python3.11/site-packages/packaging/_tokenizer.py:176:        if self.check(open_token):
./.venv-build/lib/python3.11/site-packages/packaging/_tokenizer.py:187:        if not self.check(close_token):
./.venv-build/lib/python3.11/site-packages/packaging/_tokenizer.py:189:                f"Expected matching {close_token} for {open_token}, after {around}",
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:12:from ._tokenizer import DEFAULT_RULES, Tokenizer
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:62:    return _parse_requirement(Tokenizer(source, rules=DEFAULT_RULES))
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:65:def _parse_requirement(tokenizer: Tokenizer) -> ParsedRequirement:
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:69:    tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:71:    name_token = tokenizer.expect(
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:74:    name = name_token.text
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:75:    tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:77:    extras = _parse_extras(tokenizer)
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:78:    tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:80:    url, specifier, marker = _parse_requirement_details(tokenizer)
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:81:    tokenizer.expect("END", expected="end of dependency specifier")
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:87:    tokenizer: Tokenizer,
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:98:    if tokenizer.check("AT"):
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:99:        tokenizer.read()
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:100:        tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:102:        url_start = tokenizer.position
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:103:        url = tokenizer.expect("URL", expected="URL after @").text
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:104:        if tokenizer.check("END", peek=True):
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:107:        tokenizer.expect("WS", expected="whitespace after URL")
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:110:        if tokenizer.check("END", peek=True):
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:114:            tokenizer, span_start=url_start, after="URL and whitespace"
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:117:        specifier_start = tokenizer.position
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:118:        specifier = _parse_specifier(tokenizer)
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:119:        tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:121:        if tokenizer.check("END", peek=True):
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:125:            tokenizer,
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:133:def _parse_requirement_marker(tokenizer: Tokenizer, *, span_start: int, after: str) -> MarkerList:
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:138:    if not tokenizer.check("SEMICOLON"):
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:139:        tokenizer.raise_syntax_error(
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:143:    tokenizer.read()
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:145:    marker = _parse_marker(tokenizer)
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:146:    tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:151:def _parse_extras(tokenizer: Tokenizer) -> list[str]:
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:155:    if not tokenizer.check("LEFT_BRACKET", peek=True):
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:158:    with tokenizer.enclosing_tokens(
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:163:        tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:164:        extras = _parse_extras_list(tokenizer)
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:165:        tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:170:def _parse_extras_list(tokenizer: Tokenizer) -> list[str]:
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:176:    if not tokenizer.check("IDENTIFIER"):
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:179:    extras.append(tokenizer.read().text)
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:182:        tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:183:        if tokenizer.check("IDENTIFIER", peek=True):
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:184:            tokenizer.raise_syntax_error("Expected comma between extra names")
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:185:        elif not tokenizer.check("COMMA"):
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:188:        tokenizer.read()
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:189:        tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:191:        extra_token = tokenizer.expect("IDENTIFIER", expected="extra name after comma")
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:192:        extras.append(extra_token.text)
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:197:def _parse_specifier(tokenizer: Tokenizer) -> str:
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:202:    with tokenizer.enclosing_tokens(
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:207:        tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:208:        parsed_specifiers = _parse_version_many(tokenizer)
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:209:        tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:214:def _parse_version_many(tokenizer: Tokenizer) -> str:
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:219:    while tokenizer.check("SPECIFIER"):
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:220:        span_start = tokenizer.position
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:221:        parsed_specifiers += tokenizer.read().text
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:222:        if tokenizer.check("VERSION_PREFIX_TRAIL", peek=True):
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:223:            tokenizer.raise_syntax_error(
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:226:                span_end=tokenizer.position + 1,
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:228:        if tokenizer.check("VERSION_LOCAL_LABEL_TRAIL", peek=True):
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:229:            tokenizer.raise_syntax_error(
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:232:                span_end=tokenizer.position,
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:234:        tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:235:        if not tokenizer.check("COMMA"):
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:237:        parsed_specifiers += tokenizer.read().text
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:238:        tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:247:    return _parse_full_marker(Tokenizer(source, rules=DEFAULT_RULES))
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:250:def _parse_full_marker(tokenizer: Tokenizer) -> MarkerList:
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:251:    retval = _parse_marker(tokenizer)
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:252:    tokenizer.expect("END", expected="end of marker expression")
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:256:def _parse_marker(tokenizer: Tokenizer) -> MarkerList:
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:260:    expression = [_parse_marker_atom(tokenizer)]
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:261:    while tokenizer.check("BOOLOP"):
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:262:        token = tokenizer.read()
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:263:        expr_right = _parse_marker_atom(tokenizer)
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:264:        expression.extend((token.text, expr_right))
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:268:def _parse_marker_atom(tokenizer: Tokenizer) -> MarkerAtom:
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:274:    tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:275:    if tokenizer.check("LEFT_PARENTHESIS", peek=True):
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:276:        with tokenizer.enclosing_tokens(
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:281:            tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:282:            marker: MarkerAtom = _parse_marker(tokenizer)
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:283:            tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:285:        marker = _parse_marker_item(tokenizer)
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:286:    tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:290:def _parse_marker_item(tokenizer: Tokenizer) -> MarkerItem:
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:294:    tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:295:    marker_var_left = _parse_marker_var(tokenizer)
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:296:    tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:297:    marker_op = _parse_marker_op(tokenizer)
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:298:    tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:299:    marker_var_right = _parse_marker_var(tokenizer)
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:300:    tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:304:def _parse_marker_var(tokenizer: Tokenizer) -> MarkerVar:
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:308:    if tokenizer.check("VARIABLE"):
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:309:        return process_env_var(tokenizer.read().text.replace(".", "_"))
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:310:    elif tokenizer.check("QUOTED_STRING"):
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:311:        return process_python_str(tokenizer.read().text)
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:313:        tokenizer.raise_syntax_error(message="Expected a marker variable or quoted string")
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:328:def _parse_marker_op(tokenizer: Tokenizer) -> Op:
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:332:    if tokenizer.check("IN"):
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:333:        tokenizer.read()
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:335:    elif tokenizer.check("NOT"):
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:336:        tokenizer.read()
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:337:        tokenizer.expect("WS", expected="whitespace after 'not'")
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:338:        tokenizer.expect("IN", expected="'in' after 'not'")
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:340:    elif tokenizer.check("OP"):
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:341:        return Op(tokenizer.read().text)
./.venv-build/lib/python3.11/site-packages/packaging/_parser.py:343:        return tokenizer.raise_syntax_error(
./.venv-build/lib/python3.11/site-packages/packaging/licenses/__init__.py:67:    # Pad any parentheses so tokenization can be achieved by merely splitting on
./.venv-build/lib/python3.11/site-packages/packaging/licenses/__init__.py:81:    tokens = license_expression.split()
./.venv-build/lib/python3.11/site-packages/packaging/licenses/__init__.py:86:    python_tokens = []
./.venv-build/lib/python3.11/site-packages/packaging/licenses/__init__.py:87:    for token in tokens:
./.venv-build/lib/python3.11/site-packages/packaging/licenses/__init__.py:88:        if token not in {"or", "and", "with", "(", ")"}:
./.venv-build/lib/python3.11/site-packages/packaging/licenses/__init__.py:89:            python_tokens.append("False")
./.venv-build/lib/python3.11/site-packages/packaging/licenses/__init__.py:90:        elif token == "with":
./.venv-build/lib/python3.11/site-packages/packaging/licenses/__init__.py:91:            python_tokens.append("or")
./.venv-build/lib/python3.11/site-packages/packaging/licenses/__init__.py:92:        elif token == "(" and python_tokens and python_tokens[-1] not in {"or", "and"}:
./.venv-build/lib/python3.11/site-packages/packaging/licenses/__init__.py:96:            python_tokens.append(token)
./.venv-build/lib/python3.11/site-packages/packaging/licenses/__init__.py:98:    python_expression = " ".join(python_tokens)
./.venv-build/lib/python3.11/site-packages/packaging/licenses/__init__.py:109:    normalized_tokens = []
./.venv-build/lib/python3.11/site-packages/packaging/licenses/__init__.py:110:    for token in tokens:
./.venv-build/lib/python3.11/site-packages/packaging/licenses/__init__.py:111:        if token in {"or", "and", "with", "(", ")"}:
./.venv-build/lib/python3.11/site-packages/packaging/licenses/__init__.py:112:            normalized_tokens.append(token.upper())
./.venv-build/lib/python3.11/site-packages/packaging/licenses/__init__.py:115:        if normalized_tokens and normalized_tokens[-1] == "WITH":
./.venv-build/lib/python3.11/site-packages/packaging/licenses/__init__.py:116:            if token not in EXCEPTIONS:
./.venv-build/lib/python3.11/site-packages/packaging/licenses/__init__.py:117:                message = f"Unknown license exception: {token!r}"
./.venv-build/lib/python3.11/site-packages/packaging/licenses/__init__.py:120:            normalized_tokens.append(EXCEPTIONS[token]["id"])
./.venv-build/lib/python3.11/site-packages/packaging/licenses/__init__.py:122:            if token.endswith("+"):
./.venv-build/lib/python3.11/site-packages/packaging/licenses/__init__.py:123:                final_token = token[:-1]
./.venv-build/lib/python3.11/site-packages/packaging/licenses/__init__.py:126:                final_token = token
./.venv-build/lib/python3.11/site-packages/packaging/licenses/__init__.py:129:            if final_token.startswith("licenseref-"):
./.venv-build/lib/python3.11/site-packages/packaging/licenses/__init__.py:130:                if not license_ref_allowed.match(final_token):
./.venv-build/lib/python3.11/site-packages/packaging/licenses/__init__.py:131:                    message = f"Invalid licenseref: {final_token!r}"
./.venv-build/lib/python3.11/site-packages/packaging/licenses/__init__.py:133:                normalized_tokens.append(license_refs[final_token] + suffix)
./.venv-build/lib/python3.11/site-packages/packaging/licenses/__init__.py:135:                if final_token not in LICENSES:
./.venv-build/lib/python3.11/site-packages/packaging/licenses/__init__.py:136:                    message = f"Unknown license: {final_token!r}"
./.venv-build/lib/python3.11/site-packages/packaging/licenses/__init__.py:138:                normalized_tokens.append(LICENSES[final_token]["id"] + suffix)
./.venv-build/lib/python3.11/site-packages/packaging/licenses/__init__.py:140:    normalized_expression = " ".join(normalized_tokens)
./.venv-build/lib/python3.11/site-packages/packaging/requirements.py:9:from ._tokenizer import ParserSyntaxError
./.venv-build/lib/python3.11/site-packages/iniconfig/_parse.py:37:    tokens = parse_lines(
./.venv-build/lib/python3.11/site-packages/iniconfig/_parse.py:47:    for lineno, section, name, value in tokens:
./.venv-build/lib/python3.11/site-packages/mypyc/test/testutil.py:245:            # get 'WORDSIZE*n' token
./.venv-build/lib/python3.11/site-packages/mypyc/test/testutil.py:246:            word_size_token = line[index:].split()[0]
./.venv-build/lib/python3.11/site-packages/mypyc/test/testutil.py:247:            n = int(word_size_token[10:])
./.venv-build/lib/python3.11/site-packages/mypyc/test/testutil.py:249:            result.append(line.replace(word_size_token, replace_str))
./.venv-build/lib/python3.11/site-packages/mypyc/irbuild/specialize.py:82:from mypyc.irbuild.format_str_tokenizer import (
./.venv-build/lib/python3.11/site-packages/mypyc/irbuild/specialize.py:86:    tokenizer_format_call,
./.venv-build/lib/python3.11/site-packages/mypyc/irbuild/specialize.py:664:        tokens = tokenizer_format_call(format_str)
./.venv-build/lib/python3.11/site-packages/mypyc/irbuild/specialize.py:665:        if tokens is None:
./.venv-build/lib/python3.11/site-packages/mypyc/irbuild/specialize.py:667:        literals, format_ops = tokens
./.venv-build/lib/python3.11/site-packages/mypyc/irbuild/format_str_tokenizer.py:1:"""Tokenizers for three string formatting methods"""
./.venv-build/lib/python3.11/site-packages/mypyc/irbuild/format_str_tokenizer.py:69:def tokenizer_printf_style(format_str: str) -> tuple[list[str], list[FormatOp]] | None:
./.venv-build/lib/python3.11/site-packages/mypyc/irbuild/format_str_tokenizer.py:70:    """Tokenize a printf-style format string using regex.
./.venv-build/lib/python3.11/site-packages/mypyc/irbuild/format_str_tokenizer.py:96:def tokenizer_format_call(format_str: str) -> tuple[list[str], list[FormatOp]] | None:
./.venv-build/lib/python3.11/site-packages/mypyc/irbuild/format_str_tokenizer.py:97:    """Tokenize a str.format() format string.
./.venv-build/lib/python3.11/site-packages/mypyc/irbuild/expression.py:94:from mypyc.irbuild.format_str_tokenizer import (
./.venv-build/lib/python3.11/site-packages/mypyc/irbuild/expression.py:99:    tokenizer_printf_style,
./.venv-build/lib/python3.11/site-packages/mypyc/irbuild/expression.py:918:    tokens = tokenizer_printf_style(format_expr.value)
./.venv-build/lib/python3.11/site-packages/mypyc/irbuild/expression.py:919:    if tokens is not None:
./.venv-build/lib/python3.11/site-packages/mypyc/irbuild/expression.py:920:        literals, format_ops = tokens
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:64:from .tokens import *
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:131:        token = self.get_token()
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:132:        event = StreamStartEvent(token.start_mark, token.end_mark, encoding=token.encoding)
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:142:        if not self.check_token(DirectiveToken, DocumentStartToken, StreamEndToken):
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:144:            token = self.peek_token()
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:145:            start_mark = end_mark = token.start_mark
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:160:        while self.check_token(DocumentEndToken):
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:161:            self.get_token()
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:164:        if not self.check_token(StreamEndToken):
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:165:            token = self.peek_token()
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:166:            start_mark = token.start_mark
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:168:            if not self.check_token(DocumentStartToken):
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:172:                    "expected '<document start>', but found %r" % self.peek_token().id,
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:173:                    self.peek_token().start_mark,
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:175:            token = self.get_token()
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:176:            end_mark = token.end_mark
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:184:            token = self.get_token()
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:185:            event = StreamEndEvent(token.start_mark, token.end_mark)
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:194:        token = self.peek_token()
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:195:        start_mark = end_mark = token.start_mark
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:197:        if self.check_token(DocumentEndToken):
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:198:            token = self.get_token()
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:199:            end_mark = token.end_mark
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:209:        if self.check_token(DirectiveToken, DocumentStartToken, DocumentEndToken, StreamEndToken):
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:210:            event = self.process_empty_scalar(self.peek_token().start_mark)
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:219:        while self.check_token(DirectiveToken):
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:220:            token = self.get_token()
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:221:            if token.name == "YAML":
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:224:                        None, None, "found duplicate YAML directive", token.start_mark
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:226:                major, minor = token.value
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:232:                        token.start_mark,
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:234:                self.yaml_version = token.value
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:235:            elif token.name == "TAG":
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:236:                handle, prefix = token.value
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:239:                        None, None, "duplicate tag handle %r" % handle, token.start_mark
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:277:        if self.check_token(AliasToken):
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:278:            token = self.get_token()
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:279:            event = AliasEvent(token.value, token.start_mark, token.end_mark)
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:285:            if self.check_token(AnchorToken):
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:286:                token = self.get_token()
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:287:                start_mark = token.start_mark
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:288:                end_mark = token.end_mark
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:289:                anchor = token.value
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:290:                if self.check_token(TagToken):
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:291:                    token = self.get_token()
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:292:                    tag_mark = token.start_mark
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:293:                    end_mark = token.end_mark
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:294:                    tag = token.value
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:295:            elif self.check_token(TagToken):
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:296:                token = self.get_token()
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:297:                start_mark = tag_mark = token.start_mark
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:298:                end_mark = token.end_mark
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:299:                tag = token.value
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:300:                if self.check_token(AnchorToken):
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:301:                    token = self.get_token()
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:302:                    end_mark = token.end_mark
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:303:                    anchor = token.value
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:322:                start_mark = end_mark = self.peek_token().start_mark
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:325:            if indentless_sequence and self.check_token(BlockEntryToken):
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:326:                end_mark = self.peek_token().end_mark
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:330:                if self.check_token(ScalarToken):
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:331:                    token = self.get_token()
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:332:                    end_mark = token.end_mark
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:333:                    if (token.plain and tag is None) or tag == "!":
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:340:                        anchor, tag, implicit, token.value, start_mark, end_mark, style=token.style
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:343:                elif self.check_token(FlowSequenceStartToken):
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:344:                    end_mark = self.peek_token().end_mark
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:349:                elif self.check_token(FlowMappingStartToken):
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:350:                    end_mark = self.peek_token().end_mark
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:355:                elif block and self.check_token(BlockSequenceStartToken):
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:356:                    end_mark = self.peek_token().start_mark
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:361:                elif block and self.check_token(BlockMappingStartToken):
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:362:                    end_mark = self.peek_token().start_mark
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:377:                    token = self.peek_token()
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:381:                        "expected the node content, but found %r" % token.id,
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:382:                        token.start_mark,
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:389:        token = self.get_token()
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:390:        self.marks.append(token.start_mark)
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:394:        if self.check_token(BlockEntryToken):
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:395:            token = self.get_token()
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:396:            if not self.check_token(BlockEntryToken, BlockEndToken):
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:401:                return self.process_empty_scalar(token.end_mark)
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:402:        if not self.check_token(BlockEndToken):
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:403:            token = self.peek_token()
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:407:                "expected <block end>, but found %r" % token.id,
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:408:                token.start_mark,
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:410:        token = self.get_token()
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:411:        event = SequenceEndEvent(token.start_mark, token.end_mark)
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:419:        if self.check_token(BlockEntryToken):
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:420:            token = self.get_token()
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:421:            if not self.check_token(BlockEntryToken, KeyToken, ValueToken, BlockEndToken):
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:426:                return self.process_empty_scalar(token.end_mark)
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:427:        token = self.peek_token()
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:428:        event = SequenceEndEvent(token.start_mark, token.start_mark)
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:438:        token = self.get_token()
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:439:        self.marks.append(token.start_mark)
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:443:        if self.check_token(KeyToken):
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:444:            token = self.get_token()
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:445:            if not self.check_token(KeyToken, ValueToken, BlockEndToken):
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:450:                return self.process_empty_scalar(token.end_mark)
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:451:        if not self.check_token(BlockEndToken):
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:452:            token = self.peek_token()
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:456:                "expected <block end>, but found %r" % token.id,
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:457:                token.start_mark,
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:459:        token = self.get_token()
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:460:        event = MappingEndEvent(token.start_mark, token.end_mark)
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:466:        if self.check_token(ValueToken):
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:467:            token = self.get_token()
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:468:            if not self.check_token(KeyToken, ValueToken, BlockEndToken):
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:473:                return self.process_empty_scalar(token.end_mark)
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:476:            token = self.peek_token()
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:477:            return self.process_empty_scalar(token.start_mark)
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:491:        token = self.get_token()
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:492:        self.marks.append(token.start_mark)
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:496:        if not self.check_token(FlowSequenceEndToken):
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:498:                if self.check_token(FlowEntryToken):
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:499:                    self.get_token()
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:501:                    token = self.peek_token()
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:505:                        "expected ',' or ']', but got %r" % token.id,
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:506:                        token.start_mark,
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:509:            if self.check_token(KeyToken):
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:510:                token = self.peek_token()
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:512:                    None, None, True, token.start_mark, token.end_mark, flow_style=True
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:516:            elif not self.check_token(FlowSequenceEndToken):
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:519:        token = self.get_token()
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:520:        event = SequenceEndEvent(token.start_mark, token.end_mark)
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:526:        token = self.get_token()
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:527:        if not self.check_token(ValueToken, FlowEntryToken, FlowSequenceEndToken):
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:532:            return self.process_empty_scalar(token.end_mark)
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:535:        if self.check_token(ValueToken):
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:536:            token = self.get_token()
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:537:            if not self.check_token(FlowEntryToken, FlowSequenceEndToken):
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:542:                return self.process_empty_scalar(token.end_mark)
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:545:            token = self.peek_token()
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:546:            return self.process_empty_scalar(token.start_mark)
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:550:        token = self.peek_token()
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:551:        return MappingEndEvent(token.start_mark, token.start_mark)
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:560:        token = self.get_token()
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:561:        self.marks.append(token.start_mark)
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:565:        if not self.check_token(FlowMappingEndToken):
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:567:                if self.check_token(FlowEntryToken):
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:568:                    self.get_token()
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:570:                    token = self.peek_token()
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:574:                        "expected ',' or '}', but got %r" % token.id,
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:575:                        token.start_mark,
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:577:            if self.check_token(KeyToken):
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:578:                token = self.get_token()
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:579:                if not self.check_token(ValueToken, FlowEntryToken, FlowMappingEndToken):
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:584:                    return self.process_empty_scalar(token.end_mark)
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:585:            elif not self.check_token(FlowMappingEndToken):
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:588:        token = self.get_token()
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:589:        event = MappingEndEvent(token.start_mark, token.end_mark)
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:595:        if self.check_token(ValueToken):
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:596:            token = self.get_token()
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:597:            if not self.check_token(FlowEntryToken, FlowMappingEndToken):
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:602:                return self.process_empty_scalar(token.end_mark)
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:605:            token = self.peek_token()
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:606:            return self.process_empty_scalar(token.start_mark)
./.venv-build/lib/python3.11/site-packages/yaml/parser.py:610:        return self.process_empty_scalar(self.peek_token().start_mark)
./.venv-build/lib/python3.11/site-packages/yaml/tokens.py:1:class Token(object):
./.venv-build/lib/python3.11/site-packages/yaml/tokens.py:13:# class BOMToken(Token):
./.venv-build/lib/python3.11/site-packages/yaml/tokens.py:17:class DirectiveToken(Token):
./.venv-build/lib/python3.11/site-packages/yaml/tokens.py:27:class DocumentStartToken(Token):
./.venv-build/lib/python3.11/site-packages/yaml/tokens.py:31:class DocumentEndToken(Token):
./.venv-build/lib/python3.11/site-packages/yaml/tokens.py:35:class StreamStartToken(Token):
./.venv-build/lib/python3.11/site-packages/yaml/tokens.py:44:class StreamEndToken(Token):
./.venv-build/lib/python3.11/site-packages/yaml/tokens.py:48:class BlockSequenceStartToken(Token):
./.venv-build/lib/python3.11/site-packages/yaml/tokens.py:52:class BlockMappingStartToken(Token):
./.venv-build/lib/python3.11/site-packages/yaml/tokens.py:56:class BlockEndToken(Token):
./.venv-build/lib/python3.11/site-packages/yaml/tokens.py:60:class FlowSequenceStartToken(Token):
./.venv-build/lib/python3.11/site-packages/yaml/tokens.py:64:class FlowMappingStartToken(Token):
./.venv-build/lib/python3.11/site-packages/yaml/tokens.py:68:class FlowSequenceEndToken(Token):
./.venv-build/lib/python3.11/site-packages/yaml/tokens.py:72:class FlowMappingEndToken(Token):
./.venv-build/lib/python3.11/site-packages/yaml/tokens.py:76:class KeyToken(Token):
./.venv-build/lib/python3.11/site-packages/yaml/tokens.py:80:class ValueToken(Token):
./.venv-build/lib/python3.11/site-packages/yaml/tokens.py:84:class BlockEntryToken(Token):
./.venv-build/lib/python3.11/site-packages/yaml/tokens.py:88:class FlowEntryToken(Token):
./.venv-build/lib/python3.11/site-packages/yaml/tokens.py:92:class AliasToken(Token):
./.venv-build/lib/python3.11/site-packages/yaml/tokens.py:101:class AnchorToken(Token):
./.venv-build/lib/python3.11/site-packages/yaml/tokens.py:110:class TagToken(Token):
./.venv-build/lib/python3.11/site-packages/yaml/tokens.py:119:class ScalarToken(Token):
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:1:# Scanner produces tokens of the following types:
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:29:from .tokens import *
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:39:    def __init__(self, token_number, required, index, line, column, mark):
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:40:        self.token_number = token_number
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:68:        # List of processed tokens that are not yet emitted.
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:69:        self.tokens = []
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:71:        # Add the STREAM-START token.
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:74:        # Number of tokens that were emitted through the `get_token` method.
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:75:        self.tokens_taken = 0
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:91:        # We emit the KEY token before all keys, so when we find a potential
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:108:        #   (token_number, required, index, line, column, mark)
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:110:        # '[', or '{' tokens.
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:115:    def check_token(self, *choices):
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:116:        # Check if the next token is one of the given types.
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:117:        while self.need_more_tokens():
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:118:            self.fetch_more_tokens()
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:119:        if self.tokens:
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:123:                if isinstance(self.tokens[0], choice):
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:127:    def peek_token(self):
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:128:        # Return the next token, but do not delete if from the queue.
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:129:        # Return None if no more tokens.
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:130:        while self.need_more_tokens():
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:131:            self.fetch_more_tokens()
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:132:        if self.tokens:
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:133:            return self.tokens[0]
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:137:    def get_token(self):
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:138:        # Return the next token.
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:139:        while self.need_more_tokens():
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:140:            self.fetch_more_tokens()
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:141:        if self.tokens:
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:142:            self.tokens_taken += 1
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:143:            return self.tokens.pop(0)
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:147:    def need_more_tokens(self):
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:150:        if not self.tokens:
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:152:        # The current token may be a potential simple key, so we
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:155:        if self.next_possible_simple_key() == self.tokens_taken:
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:158:    def fetch_more_tokens(self):
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:160:        # Eat whitespaces and comments until we reach the next token.
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:161:        self.scan_to_next_token()
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:166:        # Compare the current indentation and column. It may add some tokens
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:191:        #    return self.fetch_bom()    <-- issue BOMToken
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:261:            "while scanning for the next token",
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:263:            "found character %r that cannot start any token" % ch,
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:276:        #           min(self.possible_simple_keys.keys())].token_number
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:277:        min_token_number = None
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:280:            if min_token_number is None or key.token_number < min_token_number:
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:281:                min_token_number = key.token_number
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:282:        return min_token_number
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:304:        # The next token may start a simple key. We check if it's possible
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:311:        # The next token might be a simple key. Let's save it's number and
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:315:            token_number = self.tokens_taken + len(self.tokens)
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:317:                token_number, required, self.index, self.line, self.column, self.get_mark()
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:340:        ## In flow context, tokens should respect indentation.
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:356:        # In block context, we may need to issue the BLOCK-END tokens.
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:360:            self.tokens.append(BlockEndToken(mark, mark))
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:373:        # We always add STREAM-START as the first token and STREAM-END as the
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:374:        # last token.
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:376:        # Read the token.
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:380:        self.tokens.append(StreamStartToken(mark, mark, encoding=self.encoding))
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:392:        # Read the token.
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:396:        self.tokens.append(StreamEndToken(mark, mark))
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:411:        self.tokens.append(self.scan_directive())
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:414:        self.fetch_document_indicator(DocumentStartToken)
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:417:        self.fetch_document_indicator(DocumentEndToken)
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:419:    def fetch_document_indicator(self, TokenClass):
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:433:        self.tokens.append(TokenClass(start_mark, end_mark))
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:436:        self.fetch_flow_collection_start(FlowSequenceStartToken)
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:439:        self.fetch_flow_collection_start(FlowMappingStartToken)
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:441:    def fetch_flow_collection_start(self, TokenClass):
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:456:        self.tokens.append(TokenClass(start_mark, end_mark))
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:459:        self.fetch_flow_collection_end(FlowSequenceEndToken)
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:462:        self.fetch_flow_collection_end(FlowMappingEndToken)
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:464:    def fetch_flow_collection_end(self, TokenClass):
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:479:        self.tokens.append(TokenClass(start_mark, end_mark))
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:493:        self.tokens.append(FlowEntryToken(start_mark, end_mark))
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:509:                self.tokens.append(BlockSequenceStartToken(mark, mark))
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:526:        self.tokens.append(BlockEntryToken(start_mark, end_mark))
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:540:                self.tokens.append(BlockMappingStartToken(mark, mark))
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:552:        self.tokens.append(KeyToken(start_mark, end_mark))
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:562:            self.tokens.insert(key.token_number - self.tokens_taken, KeyToken(key.mark, key.mark))
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:568:                    self.tokens.insert(
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:569:                        key.token_number - self.tokens_taken,
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:570:                        BlockMappingStartToken(key.mark, key.mark),
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:597:                    self.tokens.append(BlockMappingStartToken(mark, mark))
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:609:        self.tokens.append(ValueToken(start_mark, end_mark))
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:620:        self.tokens.append(self.scan_anchor(AliasToken))
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:631:        self.tokens.append(self.scan_anchor(AnchorToken))
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:642:        self.tokens.append(self.scan_tag())
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:659:        self.tokens.append(self.scan_block_scalar(style))
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:676:        self.tokens.append(self.scan_flow_scalar(style))
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:689:        self.tokens.append(self.scan_plain())
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:761:    def scan_to_next_token(self):
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:771:        #   Tabs cannot precede tokens
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:813:        return DirectiveToken(name, value, start_mark, end_mark)
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:934:    def scan_anchor(self, TokenClass):
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:973:        return TokenClass(value, start_mark, end_mark)
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:1021:        return TagToken(value, start_mark, end_mark)
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:1095:        return ScalarToken("".join(chunks), False, start_mark, end_mark, style)
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:1212:        return ScalarToken("".join(chunks), False, start_mark, end_mark, style)
./.venv-build/lib/python3.11/site-packages/yaml/scanner.py:1383:        return ScalarToken("".join(chunks), True, start_mark, end_mark)
./.venv-build/lib/python3.11/site-packages/yaml/__init__.py:3:from .tokens import *
./.venv-build/lib/python3.11/site-packages/yaml/__init__.py:33:    Scan a YAML stream and produce scanning tokens.
./.venv-build/lib/python3.11/site-packages/yaml/__init__.py:37:        while loader.check_token():
./.venv-build/lib/python3.11/site-packages/yaml/__init__.py:38:            yield loader.get_token()
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/logging.py:275:    log.warning("password was rejected for admin site.")
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/containers.py:146:                tokens: List[Text] = []
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/containers.py:148:                    tokens.append(word)
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/containers.py:153:                        tokens.append(Text(" " * spaces[index], style=space_style))
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/containers.py:154:                self[line_index] = Text("").join(tokens)
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/console.py:2102:        password: bool = False,
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/console.py:2113:            password: (bool, optional): Hide typed text. Defaults to False.
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/console.py:2121:        if password:
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/syntax.py:27:from pip._vendor.pygments.token import (
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/syntax.py:36:    Token,
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/syntax.py:54:TokenType = Tuple[str, ...]
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/syntax.py:62:ANSI_LIGHT: Dict[TokenType, Style] = {
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/syntax.py:63:    Token: Style(),
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/syntax.py:91:ANSI_DARK: Dict[TokenType, Style] = {
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/syntax.py:92:    Token: Style(),
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/syntax.py:128:    def get_style_for_token(self, token_type: TokenType) -> Style:
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/syntax.py:129:        """Get a style for a given Pygments token."""
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/syntax.py:142:        self._style_cache: Dict[TokenType, Style] = {}
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/syntax.py:154:    def get_style_for_token(self, token_type: TokenType) -> Style:
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/syntax.py:157:            return self._style_cache[token_type]
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/syntax.py:160:                pygments_style = self._pygments_style_class.style_for_token(token_type)
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/syntax.py:173:            self._style_cache[token_type] = style
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/syntax.py:183:    def __init__(self, style_map: Dict[TokenType, Style]) -> None:
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/syntax.py:187:        self._style_cache: Dict[TokenType, Style] = {}
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/syntax.py:189:    def get_style_for_token(self, token_type: TokenType) -> Style:
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/syntax.py:192:            return self._style_cache[token_type]
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/syntax.py:198:            token = tuple(token_type)
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/syntax.py:200:            while token:
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/syntax.py:201:                _style = get_style(token)
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/syntax.py:205:                token = token[:-1]
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/syntax.py:206:            self._style_cache[token_type] = style
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/syntax.py:424:    def _get_token_color(self, token_type: TokenType) -> Optional[Color]:
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/syntax.py:425:        """Get a color (if any) for the given token.
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/syntax.py:428:            token_type (TokenType): A token type tuple from Pygments.
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/syntax.py:433:        style = self._theme.get_style_for_token(token_type)
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/syntax.py:489:        _get_theme_style = self._theme.get_style_for_token
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/syntax.py:501:                def line_tokenize() -> Iterable[Tuple[Any, str]]:
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/syntax.py:502:                    """Split tokens to one per line."""
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/syntax.py:505:                    for token_type, token in lexer.get_tokens(code):
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/syntax.py:506:                        while token:
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/syntax.py:507:                            line_token, new_line, token = token.partition("\n")
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/syntax.py:508:                            yield token_type, line_token + new_line
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/syntax.py:510:                def tokens_to_spans() -> Iterable[Tuple[str, Optional[Style]]]:
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/syntax.py:511:                    """Convert tokens to spans."""
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/syntax.py:512:                    tokens = iter(line_tokenize())
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/syntax.py:516:                    # Skip over tokens until line start
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/syntax.py:519:                            _token_type, token = next(tokens)
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/syntax.py:522:                        yield (token, None)
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/syntax.py:523:                        if token.endswith("\n"):
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/syntax.py:526:                    for token_type, token in tokens:
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/syntax.py:527:                        yield (token, _get_theme_style(token_type))
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/syntax.py:528:                        if token.endswith("\n"):
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/syntax.py:533:                text.append_tokens(tokens_to_spans())
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/syntax.py:536:                text.append_tokens(
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/syntax.py:537:                    (token, _get_theme_style(token_type))
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/syntax.py:538:                    for token_type, token in lexer.get_tokens(code)
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/syntax.py:572:        foreground_color = self._get_token_color(Token.Text)
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/syntax.py:600:                self._theme.get_style_for_token(Token.Text),
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/syntax.py:606:                self._theme.get_style_for_token(Token.Text),
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/syntax.py:670:                + self._theme.get_style_for_token(Comment)
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/syntax.py:704:                + self._theme.get_style_for_token(Comment)
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/prompt.py:37:        password (bool, optional): Enable password input. Defaults to False.
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/prompt.py:57:        password: bool = False,
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/prompt.py:67:        self.password = password
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/prompt.py:81:        password: bool = False,
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/prompt.py:97:        password: bool = False,
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/prompt.py:111:        password: bool = False,
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/prompt.py:127:            password (bool, optional): Enable password input. Defaults to False.
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/prompt.py:137:            password=password,
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/prompt.py:188:        password: bool,
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/prompt.py:196:            password (bool): Enable password entry.
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/prompt.py:201:        return console.input(prompt, password=password, stream=stream)
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/prompt.py:278:            value = self.get_input(self.console, prompt, self.password, stream=stream)
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/prompt.py:366:            password = Prompt.ask(
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/prompt.py:367:                "Please enter a password [cyan](must be at least 5 characters)",
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/prompt.py:368:                password=True,
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/prompt.py:370:            if len(password) >= 5:
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/prompt.py:372:            print("[prompt.invalid]password too short")
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/prompt.py:373:        print(f"password={password!r}")
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/_emoji_codes.py:156:    "japanese_secret_button": "ãŠ™",
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/_emoji_codes.py:2882:    "secret": "ãŠ™",
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/text.py:1013:    def append_tokens(self, tokens: Iterable[Tuple[str, Optional[StyleType]]]) -> "Text":
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/text.py:1017:            tokens (Iterable[Tuple[str, Optional[StyleType]]]): An iterable of tuples containing str content and style.
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/text.py:1026:        for content, style in tokens:
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/ansi.py:20:class _AnsiToken(NamedTuple):
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/ansi.py:21:    """Result of ansi tokenized string."""
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/ansi.py:28:def _ansi_tokenize(ansi_text: str) -> Iterable[_AnsiToken]:
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/ansi.py:29:    """Tokenize a string in to plain text and ANSI codes.
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/ansi.py:35:        AnsiToken: A named tuple of (plain, sgr, osc)
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/ansi.py:45:            yield _AnsiToken(ansi_text[position:start])
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/ansi.py:51:                yield _AnsiToken("", sgr[1:-1], osc)
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/ansi.py:53:            yield _AnsiToken("", sgr, osc)
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/ansi.py:56:        yield _AnsiToken(ansi_text[position:])
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/ansi.py:153:        for plain_text, sgr, osc in _ansi_tokenize(line):
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/pretty.py:414:    def iter_tokens(self) -> Iterable[str]:
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/pretty.py:415:        """Generate tokens for this node."""
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/pretty.py:425:                    yield from self.children[0].iter_tokens()
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/pretty.py:429:                        yield from child.iter_tokens()
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/pretty.py:447:        for token in self.iter_tokens():
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/pretty.py:448:            total_length += cell_len(token)
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/pretty.py:454:        repr_text = "".join(self.iter_tokens())
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/traceback.py:24:from pip._vendor.pygments.token import Comment, Keyword, Name, Number, Operator, String
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/traceback.py:25:from pip._vendor.pygments.token import Text as TextToken
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/traceback.py:26:from pip._vendor.pygments.token import Token
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/traceback.py:598:        token_style = theme.get_style_for_token
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/traceback.py:602:                "pretty": token_style(TextToken),
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/traceback.py:603:                "pygments.text": token_style(Token),
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/traceback.py:604:                "pygments.string": token_style(String),
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/traceback.py:605:                "pygments.function": token_style(Name.Function),
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/traceback.py:606:                "pygments.number": token_style(Number),
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/traceback.py:607:                "repr.indent": token_style(Comment) + Style(dim=True),
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/traceback.py:608:                "repr.str": token_style(String),
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/traceback.py:609:                "repr.brace": token_style(TextToken) + Style(bold=True),
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/traceback.py:610:                "repr.number": token_style(Number),
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/traceback.py:611:                "repr.bool_true": token_style(Keyword.Constant),
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/traceback.py:612:                "repr.bool_false": token_style(Keyword.Constant),
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/traceback.py:613:                "repr.none": token_style(Keyword.Constant),
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/traceback.py:614:                "scope.border": token_style(String.Delimiter),
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/traceback.py:615:                "scope.equals": token_style(Operator),
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/traceback.py:616:                "scope.key": token_style(Name),
./.venv-build/lib/python3.11/site-packages/pip/_vendor/rich/traceback.py:617:                "scope.key.special": token_style(Name.Constant) + Style(dim=True),
./.venv-build/lib/python3.11/site-packages/pip/_vendor/urllib3/packages/six.py:426:    MovedAttribute("HTTPPasswordMgr", "urllib2", "urllib.request"),
./.venv-build/lib/python3.11/site-packages/pip/_vendor/urllib3/packages/six.py:427:    MovedAttribute("HTTPPasswordMgrWithDefaultRealm", "urllib2", "urllib.request"),
./.venv-build/lib/python3.11/site-packages/pip/_vendor/urllib3/connectionpool.py:913:    ``ca_cert_dir``, ``ssl_version``, ``key_password`` are only used if :mod:`ssl`
./.venv-build/lib/python3.11/site-packages/pip/_vendor/urllib3/connectionpool.py:936:        key_password=None,
./.venv-build/lib/python3.11/site-packages/pip/_vendor/urllib3/connectionpool.py:963:        self.key_password = key_password
./.venv-build/lib/python3.11/site-packages/pip/_vendor/urllib3/connectionpool.py:979:                key_password=self.key_password,
./.venv-build/lib/python3.11/site-packages/pip/_vendor/urllib3/connectionpool.py:1033:            key_password=self.key_password,
./.venv-build/lib/python3.11/site-packages/pip/_vendor/urllib3/poolmanager.py:36:    "key_password",
./.venv-build/lib/python3.11/site-packages/pip/_vendor/urllib3/poolmanager.py:52:    "key_key_password",  # str
./.venv-build/lib/python3.11/site-packages/pip/_vendor/urllib3/connection.py:212:                "Method cannot contain non-token characters %r (found at least %r)"
./.venv-build/lib/python3.11/site-packages/pip/_vendor/urllib3/connection.py:303:        key_password=None,
./.venv-build/lib/python3.11/site-packages/pip/_vendor/urllib3/connection.py:315:        self.key_password = key_password
./.venv-build/lib/python3.11/site-packages/pip/_vendor/urllib3/connection.py:328:        key_password=None,
./.venv-build/lib/python3.11/site-packages/pip/_vendor/urllib3/connection.py:349:        self.key_password = key_password
./.venv-build/lib/python3.11/site-packages/pip/_vendor/urllib3/connection.py:418:            key_password=self.key_password,
./.venv-build/lib/python3.11/site-packages/pip/_vendor/urllib3/util/request.py:45:        Colon-separated username:password string for 'authorization: basic ...'
./.venv-build/lib/python3.11/site-packages/pip/_vendor/urllib3/util/request.py:49:        Colon-separated username:password string for 'proxy-authorization: basic ...'
./.venv-build/lib/python3.11/site-packages/pip/_vendor/urllib3/util/ssl_.py:371:    key_password=None,
./.venv-build/lib/python3.11/site-packages/pip/_vendor/urllib3/util/ssl_.py:390:    :param key_password:
./.venv-build/lib/python3.11/site-packages/pip/_vendor/urllib3/util/ssl_.py:391:        Optional password if the keyfile is encrypted.
./.venv-build/lib/python3.11/site-packages/pip/_vendor/urllib3/util/ssl_.py:418:    if keyfile and key_password is None and _is_key_file_encrypted(keyfile):
./.venv-build/lib/python3.11/site-packages/pip/_vendor/urllib3/util/ssl_.py:419:        raise SSLError("Client private key is encrypted, password is required")
./.venv-build/lib/python3.11/site-packages/pip/_vendor/urllib3/util/ssl_.py:422:        if key_password is None:
./.venv-build/lib/python3.11/site-packages/pip/_vendor/urllib3/util/ssl_.py:425:            context.load_cert_chain(certfile, keyfile, key_password)
./.venv-build/lib/python3.11/site-packages/pip/_vendor/urllib3/util/url.py:142:            >>> Url('http', 'username:password', 'host.com', 80,
./.venv-build/lib/python3.11/site-packages/pip/_vendor/urllib3/util/url.py:144:            'http://username:password@host.com:80/path?query#fragment'
./.venv-build/lib/python3.11/site-packages/pip/_vendor/urllib3/contrib/securetransport.py:863:    def load_cert_chain(self, certfile, keyfile=None, password=None):
./.venv-build/lib/python3.11/site-packages/pip/_vendor/urllib3/contrib/securetransport.py:866:        self._client_cert_passphrase = password
./.venv-build/lib/python3.11/site-packages/pip/_vendor/urllib3/contrib/_securetransport/low_level.py:212:    credentials. This keychain uses a one-time password and a temporary file to
./.venv-build/lib/python3.11/site-packages/pip/_vendor/urllib3/contrib/_securetransport/low_level.py:224:    # some random bytes to password-protect the keychain we're creating, so we
./.venv-build/lib/python3.11/site-packages/pip/_vendor/urllib3/contrib/_securetransport/low_level.py:228:    password = base64.b16encode(random_bytes[8:])  # Must be valid UTF-8
./.venv-build/lib/python3.11/site-packages/pip/_vendor/urllib3/contrib/_securetransport/low_level.py:236:        keychain_path, len(password), password, False, None, ctypes.byref(keychain)
./.venv-build/lib/python3.11/site-packages/pip/_vendor/urllib3/contrib/pyopenssl.py:468:    def load_cert_chain(self, certfile, keyfile=None, password=None):
./.venv-build/lib/python3.11/site-packages/pip/_vendor/urllib3/contrib/pyopenssl.py:470:        if password is not None:
./.venv-build/lib/python3.11/site-packages/pip/_vendor/urllib3/contrib/pyopenssl.py:471:            if not isinstance(password, six.binary_type):
./.venv-build/lib/python3.11/site-packages/pip/_vendor/urllib3/contrib/pyopenssl.py:472:                password = password.encode("utf-8")
./.venv-build/lib/python3.11/site-packages/pip/_vendor/urllib3/contrib/pyopenssl.py:473:            self._ctx.set_passwd_cb(lambda *_: password)
./.venv-build/lib/python3.11/site-packages/pip/_vendor/urllib3/contrib/ntlmpool.py:39:        pw is the password for the user.
./.venv-build/lib/python3.11/site-packages/pip/_vendor/urllib3/contrib/ntlmpool.py:105:                raise Exception("Server rejected request: wrong username or password")
./.venv-build/lib/python3.11/site-packages/pip/_vendor/urllib3/contrib/socks.py:15:- Usernames and passwords for the SOCKS proxy
./.venv-build/lib/python3.11/site-packages/pip/_vendor/urllib3/contrib/socks.py:32:When connecting to a SOCKS5 proxy the ``username`` and ``password`` portion
./.venv-build/lib/python3.11/site-packages/pip/_vendor/urllib3/contrib/socks.py:33:of the ``proxy_url`` will be sent as the username/password to authenticate
./.venv-build/lib/python3.11/site-packages/pip/_vendor/urllib3/contrib/socks.py:38:    proxy_url="socks5h://<username>:<password>@proxy-host"
./.venv-build/lib/python3.11/site-packages/pip/_vendor/urllib3/contrib/socks.py:102:                proxy_password=self._socks_options["password"],
./.venv-build/lib/python3.11/site-packages/pip/_vendor/urllib3/contrib/socks.py:169:        password=None,
./.venv-build/lib/python3.11/site-packages/pip/_vendor/urllib3/contrib/socks.py:176:        if username is None and password is None and parsed.auth is not None:
./.venv-build/lib/python3.11/site-packages/pip/_vendor/urllib3/contrib/socks.py:179:                username, password = split
./.venv-build/lib/python3.11/site-packages/pip/_vendor/urllib3/contrib/socks.py:202:            "password": password,
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/markers.py:15:from ._tokenizer import ParserSyntaxError
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_tokenizer.py:12:class Token:
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_tokenizer.py:91:class Tokenizer:
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_tokenizer.py:92:    """Context-sensitive token parsing.
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_tokenizer.py:94:    Provides methods to examine the input stream to check whether the next token
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_tokenizer.py:108:        self.next_token: Token | None = None
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_tokenizer.py:112:        """Move beyond provided token name, if at current position."""
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_tokenizer.py:117:        """Check whether the next token has the provided name.
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_tokenizer.py:119:        By default, if the check succeeds, the token *must* be read before
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_tokenizer.py:120:        another check. If `peek` is set to `True`, the token is not loaded and
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_tokenizer.py:124:            self.next_token is None
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_tokenizer.py:125:        ), f"Cannot check for {name!r}, already have {self.next_token!r}"
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_tokenizer.py:126:        assert name in self.rules, f"Unknown token name: {name!r}"
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_tokenizer.py:134:            self.next_token = Token(name, match[0], self.position)
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_tokenizer.py:137:    def expect(self, name: str, *, expected: str) -> Token:
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_tokenizer.py:138:        """Expect a certain token name next, failing with a syntax error otherwise.
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_tokenizer.py:140:        The token is *not* read.
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_tokenizer.py:146:    def read(self) -> Token:
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_tokenizer.py:147:        """Consume the next token and return it."""
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_tokenizer.py:148:        token = self.next_token
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_tokenizer.py:149:        assert token is not None
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_tokenizer.py:151:        self.position += len(token.text)
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_tokenizer.py:152:        self.next_token = None
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_tokenizer.py:154:        return token
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_tokenizer.py:175:    def enclosing_tokens(self, open_token: str, close_token: str, *, around: str) -> Iterator[None]:
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_tokenizer.py:176:        if self.check(open_token):
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_tokenizer.py:187:        if not self.check(close_token):
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_tokenizer.py:189:                f"Expected matching {close_token} for {open_token}, after {around}",
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:12:from ._tokenizer import DEFAULT_RULES, Tokenizer
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:62:    return _parse_requirement(Tokenizer(source, rules=DEFAULT_RULES))
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:65:def _parse_requirement(tokenizer: Tokenizer) -> ParsedRequirement:
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:69:    tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:71:    name_token = tokenizer.expect(
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:74:    name = name_token.text
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:75:    tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:77:    extras = _parse_extras(tokenizer)
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:78:    tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:80:    url, specifier, marker = _parse_requirement_details(tokenizer)
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:81:    tokenizer.expect("END", expected="end of dependency specifier")
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:87:    tokenizer: Tokenizer,
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:98:    if tokenizer.check("AT"):
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:99:        tokenizer.read()
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:100:        tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:102:        url_start = tokenizer.position
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:103:        url = tokenizer.expect("URL", expected="URL after @").text
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:104:        if tokenizer.check("END", peek=True):
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:107:        tokenizer.expect("WS", expected="whitespace after URL")
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:110:        if tokenizer.check("END", peek=True):
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:114:            tokenizer, span_start=url_start, after="URL and whitespace"
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:117:        specifier_start = tokenizer.position
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:118:        specifier = _parse_specifier(tokenizer)
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:119:        tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:121:        if tokenizer.check("END", peek=True):
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:125:            tokenizer,
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:133:def _parse_requirement_marker(tokenizer: Tokenizer, *, span_start: int, after: str) -> MarkerList:
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:138:    if not tokenizer.check("SEMICOLON"):
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:139:        tokenizer.raise_syntax_error(
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:143:    tokenizer.read()
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:145:    marker = _parse_marker(tokenizer)
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:146:    tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:151:def _parse_extras(tokenizer: Tokenizer) -> list[str]:
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:155:    if not tokenizer.check("LEFT_BRACKET", peek=True):
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:158:    with tokenizer.enclosing_tokens(
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:163:        tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:164:        extras = _parse_extras_list(tokenizer)
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:165:        tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:170:def _parse_extras_list(tokenizer: Tokenizer) -> list[str]:
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:176:    if not tokenizer.check("IDENTIFIER"):
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:179:    extras.append(tokenizer.read().text)
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:182:        tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:183:        if tokenizer.check("IDENTIFIER", peek=True):
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:184:            tokenizer.raise_syntax_error("Expected comma between extra names")
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:185:        elif not tokenizer.check("COMMA"):
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:188:        tokenizer.read()
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:189:        tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:191:        extra_token = tokenizer.expect("IDENTIFIER", expected="extra name after comma")
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:192:        extras.append(extra_token.text)
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:197:def _parse_specifier(tokenizer: Tokenizer) -> str:
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:202:    with tokenizer.enclosing_tokens(
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:207:        tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:208:        parsed_specifiers = _parse_version_many(tokenizer)
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:209:        tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:214:def _parse_version_many(tokenizer: Tokenizer) -> str:
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:219:    while tokenizer.check("SPECIFIER"):
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:220:        span_start = tokenizer.position
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:221:        parsed_specifiers += tokenizer.read().text
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:222:        if tokenizer.check("VERSION_PREFIX_TRAIL", peek=True):
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:223:            tokenizer.raise_syntax_error(
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:226:                span_end=tokenizer.position + 1,
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:228:        if tokenizer.check("VERSION_LOCAL_LABEL_TRAIL", peek=True):
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:229:            tokenizer.raise_syntax_error(
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:232:                span_end=tokenizer.position,
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:234:        tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:235:        if not tokenizer.check("COMMA"):
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:237:        parsed_specifiers += tokenizer.read().text
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:238:        tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:247:    return _parse_full_marker(Tokenizer(source, rules=DEFAULT_RULES))
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:250:def _parse_full_marker(tokenizer: Tokenizer) -> MarkerList:
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:251:    retval = _parse_marker(tokenizer)
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:252:    tokenizer.expect("END", expected="end of marker expression")
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:256:def _parse_marker(tokenizer: Tokenizer) -> MarkerList:
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:260:    expression = [_parse_marker_atom(tokenizer)]
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:261:    while tokenizer.check("BOOLOP"):
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:262:        token = tokenizer.read()
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:263:        expr_right = _parse_marker_atom(tokenizer)
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:264:        expression.extend((token.text, expr_right))
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:268:def _parse_marker_atom(tokenizer: Tokenizer) -> MarkerAtom:
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:274:    tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:275:    if tokenizer.check("LEFT_PARENTHESIS", peek=True):
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:276:        with tokenizer.enclosing_tokens(
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:281:            tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:282:            marker: MarkerAtom = _parse_marker(tokenizer)
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:283:            tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:285:        marker = _parse_marker_item(tokenizer)
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:286:    tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:290:def _parse_marker_item(tokenizer: Tokenizer) -> MarkerItem:
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:294:    tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:295:    marker_var_left = _parse_marker_var(tokenizer)
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:296:    tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:297:    marker_op = _parse_marker_op(tokenizer)
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:298:    tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:299:    marker_var_right = _parse_marker_var(tokenizer)
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:300:    tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:304:def _parse_marker_var(tokenizer: Tokenizer) -> MarkerVar:
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:308:    if tokenizer.check("VARIABLE"):
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:309:        return process_env_var(tokenizer.read().text.replace(".", "_"))
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:310:    elif tokenizer.check("QUOTED_STRING"):
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:311:        return process_python_str(tokenizer.read().text)
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:313:        tokenizer.raise_syntax_error(message="Expected a marker variable or quoted string")
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:328:def _parse_marker_op(tokenizer: Tokenizer) -> Op:
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:332:    if tokenizer.check("IN"):
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:333:        tokenizer.read()
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:335:    elif tokenizer.check("NOT"):
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:336:        tokenizer.read()
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:337:        tokenizer.expect("WS", expected="whitespace after 'not'")
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:338:        tokenizer.expect("IN", expected="'in' after 'not'")
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:340:    elif tokenizer.check("OP"):
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:341:        return Op(tokenizer.read().text)
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/_parser.py:343:        return tokenizer.raise_syntax_error(
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/licenses/__init__.py:67:    # Pad any parentheses so tokenization can be achieved by merely splitting on
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/licenses/__init__.py:81:    tokens = license_expression.split()
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/licenses/__init__.py:86:    python_tokens = []
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/licenses/__init__.py:87:    for token in tokens:
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/licenses/__init__.py:88:        if token not in {"or", "and", "with", "(", ")"}:
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/licenses/__init__.py:89:            python_tokens.append("False")
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/licenses/__init__.py:90:        elif token == "with":
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/licenses/__init__.py:91:            python_tokens.append("or")
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/licenses/__init__.py:92:        elif token == "(" and python_tokens and python_tokens[-1] not in {"or", "and"}:
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/licenses/__init__.py:96:            python_tokens.append(token)
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/licenses/__init__.py:98:    python_expression = " ".join(python_tokens)
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/licenses/__init__.py:109:    normalized_tokens = []
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/licenses/__init__.py:110:    for token in tokens:
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/licenses/__init__.py:111:        if token in {"or", "and", "with", "(", ")"}:
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/licenses/__init__.py:112:            normalized_tokens.append(token.upper())
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/licenses/__init__.py:115:        if normalized_tokens and normalized_tokens[-1] == "WITH":
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/licenses/__init__.py:116:            if token not in EXCEPTIONS:
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/licenses/__init__.py:117:                message = f"Unknown license exception: {token!r}"
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/licenses/__init__.py:120:            normalized_tokens.append(EXCEPTIONS[token]["id"])
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/licenses/__init__.py:122:            if token.endswith("+"):
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/licenses/__init__.py:123:                final_token = token[:-1]
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/licenses/__init__.py:126:                final_token = token
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/licenses/__init__.py:129:            if final_token.startswith("licenseref-"):
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/licenses/__init__.py:130:                if not license_ref_allowed.match(final_token):
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/licenses/__init__.py:131:                    message = f"Invalid licenseref: {final_token!r}"
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/licenses/__init__.py:133:                normalized_tokens.append(license_refs[final_token] + suffix)
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/licenses/__init__.py:135:                if final_token not in LICENSES:
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/licenses/__init__.py:136:                    message = f"Unknown license: {final_token!r}"
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/licenses/__init__.py:138:                normalized_tokens.append(LICENSES[final_token]["id"] + suffix)
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/licenses/__init__.py:140:    normalized_expression = " ".join(normalized_tokens)
./.venv-build/lib/python3.11/site-packages/pip/_vendor/packaging/requirements.py:9:from ._tokenizer import ParserSyntaxError
./.venv-build/lib/python3.11/site-packages/pip/_vendor/requests/auth.py:25:def _basic_auth_str(username, password):
./.venv-build/lib/python3.11/site-packages/pip/_vendor/requests/auth.py:45:    if not isinstance(password, basestring):
./.venv-build/lib/python3.11/site-packages/pip/_vendor/requests/auth.py:47:            "Non-string passwords will no longer be supported in Requests "
./.venv-build/lib/python3.11/site-packages/pip/_vendor/requests/auth.py:50:            "problems.".format(type(password)),
./.venv-build/lib/python3.11/site-packages/pip/_vendor/requests/auth.py:53:        password = str(password)
./.venv-build/lib/python3.11/site-packages/pip/_vendor/requests/auth.py:59:    if isinstance(password, str):
./.venv-build/lib/python3.11/site-packages/pip/_vendor/requests/auth.py:60:        password = password.encode("latin1")
./.venv-build/lib/python3.11/site-packages/pip/_vendor/requests/auth.py:62:    authstr = "Basic " + to_native_string(b64encode(b":".join((username, password))).strip())
./.venv-build/lib/python3.11/site-packages/pip/_vendor/requests/auth.py:77:    def __init__(self, username, password):
./.venv-build/lib/python3.11/site-packages/pip/_vendor/requests/auth.py:79:        self.password = password
./.venv-build/lib/python3.11/site-packages/pip/_vendor/requests/auth.py:85:                self.password == getattr(other, "password", None),
./.venv-build/lib/python3.11/site-packages/pip/_vendor/requests/auth.py:93:        r.headers["Authorization"] = _basic_auth_str(self.username, self.password)
./.venv-build/lib/python3.11/site-packages/pip/_vendor/requests/auth.py:101:        r.headers["Proxy-Authorization"] = _basic_auth_str(self.username, self.password)
./.venv-build/lib/python3.11/site-packages/pip/_vendor/requests/auth.py:108:    def __init__(self, username, password):
./.venv-build/lib/python3.11/site-packages/pip/_vendor/requests/auth.py:110:        self.password = password
./.venv-build/lib/python3.11/site-packages/pip/_vendor/requests/auth.py:187:        A1 = f"{self.username}:{realm}:{self.password}"
./.venv-build/lib/python3.11/site-packages/pip/_vendor/requests/auth.py:305:                self.password == getattr(other, "password", None),
./.venv-build/lib/python3.11/site-packages/pip/_vendor/requests/utils.py:237:                # Return with login / password
./.venv-build/lib/python3.11/site-packages/pip/_vendor/requests/utils.py:378:    >>> parse_list_header('token, "quoted value"')
./.venv-build/lib/python3.11/site-packages/pip/_vendor/requests/utils.py:379:    ['token', 'quoted value']
./.venv-build/lib/python3.11/site-packages/pip/_vendor/requests/utils.py:508:    tokens = header.split(";")
./.venv-build/lib/python3.11/site-packages/pip/_vendor/requests/utils.py:509:    content_type, params = tokens[0].strip(), tokens[1:]
./.venv-build/lib/python3.11/site-packages/pip/_vendor/requests/utils.py:1006:    username,password.
./.venv-build/lib/python3.11/site-packages/pip/_vendor/requests/utils.py:1013:        auth = (unquote(parsed.username), unquote(parsed.password))
./.venv-build/lib/python3.11/site-packages/pip/_vendor/requests/sessions.py:317:            username, password = get_auth_from_url(new_proxies[scheme])
./.venv-build/lib/python3.11/site-packages/pip/_vendor/requests/sessions.py:319:            username, password = None, None
./.venv-build/lib/python3.11/site-packages/pip/_vendor/requests/sessions.py:323:        if not scheme.startswith("https") and username and password:
./.venv-build/lib/python3.11/site-packages/pip/_vendor/requests/sessions.py:324:            headers["Proxy-Authorization"] = _basic_auth_str(username, password)
./.venv-build/lib/python3.11/site-packages/pip/_vendor/requests/adapters.py:251:            username, password = get_auth_from_url(proxy)
./.venv-build/lib/python3.11/site-packages/pip/_vendor/requests/adapters.py:255:                password=password,
./.venv-build/lib/python3.11/site-packages/pip/_vendor/requests/adapters.py:568:        username, password = get_auth_from_url(proxy)
./.venv-build/lib/python3.11/site-packages/pip/_vendor/requests/adapters.py:571:            headers["Proxy-Authorization"] = _basic_auth_str(username, password)
./.venv-build/lib/python3.11/site-packages/pip/_vendor/truststore/_api.py:31:_PasswordType: typing.TypeAlias = str | bytes | typing.Callable[[], str | bytes]
./.venv-build/lib/python3.11/site-packages/pip/_vendor/truststore/_api.py:166:        password: _PasswordType | None = None,
./.venv-build/lib/python3.11/site-packages/pip/_vendor/truststore/_api.py:168:        return self._ctx.load_cert_chain(certfile=certfile, keyfile=keyfile, password=password)
./.venv-build/lib/python3.11/site-packages/pip/_vendor/distro/distro.py:1105:        tokens = list(lexer)
./.venv-build/lib/python3.11/site-packages/pip/_vendor/distro/distro.py:1106:        for token in tokens:
./.venv-build/lib/python3.11/site-packages/pip/_vendor/distro/distro.py:1110:            # stripped, etc.), so the tokens are now either:
./.venv-build/lib/python3.11/site-packages/pip/_vendor/distro/distro.py:1113:            # Ignore any tokens that are not variable assignments
./.venv-build/lib/python3.11/site-packages/pip/_vendor/distro/distro.py:1114:            if "=" in token:
./.venv-build/lib/python3.11/site-packages/pip/_vendor/distro/distro.py:1115:                k, v = token.split("=", 1)
./.venv-build/lib/python3.11/site-packages/pip/_vendor/distlib/util.py:847:    username = password = None
./.venv-build/lib/python3.11/site-packages/pip/_vendor/distlib/util.py:853:            username, password = prefix.split(":", 1)
./.venv-build/lib/python3.11/site-packages/pip/_vendor/distlib/util.py:856:    if password:
./.venv-build/lib/python3.11/site-packages/pip/_vendor/distlib/util.py:857:        password = unquote(password)
./.venv-build/lib/python3.11/site-packages/pip/_vendor/distlib/util.py:858:    return username, password, netloc
./.venv-build/lib/python3.11/site-packages/pip/_vendor/distlib/util.py:1866:                            ("password", None),
./.venv-build/lib/python3.11/site-packages/pip/_vendor/distlib/util.py:1889:                    "password": config.get(server, "password"),
./.venv-build/lib/python3.11/site-packages/pip/_vendor/distlib/util.py:1896:    def update(self, username, password):
./.venv-build/lib/python3.11/site-packages/pip/_vendor/distlib/util.py:1904:        config.set("pypi", "password", password)
./.venv-build/lib/python3.11/site-packages/pip/_vendor/distlib/util.py:1917:    PyPIRCFile().update(index.username, index.password)
./.venv-build/lib/python3.11/site-packages/pip/_vendor/distlib/compat.py:50:        HTTPPasswordMgr,
./.venv-build/lib/python3.11/site-packages/pip/_vendor/distlib/compat.py:106:        HTTPPasswordMgr,
./.venv-build/lib/python3.11/site-packages/pip/_vendor/distlib/compat.py:402:    from tokenize import detect_encoding
./.venv-build/lib/python3.11/site-packages/pip/_vendor/distlib/compat.py:409:        """Imitates get_normal_name in tokenizer.c."""
./.venv-build/lib/python3.11/site-packages/pip/_vendor/distlib/compat.py:424:        in the same way as the tokenize() generator.
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/formatters/_mapping.py:10:        "Format tokens with BBcodes. These formatting codes are used by many bulletin boards, so you can highlight your sourcecode with pygments before posting it there.",
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/formatters/_mapping.py:31:        "Format tokens with groff escapes to change their color and font style.",
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/formatters/_mapping.py:38:        "Format tokens as HTML 4 ``<span>`` tags. By default, the content is enclosed in a ``<pre>`` tag, itself wrapped in a ``<div>`` tag (but see the `nowrap` option). The ``<div>``'s CSS class can be set by the `cssclass` option.",
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/formatters/_mapping.py:45:        "Format tokens with IRC color sequences",
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/formatters/_mapping.py:66:        "Format tokens as LaTeX code. This needs the `fancyvrb` and `color` standard packages.",
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/formatters/_mapping.py:80:        "Format tokens as Pango Markup code. It can then be rendered to an SVG.",
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/formatters/_mapping.py:82:    "RawTokenFormatter": (
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/formatters/_mapping.py:84:        "Raw tokens",
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/formatters/_mapping.py:85:        ("raw", "tokens"),
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/formatters/_mapping.py:87:        "Format tokens as a raw representation for storing token streams.",
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/formatters/_mapping.py:94:        "Format tokens as RTF markup. This formatter automatically outputs full RTF documents with color information and other useful stuff. Perfect for Copy and Paste into Microsoft(R) Word(R) documents.",
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/formatters/_mapping.py:101:        "Format tokens as an SVG graphics file.  This formatter is still experimental. Each line of code is a ``<text>`` element with explicit ``x`` and ``y`` coordinates containing ``<tspan>`` elements with the individual token styles.",
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/formatters/_mapping.py:108:        "Format tokens with ANSI color sequences, for output in a 256-color terminal or console.  Like in `TerminalFormatter` color sequences are terminated at newlines, so that paging the output works correctly.",
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/formatters/_mapping.py:115:        "Format tokens with ANSI color sequences, for output in a text console. Color sequences are terminated at newlines, so that paging the output works correctly.",
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/formatters/_mapping.py:122:        "Format tokens with ANSI color sequences, for output in a true-color terminal or console.  Like in `TerminalFormatter` color sequences are terminated at newlines, so that paging the output works correctly.",
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/formatters/_mapping.py:129:        "Format tokens as appropriate for a new testcase.",
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/formatter.py:27:    Converts a token stream to text.
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/formatter.py:58:        convert the Unicode token strings to byte strings in the
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/formatter.py:114:    def format(self, tokensource, outfile):
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/formatter.py:116:        This method must format the tokens from the `tokensource` iterable and
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/formatter.py:119:        Formatter options can control how exactly the tokens are converted.
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/formatter.py:124:        return self.format_unencoded(tokensource, outfile)
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/token.py:2:pygments.token
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/token.py:5:Basic token types and the standard tokens.
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/token.py:12:class _TokenType(tuple):
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/token.py:34:        new = _TokenType(self + (val,))
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/token.py:41:        return "Token" + (self and "." or "") + ".".join(self)
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/token.py:52:Token = _TokenType()
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/token.py:54:# Special token types
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/token.py:55:Text = Token.Text
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/token.py:57:Escape = Token.Escape
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/token.py:58:Error = Token.Error
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/token.py:60:Other = Token.Other
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/token.py:62:# Common token types for source code
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/token.py:63:Keyword = Token.Keyword
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/token.py:64:Name = Token.Name
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/token.py:65:Literal = Token.Literal
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/token.py:68:Punctuation = Token.Punctuation
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/token.py:69:Operator = Token.Operator
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/token.py:70:Comment = Token.Comment
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/token.py:73:Generic = Token.Generic
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/token.py:75:# String and some others are not direct children of Token.
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/token.py:77:Token.Token = Token
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/token.py:78:Token.String = String
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/token.py:79:Token.Number = Number
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/token.py:82:def is_token_subtype(ttype, other):
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/token.py:91:def string_to_tokentype(s):
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/token.py:93:    Convert a string into a token type::
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/token.py:95:        >>> string_to_token('String.Double')
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/token.py:96:        Token.Literal.String.Double
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/token.py:97:        >>> string_to_token('Token.Literal.Number')
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/token.py:98:        Token.Literal.Number
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/token.py:99:        >>> string_to_token('')
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/token.py:100:        Token
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/token.py:102:    Tokens that are already tokens are returned unchanged:
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/token.py:104:        >>> string_to_token(String)
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/token.py:105:        Token.Literal.String
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/token.py:107:    if isinstance(s, _TokenType):
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/token.py:110:        return Token
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/token.py:111:    node = Token
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/token.py:117:# Map standard token types to short names, used in CSS class naming.
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/token.py:121:    Token: "",
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/style.py:11:from pip._vendor.pygments.token import Token, STANDARD_TYPES
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/style.py:62:        for token in STANDARD_TYPES:
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/style.py:63:            if token not in obj.styles:
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/style.py:64:                obj.styles[token] = ""
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/style.py:84:            for token in ttype.split():
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/style.py:85:                if token in _styles:
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/style.py:87:                ndef = _styles.get(token.parent, None)
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/style.py:88:                styledefs = obj.styles.get(token, "").split()
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/style.py:89:                if not ndef or token is None:
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/style.py:91:                elif "noinherit" in styledefs and token is not Token:
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/style.py:92:                    ndef = _styles[Token][:]
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/style.py:95:                _styles[token] = ndef
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/style.py:96:                for styledef in obj.styles.get(token, "").split():
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/style.py:126:    def style_for_token(cls, token):
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/style.py:127:        t = cls._styles[token]
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/style.py:159:    def styles_token(cls, ttype):
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/style.py:163:        for token in cls._styles:
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/style.py:164:            yield token, cls.style_for_token(token)
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/style.py:190:    #: Style definitions for individual token types.
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexers/_mapping.py:2760:    "RawTokenLexer": (
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexers/_mapping.py:2762:        "Raw token data",
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexers/_mapping.py:2765:        ("application/x-pygments-tokens",),
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexers/python.py:25:from pip._vendor.pygments.token import (
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexers/python.py:130:    tokens = {
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexers/python.py:746:    tokens = {
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexers/python.py:1200:    Code tokens are output as ``Token.Other.Code``, traceback tokens as
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexers/python.py:1201:    ``Token.Other.Traceback``.
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexers/python.py:1203:    tokens = {
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexers/python.py:1270:        # different tokens.  TODO: DelegatingLexer should support this
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexers/python.py:1272:        # distinguishing tokens. Then we wouldn't need this intermediary
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexers/python.py:1297:    tokens = {
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexers/python.py:1359:    tokens = {
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexers/python.py:1402:    tokens = {
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexers/python.py:1727:    tokens = {
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexers/python.py:2298:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexers/python.py:2299:        for index, token, value in PythonLexer.get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexers/python.py:2300:            if token is Name and value in self.EXTRA_KEYWORDS:
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexers/python.py:2303:                yield index, token, value
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:17:from pip._vendor.pygments.token import Error, Text, Other, Whitespace, _TokenType
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:276:    def get_tokens(self, text, unfiltered=False):
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:280:        iterable of ``(tokentype, value)`` pairs from `text`.
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:284:        (`stripnl`, `stripall` and so on), and then yields all tokens
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:285:        from `get_tokens_unprocessed()`, with the ``index`` dropped.
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:293:            for _, t, v in self.get_tokens_unprocessed(text):
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:301:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:304:        ``(index, tokentype, value)`` tuples where ``index`` is the starting
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:305:        position of the token within the input text.
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:317:    lexer, afterwards all ``Other`` tokens are lexed using the root
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:329:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:333:        for i, t, v in self.language_lexer.get_tokens_unprocessed(text):
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:343:        return do_insertions(insertions, self.root_lexer.get_tokens_unprocessed(buffered))
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:420:            elif type(action) is _TokenType:
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:481:            for i, t, v in lx.get_tokens_unprocessed(match.group(), **gt_kwargs):
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:494:            for i, t, v in lx.get_tokens_unprocessed(match.group(), **gt_kwargs):
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:505:    For example default('#pop') is equivalent to ('', Token, '#pop')
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:534:    Metaclass for RegexLexer, creates the self._tokens attribute from
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:535:    self.tokens on the first instantiation.
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:539:        """Preprocess the regular expression component of a token definition."""
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:544:    def _process_token(cls, token):
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:545:        """Preprocess the token component of a token definition."""
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:546:        assert type(token) is _TokenType or callable(
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:547:            token
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:548:        ), f"token type must be simple type or callable, not {token!r}"
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:549:        return token
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:552:        """Preprocess the state transition action of a token definition."""
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:569:            itokens = []
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:572:                itokens.extend(cls._process_state(unprocessed, processed, istate))
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:573:            processed[tmp_state] = itokens
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:591:        tokens = processed[state] = []
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:597:                tokens.extend(cls._process_state(unprocessed, processed, str(tdef)))
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:606:                tokens.append((re.compile("").match, None, new_state))
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:618:            token = cls._process_token(tdef[1])
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:625:            tokens.append((rex, token, new_state))
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:626:        return tokens
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:628:    def process_tokendef(cls, name, tokendefs=None):
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:629:        """Preprocess a dictionary of token definitions."""
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:630:        processed = cls._all_tokens[name] = {}
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:631:        tokendefs = tokendefs or cls.tokens[name]
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:632:        for state in list(tokendefs):
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:633:            cls._process_state(tokendefs, processed, state)
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:636:    def get_tokendefs(cls):
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:638:        Merge tokens from superclasses in MRO order, returning a single tokendef
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:648:        tokens = {}
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:651:            toks = c.__dict__.get("tokens", {})
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:654:                curitems = tokens.get(state)
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:660:                    tokens[state] = items
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:683:        return tokens
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:686:        """Instantiate cls after preprocessing its token definitions."""
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:687:        if "_tokens" not in cls.__dict__:
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:688:            cls._all_tokens = {}
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:690:            if hasattr(cls, "token_variants") and cls.token_variants:
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:694:                cls._tokens = cls.process_tokendef("", cls.get_tokendefs())
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:713:    #: Dict of ``{'state': [(regex, tokentype, new_state), ...], ...}``
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:732:    tokens = {}
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:734:    def get_tokens_unprocessed(self, text, stack=("root",)):
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:736:        Split ``text`` into (tokentype, text) pairs.
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:741:        tokendefs = self._tokens
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:743:        statetokens = tokendefs[statestack[-1]]
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:745:            for rexmatch, action, new_state in statetokens:
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:749:                        if type(action) is _TokenType:
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:777:                        statetokens = tokendefs[statestack[-1]]
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:780:                # We are here only if all state tokens have been considered
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:786:                        statetokens = tokendefs["root"]
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:816:    def get_tokens_unprocessed(self, text=None, context=None):
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:818:        Split ``text`` into (tokentype, text) pairs.
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:821:        tokendefs = self._tokens
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:824:            statetokens = tokendefs["root"]
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:827:            statetokens = tokendefs[ctx.stack[-1]]
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:830:            for rexmatch, action, new_state in statetokens:
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:834:                        if type(action) is _TokenType:
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:841:                                statetokens = tokendefs[ctx.stack[-1]]
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:864:                        statetokens = tokendefs[ctx.stack[-1]]
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:873:                        statetokens = tokendefs["root"]
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:883:def do_insertions(insertions, tokens):
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:888:    ``insertions`` is a list of ``(index, itokens)`` pairs.
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:889:    Each ``itokens`` iterable should be inserted at position
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:890:    ``index`` into the token stream given by the ``tokens``
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:893:    The result is a combined token stream.
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:899:        index, itokens = next(insertions)
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:902:        yield from tokens
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:908:    # iterate over the token stream where we want to insert
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:909:    # the tokens from the insertion list.
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:910:    for i, t, v in tokens:
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:920:            for it_index, it_token, it_value in itokens:
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:921:                yield realpos, it_token, it_value
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:925:                index, itokens = next(insertions)
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:933:    # leftover tokens
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:935:        # no normal tokens, set realpos to zero
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:937:        for p, t, v in itokens:
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:941:            index, itokens = next(insertions)
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:975:    def get_tokens_unprocessed(self, text, stack=("root",)):
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py:978:        yield from RegexLexer.get_tokens_unprocessed(self, text, stack)
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/__init__.py:39:    and return an iterable of tokens. Currently, this only calls
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/__init__.py:40:    `lexer.get_tokens()`.
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/__init__.py:43:        return lexer.get_tokens(code)
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/__init__.py:53:def format(tokens, formatter, outfile=None):  # pylint: disable=redefined-builtin
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/__init__.py:55:    Format ``tokens`` (an iterable of tokens) with the formatter ``formatter``
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/__init__.py:65:            formatter.format(tokens, realoutfile)
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/__init__.py:68:            formatter.format(tokens, outfile)
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/filters/__init__.py:14:from pip._vendor.pygments.token import (
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/filters/__init__.py:21:    string_to_tokentype,
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/filters/__init__.py:723:    """Highlight a normal Name (and Name.*) token with a different token type.
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/filters/__init__.py:729:            tokentype=Name.Function,
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/filters/__init__.py:733:    as functions. `Name.Function` is the default token type.
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/filters/__init__.py:738:      A list of names that should be given the different token type.
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/filters/__init__.py:740:    `tokentype` : TokenType or string
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/filters/__init__.py:741:      A token type or a string containing a token type name that is
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/filters/__init__.py:749:        tokentype = options.get("tokentype")
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/filters/__init__.py:750:        if tokentype:
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/filters/__init__.py:751:            self.tokentype = string_to_tokentype(tokentype)
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/filters/__init__.py:753:            self.tokentype = Name.Function
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/filters/__init__.py:758:                yield self.tokentype, value
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/filters/__init__.py:763:class ErrorToken(Exception):
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/filters/__init__.py:767:class RaiseOnErrorTokenFilter(Filter):
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/filters/__init__.py:768:    """Raise an exception when the lexer generates an error token.
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/filters/__init__.py:774:      The default is `pygments.filters.ErrorToken`.
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/filters/__init__.py:781:        self.exception = options.get("excclass", ErrorToken)
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/filters/__init__.py:818:    `wstokentype` : bool
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/filters/__init__.py:819:      If true, give whitespace the special `Whitespace` token type.  This allows
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/filters/__init__.py:839:        self.wstt = get_bool_opt(options, "wstokentype", True)
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/filters/__init__.py:901:            # Remove ``left`` tokens from first line, ``n`` from all others.
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/filters/__init__.py:912:class TokenMergeFilter(Filter):
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/filters/__init__.py:913:    """Merges consecutive tokens with the same token type in the output
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/filters/__init__.py:941:    "raiseonerror": RaiseOnErrorTokenFilter,
./.venv-build/lib/python3.11/site-packages/pip/_vendor/pygments/filters/__init__.py:944:    "tokenmerge": TokenMergeFilter,
./.venv-build/lib/python3.11/site-packages/pip/_internal/vcs/versioncontrol.py:363:        information can be provided via the --username and --password options
./.venv-build/lib/python3.11/site-packages/pip/_internal/vcs/versioncontrol.py:367:        Returns: (netloc, (username, password)).
./.venv-build/lib/python3.11/site-packages/pip/_internal/vcs/versioncontrol.py:377:        Returns: (url, rev, (username, password)).
./.venv-build/lib/python3.11/site-packages/pip/_internal/vcs/versioncontrol.py:402:    def make_rev_args(username: str | None, password: HiddenText | None) -> CommandArgs:
./.venv-build/lib/python3.11/site-packages/pip/_internal/vcs/versioncontrol.py:413:        secret_url, rev, user_pass = self.get_url_rev_and_auth(url.secret)
./.venv-build/lib/python3.11/site-packages/pip/_internal/vcs/versioncontrol.py:414:        username, secret_password = user_pass
./.venv-build/lib/python3.11/site-packages/pip/_internal/vcs/versioncontrol.py:415:        password: HiddenText | None = None
./.venv-build/lib/python3.11/site-packages/pip/_internal/vcs/versioncontrol.py:416:        if secret_password is not None:
./.venv-build/lib/python3.11/site-packages/pip/_internal/vcs/versioncontrol.py:417:            password = hide_value(secret_password)
./.venv-build/lib/python3.11/site-packages/pip/_internal/vcs/versioncontrol.py:418:        extra_args = self.make_rev_args(username, password)
./.venv-build/lib/python3.11/site-packages/pip/_internal/vcs/versioncontrol.py:421:        return hide_url(secret_url), rev_options
./.venv-build/lib/python3.11/site-packages/pip/_internal/vcs/versioncontrol.py:511:            if self.compare_urls(existing_url, url.secret):
./.venv-build/lib/python3.11/site-packages/pip/_internal/vcs/mercurial.py:77:            config.set("paths", "default", url.secret)
./.venv-build/lib/python3.11/site-packages/pip/_internal/vcs/subversion.py:80:        --username and --password options instead of via the URL.
./.venv-build/lib/python3.11/site-packages/pip/_internal/vcs/subversion.py:83:            # The --username and --password options can't be used for
./.venv-build/lib/python3.11/site-packages/pip/_internal/vcs/subversion.py:98:    def make_rev_args(username: str | None, password: HiddenText | None) -> CommandArgs:
./.venv-build/lib/python3.11/site-packages/pip/_internal/vcs/subversion.py:102:        if password:
./.venv-build/lib/python3.11/site-packages/pip/_internal/vcs/subversion.py:103:            extra_args += ["--password", password]
./.venv-build/lib/python3.11/site-packages/pip/_internal/vcs/subversion.py:160:                # is being used to prompt for passwords, because passwords
./.venv-build/lib/python3.11/site-packages/pip/_internal/vcs/subversion.py:269:        # the user can be prompted for a password, if required.
./.venv-build/lib/python3.11/site-packages/pip/_internal/models/link.py:422:            # includes a username and password.
./.venv-build/lib/python3.11/site-packages/pip/_internal/models/direct_url.py:171:        """url with user:password part removed unless it is formed with
./.venv-build/lib/python3.11/site-packages/pip/_internal/req/req_file.py:440:    tokens = line.split(" ")
./.venv-build/lib/python3.11/site-packages/pip/_internal/req/req_file.py:442:    options = tokens[:]
./.venv-build/lib/python3.11/site-packages/pip/_internal/req/req_file.py:443:    for token in tokens:
./.venv-build/lib/python3.11/site-packages/pip/_internal/req/req_file.py:444:        if token.startswith(("-", "--")):
./.venv-build/lib/python3.11/site-packages/pip/_internal/req/req_file.py:447:            args.append(token)
./.venv-build/lib/python3.11/site-packages/pip/_internal/commands/completion.py:41:                (commandline --current-process --tokenize --cut-at-cursor) \\
./.venv-build/lib/python3.11/site-packages/pip/_internal/commands/completion.py:42:                (commandline --current-token --cut-at-cursor)
./.venv-build/lib/python3.11/site-packages/pip/_internal/utils/wheel.py:71:        # and RuntimeError for password-protected files
./.venv-build/lib/python3.11/site-packages/pip/_internal/utils/subprocess.py:55:    return [arg.secret if isinstance(arg, HiddenText) else arg for arg in args]
./.venv-build/lib/python3.11/site-packages/pip/_internal/utils/misc.py:236:def ask_password(message: str) -> str:
./.venv-build/lib/python3.11/site-packages/pip/_internal/utils/misc.py:237:    """Ask for a password interactively."""
./.venv-build/lib/python3.11/site-packages/pip/_internal/utils/misc.py:425:    Returns: (netloc, (username, password)).
./.venv-build/lib/python3.11/site-packages/pip/_internal/utils/misc.py:432:    # the password attribute of urlsplit()'s return value).
./.venv-build/lib/python3.11/site-packages/pip/_internal/utils/misc.py:438:        # using the password attribute of the return value)
./.venv-build/lib/python3.11/site-packages/pip/_internal/utils/misc.py:456:        - "accesstoken@example.com" returns "****@example.com"
./.venv-build/lib/python3.11/site-packages/pip/_internal/utils/misc.py:458:    netloc, (user, password) = split_auth_from_netloc(netloc)
./.venv-build/lib/python3.11/site-packages/pip/_internal/utils/misc.py:461:    if password is None:
./.venv-build/lib/python3.11/site-packages/pip/_internal/utils/misc.py:463:        password = ""
./.venv-build/lib/python3.11/site-packages/pip/_internal/utils/misc.py:466:        password = ":****"
./.venv-build/lib/python3.11/site-packages/pip/_internal/utils/misc.py:467:    return f"{user}{password}@{netloc}"
./.venv-build/lib/python3.11/site-packages/pip/_internal/utils/misc.py:504:    Returns: (url_without_auth, netloc, (username, password))
./.venv-build/lib/python3.11/site-packages/pip/_internal/utils/misc.py:511:    """Return a copy of url with 'username:password@' removed."""
./.venv-build/lib/python3.11/site-packages/pip/_internal/utils/misc.py:518:    """Replace the password in a given url with ****."""
./.venv-build/lib/python3.11/site-packages/pip/_internal/utils/misc.py:523:    """Replace the password in a given requirement url with ****."""
./.venv-build/lib/python3.11/site-packages/pip/_internal/utils/misc.py:531:    secret: str
./.venv-build/lib/python3.11/site-packages/pip/_internal/utils/misc.py:547:        return self.secret == other.secret
./.venv-build/lib/python3.11/site-packages/pip/_internal/network/auth.py:30:    ask_password,
./.venv-build/lib/python3.11/site-packages/pip/_internal/network/auth.py:44:    password: str
./.venv-build/lib/python3.11/site-packages/pip/_internal/network/auth.py:56:    def save_auth_info(self, url: str, username: str, password: str) -> None: ...
./.venv-build/lib/python3.11/site-packages/pip/_internal/network/auth.py:67:    def save_auth_info(self, url: str, username: str, password: str) -> None:
./.venv-build/lib/python3.11/site-packages/pip/_internal/network/auth.py:89:                return cred.username, cred.password
./.venv-build/lib/python3.11/site-packages/pip/_internal/network/auth.py:93:            logger.debug("Getting password from keyring for %s", url)
./.venv-build/lib/python3.11/site-packages/pip/_internal/network/auth.py:94:            password = self.keyring.get_password(url, username)
./.venv-build/lib/python3.11/site-packages/pip/_internal/network/auth.py:95:            if password:
./.venv-build/lib/python3.11/site-packages/pip/_internal/network/auth.py:96:                return username, password
./.venv-build/lib/python3.11/site-packages/pip/_internal/network/auth.py:99:    def save_auth_info(self, url: str, username: str, password: str) -> None:
./.venv-build/lib/python3.11/site-packages/pip/_internal/network/auth.py:100:        self.keyring.set_password(url, username, password)
./.venv-build/lib/python3.11/site-packages/pip/_internal/network/auth.py:121:            password = self._get_password(url, username)
./.venv-build/lib/python3.11/site-packages/pip/_internal/network/auth.py:122:            if password is not None:
./.venv-build/lib/python3.11/site-packages/pip/_internal/network/auth.py:123:                return username, password
./.venv-build/lib/python3.11/site-packages/pip/_internal/network/auth.py:126:    def save_auth_info(self, url: str, username: str, password: str) -> None:
./.venv-build/lib/python3.11/site-packages/pip/_internal/network/auth.py:127:        return self._set_password(url, username, password)
./.venv-build/lib/python3.11/site-packages/pip/_internal/network/auth.py:129:    def _get_password(self, service_name: str, username: str) -> str | None:
./.venv-build/lib/python3.11/site-packages/pip/_internal/network/auth.py:130:        """Mirror the implementation of keyring.get_password using cli"""
./.venv-build/lib/python3.11/site-packages/pip/_internal/network/auth.py:147:    def _set_password(self, service_name: str, username: str, password: str) -> None:
./.venv-build/lib/python3.11/site-packages/pip/_internal/network/auth.py:148:        """Mirror the implementation of keyring.set_password using cli"""
./.venv-build/lib/python3.11/site-packages/pip/_internal/network/auth.py:155:            input=f"{password}{os.linesep}".encode(),
./.venv-build/lib/python3.11/site-packages/pip/_internal/network/auth.py:234:        self.passwords: dict[str, AuthInfo] = {}
./.venv-build/lib/python3.11/site-packages/pip/_internal/network/auth.py:293:        The provided url should have had its username and password
./.venv-build/lib/python3.11/site-packages/pip/_internal/network/auth.py:344:        url, netloc, url_user_password = split_auth_netloc_from_url(
./.venv-build/lib/python3.11/site-packages/pip/_internal/network/auth.py:349:        username, password = url_user_password
./.venv-build/lib/python3.11/site-packages/pip/_internal/network/auth.py:350:        if username is not None and password is not None:
./.venv-build/lib/python3.11/site-packages/pip/_internal/network/auth.py:352:            return url_user_password
./.venv-build/lib/python3.11/site-packages/pip/_internal/network/auth.py:360:                index_url, _, index_url_user_password = index_info
./.venv-build/lib/python3.11/site-packages/pip/_internal/network/auth.py:364:        if index_url and index_url_user_password[0] is not None:
./.venv-build/lib/python3.11/site-packages/pip/_internal/network/auth.py:365:            username, password = index_url_user_password
./.venv-build/lib/python3.11/site-packages/pip/_internal/network/auth.py:366:            if username is not None and password is not None:
./.venv-build/lib/python3.11/site-packages/pip/_internal/network/auth.py:368:                return index_url_user_password
./.venv-build/lib/python3.11/site-packages/pip/_internal/network/auth.py:377:        # If we don't have a password and keyring is available, use it.
./.venv-build/lib/python3.11/site-packages/pip/_internal/network/auth.py:390:        return username, password
./.venv-build/lib/python3.11/site-packages/pip/_internal/network/auth.py:398:        Returns (url_without_credentials, username, password). Note
./.venv-build/lib/python3.11/site-packages/pip/_internal/network/auth.py:400:        function may return a different username and password.
./.venv-build/lib/python3.11/site-packages/pip/_internal/network/auth.py:405:        username, password = self._get_new_credentials(original_url)
./.venv-build/lib/python3.11/site-packages/pip/_internal/network/auth.py:408:        # Do this if either the username or the password is missing.
./.venv-build/lib/python3.11/site-packages/pip/_internal/network/auth.py:410:        # the username in the index url, but the password comes from keyring.
./.venv-build/lib/python3.11/site-packages/pip/_internal/network/auth.py:411:        if (username is None or password is None) and netloc in self.passwords:
./.venv-build/lib/python3.11/site-packages/pip/_internal/network/auth.py:412:            un, pw = self.passwords[netloc]
./.venv-build/lib/python3.11/site-packages/pip/_internal/network/auth.py:416:                username, password = un, pw
./.venv-build/lib/python3.11/site-packages/pip/_internal/network/auth.py:418:        if username is not None or password is not None:
./.venv-build/lib/python3.11/site-packages/pip/_internal/network/auth.py:419:            # Convert the username and password if they're None, so that
./.venv-build/lib/python3.11/site-packages/pip/_internal/network/auth.py:424:            password = password or ""
./.venv-build/lib/python3.11/site-packages/pip/_internal/network/auth.py:427:            self.passwords[netloc] = (username, password)
./.venv-build/lib/python3.11/site-packages/pip/_internal/network/auth.py:431:            (username is not None and password is not None)
./.venv-build/lib/python3.11/site-packages/pip/_internal/network/auth.py:433:            or (username is None and password is None)
./.venv-build/lib/python3.11/site-packages/pip/_internal/network/auth.py:436:        return url, username, password
./.venv-build/lib/python3.11/site-packages/pip/_internal/network/auth.py:440:        url, username, password = self._get_url_and_credentials(req.url)
./.venv-build/lib/python3.11/site-packages/pip/_internal/network/auth.py:445:        if username is not None and password is not None:
./.venv-build/lib/python3.11/site-packages/pip/_internal/network/auth.py:447:            req = HTTPBasicAuth(username, password)(req)
./.venv-build/lib/python3.11/site-packages/pip/_internal/network/auth.py:455:    def _prompt_for_password(self, netloc: str) -> tuple[str | None, str | None, bool]:
./.venv-build/lib/python3.11/site-packages/pip/_internal/network/auth.py:463:        password = ask_password("Password: ")
./.venv-build/lib/python3.11/site-packages/pip/_internal/network/auth.py:464:        return username, password, True
./.venv-build/lib/python3.11/site-packages/pip/_internal/network/auth.py:467:    def _should_save_password_to_keyring(self) -> bool:
./.venv-build/lib/python3.11/site-packages/pip/_internal/network/auth.py:478:        username, password = None, None
./.venv-build/lib/python3.11/site-packages/pip/_internal/network/auth.py:482:            username, password = self._get_new_credentials(
./.venv-build/lib/python3.11/site-packages/pip/_internal/network/auth.py:489:        if not self.prompting and not username and not password:
./.venv-build/lib/python3.11/site-packages/pip/_internal/network/auth.py:494:        # Prompt the user for a new username and password
./.venv-build/lib/python3.11/site-packages/pip/_internal/network/auth.py:496:        if not username and not password:
./.venv-build/lib/python3.11/site-packages/pip/_internal/network/auth.py:497:            username, password, save = self._prompt_for_password(parsed.netloc)
./.venv-build/lib/python3.11/site-packages/pip/_internal/network/auth.py:499:        # Store the new username and password to use for future requests
./.venv-build/lib/python3.11/site-packages/pip/_internal/network/auth.py:501:        if username is not None and password is not None:
./.venv-build/lib/python3.11/site-packages/pip/_internal/network/auth.py:502:            self.passwords[parsed.netloc] = (username, password)
./.venv-build/lib/python3.11/site-packages/pip/_internal/network/auth.py:504:            # Prompt to save the password to keyring
./.venv-build/lib/python3.11/site-packages/pip/_internal/network/auth.py:505:            if save and self._should_save_password_to_keyring():
./.venv-build/lib/python3.11/site-packages/pip/_internal/network/auth.py:509:                    password=password,
./.venv-build/lib/python3.11/site-packages/pip/_internal/network/auth.py:519:        # Add our new username and password to the request
./.venv-build/lib/python3.11/site-packages/pip/_internal/network/auth.py:520:        req = HTTPBasicAuth(username or "", password or "")(resp.request)
./.venv-build/lib/python3.11/site-packages/pip/_internal/network/auth.py:552:                self.keyring_provider.save_auth_info(creds.url, creds.username, creds.password)
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/driver.py:31:from blib2to3.pgen2.tokenize import TokenInfo
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/driver.py:35:from . import grammar, parse, pgen, token, tokenize
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/driver.py:44:    tokens: list[Any] = field(default_factory=list)
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/driver.py:47:        total_eaten = len(self.tokens)
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/driver.py:51:class TokenProxy:
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/driver.py:53:        self._tokens = generator
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/driver.py:58:    def release(self) -> Iterator["TokenProxy"]:
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/driver.py:69:        eaten_tokens = self._release_ranges[-1].tokens
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/driver.py:70:        if point < len(eaten_tokens):
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/driver.py:71:            return eaten_tokens[point]
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/driver.py:73:            while point >= len(eaten_tokens):
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/driver.py:74:                token = next(self._tokens)
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/driver.py:75:                eaten_tokens.append(token)
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/driver.py:76:            return token
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/driver.py:78:    def __iter__(self) -> "TokenProxy":
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/driver.py:83:        # return the eaten token, if not just go further on the given
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/driver.py:84:        # token producer.
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/driver.py:90:                token = release_range.tokens[self._counter - start]
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/driver.py:93:            token = next(self._tokens)
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/driver.py:95:        return token
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/driver.py:115:    def parse_tokens(self, tokens: Iterable[TokenInfo], debug: bool = False) -> NL:
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/driver.py:116:        """Parse a series of tokens and return the syntax tree."""
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/driver.py:117:        # XXX Move the prefix computation into a wrapper around tokenize.
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/driver.py:118:        proxy = TokenProxy(tokens)
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/driver.py:141:            if type in (tokenize.COMMENT, tokenize.NL):
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/driver.py:148:            if type == token.OP:
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/driver.py:152:                self.logger.debug("%s %r (prefix=%r)", token.tok_name[type], value, prefix)
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/driver.py:153:            if type == token.INDENT:
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/driver.py:158:            elif type == token.DEDENT:
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/driver.py:161:            if p.addtoken(cast(int, type), value, (prefix, start)):
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/driver.py:166:            if type in {token.INDENT, token.DEDENT}:
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/driver.py:169:            # FSTRING_MIDDLE is the only token that can end with a newline, and
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/driver.py:171:            if value.endswith("\n") and type != token.FSTRING_MIDDLE:
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/driver.py:189:        tokens = tokenize.tokenize(text, grammar=self.grammar)
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/driver.py:190:        return self.parse_tokens(tokens, debug)
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:6:"""Tokenization help for Python programs.
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:8:generate_tokens(readline) is a generator that breaks a stream of
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:9:text into Python tokens.  It accepts a readline-like method which is called
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:13:    the token type (see token.py)
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:14:    the token (a string)
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:15:    the starting (row, column) indices of the token (a 2-tuple of ints)
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:16:    the ending (row, column) indices of the token (a 2-tuple of ints)
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:19:It is designed to match the working of the Python tokenizer exactly, except
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:20:that it produces COMMENT tokens for comments and gives type OP for all
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:24:    tokenize_loop(readline, tokeneater)
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:25:    tokenize(readline, tokeneater=printtoken)
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:26:are the same, except instead of generating tokens, tokeneater is a callback
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:28:each time a new token is found."""
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:35:from blib2to3.pgen2.token import (
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:41:    ERRORTOKEN,
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:58:import pytokens
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:59:from pytokens import TokenType
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:61:from . import token as _token
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:63:__all__ = [x for x in dir(_token) if x[0] != "_"] + [
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:64:    "tokenize",
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:65:    "generate_tokens",
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:66:    "untokenize",
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:68:del _token
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:71:TokenInfo = tuple[int, str, Coord, Coord, str]
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:73:TOKEN_TYPE_MAP = {
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:74:    TokenType.indent: INDENT,
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:75:    TokenType.dedent: DEDENT,
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:76:    TokenType.newline: NEWLINE,
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:77:    TokenType.nl: NL,
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:78:    TokenType.comment: COMMENT,
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:79:    TokenType.semicolon: OP,
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:80:    TokenType.lparen: OP,
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:81:    TokenType.rparen: OP,
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:82:    TokenType.lbracket: OP,
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:83:    TokenType.rbracket: OP,
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:84:    TokenType.lbrace: OP,
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:85:    TokenType.rbrace: OP,
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:86:    TokenType.colon: OP,
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:87:    TokenType.op: OP,
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:88:    TokenType.identifier: NAME,
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:89:    TokenType.number: NUMBER,
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:90:    TokenType.string: STRING,
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:91:    TokenType.fstring_start: FSTRING_START,
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:92:    TokenType.fstring_middle: FSTRING_MIDDLE,
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:93:    TokenType.fstring_end: FSTRING_END,
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:94:    TokenType.endmarker: ENDMARKER,
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:98:class TokenError(Exception): ...
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:102:    token: pytokens.Token, source: str, prev_token: Optional[pytokens.Token]
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:103:) -> pytokens.Token:
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:105:    Black treats `\\\n` at the end of a line as a 'NL' token, while it
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:111:        token.type == TokenType.whitespace
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:112:        and prev_token is not None
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:113:        and prev_token.type not in (TokenType.nl, TokenType.newline)
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:115:        token_str = source[token.start_index : token.end_index]
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:116:        if token_str.startswith("\\\r\n"):
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:117:            return pytokens.Token(
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:118:                TokenType.nl,
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:119:                token.start_index,
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:120:                token.start_index + 3,
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:121:                token.start_line,
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:122:                token.start_col,
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:123:                token.start_line,
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:124:                token.start_col + 3,
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:126:        elif token_str.startswith("\\\n") or token_str.startswith("\\\r"):
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:127:            return pytokens.Token(
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:128:                TokenType.nl,
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:129:                token.start_index,
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:130:                token.start_index + 2,
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:131:                token.start_line,
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:132:                token.start_col,
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:133:                token.start_line,
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:134:                token.start_col + 2,
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:137:    return token
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:140:def tokenize(source: str, grammar: Optional[Grammar] = None) -> Iterator[TokenInfo]:
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:142:    lines += [""]  # For newline tokens in files that don't end in a newline
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:145:    prev_token: Optional[pytokens.Token] = None
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:147:        for token in pytokens.tokenize(source):
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:148:            token = transform_whitespace(token, source, prev_token)
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:150:            line, column = token.start_line, token.start_col
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:151:            if token.type == TokenType.whitespace:
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:154:            token_str = source[token.start_index : token.end_index]
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:156:            if token.type == TokenType.newline and token_str == "":
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:157:                # Black doesn't yield empty newline tokens at the end of a file
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:159:                prev_token = token
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:162:            source_line = lines[token.start_line - 1]
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:164:            if token.type == TokenType.identifier and token_str in ("async", "await"):
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:165:                # Black uses `async` and `await` token types just for those two keywords
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:167:                    ASYNC if token_str == "async" else AWAIT,
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:168:                    token_str,
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:169:                    (token.start_line, token.start_col),
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:170:                    (token.end_line, token.end_col),
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:173:            elif token.type == TokenType.op and token_str == "...":
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:174:                # Black doesn't have an ellipsis token yet, yield 3 DOTs instead
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:175:                assert token.start_line == token.end_line
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:176:                assert token.end_col == token.start_col + 3
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:178:                token_str = "."
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:179:                for start_col in range(token.start_col, token.start_col + 3):
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:182:                        TOKEN_TYPE_MAP[token.type],
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:183:                        token_str,
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:184:                        (token.start_line, start_col),
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:185:                        (token.end_line, end_col),
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:190:                    TOKEN_TYPE_MAP[token.type],
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:191:                    token_str,
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:192:                    (token.start_line, token.start_col),
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:193:                    (token.end_line, token.end_col),
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:196:            prev_token = token
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:198:    except pytokens.UnexpectedEOF:
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:199:        raise TokenError("Unexpected EOF in multi-line statement", (line, column))
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:200:    except pytokens.TokenizeError as exc:
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:201:        raise TokenError(f"Failed to parse: {type(exc).__name__}", (line, column))
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:204:def printtoken(
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:205:    type: int, token: str, srow_col: Coord, erow_col: Coord, line: str
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:209:    print(f"{srow},{scol}-{erow},{ecol}:\t{tok_name[type]}\t{token!r}")
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:214:        token_iterator = tokenize(open(sys.argv[1]).read())
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:216:        token_iterator = tokenize(sys.stdin.read())
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:218:    for tok in token_iterator:
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/tokenize.py:219:        printtoken(*tok)
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/grammar.py:10:token module; the Python tokenize module reports all operators as the
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/grammar.py:11:fallback token code OP, but the parser needs the actual token code.
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/grammar.py:22:from . import token
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/grammar.py:50:                     them from token numbers, which are between 0 and
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/grammar.py:66:                     above, and first is a set of tokens that can
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/grammar.py:70:    labels        -- a list of (x, y) pairs where x is either a token
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/grammar.py:81:    tokens        -- a dict mapping token numbers to arc labels.
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/grammar.py:93:        self.tokens: dict[int, int] = {}
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/grammar.py:140:            "tokens",
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/grammar.py:168:# Map from operator to number (since tokenize doesn't do this)
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/grammar.py:226:        opmap[op] = getattr(token, name)
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/token.py:1:"""Token constants (from "token.h")."""
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/token.py:5:#  Taken from Python (r53757) and modified to include some tokens
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/token.py:6:#   originally monkeypatched in by pgen2.tokenize
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/token.py:67:ERRORTOKEN: Final = 58
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/token.py:73:N_TOKENS: Final = 64
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/conv.py:14:Note that the token numbers are constants determined by the standard
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/conv.py:15:Python tokenizer.  The standard token module defines these numbers and
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/conv.py:16:their names (the names are not used much).  The token numbers are
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/conv.py:17:hardcoded into the Python tokenizer and into pgen.  A Python
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/conv.py:18:implementation of the Python tokenizer is also available, in the
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/conv.py:19:standard tokenize module.
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/conv.py:35:from pgen2 import grammar, token
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/conv.py:251:        self.tokens = {}  # map from numeric token values to arc labels
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/conv.py:253:            if type == token.NAME and value is not None:
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/conv.py:256:                self.tokens[type] = ilabel
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/pgen.py:8:from blib2to3.pgen2 import grammar, token, tokenize
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/pgen.py:9:from blib2to3.pgen2.tokenize import TokenInfo
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/pgen.py:21:    generator: Iterator[TokenInfo]
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/pgen.py:30:        self.generator = tokenize.tokenize(stream.read())
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/pgen.py:31:        self.gettoken()  # Initialize lookahead
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/pgen.py:35:        self.first = {}  # map from symbol name to set of tokens
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/pgen.py:77:            # Either a symbol name or a named token
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/pgen.py:87:                # A named token (NAME, NUMBER, STRING)
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/pgen.py:88:                itoken = getattr(token, label, None)
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/pgen.py:89:                assert isinstance(itoken, int), label
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/pgen.py:90:                assert itoken in token.tok_name, label
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/pgen.py:91:                if itoken in c.tokens:
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/pgen.py:92:                    return c.tokens[itoken]
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/pgen.py:94:                    c.labels.append((itoken, None))
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/pgen.py:95:                    c.tokens[itoken] = ilabel
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/pgen.py:111:                    c.labels.append((token.NAME, value))
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/pgen.py:115:                # An operator (any non-numeric token)
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/pgen.py:116:                itoken = grammar.opmap[value]  # Fails if unknown token
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/pgen.py:117:                if itoken in c.tokens:
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/pgen.py:118:                    return c.tokens[itoken]
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/pgen.py:120:                    c.labels.append((itoken, None))
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/pgen.py:121:                    c.tokens[itoken] = ilabel
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/pgen.py:168:        while self.type != token.ENDMARKER:
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/pgen.py:169:            while self.type == token.NEWLINE:
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/pgen.py:170:                self.gettoken()
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/pgen.py:172:            name = self.expect(token.NAME)
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/pgen.py:173:            self.expect(token.OP, ":")
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/pgen.py:175:            self.expect(token.NEWLINE)
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/pgen.py:283:                self.gettoken()
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/pgen.py:292:        while self.value in ("(", "[") or self.type in (token.NAME, token.STRING):
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/pgen.py:301:            self.gettoken()
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/pgen.py:303:            self.expect(token.OP, "]")
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/pgen.py:311:            self.gettoken()
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/pgen.py:321:            self.gettoken()
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/pgen.py:323:            self.expect(token.OP, ")")
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/pgen.py:325:        elif self.type in (token.NAME, token.STRING):
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/pgen.py:329:            self.gettoken()
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/pgen.py:338:        self.gettoken()
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/pgen.py:341:    def gettoken(self) -> None:
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/pgen.py:343:        while tup[0] in (tokenize.COMMENT, tokenize.NL):
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/pgen.py:346:        # print token.tok_name[self.type], repr(self.value)
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/parse.py:20:from . import grammar, token, tokenize
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/parse.py:23:    from blib2to3.pgen2.driver import TokenProxy
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/parse.py:91:    def add_token(self, tok_type: int, tok_val: str, raw: bool = False) -> None:
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/parse.py:95:                    self.parser._addtoken(ilabel, tok_type, tok_val, self.context)
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/parse.py:97:                    self.parser.addtoken(tok_type, tok_val, self.context)
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/parse.py:132:    <for each input token>:
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/parse.py:133:        if p.addtoken(...):           # parse a token; may raise ParseError
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/parse.py:139:    A Parser instance contains state pertaining to the current token
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/parse.py:141:    to parse separate token sequences.
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/parse.py:143:    See driver.py for how to get input tokens by tokenizing a file or
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/parse.py:146:    Parsing is complete when addtoken() returns True; the root of the
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/parse.py:148:    instance variable.  When a syntax error occurs, addtoken() raises
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/parse.py:179:        tuple, where type is the node type (a token or symbol number),
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/parse.py:180:        value is None for symbols and a string for tokens, context is
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/parse.py:183:        symbols, and None for tokens.
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/parse.py:193:        self.last_token: Optional[int] = None
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/parse.py:195:    def setup(self, proxy: "TokenProxy", start: Optional[int] = None) -> None:
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/parse.py:219:        self.last_token = None
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/parse.py:221:    def addtoken(self, type: int, value: str, context: Context) -> bool:
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/parse.py:222:        """Add a token; return True iff this is the end of the program."""
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/parse.py:223:        # Map from token to label
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/parse.py:231:            return self._addtoken(ilabel, type, value, context)
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/parse.py:245:            recorder.add_token(type, value, raw=True)
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/parse.py:247:            next_token_value = value
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/parse.py:248:            while recorder.determine_route(next_token_value) is None:
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/parse.py:253:                next_token_type, next_token_value, *_ = proxy.eat(counter)
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/parse.py:254:                if next_token_type in (tokenize.COMMENT, tokenize.NL):
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/parse.py:258:                if next_token_type == tokenize.OP:
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/parse.py:259:                    next_token_type = grammar.opmap[next_token_value]
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/parse.py:261:                recorder.add_token(next_token_type, next_token_value)
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/parse.py:264:            ilabel = cast(int, recorder.determine_route(next_token_value, force=force))
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/parse.py:267:        return self._addtoken(ilabel, type, value, context)
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/parse.py:269:    def _addtoken(self, ilabel: int, type: int, value: str, context: Context) -> bool:
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/parse.py:270:        # Loop until the token is shifted; may raise exceptions
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/parse.py:289:                    # Shift a token; we're done with it
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/parse.py:300:                    # Done with this token
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/parse.py:301:                    self.last_token = type
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/parse.py:309:                        # Done parsing, but another token is input
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/parse.py:316:        """Turn a token into a label.  (Internal)
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/parse.py:320:        if type == token.NAME:
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/parse.py:327:                assert type in self.grammar.tokens
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/parse.py:333:                if self.last_token not in (
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/parse.py:335:                    token.INDENT,
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/parse.py:336:                    token.DEDENT,
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/parse.py:337:                    token.NEWLINE,
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/parse.py:338:                    token.SEMI,
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/parse.py:339:                    token.COLON,
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/parse.py:341:                    return [self.grammar.tokens[type]]
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/parse.py:343:                    self.grammar.tokens[type],
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/parse.py:347:        ilabel = self.grammar.tokens.get(type)
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/parse.py:349:            raise ParseError("bad token", type, value, context)
./.venv-build/lib/python3.11/site-packages/blib2to3/pgen2/parse.py:353:        """Shift a token.  (Internal)"""
./.venv-build/lib/python3.11/site-packages/blib2to3/pytree.py:7:This is a very concrete parse tree; we need to keep every token and
./.venv-build/lib/python3.11/site-packages/blib2to3/pytree.py:8:even the comments and whitespace between tokens.
./.venv-build/lib/python3.11/site-packages/blib2to3/pytree.py:38:        # printing tokens is possible but not as useful
./.venv-build/lib/python3.11/site-packages/blib2to3/pytree.py:39:        # from .pgen2 import token // token.__dict__.items():
./.venv-build/lib/python3.11/site-packages/blib2to3/pytree.py:65:    type: int  # int: token number (< 256) or symbol number (>= 256)
./.venv-build/lib/python3.11/site-packages/blib2to3/pytree.py:378:    _prefix = ""  # Whitespace and comments preceding this token in the input
./.venv-build/lib/python3.11/site-packages/blib2to3/pytree.py:379:    lineno: int = 0  # Line where this token starts in the input
./.venv-build/lib/python3.11/site-packages/blib2to3/pytree.py:380:    column: int = 0  # Column where this token starts in the input
./.venv-build/lib/python3.11/site-packages/blib2to3/pytree.py:399:        Takes a type constant (a token number < 256), a string value, and an
./.venv-build/lib/python3.11/site-packages/blib2to3/pytree.py:417:        from .pgen2.token import tok_name
./.venv-build/lib/python3.11/site-packages/blib2to3/pytree.py:460:        The whitespace and comments preceding this token in the input.
./.venv-build/lib/python3.11/site-packages/blib2to3/pytree.py:497:    It looks for a specific node type (token or symbol), and
./.venv-build/lib/python3.11/site-packages/blib2to3/pytree.py:510:    type = None  # Node type (token if < 256, symbol if >= 256)
./.venv-build/lib/python3.11/site-packages/blib2to3/pytree.py:594:        The type, if given must be a token type (< 256).  If not given,
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/markers.py:21:from ._tokenizer import ParserSyntaxError
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_tokenizer.py:10:class Token:
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_tokenizer.py:88:class Tokenizer:
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_tokenizer.py:89:    """Context-sensitive token parsing.
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_tokenizer.py:91:    Provides methods to examine the input stream to check whether the next token
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_tokenizer.py:105:        self.next_token: Optional[Token] = None
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_tokenizer.py:109:        """Move beyond provided token name, if at current position."""
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_tokenizer.py:114:        """Check whether the next token has the provided name.
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_tokenizer.py:116:        By default, if the check succeeds, the token *must* be read before
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_tokenizer.py:117:        another check. If `peek` is set to `True`, the token is not loaded and
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_tokenizer.py:121:            self.next_token is None
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_tokenizer.py:122:        ), f"Cannot check for {name!r}, already have {self.next_token!r}"
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_tokenizer.py:123:        assert name in self.rules, f"Unknown token name: {name!r}"
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_tokenizer.py:131:            self.next_token = Token(name, match[0], self.position)
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_tokenizer.py:134:    def expect(self, name: str, *, expected: str) -> Token:
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_tokenizer.py:135:        """Expect a certain token name next, failing with a syntax error otherwise.
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_tokenizer.py:137:        The token is *not* read.
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_tokenizer.py:143:    def read(self) -> Token:
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_tokenizer.py:144:        """Consume the next token and return it."""
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_tokenizer.py:145:        token = self.next_token
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_tokenizer.py:146:        assert token is not None
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_tokenizer.py:148:        self.position += len(token.text)
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_tokenizer.py:149:        self.next_token = None
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_tokenizer.py:151:        return token
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_tokenizer.py:172:    def enclosing_tokens(self, open_token: str, close_token: str, *, around: str) -> Iterator[None]:
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_tokenizer.py:173:        if self.check(open_token):
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_tokenizer.py:184:        if not self.check(close_token):
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_tokenizer.py:186:                f"Expected matching {close_token} for {open_token}, after {around}",
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:10:from ._tokenizer import DEFAULT_RULES, Tokenizer
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:64:    return _parse_requirement(Tokenizer(source, rules=DEFAULT_RULES))
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:67:def _parse_requirement(tokenizer: Tokenizer) -> ParsedRequirement:
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:71:    tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:73:    name_token = tokenizer.expect(
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:76:    name = name_token.text
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:77:    tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:79:    extras = _parse_extras(tokenizer)
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:80:    tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:82:    url, specifier, marker = _parse_requirement_details(tokenizer)
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:83:    tokenizer.expect("END", expected="end of dependency specifier")
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:89:    tokenizer: Tokenizer,
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:100:    if tokenizer.check("AT"):
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:101:        tokenizer.read()
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:102:        tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:104:        url_start = tokenizer.position
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:105:        url = tokenizer.expect("URL", expected="URL after @").text
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:106:        if tokenizer.check("END", peek=True):
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:109:        tokenizer.expect("WS", expected="whitespace after URL")
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:112:        if tokenizer.check("END", peek=True):
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:116:            tokenizer, span_start=url_start, after="URL and whitespace"
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:119:        specifier_start = tokenizer.position
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:120:        specifier = _parse_specifier(tokenizer)
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:121:        tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:123:        if tokenizer.check("END", peek=True):
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:127:            tokenizer,
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:135:def _parse_requirement_marker(tokenizer: Tokenizer, *, span_start: int, after: str) -> MarkerList:
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:140:    if not tokenizer.check("SEMICOLON"):
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:141:        tokenizer.raise_syntax_error(
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:145:    tokenizer.read()
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:147:    marker = _parse_marker(tokenizer)
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:148:    tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:153:def _parse_extras(tokenizer: Tokenizer) -> List[str]:
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:157:    if not tokenizer.check("LEFT_BRACKET", peek=True):
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:160:    with tokenizer.enclosing_tokens(
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:165:        tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:166:        extras = _parse_extras_list(tokenizer)
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:167:        tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:172:def _parse_extras_list(tokenizer: Tokenizer) -> List[str]:
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:178:    if not tokenizer.check("IDENTIFIER"):
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:181:    extras.append(tokenizer.read().text)
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:184:        tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:185:        if tokenizer.check("IDENTIFIER", peek=True):
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:186:            tokenizer.raise_syntax_error("Expected comma between extra names")
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:187:        elif not tokenizer.check("COMMA"):
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:190:        tokenizer.read()
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:191:        tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:193:        extra_token = tokenizer.expect("IDENTIFIER", expected="extra name after comma")
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:194:        extras.append(extra_token.text)
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:199:def _parse_specifier(tokenizer: Tokenizer) -> str:
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:204:    with tokenizer.enclosing_tokens(
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:209:        tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:210:        parsed_specifiers = _parse_version_many(tokenizer)
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:211:        tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:216:def _parse_version_many(tokenizer: Tokenizer) -> str:
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:221:    while tokenizer.check("SPECIFIER"):
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:222:        span_start = tokenizer.position
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:223:        parsed_specifiers += tokenizer.read().text
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:224:        if tokenizer.check("VERSION_PREFIX_TRAIL", peek=True):
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:225:            tokenizer.raise_syntax_error(
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:228:                span_end=tokenizer.position + 1,
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:230:        if tokenizer.check("VERSION_LOCAL_LABEL_TRAIL", peek=True):
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:231:            tokenizer.raise_syntax_error(
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:234:                span_end=tokenizer.position,
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:236:        tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:237:        if not tokenizer.check("COMMA"):
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:239:        parsed_specifiers += tokenizer.read().text
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:240:        tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:249:    return _parse_full_marker(Tokenizer(source, rules=DEFAULT_RULES))
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:252:def _parse_full_marker(tokenizer: Tokenizer) -> MarkerList:
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:253:    retval = _parse_marker(tokenizer)
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:254:    tokenizer.expect("END", expected="end of marker expression")
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:258:def _parse_marker(tokenizer: Tokenizer) -> MarkerList:
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:262:    expression = [_parse_marker_atom(tokenizer)]
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:263:    while tokenizer.check("BOOLOP"):
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:264:        token = tokenizer.read()
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:265:        expr_right = _parse_marker_atom(tokenizer)
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:266:        expression.extend((token.text, expr_right))
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:270:def _parse_marker_atom(tokenizer: Tokenizer) -> MarkerAtom:
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:276:    tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:277:    if tokenizer.check("LEFT_PARENTHESIS", peek=True):
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:278:        with tokenizer.enclosing_tokens(
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:283:            tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:284:            marker: MarkerAtom = _parse_marker(tokenizer)
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:285:            tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:287:        marker = _parse_marker_item(tokenizer)
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:288:    tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:292:def _parse_marker_item(tokenizer: Tokenizer) -> MarkerItem:
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:296:    tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:297:    marker_var_left = _parse_marker_var(tokenizer)
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:298:    tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:299:    marker_op = _parse_marker_op(tokenizer)
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:300:    tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:301:    marker_var_right = _parse_marker_var(tokenizer)
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:302:    tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:306:def _parse_marker_var(tokenizer: Tokenizer) -> MarkerVar:
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:310:    if tokenizer.check("VARIABLE"):
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:311:        return process_env_var(tokenizer.read().text.replace(".", "_"))
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:312:    elif tokenizer.check("QUOTED_STRING"):
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:313:        return process_python_str(tokenizer.read().text)
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:315:        tokenizer.raise_syntax_error(message="Expected a marker variable or quoted string")
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:330:def _parse_marker_op(tokenizer: Tokenizer) -> Op:
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:334:    if tokenizer.check("IN"):
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:335:        tokenizer.read()
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:337:    elif tokenizer.check("NOT"):
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:338:        tokenizer.read()
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:339:        tokenizer.expect("WS", expected="whitespace after 'not'")
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:340:        tokenizer.expect("IN", expected="'in' after 'not'")
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:342:    elif tokenizer.check("OP"):
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:343:        return Op(tokenizer.read().text)
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/_parser.py:345:        return tokenizer.raise_syntax_error(
./.venv-build/lib/python3.11/site-packages/wheel/vendored/packaging/requirements.py:8:from ._tokenizer import ParserSyntaxError
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/markers.py:15:from ._tokenizer import ParserSyntaxError
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_tokenizer.py:12:class Token:
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_tokenizer.py:90:class Tokenizer:
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_tokenizer.py:91:    """Context-sensitive token parsing.
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_tokenizer.py:93:    Provides methods to examine the input stream to check whether the next token
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_tokenizer.py:107:        self.next_token: Token | None = None
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_tokenizer.py:111:        """Move beyond provided token name, if at current position."""
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_tokenizer.py:116:        """Check whether the next token has the provided name.
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_tokenizer.py:118:        By default, if the check succeeds, the token *must* be read before
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_tokenizer.py:119:        another check. If `peek` is set to `True`, the token is not loaded and
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_tokenizer.py:123:            self.next_token is None
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_tokenizer.py:124:        ), f"Cannot check for {name!r}, already have {self.next_token!r}"
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_tokenizer.py:125:        assert name in self.rules, f"Unknown token name: {name!r}"
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_tokenizer.py:133:            self.next_token = Token(name, match[0], self.position)
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_tokenizer.py:136:    def expect(self, name: str, *, expected: str) -> Token:
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_tokenizer.py:137:        """Expect a certain token name next, failing with a syntax error otherwise.
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_tokenizer.py:139:        The token is *not* read.
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_tokenizer.py:145:    def read(self) -> Token:
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_tokenizer.py:146:        """Consume the next token and return it."""
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_tokenizer.py:147:        token = self.next_token
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_tokenizer.py:148:        assert token is not None
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_tokenizer.py:150:        self.position += len(token.text)
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_tokenizer.py:151:        self.next_token = None
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_tokenizer.py:153:        return token
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_tokenizer.py:174:    def enclosing_tokens(self, open_token: str, close_token: str, *, around: str) -> Iterator[None]:
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_tokenizer.py:175:        if self.check(open_token):
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_tokenizer.py:186:        if not self.check(close_token):
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_tokenizer.py:188:                f"Expected matching {close_token} for {open_token}, after {around}",
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:12:from ._tokenizer import DEFAULT_RULES, Tokenizer
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:62:    return _parse_requirement(Tokenizer(source, rules=DEFAULT_RULES))
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:65:def _parse_requirement(tokenizer: Tokenizer) -> ParsedRequirement:
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:69:    tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:71:    name_token = tokenizer.expect(
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:74:    name = name_token.text
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:75:    tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:77:    extras = _parse_extras(tokenizer)
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:78:    tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:80:    url, specifier, marker = _parse_requirement_details(tokenizer)
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:81:    tokenizer.expect("END", expected="end of dependency specifier")
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:87:    tokenizer: Tokenizer,
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:98:    if tokenizer.check("AT"):
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:99:        tokenizer.read()
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:100:        tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:102:        url_start = tokenizer.position
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:103:        url = tokenizer.expect("URL", expected="URL after @").text
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:104:        if tokenizer.check("END", peek=True):
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:107:        tokenizer.expect("WS", expected="whitespace after URL")
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:110:        if tokenizer.check("END", peek=True):
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:114:            tokenizer, span_start=url_start, after="URL and whitespace"
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:117:        specifier_start = tokenizer.position
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:118:        specifier = _parse_specifier(tokenizer)
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:119:        tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:121:        if tokenizer.check("END", peek=True):
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:125:            tokenizer,
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:133:def _parse_requirement_marker(tokenizer: Tokenizer, *, span_start: int, after: str) -> MarkerList:
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:138:    if not tokenizer.check("SEMICOLON"):
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:139:        tokenizer.raise_syntax_error(
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:143:    tokenizer.read()
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:145:    marker = _parse_marker(tokenizer)
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:146:    tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:151:def _parse_extras(tokenizer: Tokenizer) -> list[str]:
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:155:    if not tokenizer.check("LEFT_BRACKET", peek=True):
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:158:    with tokenizer.enclosing_tokens(
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:163:        tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:164:        extras = _parse_extras_list(tokenizer)
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:165:        tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:170:def _parse_extras_list(tokenizer: Tokenizer) -> list[str]:
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:176:    if not tokenizer.check("IDENTIFIER"):
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:179:    extras.append(tokenizer.read().text)
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:182:        tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:183:        if tokenizer.check("IDENTIFIER", peek=True):
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:184:            tokenizer.raise_syntax_error("Expected comma between extra names")
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:185:        elif not tokenizer.check("COMMA"):
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:188:        tokenizer.read()
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:189:        tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:191:        extra_token = tokenizer.expect("IDENTIFIER", expected="extra name after comma")
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:192:        extras.append(extra_token.text)
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:197:def _parse_specifier(tokenizer: Tokenizer) -> str:
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:202:    with tokenizer.enclosing_tokens(
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:207:        tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:208:        parsed_specifiers = _parse_version_many(tokenizer)
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:209:        tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:214:def _parse_version_many(tokenizer: Tokenizer) -> str:
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:219:    while tokenizer.check("SPECIFIER"):
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:220:        span_start = tokenizer.position
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:221:        parsed_specifiers += tokenizer.read().text
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:222:        if tokenizer.check("VERSION_PREFIX_TRAIL", peek=True):
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:223:            tokenizer.raise_syntax_error(
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:226:                span_end=tokenizer.position + 1,
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:228:        if tokenizer.check("VERSION_LOCAL_LABEL_TRAIL", peek=True):
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:229:            tokenizer.raise_syntax_error(
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:232:                span_end=tokenizer.position,
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:234:        tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:235:        if not tokenizer.check("COMMA"):
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:237:        parsed_specifiers += tokenizer.read().text
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:238:        tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:247:    return _parse_full_marker(Tokenizer(source, rules=DEFAULT_RULES))
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:250:def _parse_full_marker(tokenizer: Tokenizer) -> MarkerList:
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:251:    retval = _parse_marker(tokenizer)
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:252:    tokenizer.expect("END", expected="end of marker expression")
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:256:def _parse_marker(tokenizer: Tokenizer) -> MarkerList:
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:260:    expression = [_parse_marker_atom(tokenizer)]
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:261:    while tokenizer.check("BOOLOP"):
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:262:        token = tokenizer.read()
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:263:        expr_right = _parse_marker_atom(tokenizer)
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:264:        expression.extend((token.text, expr_right))
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:268:def _parse_marker_atom(tokenizer: Tokenizer) -> MarkerAtom:
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:274:    tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:275:    if tokenizer.check("LEFT_PARENTHESIS", peek=True):
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:276:        with tokenizer.enclosing_tokens(
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:281:            tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:282:            marker: MarkerAtom = _parse_marker(tokenizer)
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:283:            tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:285:        marker = _parse_marker_item(tokenizer)
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:286:    tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:290:def _parse_marker_item(tokenizer: Tokenizer) -> MarkerItem:
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:294:    tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:295:    marker_var_left = _parse_marker_var(tokenizer)
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:296:    tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:297:    marker_op = _parse_marker_op(tokenizer)
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:298:    tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:299:    marker_var_right = _parse_marker_var(tokenizer)
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:300:    tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:304:def _parse_marker_var(tokenizer: Tokenizer) -> MarkerVar:
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:308:    if tokenizer.check("VARIABLE"):
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:309:        return process_env_var(tokenizer.read().text.replace(".", "_"))
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:310:    elif tokenizer.check("QUOTED_STRING"):
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:311:        return process_python_str(tokenizer.read().text)
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:313:        tokenizer.raise_syntax_error(message="Expected a marker variable or quoted string")
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:328:def _parse_marker_op(tokenizer: Tokenizer) -> Op:
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:332:    if tokenizer.check("IN"):
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:333:        tokenizer.read()
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:335:    elif tokenizer.check("NOT"):
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:336:        tokenizer.read()
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:337:        tokenizer.expect("WS", expected="whitespace after 'not'")
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:338:        tokenizer.expect("IN", expected="'in' after 'not'")
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:340:    elif tokenizer.check("OP"):
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:341:        return Op(tokenizer.read().text)
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py:343:        return tokenizer.raise_syntax_error(
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/licenses/__init__.py:67:    # Pad any parentheses so tokenization can be achieved by merely splitting on
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/licenses/__init__.py:81:    tokens = license_expression.split()
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/licenses/__init__.py:86:    python_tokens = []
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/licenses/__init__.py:87:    for token in tokens:
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/licenses/__init__.py:88:        if token not in {"or", "and", "with", "(", ")"}:
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/licenses/__init__.py:89:            python_tokens.append("False")
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/licenses/__init__.py:90:        elif token == "with":
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/licenses/__init__.py:91:            python_tokens.append("or")
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/licenses/__init__.py:92:        elif token == "(" and python_tokens and python_tokens[-1] not in {"or", "and"}:
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/licenses/__init__.py:96:            python_tokens.append(token)
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/licenses/__init__.py:98:    python_expression = " ".join(python_tokens)
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/licenses/__init__.py:109:    normalized_tokens = []
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/licenses/__init__.py:110:    for token in tokens:
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/licenses/__init__.py:111:        if token in {"or", "and", "with", "(", ")"}:
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/licenses/__init__.py:112:            normalized_tokens.append(token.upper())
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/licenses/__init__.py:115:        if normalized_tokens and normalized_tokens[-1] == "WITH":
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/licenses/__init__.py:116:            if token not in EXCEPTIONS:
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/licenses/__init__.py:117:                message = f"Unknown license exception: {token!r}"
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/licenses/__init__.py:120:            normalized_tokens.append(EXCEPTIONS[token]["id"])
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/licenses/__init__.py:122:            if token.endswith("+"):
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/licenses/__init__.py:123:                final_token = token[:-1]
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/licenses/__init__.py:126:                final_token = token
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/licenses/__init__.py:129:            if final_token.startswith("licenseref-"):
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/licenses/__init__.py:130:                if not license_ref_allowed.match(final_token):
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/licenses/__init__.py:131:                    message = f"Invalid licenseref: {final_token!r}"
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/licenses/__init__.py:133:                normalized_tokens.append(license_refs[final_token] + suffix)
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/licenses/__init__.py:135:                if final_token not in LICENSES:
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/licenses/__init__.py:136:                    message = f"Unknown license: {final_token!r}"
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/licenses/__init__.py:138:                normalized_tokens.append(LICENSES[final_token]["id"] + suffix)
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/licenses/__init__.py:140:    normalized_expression = " ".join(normalized_tokens)
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/packaging/requirements.py:9:from ._tokenizer import ParserSyntaxError
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/markers.py:21:from ._tokenizer import ParserSyntaxError
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_tokenizer.py:10:class Token:
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_tokenizer.py:88:class Tokenizer:
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_tokenizer.py:89:    """Context-sensitive token parsing.
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_tokenizer.py:91:    Provides methods to examine the input stream to check whether the next token
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_tokenizer.py:105:        self.next_token: Optional[Token] = None
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_tokenizer.py:109:        """Move beyond provided token name, if at current position."""
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_tokenizer.py:114:        """Check whether the next token has the provided name.
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_tokenizer.py:116:        By default, if the check succeeds, the token *must* be read before
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_tokenizer.py:117:        another check. If `peek` is set to `True`, the token is not loaded and
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_tokenizer.py:121:            self.next_token is None
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_tokenizer.py:122:        ), f"Cannot check for {name!r}, already have {self.next_token!r}"
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_tokenizer.py:123:        assert name in self.rules, f"Unknown token name: {name!r}"
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_tokenizer.py:131:            self.next_token = Token(name, match[0], self.position)
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_tokenizer.py:134:    def expect(self, name: str, *, expected: str) -> Token:
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_tokenizer.py:135:        """Expect a certain token name next, failing with a syntax error otherwise.
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_tokenizer.py:137:        The token is *not* read.
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_tokenizer.py:143:    def read(self) -> Token:
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_tokenizer.py:144:        """Consume the next token and return it."""
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_tokenizer.py:145:        token = self.next_token
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_tokenizer.py:146:        assert token is not None
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_tokenizer.py:148:        self.position += len(token.text)
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_tokenizer.py:149:        self.next_token = None
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_tokenizer.py:151:        return token
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_tokenizer.py:172:    def enclosing_tokens(self, open_token: str, close_token: str, *, around: str) -> Iterator[None]:
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_tokenizer.py:173:        if self.check(open_token):
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_tokenizer.py:184:        if not self.check(close_token):
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_tokenizer.py:186:                f"Expected matching {close_token} for {open_token}, after {around}",
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:10:from ._tokenizer import DEFAULT_RULES, Tokenizer
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:64:    return _parse_requirement(Tokenizer(source, rules=DEFAULT_RULES))
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:67:def _parse_requirement(tokenizer: Tokenizer) -> ParsedRequirement:
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:71:    tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:73:    name_token = tokenizer.expect(
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:76:    name = name_token.text
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:77:    tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:79:    extras = _parse_extras(tokenizer)
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:80:    tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:82:    url, specifier, marker = _parse_requirement_details(tokenizer)
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:83:    tokenizer.expect("END", expected="end of dependency specifier")
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:89:    tokenizer: Tokenizer,
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:100:    if tokenizer.check("AT"):
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:101:        tokenizer.read()
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:102:        tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:104:        url_start = tokenizer.position
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:105:        url = tokenizer.expect("URL", expected="URL after @").text
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:106:        if tokenizer.check("END", peek=True):
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:109:        tokenizer.expect("WS", expected="whitespace after URL")
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:112:        if tokenizer.check("END", peek=True):
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:116:            tokenizer, span_start=url_start, after="URL and whitespace"
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:119:        specifier_start = tokenizer.position
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:120:        specifier = _parse_specifier(tokenizer)
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:121:        tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:123:        if tokenizer.check("END", peek=True):
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:127:            tokenizer,
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:135:def _parse_requirement_marker(tokenizer: Tokenizer, *, span_start: int, after: str) -> MarkerList:
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:140:    if not tokenizer.check("SEMICOLON"):
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:141:        tokenizer.raise_syntax_error(
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:145:    tokenizer.read()
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:147:    marker = _parse_marker(tokenizer)
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:148:    tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:153:def _parse_extras(tokenizer: Tokenizer) -> List[str]:
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:157:    if not tokenizer.check("LEFT_BRACKET", peek=True):
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:160:    with tokenizer.enclosing_tokens(
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:165:        tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:166:        extras = _parse_extras_list(tokenizer)
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:167:        tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:172:def _parse_extras_list(tokenizer: Tokenizer) -> List[str]:
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:178:    if not tokenizer.check("IDENTIFIER"):
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:181:    extras.append(tokenizer.read().text)
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:184:        tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:185:        if tokenizer.check("IDENTIFIER", peek=True):
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:186:            tokenizer.raise_syntax_error("Expected comma between extra names")
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:187:        elif not tokenizer.check("COMMA"):
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:190:        tokenizer.read()
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:191:        tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:193:        extra_token = tokenizer.expect("IDENTIFIER", expected="extra name after comma")
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:194:        extras.append(extra_token.text)
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:199:def _parse_specifier(tokenizer: Tokenizer) -> str:
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:204:    with tokenizer.enclosing_tokens(
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:209:        tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:210:        parsed_specifiers = _parse_version_many(tokenizer)
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:211:        tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:216:def _parse_version_many(tokenizer: Tokenizer) -> str:
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:221:    while tokenizer.check("SPECIFIER"):
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:222:        span_start = tokenizer.position
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:223:        parsed_specifiers += tokenizer.read().text
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:224:        if tokenizer.check("VERSION_PREFIX_TRAIL", peek=True):
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:225:            tokenizer.raise_syntax_error(
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:228:                span_end=tokenizer.position + 1,
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:230:        if tokenizer.check("VERSION_LOCAL_LABEL_TRAIL", peek=True):
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:231:            tokenizer.raise_syntax_error(
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:234:                span_end=tokenizer.position,
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:236:        tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:237:        if not tokenizer.check("COMMA"):
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:239:        parsed_specifiers += tokenizer.read().text
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:240:        tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:249:    return _parse_full_marker(Tokenizer(source, rules=DEFAULT_RULES))
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:252:def _parse_full_marker(tokenizer: Tokenizer) -> MarkerList:
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:253:    retval = _parse_marker(tokenizer)
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:254:    tokenizer.expect("END", expected="end of marker expression")
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:258:def _parse_marker(tokenizer: Tokenizer) -> MarkerList:
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:262:    expression = [_parse_marker_atom(tokenizer)]
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:263:    while tokenizer.check("BOOLOP"):
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:264:        token = tokenizer.read()
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:265:        expr_right = _parse_marker_atom(tokenizer)
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:266:        expression.extend((token.text, expr_right))
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:270:def _parse_marker_atom(tokenizer: Tokenizer) -> MarkerAtom:
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:276:    tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:277:    if tokenizer.check("LEFT_PARENTHESIS", peek=True):
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:278:        with tokenizer.enclosing_tokens(
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:283:            tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:284:            marker: MarkerAtom = _parse_marker(tokenizer)
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:285:            tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:287:        marker = _parse_marker_item(tokenizer)
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:288:    tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:292:def _parse_marker_item(tokenizer: Tokenizer) -> MarkerItem:
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:296:    tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:297:    marker_var_left = _parse_marker_var(tokenizer)
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:298:    tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:299:    marker_op = _parse_marker_op(tokenizer)
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:300:    tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:301:    marker_var_right = _parse_marker_var(tokenizer)
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:302:    tokenizer.consume("WS")
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:306:def _parse_marker_var(tokenizer: Tokenizer) -> MarkerVar:
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:310:    if tokenizer.check("VARIABLE"):
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:311:        return process_env_var(tokenizer.read().text.replace(".", "_"))
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:312:    elif tokenizer.check("QUOTED_STRING"):
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:313:        return process_python_str(tokenizer.read().text)
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:315:        tokenizer.raise_syntax_error(message="Expected a marker variable or quoted string")
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:330:def _parse_marker_op(tokenizer: Tokenizer) -> Op:
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:334:    if tokenizer.check("IN"):
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:335:        tokenizer.read()
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:337:    elif tokenizer.check("NOT"):
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:338:        tokenizer.read()
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:339:        tokenizer.expect("WS", expected="whitespace after 'not'")
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:340:        tokenizer.expect("IN", expected="'in' after 'not'")
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:342:    elif tokenizer.check("OP"):
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:343:        return Op(tokenizer.read().text)
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:345:        return tokenizer.raise_syntax_error(
./.venv-build/lib/python3.11/site-packages/setuptools/_vendor/wheel/vendored/packaging/requirements.py:8:from ._tokenizer import ParserSyntaxError
./.venv-build/lib/python3.11/site-packages/setuptools/tests/config/test_expand.py:47:    secrets = Path(str(dir_) + "secrets")
./.venv-build/lib/python3.11/site-packages/setuptools/tests/config/test_expand.py:48:    secrets.mkdir(exist_ok=True)
./.venv-build/lib/python3.11/site-packages/setuptools/tests/config/test_expand.py:49:    write_files({"secrets.txt": "secret keys"}, secrets)
./.venv-build/lib/python3.11/site-packages/setuptools/tests/config/test_expand.py:59:        cannot_access_secrets_msg = r"Cannot access '.*secrets\.txt'"
./.venv-build/lib/python3.11/site-packages/setuptools/tests/config/test_expand.py:60:        with pytest.raises(DistutilsOptionError, match=cannot_access_secrets_msg):
./.venv-build/lib/python3.11/site-packages/setuptools/tests/config/test_expand.py:61:            expand.read_files(["../dir_secrets/secrets.txt"])
./.venv-build/lib/python3.11/site-packages/setuptools/build_meta.py:38:import tokenize
./.venv-build/lib/python3.11/site-packages/setuptools/build_meta.py:135:    return tokenize.open(setup_script)
./.venv-build/lib/python3.11/site-packages/setuptools/_imp.py:9:import tokenize
./.venv-build/lib/python3.11/site-packages/setuptools/_imp.py:62:            file = tokenize.open(path)
./.venv-build/lib/python3.11/site-packages/setuptools/launch.py:10:import tokenize
./.venv-build/lib/python3.11/site-packages/setuptools/launch.py:27:    open_ = getattr(tokenize, "open", open)
./.venv-build/lib/python3.11/site-packages/setuptools/_distutils/core.py:13:import tokenize
./.venv-build/lib/python3.11/site-packages/setuptools/_distutils/core.py:265:            # tokenize.open supports automatic encoding detection
./.venv-build/lib/python3.11/site-packages/setuptools/_distutils/core.py:266:            with tokenize.open(script_name) as f:
./.venv-build/lib/python3.11/site-packages/setuptools/_distutils/util.py:177:            # password database, do nothing
./.venv-build/lib/python3.11/site-packages/setuptools/_distutils/dist.py:235:        self.password = ""
./.venv-build/lib/python3.11/site-packages/setuptools/_distutils/command/build_scripts.py:7:import tokenize
./.venv-build/lib/python3.11/site-packages/setuptools/_distutils/command/build_scripts.py:91:            f = tokenize.open(script)
./.venv-build/lib/python3.11/site-packages/botocore/docs/paginator.py:140:            "specified in max-items then a <code>NextToken</code> "
./.venv-build/lib/python3.11/site-packages/botocore/docs/paginator.py:153:    pagination_config_members["StartingToken"] = DocumentedShape(
./.venv-build/lib/python3.11/site-packages/botocore/docs/paginator.py:154:        name="StartingToken",
./.venv-build/lib/python3.11/site-packages/botocore/docs/paginator.py:157:            "<p>A token to specify where to start paginating. "
./.venv-build/lib/python3.11/site-packages/botocore/docs/paginator.py:158:            "This is the <code>NextToken</code> from a previous "
./.venv-build/lib/python3.11/site-packages/botocore/docs/paginator.py:176:            name="NextToken",
./.venv-build/lib/python3.11/site-packages/botocore/docs/paginator.py:178:            documentation=("<p>A token to resume pagination.</p>"),
./.venv-build/lib/python3.11/site-packages/botocore/docs/paginator.py:184:    # Add the normal input token of the method to a list
./.venv-build/lib/python3.11/site-packages/botocore/docs/paginator.py:186:    if isinstance(paginator_config["input_token"], list):
./.venv-build/lib/python3.11/site-packages/botocore/docs/paginator.py:187:        service_pagination_params += paginator_config["input_token"]
./.venv-build/lib/python3.11/site-packages/botocore/docs/paginator.py:189:        service_pagination_params.append(paginator_config["input_token"])
./.venv-build/lib/python3.11/site-packages/botocore/docs/paginator.py:195:    # Hide the output tokens in the documentation.
./.venv-build/lib/python3.11/site-packages/botocore/docs/paginator.py:197:    if isinstance(paginator_config["output_token"], list):
./.venv-build/lib/python3.11/site-packages/botocore/docs/paginator.py:198:        service_pagination_response_params += paginator_config["output_token"]
./.venv-build/lib/python3.11/site-packages/botocore/docs/paginator.py:200:        service_pagination_response_params.append(paginator_config["output_token"])
./.venv-build/lib/python3.11/site-packages/botocore/docs/params.py:275:        if "idempotencyToken" in shape.metadata:
./.venv-build/lib/python3.11/site-packages/botocore/docs/service.py:34:            aws_secret_access_key="bar",
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:43:    UnauthorizedSSOTokenError,
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:46:from botocore.tokens import SSOTokenProvider
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:51:    FileWebIdentityTokenLoader,
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:54:    SSOTokenLoader,
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:63:    ["access_key", "secret_key", "token", "account_id"],
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:142:        # export AWS_SECRET_ACCESS_KEY=bar
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:172:    def __init__(self, session, cache=None, region_name=None, sso_token_cache=None):
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:176:        self._sso_token_cache = sso_token_cache
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:225:            token_cache=self._sso_token_cache,
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:226:            token_provider=SSOTokenProvider(
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:228:                cache=self._sso_token_cache,
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:274:            "secret_key": credentials["SecretAccessKey"],
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:275:            "token": credentials["SessionToken"],
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:305:    :param str secret_key: The secret key part of the credentials.
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:306:    :param str token: The security token, valid only for session credentials.
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:312:    def __init__(self, access_key, secret_key, token=None, method=None, account_id=None):
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:314:        self.secret_key = secret_key
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:315:        self.token = token
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:332:        self.secret_key = botocore.compat.ensure_unicode(self.secret_key)
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:335:        return ReadOnlyCredentials(self.access_key, self.secret_key, self.token, self.account_id)
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:350:    :param str secret_key: The secret key part of the credentials.
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:351:    :param str token: The security token, valid only for session credentials.
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:369:        secret_key,
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:370:        token,
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:381:        self._secret_key = secret_key
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:382:        self._token = token
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:388:        self._frozen_credentials = ReadOnlyCredentials(access_key, secret_key, token, account_id)
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:397:        self._secret_key = botocore.compat.ensure_unicode(self._secret_key)
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:416:            secret_key=metadata["secret_key"],
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:417:            token=metadata["token"],
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:440:    def secret_key(self):
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:446:        return self._secret_key
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:448:    @secret_key.setter
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:449:    def secret_key(self, value):
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:450:        self._secret_key = value
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:453:    def token(self):
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:459:        return self._token
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:461:    @token.setter
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:462:    def token(self, value):
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:463:        self._token = value
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:571:            self._access_key, self._secret_key, self._token, self._account_id
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:588:        expected_keys = ["access_key", "secret_key", "token", "expiry_time"]
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:602:        self.secret_key = data["secret_key"]
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:603:        self.token = data["token"]
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:612:        The ``access_key``, ``secret_key``, and ``token`` properties
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:622:            tmp.secret_key  ---> expired? yes, refresh and return t2.secret_key
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:624:        This means we're using the access key from t1 with the secret key
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:656:        self._secret_key = None
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:657:        self._token = None
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:715:            "secret_key": creds["SecretAccessKey"],
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:716:            "token": creds["SessionToken"],
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:876:            token_code = self._mfa_prompter(prompt)
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:877:            assume_role_kwargs["TokenCode"] = token_code
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:892:            aws_secret_access_key=frozen_credentials.secret_key,
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:893:            aws_session_token=frozen_credentials.token,
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:901:        web_identity_token_loader,
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:912:        :type web_identity_token_loader: callable
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:913:        :param web_identity_token_loader: A callable that takes no arguments
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:914:        and returns a web identity token str.
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:934:        self._web_identity_token_loader = web_identity_token_loader
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:949:        # the token, explicitly configure the client to not sign requests.
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:959:        identity_token = self._web_identity_token_loader()
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:960:        assume_role_kwargs["WebIdentityToken"] = identity_token
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:992:        ``access_key/secret_key/token`` themselves.
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:1035:            secret_key=creds_dict["secret_key"],
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:1036:            token=creds_dict.get("token"),
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:1062:                "secret_key": parsed["SecretAccessKey"],
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:1063:                "token": parsed.get("SessionToken"),
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:1121:    SECRET_KEY = "AWS_SECRET_ACCESS_KEY"
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:1122:    # The token can come from either of these env var.
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:1123:    # AWS_SESSION_TOKEN is what other AWS SDKs have standardized on.
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:1124:    TOKENS = ["AWS_SECURITY_TOKEN", "AWS_SESSION_TOKEN"]
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:1138:            * ``secret_key``
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:1139:            * ``token``
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:1154:            var_mapping["secret_key"] = self.SECRET_KEY
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:1155:            var_mapping["token"] = self.TOKENS
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:1160:            var_mapping["secret_key"] = mapping.get("secret_key", self.SECRET_KEY)
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:1161:            var_mapping["token"] = mapping.get("token", self.TOKENS)
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:1162:            if not isinstance(var_mapping["token"], list):
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:1163:                var_mapping["token"] = [var_mapping["token"]]
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:1186:                    credentials["secret_key"],
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:1187:                    credentials["token"],
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:1196:                credentials["secret_key"],
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:1197:                credentials["token"],
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:1217:            secret_key = environ.get(mapping["secret_key"], "")
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:1218:            if not secret_key:
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:1219:                raise PartialCredentialsError(provider=method, cred_var=mapping["secret_key"])
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:1220:            credentials["secret_key"] = secret_key
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:1222:            credentials["token"] = None
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:1223:            for token_env_var in mapping["token"]:
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:1224:                token = environ.get(token_env_var, "")
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:1225:                if token:
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:1226:                    credentials["token"] = token
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:1252:    SECRET_KEY = "AWSSecretKey"
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:1272:                secret_key = creds[self.SECRET_KEY]
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:1273:                # EC2 creds file doesn't support session tokens.
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:1274:                return Credentials(access_key, secret_key, method=self.METHOD)
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:1284:    SECRET_KEY = "aws_secret_access_key"
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:1286:    # aws_security_token, but the SDKs are standardizing on aws_session_token
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:1288:    TOKENS = ["aws_security_token", "aws_session_token"]
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:1312:                access_key, secret_key = self._extract_creds_from_mapping(
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:1313:                    config, self.ACCESS_KEY, self.SECRET_KEY
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:1315:                token = self._get_session_token(config)
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:1320:                    secret_key,
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:1321:                    token,
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:1326:    def _get_session_token(self, config):
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:1327:        for token_envvar in self.TOKENS:
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:1328:            if token_envvar in config:
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:1329:                return config[token_envvar]
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:1342:    SECRET_KEY = "aws_secret_access_key"
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:1344:    # aws_security_token, but the SDKs are standardizing on aws_session_token
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:1346:    TOKENS = ["aws_security_token", "aws_session_token"]
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:1380:                access_key, secret_key = self._extract_creds_from_mapping(
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:1381:                    profile_config, self.ACCESS_KEY, self.SECRET_KEY
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:1383:                token = self._get_session_token(profile_config)
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:1388:                    secret_key,
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:1389:                    token,
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:1396:    def _get_session_token(self, profile_config):
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:1397:        for token_name in self.TOKENS:
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:1398:            if token_name in profile_config:
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:1399:                return profile_config[token_name]
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:1412:    SECRET_KEY = "aws_secret_access_key"
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:1440:                    access_key, secret_key = self._extract_creds_from_mapping(
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:1441:                        credentials, self.ACCESS_KEY, self.SECRET_KEY
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:1444:                    return Credentials(access_key, secret_key, method=self.METHOD)
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:1456:    WEB_IDENTITY_TOKE_FILE_VAR = "web_identity_token_file"
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:1704:        static_keys = ["aws_secret_access_key", "aws_access_key_id"]
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:1750:                secret_key=profile["aws_secret_access_key"],
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:1751:                token=profile.get("aws_session_token"),
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:1776:        "web_identity_token_file": "AWS_WEB_IDENTITY_TOKEN_FILE",
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:1788:        token_loader_cls=None,
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:1796:        if token_loader_cls is None:
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:1797:            token_loader_cls = FileWebIdentityTokenLoader
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:1798:        self._token_loader_cls = token_loader_cls
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:1822:            self._feature_ids.add("CREDENTIALS_ENV_VARS_STS_WEB_ID_TOKEN")
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:1827:            self._feature_ids.add("CREDENTIALS_PROFILE_STS_WEB_ID_TOKEN")
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:1833:        token_path = self._get_config("web_identity_token_file")
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:1834:        if not token_path:
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:1836:        token_loader = self._token_loader_cls(token_path)
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:1855:            web_identity_token_loader=token_loader,
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:1966:    ENV_VAR_AUTH_TOKEN = "AWS_CONTAINER_AUTHORIZATION_TOKEN"
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:1967:    ENV_VAR_AUTH_TOKEN_FILE = "AWS_CONTAINER_AUTHORIZATION_TOKEN_FILE"
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:1992:            secret_key=creds["secret_key"],
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:1993:            token=creds["token"],
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:2001:        auth_token = None
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:2002:        if self.ENV_VAR_AUTH_TOKEN_FILE in self._environ:
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:2003:            auth_token_file_path = self._environ[self.ENV_VAR_AUTH_TOKEN_FILE]
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:2004:            with open(auth_token_file_path) as token_file:
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:2005:                auth_token = token_file.read()
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:2006:        elif self.ENV_VAR_AUTH_TOKEN in self._environ:
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:2007:            auth_token = self._environ[self.ENV_VAR_AUTH_TOKEN]
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:2008:        if auth_token is not None:
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:2009:            self._validate_auth_token(auth_token)
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:2010:            return {"Authorization": auth_token}
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:2012:    def _validate_auth_token(self, auth_token):
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:2013:        if "\r" in auth_token or "\n" in auth_token:
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:2014:            raise ValueError("Auth token value is not a legal header value")
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:2027:                "secret_key": response["SecretAccessKey"],
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:2028:                "token": response["Token"],
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:2149:        token_loader=None,
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:2152:        token_provider=None,
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:2161:        self._token_loader = token_loader
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:2162:        self._token_provider = token_provider
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:2202:        if self._token_provider:
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:2203:            initial_token_data = self._token_provider.load_token()
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:2204:            token = initial_token_data.get_frozen_token().token
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:2206:            token_dict = self._token_loader(self._start_url)
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:2207:            token = token_dict["accessToken"]
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:2209:            # raise an UnauthorizedSSOTokenError if the loaded legacy token
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:2211:            # expired token.
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:2212:            expiration = dateutil.parser.parse(token_dict["expiresAt"])
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:2215:                raise UnauthorizedSSOTokenError()
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:2220:            "accessToken": token,
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:2226:            raise UnauthorizedSSOTokenError()
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:2233:                "SecretAccessKey": credentials["secretAccessKey"],
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:2234:                "SessionToken": credentials["sessionToken"],
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:2245:    _SSO_TOKEN_CACHE_DIR = os.path.expanduser(os.path.join("~", ".aws", "sso", "cache"))
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:2262:        token_cache=None,
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:2263:        token_provider=None,
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:2265:        if token_cache is None:
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:2266:            token_cache = JSONFileCache(self._SSO_TOKEN_CACHE_DIR)
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:2267:        self._token_cache = token_cache
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:2268:        self._token_provider = token_provider
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:2345:            "token_loader": SSOTokenLoader(cache=self._token_cache),
./.venv-build/lib/python3.11/site-packages/botocore/credentials.py:2351:            fetcher_kwargs["token_provider"] = self._token_provider
./.venv-build/lib/python3.11/site-packages/botocore/exceptions.py:165:class NoAuthTokenError(BotoCoreError):
./.venv-build/lib/python3.11/site-packages/botocore/exceptions.py:167:    No authorization token could be found.
./.venv-build/lib/python3.11/site-packages/botocore/exceptions.py:170:    fmt = "Unable to locate authorization token"
./.venv-build/lib/python3.11/site-packages/botocore/exceptions.py:173:class TokenRetrievalError(BotoCoreError):
./.venv-build/lib/python3.11/site-packages/botocore/exceptions.py:175:    Error attempting to retrieve a token from a remote source.
./.venv-build/lib/python3.11/site-packages/botocore/exceptions.py:177:    :ivar provider: The name of the token provider.
./.venv-build/lib/python3.11/site-packages/botocore/exceptions.py:178:    :ivar error_msg: The msg explaining why the token could not be retrieved.
./.venv-build/lib/python3.11/site-packages/botocore/exceptions.py:182:    fmt = "Error when retrieving token from {provider}: {error_msg}"
./.venv-build/lib/python3.11/site-packages/botocore/exceptions.py:692:    fmt = "Cannot refresh credentials: MFA token required."
./.venv-build/lib/python3.11/site-packages/botocore/exceptions.py:726:        "access token from SSO."
./.venv-build/lib/python3.11/site-packages/botocore/exceptions.py:730:class SSOTokenLoadError(SSOError):
./.venv-build/lib/python3.11/site-packages/botocore/exceptions.py:731:    fmt = "Error loading SSO Token: {error_msg}"
./.venv-build/lib/python3.11/site-packages/botocore/exceptions.py:734:class UnauthorizedSSOTokenError(SSOError):
./.venv-build/lib/python3.11/site-packages/botocore/retries/bucket.py:1:"""This module implements token buckets used for client side throttling."""
./.venv-build/lib/python3.11/site-packages/botocore/retries/bucket.py:20:class TokenBucket:
./.venv-build/lib/python3.11/site-packages/botocore/retries/bucket.py:42:            # tokens we might have based on the current rate.  If we don't
./.venv-build/lib/python3.11/site-packages/botocore/retries/bucket.py:66:        """Acquire token or return amount of time until next token available.
./.venv-build/lib/python3.11/site-packages/botocore/retries/adaptive.py:13:    token_bucket = bucket.TokenBucket(max_rate=1, clock=clock)
./.venv-build/lib/python3.11/site-packages/botocore/retries/adaptive.py:21:        token_bucket=token_bucket,
./.venv-build/lib/python3.11/site-packages/botocore/retries/adaptive.py:43:        token_bucket,
./.venv-build/lib/python3.11/site-packages/botocore/retries/adaptive.py:49:        self._token_bucket = token_bucket
./.venv-build/lib/python3.11/site-packages/botocore/retries/adaptive.py:57:            self._token_bucket.acquire()
./.venv-build/lib/python3.11/site-packages/botocore/retries/adaptive.py:70:                    rate_to_use = min(measured_rate, self._token_bucket.max_rate)
./.venv-build/lib/python3.11/site-packages/botocore/retries/adaptive.py:74:                    "measured rate: %s, token bucket capacity "
./.venv-build/lib/python3.11/site-packages/botocore/retries/adaptive.py:78:                    self._token_bucket.available_capacity,
./.venv-build/lib/python3.11/site-packages/botocore/retries/adaptive.py:81:            self._token_bucket.max_rate = min(new_rate, self._MAX_RATE_ADJUST_SCALE * measured_rate)
./.venv-build/lib/python3.11/site-packages/botocore/session.py:29:import botocore.tokens
./.venv-build/lib/python3.11/site-packages/botocore/session.py:146:        self._auth_token = None
./.venv-build/lib/python3.11/site-packages/botocore/session.py:165:        self._register_token_provider()
./.venv-build/lib/python3.11/site-packages/botocore/session.py:180:    def _register_token_provider(self):
./.venv-build/lib/python3.11/site-packages/botocore/session.py:181:        self._components.lazy_register_component("token_provider", self._create_token_resolver)
./.venv-build/lib/python3.11/site-packages/botocore/session.py:183:    def _create_token_resolver(self):
./.venv-build/lib/python3.11/site-packages/botocore/session.py:184:        return botocore.tokens.create_token_resolver(self)
./.venv-build/lib/python3.11/site-packages/botocore/session.py:462:    def set_credentials(self, access_key, secret_key, token=None, account_id=None):
./.venv-build/lib/python3.11/site-packages/botocore/session.py:472:        :type secret_key: str
./.venv-build/lib/python3.11/site-packages/botocore/session.py:473:        :param secret_key: The secret key part of the credentials.
./.venv-build/lib/python3.11/site-packages/botocore/session.py:475:        :type token: str
./.venv-build/lib/python3.11/site-packages/botocore/session.py:476:        :param token: An option session token used by STS session
./.venv-build/lib/python3.11/site-packages/botocore/session.py:483:            access_key, secret_key, token, account_id=account_id
./.venv-build/lib/python3.11/site-packages/botocore/session.py:501:    def get_auth_token(self, **kwargs):
./.venv-build/lib/python3.11/site-packages/botocore/session.py:503:        Return the :class:`botocore.tokens.AuthToken` object associated with
./.venv-build/lib/python3.11/site-packages/botocore/session.py:504:        this session. If the authorization token has not yet been loaded, this
./.venv-build/lib/python3.11/site-packages/botocore/session.py:506:        return the cached authorization token.
./.venv-build/lib/python3.11/site-packages/botocore/session.py:509:        provider = self._components.get_component("token_provider")
./.venv-build/lib/python3.11/site-packages/botocore/session.py:513:            auth_token = provider.load_token(signing_name=signing_name)
./.venv-build/lib/python3.11/site-packages/botocore/session.py:514:            if auth_token is not None:
./.venv-build/lib/python3.11/site-packages/botocore/session.py:515:                return auth_token
./.venv-build/lib/python3.11/site-packages/botocore/session.py:517:        if self._auth_token is None:
./.venv-build/lib/python3.11/site-packages/botocore/session.py:518:            self._auth_token = provider.load_token()
./.venv-build/lib/python3.11/site-packages/botocore/session.py:519:        return self._auth_token
./.venv-build/lib/python3.11/site-packages/botocore/session.py:822:        aws_secret_access_key=None,
./.venv-build/lib/python3.11/site-packages/botocore/session.py:823:        aws_session_token=None,
./.venv-build/lib/python3.11/site-packages/botocore/session.py:875:        :type aws_secret_access_key: string
./.venv-build/lib/python3.11/site-packages/botocore/session.py:876:        :param aws_secret_access_key: The secret key to use when creating
./.venv-build/lib/python3.11/site-packages/botocore/session.py:879:        :type aws_session_token: string
./.venv-build/lib/python3.11/site-packages/botocore/session.py:880:        :param aws_session_token: The session token to use when creating
./.venv-build/lib/python3.11/site-packages/botocore/session.py:925:        elif aws_access_key_id is not None and aws_secret_access_key is not None:
./.venv-build/lib/python3.11/site-packages/botocore/session.py:928:                secret_key=aws_secret_access_key,
./.venv-build/lib/python3.11/site-packages/botocore/session.py:929:                token=aws_session_token,
./.venv-build/lib/python3.11/site-packages/botocore/session.py:932:        elif self._missing_cred_vars(aws_access_key_id, aws_secret_access_key):
./.venv-build/lib/python3.11/site-packages/botocore/session.py:935:                cred_var=self._missing_cred_vars(aws_access_key_id, aws_secret_access_key),
./.venv-build/lib/python3.11/site-packages/botocore/session.py:939:                aws_session_token, aws_account_id
./.venv-build/lib/python3.11/site-packages/botocore/session.py:943:                    "an access key id and secret key on the session or client: %s",
./.venv-build/lib/python3.11/site-packages/botocore/session.py:949:        auth_token = self.get_auth_token()
./.venv-build/lib/python3.11/site-packages/botocore/session.py:985:            auth_token_resolver=self.get_auth_token,
./.venv-build/lib/python3.11/site-packages/botocore/session.py:997:            auth_token=auth_token,
./.venv-build/lib/python3.11/site-packages/botocore/session.py:1052:    def _missing_cred_vars(self, access_key, secret_key):
./.venv-build/lib/python3.11/site-packages/botocore/session.py:1053:        if access_key is not None and secret_key is None:
./.venv-build/lib/python3.11/site-packages/botocore/session.py:1054:            return "aws_secret_access_key"
./.venv-build/lib/python3.11/site-packages/botocore/session.py:1055:        if secret_key is not None and access_key is None:
./.venv-build/lib/python3.11/site-packages/botocore/session.py:1112:    def _get_ignored_credentials(self, aws_session_token, aws_account_id):
./.venv-build/lib/python3.11/site-packages/botocore/session.py:1114:        if aws_session_token:
./.venv-build/lib/python3.11/site-packages/botocore/session.py:1115:            credential_inputs.append("aws_session_token")
./.venv-build/lib/python3.11/site-packages/botocore/auth.py:41:    NoAuthTokenError,
./.venv-build/lib/python3.11/site-packages/botocore/auth.py:106:    REQUIRES_TOKEN = False
./.venv-build/lib/python3.11/site-packages/botocore/auth.py:112:class TokenSigner(BaseSigner):
./.venv-build/lib/python3.11/site-packages/botocore/auth.py:113:    REQUIRES_TOKEN = True
./.venv-build/lib/python3.11/site-packages/botocore/auth.py:115:    Signers that expect an authorization token to perform the authorization
./.venv-build/lib/python3.11/site-packages/botocore/auth.py:118:    def __init__(self, auth_token):
./.venv-build/lib/python3.11/site-packages/botocore/auth.py:119:        self.auth_token = auth_token
./.venv-build/lib/python3.11/site-packages/botocore/auth.py:137:        lhmac = hmac.new(self.credentials.secret_key.encode("utf-8"), digestmod=sha256)
./.venv-build/lib/python3.11/site-packages/botocore/auth.py:174:        if self.credentials.token:
./.venv-build/lib/python3.11/site-packages/botocore/auth.py:175:            params["SecurityToken"] = self.credentials.token
./.venv-build/lib/python3.11/site-packages/botocore/auth.py:191:        if self.credentials.token:
./.venv-build/lib/python3.11/site-packages/botocore/auth.py:192:            if "X-Amz-Security-Token" in request.headers:
./.venv-build/lib/python3.11/site-packages/botocore/auth.py:193:                del request.headers["X-Amz-Security-Token"]
./.venv-build/lib/python3.11/site-packages/botocore/auth.py:194:            request.headers["X-Amz-Security-Token"] = self.credentials.token
./.venv-build/lib/python3.11/site-packages/botocore/auth.py:195:        new_hmac = hmac.new(self.credentials.secret_key.encode("utf-8"), digestmod=sha256)
./.venv-build/lib/python3.11/site-packages/botocore/auth.py:399:        key = self.credentials.secret_key
./.venv-build/lib/python3.11/site-packages/botocore/auth.py:436:        if self.credentials.token:
./.venv-build/lib/python3.11/site-packages/botocore/auth.py:437:            if "X-Amz-Security-Token" in request.headers:
./.venv-build/lib/python3.11/site-packages/botocore/auth.py:438:                del request.headers["X-Amz-Security-Token"]
./.venv-build/lib/python3.11/site-packages/botocore/auth.py:439:            request.headers["X-Amz-Security-Token"] = self.credentials.token
./.venv-build/lib/python3.11/site-packages/botocore/auth.py:529:        if "x-amz-s3session-token" not in request.headers:
./.venv-build/lib/python3.11/site-packages/botocore/auth.py:530:            request.headers["x-amz-s3session-token"] = self.credentials.token
./.venv-build/lib/python3.11/site-packages/botocore/auth.py:531:        # S3Express does not support STS' X-Amz-Security-Token
./.venv-build/lib/python3.11/site-packages/botocore/auth.py:532:        if "X-Amz-Security-Token" in request.headers:
./.venv-build/lib/python3.11/site-packages/botocore/auth.py:533:            del request.headers["X-Amz-Security-Token"]
./.venv-build/lib/python3.11/site-packages/botocore/auth.py:564:        if self.credentials.token is not None:
./.venv-build/lib/python3.11/site-packages/botocore/auth.py:565:            fields["X-Amz-S3session-Token"] = self.credentials.token
./.venv-build/lib/python3.11/site-packages/botocore/auth.py:566:            conditions.append({"X-Amz-S3session-Token": self.credentials.token})
./.venv-build/lib/python3.11/site-packages/botocore/auth.py:618:        if self.credentials.token is not None:
./.venv-build/lib/python3.11/site-packages/botocore/auth.py:619:            auth_params["X-Amz-S3session-Token"] = self.credentials.token
./.venv-build/lib/python3.11/site-packages/botocore/auth.py:704:        if self.credentials.token is not None:
./.venv-build/lib/python3.11/site-packages/botocore/auth.py:705:            auth_params["X-Amz-Security-Token"] = self.credentials.token
./.venv-build/lib/python3.11/site-packages/botocore/auth.py:809:        if self.credentials.token is not None:
./.venv-build/lib/python3.11/site-packages/botocore/auth.py:810:            fields["x-amz-security-token"] = self.credentials.token
./.venv-build/lib/python3.11/site-packages/botocore/auth.py:811:            conditions.append({"x-amz-security-token": self.credentials.token})
./.venv-build/lib/python3.11/site-packages/botocore/auth.py:867:        new_hmac = hmac.new(self.credentials.secret_key.encode("utf-8"), digestmod=sha1)
./.venv-build/lib/python3.11/site-packages/botocore/auth.py:944:        if self.credentials.token:
./.venv-build/lib/python3.11/site-packages/botocore/auth.py:945:            del headers["x-amz-security-token"]
./.venv-build/lib/python3.11/site-packages/botocore/auth.py:946:            headers["x-amz-security-token"] = self.credentials.token
./.venv-build/lib/python3.11/site-packages/botocore/auth.py:1057:        if self.credentials.token is not None:
./.venv-build/lib/python3.11/site-packages/botocore/auth.py:1058:            fields["x-amz-security-token"] = self.credentials.token
./.venv-build/lib/python3.11/site-packages/botocore/auth.py:1059:            conditions.append({"x-amz-security-token": self.credentials.token})
./.venv-build/lib/python3.11/site-packages/botocore/auth.py:1070:class BearerAuth(TokenSigner):
./.venv-build/lib/python3.11/site-packages/botocore/auth.py:1072:    Performs bearer token authorization by placing the bearer token in the
./.venv-build/lib/python3.11/site-packages/botocore/auth.py:1079:        if self.auth_token is None:
./.venv-build/lib/python3.11/site-packages/botocore/auth.py:1080:            raise NoAuthTokenError()
./.venv-build/lib/python3.11/site-packages/botocore/auth.py:1082:        auth_header = f"Bearer {self.auth_token.token}"
./.venv-build/lib/python3.11/site-packages/botocore/utils.py:83:    SSOTokenLoadError,
./.venv-build/lib/python3.11/site-packages/botocore/utils.py:176:    "secretsmanager": "secrets-manager",
./.venv-build/lib/python3.11/site-packages/botocore/utils.py:371:    token = set_plugin_context(ctx)
./.venv-build/lib/python3.11/site-packages/botocore/utils.py:375:        reset_plugin_context(token)
./.venv-build/lib/python3.11/site-packages/botocore/utils.py:391:    _TOKEN_PATH = "latest/api/token"
./.venv-build/lib/python3.11/site-packages/botocore/utils.py:392:    _TOKEN_TTL = "21600"
./.venv-build/lib/python3.11/site-packages/botocore/utils.py:456:    def _fetch_metadata_token(self):
./.venv-build/lib/python3.11/site-packages/botocore/utils.py:458:        url = self._construct_url(self._TOKEN_PATH)
./.venv-build/lib/python3.11/site-packages/botocore/utils.py:460:            "x-aws-ec2-metadata-token-ttl-seconds": self._TOKEN_TTL,
./.venv-build/lib/python3.11/site-packages/botocore/utils.py:490:    def _get_request(self, url_path, retry_func, token=None):
./.venv-build/lib/python3.11/site-packages/botocore/utils.py:503:        :type token: str
./.venv-build/lib/python3.11/site-packages/botocore/utils.py:504:        :param token: Metadata token to send along with GET requests to IMDS.
./.venv-build/lib/python3.11/site-packages/botocore/utils.py:507:        if not token:
./.venv-build/lib/python3.11/site-packages/botocore/utils.py:513:        if token is not None:
./.venv-build/lib/python3.11/site-packages/botocore/utils.py:514:            headers["x-aws-ec2-metadata-token"] = token
./.venv-build/lib/python3.11/site-packages/botocore/utils.py:544:                error_msg="Unable to retrieve token for use in IMDSv2 call and IMDSv1 has been disabled"
./.venv-build/lib/python3.11/site-packages/botocore/utils.py:575:        "SecretAccessKey",
./.venv-build/lib/python3.11/site-packages/botocore/utils.py:576:        "Token",
./.venv-build/lib/python3.11/site-packages/botocore/utils.py:582:            token = self._fetch_metadata_token()
./.venv-build/lib/python3.11/site-packages/botocore/utils.py:583:            role_name = self._get_iam_role(token)
./.venv-build/lib/python3.11/site-packages/botocore/utils.py:584:            credentials = self._get_credentials(role_name, token)
./.venv-build/lib/python3.11/site-packages/botocore/utils.py:589:                    "secret_key": credentials["SecretAccessKey"],
./.venv-build/lib/python3.11/site-packages/botocore/utils.py:590:                    "token": credentials["Token"],
./.venv-build/lib/python3.11/site-packages/botocore/utils.py:620:    def _get_iam_role(self, token=None):
./.venv-build/lib/python3.11/site-packages/botocore/utils.py:624:            token=token,
./.venv-build/lib/python3.11/site-packages/botocore/utils.py:627:    def _get_credentials(self, role_name, token=None):
./.venv-build/lib/python3.11/site-packages/botocore/utils.py:631:            token=token,
./.venv-build/lib/python3.11/site-packages/botocore/utils.py:776:        token = self._fetch_metadata_token()
./.venv-build/lib/python3.11/site-packages/botocore/utils.py:780:            token=token,
./.venv-build/lib/python3.11/site-packages/botocore/utils.py:1607:                "secret_key": creds["SecretAccessKey"],
./.venv-build/lib/python3.11/site-packages/botocore/utils.py:1608:                "token": creds["SessionToken"],
./.venv-build/lib/python3.11/site-packages/botocore/utils.py:3226:class FileWebIdentityTokenLoader:
./.venv-build/lib/python3.11/site-packages/botocore/utils.py:3227:    def __init__(self, web_identity_token_path, _open=open):
./.venv-build/lib/python3.11/site-packages/botocore/utils.py:3228:        self._web_identity_token_path = web_identity_token_path
./.venv-build/lib/python3.11/site-packages/botocore/utils.py:3232:        with self._open(self._web_identity_token_path) as token_file:
./.venv-build/lib/python3.11/site-packages/botocore/utils.py:3233:            return token_file.read()
./.venv-build/lib/python3.11/site-packages/botocore/utils.py:3236:class SSOTokenLoader:
./.venv-build/lib/python3.11/site-packages/botocore/utils.py:3248:    def save_token(self, start_url, token, session_name=None):
./.venv-build/lib/python3.11/site-packages/botocore/utils.py:3250:        self._cache[cache_key] = token
./.venv-build/lib/python3.11/site-packages/botocore/utils.py:3254:        logger.debug("Checking for cached token at: %s", cache_key)
./.venv-build/lib/python3.11/site-packages/botocore/utils.py:3259:            error_msg = f"Token for {name} does not exist"
./.venv-build/lib/python3.11/site-packages/botocore/utils.py:3260:            raise SSOTokenLoadError(error_msg=error_msg)
./.venv-build/lib/python3.11/site-packages/botocore/utils.py:3262:        token = self._cache[cache_key]
./.venv-build/lib/python3.11/site-packages/botocore/utils.py:3263:        if "accessToken" not in token or "expiresAt" not in token:
./.venv-build/lib/python3.11/site-packages/botocore/utils.py:3264:            error_msg = f"Token for {start_url} is invalid"
./.venv-build/lib/python3.11/site-packages/botocore/utils.py:3265:            raise SSOTokenLoadError(error_msg=error_msg)
./.venv-build/lib/python3.11/site-packages/botocore/utils.py:3266:        return token
./.venv-build/lib/python3.11/site-packages/botocore/utils.py:3450:def get_token_from_environment(signing_name, environ=None):
./.venv-build/lib/python3.11/site-packages/botocore/utils.py:3462:    return f"AWS_BEARER_TOKEN_{bearer_name}"
./.venv-build/lib/python3.11/site-packages/botocore/utils.py:3530:    "secretsmanager": "secrets-manager",
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:30:class TokenEncoder:
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:37:    This is intended for use in encoding pagination tokens, which in some
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:41:    def encode(self, token):
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:44:        :type token: dict
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:45:        :param token: A dictionary containing pagination information,
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:46:            particularly the service pagination token(s) but also other boto
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:55:            json_string = json.dumps(token)
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:58:            encoded_token, encoded_keys = self._encode(token, [])
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:62:            encoded_token["boto_encoded_keys"] = encoded_keys
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:65:            json_string = json.dumps(encoded_token)
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:67:        # base64 encode the json string to produce an opaque token string.
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:108:class TokenDecoder:
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:109:    """Decodes token strings back into dictionaries.
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:111:    This performs the inverse operation to the TokenEncoder, accepting
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:115:    def decode(self, token):
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:118:        :type token: str
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:119:        :param token: A token string given by the botocore pagination
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:124:            particularly the service pagination token(s) but also other boto
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:127:        json_string = base64.b64decode(token.encode("utf-8")).decode("utf-8")
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:128:        decoded_token = json.loads(json_string)
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:132:        encoded_keys = decoded_token.pop("boto_encoded_keys", None)
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:134:            return decoded_token
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:136:            return self._decode(decoded_token, encoded_keys)
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:138:    def _decode(self, token, encoded_keys):
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:141:            encoded = self._path_get(token, key)
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:143:            self._path_set(token, key, decoded)
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:144:        return token
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:197:        input_token,
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:198:        output_token,
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:204:        starting_token,
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:209:        self._input_token = input_token
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:210:        self._output_token = output_token
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:215:        self._starting_token = starting_token
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:218:        self._resume_token = None
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:221:        self._token_encoder = TokenEncoder()
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:222:        self._token_decoder = TokenDecoder()
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:229:    def resume_token(self):
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:230:        """Token to specify to resume pagination."""
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:231:        return self._resume_token
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:233:    @resume_token.setter
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:234:    def resume_token(self, value):
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:236:            raise ValueError(f"Bad starting token: {value}")
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:239:            token_keys = sorted(self._input_token + ["boto_truncate_amount"])
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:241:            token_keys = sorted(self._input_token)
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:244:        if token_keys == dict_keys:
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:245:            self._resume_token = self._token_encoder.encode(value)
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:247:            raise ValueError(f"Bad starting token: {value}")
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:255:        previous_next_token = None
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:256:        next_token = {key: None for key in self._input_token}
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:257:        if self._starting_token is not None:
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:258:            # If the starting token exists, populate the next_token with the
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:260:            # pagination token on hand if we need to truncate after the
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:262:            next_token = self._parse_starting_token()[0]
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:274:                # possibly have a resume/starting token that tells us where
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:276:                if self._starting_token is not None:
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:300:                    next_token,
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:307:                next_token = self._get_next_token(parsed)
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:308:                if all(t is None for t in next_token.values()):
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:312:                    # next token to be the resume token.
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:313:                    self.resume_token = next_token
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:315:                if previous_next_token is not None and previous_next_token == next_token:
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:316:                    message = f"The same next token was received twice: {next_token}"
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:318:                self._inject_token_into_kwargs(current_kwargs, next_token)
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:319:                previous_next_token = next_token
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:361:        # If the user has specified a starting token we need to
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:363:        if self._starting_token is not None:
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:365:            # token specified.
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:366:            next_token = self._parse_starting_token()[0]
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:367:            self._inject_token_into_kwargs(op_kwargs, next_token)
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:373:    def _inject_token_into_kwargs(self, op_kwargs, next_token):
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:374:        for name, token in next_token.items():
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:375:            if (token is not None) and (token != "None"):
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:376:                op_kwargs[name] = token
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:383:        starting_truncation = self._parse_starting_token()[1]
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:393:        for token in self.result_keys:
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:394:            if token == primary_result_key:
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:396:            sample = token.search(parsed)
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:406:                # due to a StartingToken.
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:410:            set_value_from_jmespath(parsed, token.expression, empty_value)
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:419:        next_token,
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:438:        next_token["boto_truncate_amount"] = amount_to_keep + starting_truncation
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:439:        self.resume_token = next_token
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:441:    def _get_next_token(self, parsed):
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:445:        next_tokens = {}
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:446:        for output_token, input_key in zip(self._output_token, self._input_token):
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:447:            next_token = output_token.search(parsed)
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:448:            # We do not want to include any empty strings as actual tokens.
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:450:            if next_token:
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:451:                next_tokens[input_key] = next_token
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:453:                next_tokens[input_key] = None
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:454:        return next_tokens
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:508:        if self.resume_token is not None:
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:509:            complete_result["NextToken"] = self.resume_token
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:512:    def _parse_starting_token(self):
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:513:        if self._starting_token is None:
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:516:        # The starting token is a dict passed as a base64 encoded string.
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:517:        next_token = self._starting_token
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:519:            next_token = self._token_decoder.decode(next_token)
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:521:            if "boto_truncate_amount" in next_token:
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:522:                index = next_token.get("boto_truncate_amount")
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:523:                del next_token["boto_truncate_amount"]
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:525:            next_token, index = self._parse_starting_token_deprecated()
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:526:        return next_token, index
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:528:    def _parse_starting_token_deprecated(self):
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:530:        This handles parsing of old style starting tokens, and attempts to
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:534:            "Attempting to fall back to old starting token parser. For token: %s",
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:535:            self._starting_token,
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:537:        if self._starting_token is None:
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:540:        parts = self._starting_token.split("___")
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:541:        next_token = []
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:543:        if len(parts) == len(self._input_token) + 1:
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:547:                # This doesn't look like a valid old-style token, so we're
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:548:                # passing it along as an opaque service token.
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:549:                parts = [self._starting_token]
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:553:                next_token.append(None)
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:555:                next_token.append(part)
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:556:        return self._convert_deprecated_starting_token(next_token), index
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:558:    def _convert_deprecated_starting_token(self, deprecated_token):
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:560:        This attempts to convert a deprecated starting token into the new
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:563:        len_deprecated_token = len(deprecated_token)
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:564:        len_input_token = len(self._input_token)
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:565:        if len_deprecated_token > len_input_token:
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:566:            raise ValueError(f"Bad starting token: {self._starting_token}")
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:567:        elif len_deprecated_token < len_input_token:
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:569:                "Old format starting token does not contain all input "
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:570:                "tokens. Setting the rest, in order, as None."
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:572:            for i in range(len_input_token - len_deprecated_token):
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:573:                deprecated_token.append(None)
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:574:        return dict(zip(self._input_token, deprecated_token))
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:584:        self._output_token = self._get_output_tokens(self._pagination_cfg)
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:585:        self._input_token = self._get_input_tokens(self._pagination_cfg)
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:586:        self._more_results = self._get_more_results_token(self._pagination_cfg)
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:601:    def _get_output_tokens(self, config):
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:603:        output_token = config["output_token"]
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:604:        if not isinstance(output_token, list):
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:605:            output_token = [output_token]
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:606:        for config in output_token:
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:610:    def _get_input_tokens(self, config):
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:611:        input_token = self._pagination_cfg["input_token"]
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:612:        if not isinstance(input_token, list):
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:613:            input_token = [input_token]
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:614:        return input_token
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:616:    def _get_more_results_token(self, config):
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:643:            self._input_token,
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:644:            self._output_token,
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:650:            page_params["StartingToken"],
./.venv-build/lib/python3.11/site-packages/botocore/paginate.py:676:            "StartingToken": pagination_config.get("StartingToken", None),
./.venv-build/lib/python3.11/site-packages/botocore/handlers.py:65:    add_dsql_generate_db_auth_token_methods,
./.venv-build/lib/python3.11/site-packages/botocore/handlers.py:66:    add_generate_db_auth_token,
./.venv-build/lib/python3.11/site-packages/botocore/handlers.py:75:    get_token_from_environment,
./.venv-build/lib/python3.11/site-packages/botocore/handlers.py:281:                "injecting idempotency token (%s) into param '%s'.",
./.venv-build/lib/python3.11/site-packages/botocore/handlers.py:816:    # Delimiter, Prefix, ContinuationToken, Key, and StartAfter.
./.venv-build/lib/python3.11/site-packages/botocore/handlers.py:1288:    authentication scheme preferences, and the availability of a bearer token.
./.venv-build/lib/python3.11/site-packages/botocore/handlers.py:1314:    # Prefer 'bearer' signature version if a bearer token is available, and it
./.venv-build/lib/python3.11/site-packages/botocore/handlers.py:1343:    has_token = get_token_from_environment(signing_name) is not None
./.venv-build/lib/python3.11/site-packages/botocore/handlers.py:1345:    # Prefer 'bearer' if a bearer token is available, and either:
./.venv-build/lib/python3.11/site-packages/botocore/handlers.py:1348:    return has_token and (resolved_signature_version == "bearer" or not has_in_code_configuration)
./.venv-build/lib/python3.11/site-packages/botocore/handlers.py:1353:    Returns a set of services that support bearer token authentication.
./.venv-build/lib/python3.11/site-packages/botocore/handlers.py:1454:    ("choose-signer.cognito-identity.GetOpenIdToken", disable_signing),
./.venv-build/lib/python3.11/site-packages/botocore/handlers.py:1577:    ("creating-client-class.dsql", add_dsql_generate_db_auth_token_methods),
./.venv-build/lib/python3.11/site-packages/botocore/handlers.py:1581:    ("creating-client-class.rds", add_generate_db_auth_token),
./.venv-build/lib/python3.11/site-packages/botocore/useragent.py:83:    "CREDENTIALS_ENV_VARS_STS_WEB_ID_TOKEN": "h",
./.venv-build/lib/python3.11/site-packages/botocore/useragent.py:89:    "CREDENTIALS_PROFILE_STS_WEB_ID_TOKEN": "q",
./.venv-build/lib/python3.11/site-packages/botocore/signers.py:28:from botocore.tokens import FrozenAuthToken
./.venv-build/lib/python3.11/site-packages/botocore/signers.py:79:        auth_token=None,
./.venv-build/lib/python3.11/site-packages/botocore/signers.py:85:        self._auth_token = auth_token
./.venv-build/lib/python3.11/site-packages/botocore/signers.py:277:        if cls.REQUIRES_TOKEN is True:
./.venv-build/lib/python3.11/site-packages/botocore/signers.py:278:            if self._auth_token and not isinstance(self._auth_token, FrozenAuthToken):
./.venv-build/lib/python3.11/site-packages/botocore/signers.py:279:                frozen_token = self._auth_token.get_frozen_token()
./.venv-build/lib/python3.11/site-packages/botocore/signers.py:281:                frozen_token = self._auth_token
./.venv-build/lib/python3.11/site-packages/botocore/signers.py:282:            auth = cls(frozen_token)
./.venv-build/lib/python3.11/site-packages/botocore/signers.py:361:            private_key = open('private_key.pem', 'r').read()
./.venv-build/lib/python3.11/site-packages/botocore/signers.py:364:                rsa.PrivateKey.load_pkcs1(private_key.encode('utf8')),
./.venv-build/lib/python3.11/site-packages/botocore/signers.py:481:def add_generate_db_auth_token(class_attributes, **kwargs):
./.venv-build/lib/python3.11/site-packages/botocore/signers.py:482:    class_attributes["generate_db_auth_token"] = generate_db_auth_token
./.venv-build/lib/python3.11/site-packages/botocore/signers.py:485:def add_dsql_generate_db_auth_token_methods(class_attributes, **kwargs):
./.venv-build/lib/python3.11/site-packages/botocore/signers.py:486:    class_attributes["generate_db_connect_auth_token"] = dsql_generate_db_connect_auth_token
./.venv-build/lib/python3.11/site-packages/botocore/signers.py:487:    class_attributes["generate_db_connect_admin_auth_token"] = (
./.venv-build/lib/python3.11/site-packages/botocore/signers.py:488:        dsql_generate_db_connect_admin_auth_token
./.venv-build/lib/python3.11/site-packages/botocore/signers.py:492:def generate_db_auth_token(self, DBHostname, Port, DBUsername, Region=None):
./.venv-build/lib/python3.11/site-packages/botocore/signers.py:493:    """Generates an auth token used to connect to a db with IAM credentials.
./.venv-build/lib/python3.11/site-packages/botocore/signers.py:508:    :return: A presigned url which can be used as an auth token.
./.venv-build/lib/python3.11/site-packages/botocore/signers.py:546:def _dsql_generate_db_auth_token(self, Hostname, Action, Region=None, ExpiresIn=900):
./.venv-build/lib/python3.11/site-packages/botocore/signers.py:547:    """Generate a DSQL database token for an arbitrary action.
./.venv-build/lib/python3.11/site-packages/botocore/signers.py:559:    :param ExpiresIn: The token expiry duration in seconds (default is 900 seconds).
./.venv-build/lib/python3.11/site-packages/botocore/signers.py:561:    :return: A presigned url which can be used as an auth token.
./.venv-build/lib/python3.11/site-packages/botocore/signers.py:595:def dsql_generate_db_connect_auth_token(self, Hostname, Region=None, ExpiresIn=900):
./.venv-build/lib/python3.11/site-packages/botocore/signers.py:596:    """Generate a DSQL database token for the "DbConnect" action.
./.venv-build/lib/python3.11/site-packages/botocore/signers.py:605:    :param ExpiresIn: The token expiry duration in seconds (default is 900 seconds).
./.venv-build/lib/python3.11/site-packages/botocore/signers.py:607:    :return: A presigned url which can be used as an auth token.
./.venv-build/lib/python3.11/site-packages/botocore/signers.py:609:    return _dsql_generate_db_auth_token(self, Hostname, "DbConnect", Region, ExpiresIn)
./.venv-build/lib/python3.11/site-packages/botocore/signers.py:612:def dsql_generate_db_connect_admin_auth_token(self, Hostname, Region=None, ExpiresIn=900):
./.venv-build/lib/python3.11/site-packages/botocore/signers.py:613:    """Generate a DSQL database token for the "DbConnectAdmin" action.
./.venv-build/lib/python3.11/site-packages/botocore/signers.py:622:    :param ExpiresIn: The token expiry duration in seconds (default is 900 seconds).
./.venv-build/lib/python3.11/site-packages/botocore/signers.py:624:    :return: A presigned url which can be used as an auth token.
./.venv-build/lib/python3.11/site-packages/botocore/signers.py:626:    return _dsql_generate_db_auth_token(self, Hostname, "DbConnectAdmin", Region, ExpiresIn)
./.venv-build/lib/python3.11/site-packages/botocore/client.py:85:        auth_token_resolver=None,
./.venv-build/lib/python3.11/site-packages/botocore/client.py:101:        self._auth_token_resolver = auth_token_resolver
./.venv-build/lib/python3.11/site-packages/botocore/client.py:114:        auth_token=None,
./.venv-build/lib/python3.11/site-packages/botocore/client.py:145:        if token := self._evaluate_client_specific_token(service_model.signing_name):
./.venv-build/lib/python3.11/site-packages/botocore/client.py:146:            auth_token = token
./.venv-build/lib/python3.11/site-packages/botocore/client.py:157:            auth_token,
./.venv-build/lib/python3.11/site-packages/botocore/client.py:492:        auth_token,
./.venv-build/lib/python3.11/site-packages/botocore/client.py:515:            auth_token,
./.venv-build/lib/python3.11/site-packages/botocore/client.py:563:    def _evaluate_client_specific_token(self, signing_name):
./.venv-build/lib/python3.11/site-packages/botocore/client.py:564:        # Resolves an auth_token for the given signing_name.
./.venv-build/lib/python3.11/site-packages/botocore/client.py:566:        resolver = self._auth_token_resolver
./.venv-build/lib/python3.11/site-packages/botocore/crt/auth.py:43:        "X-Amz-Security-Token",
./.venv-build/lib/python3.11/site-packages/botocore/crt/auth.py:73:            secret_access_key=self.credentials.secret_key,
./.venv-build/lib/python3.11/site-packages/botocore/crt/auth.py:74:            session_token=self.credentials.token,
./.venv-build/lib/python3.11/site-packages/botocore/crt/auth.py:234:        "X-Amz-Security-Token",
./.venv-build/lib/python3.11/site-packages/botocore/crt/auth.py:260:            secret_access_key=self.credentials.secret_key,
./.venv-build/lib/python3.11/site-packages/botocore/crt/auth.py:261:            session_token=self.credentials.token,
./.venv-build/lib/python3.11/site-packages/botocore/plugin.py:47:    token = _plugin_context.set(ctx)
./.venv-build/lib/python3.11/site-packages/botocore/plugin.py:48:    return token
./.venv-build/lib/python3.11/site-packages/botocore/plugin.py:51:def reset_plugin_context(token):
./.venv-build/lib/python3.11/site-packages/botocore/plugin.py:53:    _plugin_context.reset(token)
./.venv-build/lib/python3.11/site-packages/botocore/context.py:52:    :rtype: contextvars.Token
./.venv-build/lib/python3.11/site-packages/botocore/context.py:53:    :returns: Token object used to revert the context variable to what it was
./.venv-build/lib/python3.11/site-packages/botocore/context.py:56:    token = _context.set(ctx)
./.venv-build/lib/python3.11/site-packages/botocore/context.py:57:    return token
./.venv-build/lib/python3.11/site-packages/botocore/context.py:60:def reset_context(token):
./.venv-build/lib/python3.11/site-packages/botocore/context.py:63:    :type token: contextvars.Token
./.venv-build/lib/python3.11/site-packages/botocore/context.py:64:    :param token: Token object to reset the context variable.
./.venv-build/lib/python3.11/site-packages/botocore/context.py:66:    _context.reset(token)
./.venv-build/lib/python3.11/site-packages/botocore/context.py:93:    token = set_context(new)
./.venv-build/lib/python3.11/site-packages/botocore/context.py:97:        reset_context(token)
./.venv-build/lib/python3.11/site-packages/botocore/monitoring.py:429:        if "X-Amz-Security-Token" in request_headers:
./.venv-build/lib/python3.11/site-packages/botocore/monitoring.py:430:            event_dict["SessionToken"] = request_headers["X-Amz-Security-Token"]
./.venv-build/lib/python3.11/site-packages/botocore/tokens.py:29:    TokenRetrievalError,
./.venv-build/lib/python3.11/site-packages/botocore/tokens.py:34:    SSOTokenLoader,
./.venv-build/lib/python3.11/site-packages/botocore/tokens.py:36:    get_token_from_environment,
./.venv-build/lib/python3.11/site-packages/botocore/tokens.py:46:def create_token_resolver(session):
./.venv-build/lib/python3.11/site-packages/botocore/tokens.py:48:        ScopedEnvTokenProvider(session),
./.venv-build/lib/python3.11/site-packages/botocore/tokens.py:49:        SSOTokenProvider(session),
./.venv-build/lib/python3.11/site-packages/botocore/tokens.py:51:    return TokenProviderChain(providers=providers)
./.venv-build/lib/python3.11/site-packages/botocore/tokens.py:64:class FrozenAuthToken(NamedTuple):
./.venv-build/lib/python3.11/site-packages/botocore/tokens.py:65:    token: str
./.venv-build/lib/python3.11/site-packages/botocore/tokens.py:69:class DeferredRefreshableToken:
./.venv-build/lib/python3.11/site-packages/botocore/tokens.py:73:    # The time at which all threads will block waiting for a refreshed token
./.venv-build/lib/python3.11/site-packages/botocore/tokens.py:83:        # The frozen token is protected by this lock
./.venv-build/lib/python3.11/site-packages/botocore/tokens.py:85:        self._frozen_token = None
./.venv-build/lib/python3.11/site-packages/botocore/tokens.py:88:    def get_frozen_token(self):
./.venv-build/lib/python3.11/site-packages/botocore/tokens.py:90:        return self._frozen_token
./.venv-build/lib/python3.11/site-packages/botocore/tokens.py:116:            self._frozen_token = self._refresh_using()
./.venv-build/lib/python3.11/site-packages/botocore/tokens.py:119:                "Refreshing token failed during the %s refresh period.",
./.venv-build/lib/python3.11/site-packages/botocore/tokens.py:129:            raise TokenRetrievalError(
./.venv-build/lib/python3.11/site-packages/botocore/tokens.py:131:                error_msg="Token has expired and refresh failed",
./.venv-build/lib/python3.11/site-packages/botocore/tokens.py:135:        if self._frozen_token is None:
./.venv-build/lib/python3.11/site-packages/botocore/tokens.py:138:        expiration = self._frozen_token.expiration
./.venv-build/lib/python3.11/site-packages/botocore/tokens.py:143:        if self._frozen_token is None:
./.venv-build/lib/python3.11/site-packages/botocore/tokens.py:144:            # We don't have a token yet, mandatory refresh
./.venv-build/lib/python3.11/site-packages/botocore/tokens.py:147:        expiration = self._frozen_token.expiration
./.venv-build/lib/python3.11/site-packages/botocore/tokens.py:166:class TokenProviderChain:
./.venv-build/lib/python3.11/site-packages/botocore/tokens.py:172:    def load_token(self, **kwargs):
./.venv-build/lib/python3.11/site-packages/botocore/tokens.py:174:            token = provider.load_token(**kwargs)
./.venv-build/lib/python3.11/site-packages/botocore/tokens.py:175:            if token is not None:
./.venv-build/lib/python3.11/site-packages/botocore/tokens.py:176:                return token
./.venv-build/lib/python3.11/site-packages/botocore/tokens.py:180:class SSOTokenProvider:
./.venv-build/lib/python3.11/site-packages/botocore/tokens.py:183:    _SSO_TOKEN_CACHE_DIR = os.path.expanduser(os.path.join("~", ".aws", "sso", "cache"))
./.venv-build/lib/python3.11/site-packages/botocore/tokens.py:188:    _GRANT_TYPE = "refresh_token"
./.venv-build/lib/python3.11/site-packages/botocore/tokens.py:195:                self._SSO_TOKEN_CACHE_DIR,
./.venv-build/lib/python3.11/site-packages/botocore/tokens.py:200:        self._token_loader = SSOTokenLoader(cache=self._cache)
./.venv-build/lib/python3.11/site-packages/botocore/tokens.py:220:                f'token provider but the "{sso_session_name}" sso_session '
./.venv-build/lib/python3.11/site-packages/botocore/tokens.py:233:                f"token provider but is missing the following configuration: "
./.venv-build/lib/python3.11/site-packages/botocore/tokens.py:256:    def _attempt_create_token(self, token):
./.venv-build/lib/python3.11/site-packages/botocore/tokens.py:257:        response = self._client.create_token(
./.venv-build/lib/python3.11/site-packages/botocore/tokens.py:259:            clientId=token["clientId"],
./.venv-build/lib/python3.11/site-packages/botocore/tokens.py:260:            clientSecret=token["clientSecret"],
./.venv-build/lib/python3.11/site-packages/botocore/tokens.py:261:            refreshToken=token["refreshToken"],
./.venv-build/lib/python3.11/site-packages/botocore/tokens.py:264:        new_token = {
./.venv-build/lib/python3.11/site-packages/botocore/tokens.py:267:            "accessToken": response["accessToken"],
./.venv-build/lib/python3.11/site-packages/botocore/tokens.py:269:            # Cache the registration alongside the token
./.venv-build/lib/python3.11/site-packages/botocore/tokens.py:270:            "clientId": token["clientId"],
./.venv-build/lib/python3.11/site-packages/botocore/tokens.py:271:            "clientSecret": token["clientSecret"],
./.venv-build/lib/python3.11/site-packages/botocore/tokens.py:272:            "registrationExpiresAt": token["registrationExpiresAt"],
./.venv-build/lib/python3.11/site-packages/botocore/tokens.py:274:        if "refreshToken" in response:
./.venv-build/lib/python3.11/site-packages/botocore/tokens.py:275:            new_token["refreshToken"] = response["refreshToken"]
./.venv-build/lib/python3.11/site-packages/botocore/tokens.py:276:        logger.info("SSO Token refresh succeeded")
./.venv-build/lib/python3.11/site-packages/botocore/tokens.py:277:        return new_token
./.venv-build/lib/python3.11/site-packages/botocore/tokens.py:279:    def _refresh_access_token(self, token):
./.venv-build/lib/python3.11/site-packages/botocore/tokens.py:281:            "refreshToken",
./.venv-build/lib/python3.11/site-packages/botocore/tokens.py:283:            "clientSecret",
./.venv-build/lib/python3.11/site-packages/botocore/tokens.py:286:        missing_keys = [k for k in keys if k not in token]
./.venv-build/lib/python3.11/site-packages/botocore/tokens.py:288:            msg = f"Unable to refresh SSO token: missing keys: {missing_keys}"
./.venv-build/lib/python3.11/site-packages/botocore/tokens.py:292:        expiry = dateutil.parser.parse(token["registrationExpiresAt"])
./.venv-build/lib/python3.11/site-packages/botocore/tokens.py:294:            logger.info("SSO token registration expired at %s", expiry)
./.venv-build/lib/python3.11/site-packages/botocore/tokens.py:298:            return self._attempt_create_token(token)
./.venv-build/lib/python3.11/site-packages/botocore/tokens.py:300:            logger.warning("SSO token refresh attempt failed", exc_info=True)
./.venv-build/lib/python3.11/site-packages/botocore/tokens.py:306:        logger.info("Loading cached SSO token for %s", session_name)
./.venv-build/lib/python3.11/site-packages/botocore/tokens.py:307:        token_dict = self._token_loader(start_url, session_name=session_name)
./.venv-build/lib/python3.11/site-packages/botocore/tokens.py:308:        expiration = dateutil.parser.parse(token_dict["expiresAt"])
./.venv-build/lib/python3.11/site-packages/botocore/tokens.py:309:        logger.debug("Cached SSO token expires at %s", expiration)
./.venv-build/lib/python3.11/site-packages/botocore/tokens.py:313:            new_token_dict = self._refresh_access_token(token_dict)
./.venv-build/lib/python3.11/site-packages/botocore/tokens.py:314:            if new_token_dict is not None:
./.venv-build/lib/python3.11/site-packages/botocore/tokens.py:315:                token_dict = new_token_dict
./.venv-build/lib/python3.11/site-packages/botocore/tokens.py:316:                expiration = token_dict["expiresAt"]
./.venv-build/lib/python3.11/site-packages/botocore/tokens.py:317:                self._token_loader.save_token(start_url, token_dict, session_name=session_name)
./.venv-build/lib/python3.11/site-packages/botocore/tokens.py:319:        return FrozenAuthToken(token_dict["accessToken"], expiration=expiration)
./.venv-build/lib/python3.11/site-packages/botocore/tokens.py:321:    def load_token(self, **kwargs):
./.venv-build/lib/python3.11/site-packages/botocore/tokens.py:325:        return DeferredRefreshableToken(self.METHOD, self._refresher, time_fetcher=self._now)
./.venv-build/lib/python3.11/site-packages/botocore/tokens.py:328:class ScopedEnvTokenProvider:
./.venv-build/lib/python3.11/site-packages/botocore/tokens.py:330:    Token provider that loads tokens from environment variables scoped to
./.venv-build/lib/python3.11/site-packages/botocore/tokens.py:342:    def load_token(self, **kwargs):
./.venv-build/lib/python3.11/site-packages/botocore/tokens.py:347:        token = get_token_from_environment(signing_name, self.environ)
./.venv-build/lib/python3.11/site-packages/botocore/tokens.py:349:        if token is not None:
./.venv-build/lib/python3.11/site-packages/botocore/tokens.py:350:            logger.info("Found token in environment variables.")
./.venv-build/lib/python3.11/site-packages/botocore/tokens.py:351:            return FrozenAuthToken(token)
./.venv-build/lib/python3.11/site-packages/botocore/httpsession.py:195:    :param proxy_url: The proxy url, i.e. https://username:password@proxy.com
./.venv-build/lib/python3.11/site-packages/botocore/httpsession.py:203:    if parsed_url.password:
./.venv-build/lib/python3.11/site-packages/botocore/httpsession.py:204:        proxy_url = proxy_url.replace(parsed_url.password, mask, 1)
./.venv-build/lib/python3.11/site-packages/botocore/httpsession.py:241:        username, password = self._get_auth_from_url(proxy_url)
./.venv-build/lib/python3.11/site-packages/botocore/httpsession.py:242:        if username and password:
./.venv-build/lib/python3.11/site-packages/botocore/httpsession.py:243:            basic_auth = self._construct_basic_auth(username, password)
./.venv-build/lib/python3.11/site-packages/botocore/httpsession.py:259:    def _construct_basic_auth(self, username, password):
./.venv-build/lib/python3.11/site-packages/botocore/httpsession.py:260:        auth_str = f"{username}:{password}"
./.venv-build/lib/python3.11/site-packages/botocore/httpsession.py:267:            return unquote(parsed_url.username), unquote(parsed_url.password)
./.venv-build/lib/python3.11/site-packages/botocore/model.py:89:        "idempotencyToken",
./.venv-build/lib/python3.11/site-packages/botocore/model.py:181:            * idempotencyToken
./.venv-build/lib/python3.11/site-packages/botocore/model.py:617:            if "idempotencyToken" in shape.metadata and shape.metadata["idempotencyToken"]
./.venv-build/lib/python3.11/site-packages/botocore/vendored/six.py:429:    MovedAttribute("HTTPPasswordMgr", "urllib2", "urllib.request"),
./.venv-build/lib/python3.11/site-packages/botocore/vendored/six.py:430:    MovedAttribute("HTTPPasswordMgrWithDefaultRealm", "urllib2", "urllib.request"),
./.venv-build/lib/python3.11/site-packages/botocore/args.py:117:        auth_token=None,
./.venv-build/lib/python3.11/site-packages/botocore/args.py:152:            auth_token,
./.venv-build/lib/python3.11/site-packages/_pytest/compat.py:44:    token = 0
./.venv-build/lib/python3.11/site-packages/_pytest/compat.py:45:NOTSET: Final = NotSetType.token
./.venv-build/lib/python3.11/site-packages/_pytest/assertion/rewrite.py:24:import tokenize
./.venv-build/lib/python3.11/site-packages/_pytest/assertion/rewrite.py:571:    tokens = tokenize.tokenize(io.BytesIO(src).readline)
./.venv-build/lib/python3.11/site-packages/_pytest/assertion/rewrite.py:572:    for tp, source, (lineno, offset), _, line in tokens:
./.venv-build/lib/python3.11/site-packages/_pytest/assertion/rewrite.py:573:        if tp == tokenize.NAME and source == "assert":
./.venv-build/lib/python3.11/site-packages/_pytest/assertion/rewrite.py:577:            if tp == tokenize.OP and source in "([{":
./.venv-build/lib/python3.11/site-packages/_pytest/assertion/rewrite.py:579:            elif tp == tokenize.OP and source in ")]}":
./.venv-build/lib/python3.11/site-packages/_pytest/assertion/rewrite.py:586:            elif depth == 0 and tp == tokenize.OP and source == ",":
./.venv-build/lib/python3.11/site-packages/_pytest/assertion/rewrite.py:598:            elif tp in {tokenize.NEWLINE, tokenize.ENDMARKER}:
./.venv-build/lib/python3.11/site-packages/_pytest/mark/structures.py:46:    token = 0
./.venv-build/lib/python3.11/site-packages/_pytest/mark/structures.py:50:HIDDEN_PARAM = _HiddenParam.token
./.venv-build/lib/python3.11/site-packages/_pytest/mark/expression.py:46:class TokenType(enum.Enum):
./.venv-build/lib/python3.11/site-packages/_pytest/mark/expression.py:60:class Token:
./.venv-build/lib/python3.11/site-packages/_pytest/mark/expression.py:62:    type: TokenType
./.venv-build/lib/python3.11/site-packages/_pytest/mark/expression.py:83:    __slots__ = ("current", "tokens")
./.venv-build/lib/python3.11/site-packages/_pytest/mark/expression.py:86:        self.tokens = self.lex(input)
./.venv-build/lib/python3.11/site-packages/_pytest/mark/expression.py:87:        self.current = next(self.tokens)
./.venv-build/lib/python3.11/site-packages/_pytest/mark/expression.py:89:    def lex(self, input: str) -> Iterator[Token]:
./.venv-build/lib/python3.11/site-packages/_pytest/mark/expression.py:95:                yield Token(TokenType.LPAREN, "(", pos)
./.venv-build/lib/python3.11/site-packages/_pytest/mark/expression.py:98:                yield Token(TokenType.RPAREN, ")", pos)
./.venv-build/lib/python3.11/site-packages/_pytest/mark/expression.py:101:                yield Token(TokenType.EQUAL, "=", pos)
./.venv-build/lib/python3.11/site-packages/_pytest/mark/expression.py:104:                yield Token(TokenType.COMMA, ",", pos)
./.venv-build/lib/python3.11/site-packages/_pytest/mark/expression.py:119:                yield Token(TokenType.STRING, value, pos)
./.venv-build/lib/python3.11/site-packages/_pytest/mark/expression.py:126:                        yield Token(TokenType.OR, value, pos)
./.venv-build/lib/python3.11/site-packages/_pytest/mark/expression.py:128:                        yield Token(TokenType.AND, value, pos)
./.venv-build/lib/python3.11/site-packages/_pytest/mark/expression.py:130:                        yield Token(TokenType.NOT, value, pos)
./.venv-build/lib/python3.11/site-packages/_pytest/mark/expression.py:132:                        yield Token(TokenType.IDENT, value, pos)
./.venv-build/lib/python3.11/site-packages/_pytest/mark/expression.py:139:        yield Token(TokenType.EOF, "", pos)
./.venv-build/lib/python3.11/site-packages/_pytest/mark/expression.py:142:    def accept(self, type: TokenType, *, reject: Literal[True]) -> Token: ...
./.venv-build/lib/python3.11/site-packages/_pytest/mark/expression.py:145:    def accept(self, type: TokenType, *, reject: Literal[False] = False) -> Token | None: ...
./.venv-build/lib/python3.11/site-packages/_pytest/mark/expression.py:147:    def accept(self, type: TokenType, *, reject: bool = False) -> Token | None:
./.venv-build/lib/python3.11/site-packages/_pytest/mark/expression.py:149:            token = self.current
./.venv-build/lib/python3.11/site-packages/_pytest/mark/expression.py:150:            if token.type is not TokenType.EOF:
./.venv-build/lib/python3.11/site-packages/_pytest/mark/expression.py:151:                self.current = next(self.tokens)
./.venv-build/lib/python3.11/site-packages/_pytest/mark/expression.py:152:            return token
./.venv-build/lib/python3.11/site-packages/_pytest/mark/expression.py:157:    def reject(self, expected: Sequence[TokenType]) -> NoReturn:
./.venv-build/lib/python3.11/site-packages/_pytest/mark/expression.py:174:    if s.accept(TokenType.EOF):
./.venv-build/lib/python3.11/site-packages/_pytest/mark/expression.py:178:        s.accept(TokenType.EOF, reject=True)
./.venv-build/lib/python3.11/site-packages/_pytest/mark/expression.py:184:    while s.accept(TokenType.OR):
./.venv-build/lib/python3.11/site-packages/_pytest/mark/expression.py:192:    while s.accept(TokenType.AND):
./.venv-build/lib/python3.11/site-packages/_pytest/mark/expression.py:199:    if s.accept(TokenType.NOT):
./.venv-build/lib/python3.11/site-packages/_pytest/mark/expression.py:201:    if s.accept(TokenType.LPAREN):
./.venv-build/lib/python3.11/site-packages/_pytest/mark/expression.py:203:        s.accept(TokenType.RPAREN, reject=True)
./.venv-build/lib/python3.11/site-packages/_pytest/mark/expression.py:205:    ident = s.accept(TokenType.IDENT)
./.venv-build/lib/python3.11/site-packages/_pytest/mark/expression.py:208:        if s.accept(TokenType.LPAREN):
./.venv-build/lib/python3.11/site-packages/_pytest/mark/expression.py:210:            s.accept(TokenType.RPAREN, reject=True)
./.venv-build/lib/python3.11/site-packages/_pytest/mark/expression.py:215:    s.reject((TokenType.NOT, TokenType.LPAREN, TokenType.IDENT))
./.venv-build/lib/python3.11/site-packages/_pytest/mark/expression.py:222:    keyword_name = s.accept(TokenType.IDENT, reject=True)
./.venv-build/lib/python3.11/site-packages/_pytest/mark/expression.py:233:    s.accept(TokenType.EQUAL, reject=True)
./.venv-build/lib/python3.11/site-packages/_pytest/mark/expression.py:235:    if value_token := s.accept(TokenType.STRING):
./.venv-build/lib/python3.11/site-packages/_pytest/mark/expression.py:236:        value: str | int | bool | None = value_token.value[1:-1]  # strip quotes
./.venv-build/lib/python3.11/site-packages/_pytest/mark/expression.py:238:        value_token = s.accept(TokenType.IDENT, reject=True)
./.venv-build/lib/python3.11/site-packages/_pytest/mark/expression.py:239:        if (number := value_token.value).isdigit() or (
./.venv-build/lib/python3.11/site-packages/_pytest/mark/expression.py:243:        elif value_token.value in BUILTIN_MATCHERS:
./.venv-build/lib/python3.11/site-packages/_pytest/mark/expression.py:244:            value = BUILTIN_MATCHERS[value_token.value]
./.venv-build/lib/python3.11/site-packages/_pytest/mark/expression.py:247:                value_token.pos + 1,
./.venv-build/lib/python3.11/site-packages/_pytest/mark/expression.py:248:                f'unexpected character/s "{value_token.value}"',
./.venv-build/lib/python3.11/site-packages/_pytest/mark/expression.py:257:    while s.accept(TokenType.COMMA):
./.venv-build/lib/python3.11/site-packages/_pytest/_code/source.py:10:import tokenize
./.venv-build/lib/python3.11/site-packages/_pytest/_code/source.py:209:            for tok in tokenize.generate_tokens(lambda: next(it)):
./.venv-build/lib/python3.11/site-packages/_pytest/_code/source.py:210:                block_finder.tokeneater(*tok)
./.venv-build/lib/python3.11/site-packages/_pytest/python.py:877:        Format is <prm_1_token>-...-<prm_n_token>[counter], where prm_x_token is
./.venv-build/lib/python3.11/site-packages/boto3/session.py:39:    :type aws_secret_access_key: string
./.venv-build/lib/python3.11/site-packages/boto3/session.py:40:    :param aws_secret_access_key: AWS secret access key
./.venv-build/lib/python3.11/site-packages/boto3/session.py:41:    :type aws_session_token: string
./.venv-build/lib/python3.11/site-packages/boto3/session.py:42:    :param aws_session_token: AWS temporary session token
./.venv-build/lib/python3.11/site-packages/boto3/session.py:58:        aws_secret_access_key=None,
./.venv-build/lib/python3.11/site-packages/boto3/session.py:59:        aws_session_token=None,
./.venv-build/lib/python3.11/site-packages/boto3/session.py:86:            "aws_secret_access_key": aws_secret_access_key,
./.venv-build/lib/python3.11/site-packages/boto3/session.py:87:            "aws_session_token": aws_session_token,
./.venv-build/lib/python3.11/site-packages/boto3/session.py:236:        aws_secret_access_key=None,
./.venv-build/lib/python3.11/site-packages/boto3/session.py:237:        aws_session_token=None,
./.venv-build/lib/python3.11/site-packages/boto3/session.py:290:        :type aws_secret_access_key: string
./.venv-build/lib/python3.11/site-packages/boto3/session.py:291:        :param aws_secret_access_key: The secret key to use when creating
./.venv-build/lib/python3.11/site-packages/boto3/session.py:294:        :type aws_session_token: string
./.venv-build/lib/python3.11/site-packages/boto3/session.py:295:        :param aws_session_token: The session token to use when creating
./.venv-build/lib/python3.11/site-packages/boto3/session.py:321:            "aws_secret_access_key": aws_secret_access_key,
./.venv-build/lib/python3.11/site-packages/boto3/session.py:322:            "aws_session_token": aws_session_token,
./.venv-build/lib/python3.11/site-packages/boto3/session.py:342:        aws_secret_access_key=None,
./.venv-build/lib/python3.11/site-packages/boto3/session.py:343:        aws_session_token=None,
./.venv-build/lib/python3.11/site-packages/boto3/session.py:395:        :type aws_secret_access_key: string
./.venv-build/lib/python3.11/site-packages/boto3/session.py:396:        :param aws_secret_access_key: The secret key to use when creating
./.venv-build/lib/python3.11/site-packages/boto3/session.py:399:        :type aws_session_token: string
./.venv-build/lib/python3.11/site-packages/boto3/session.py:400:        :param aws_session_token: The session token to use when creating
./.venv-build/lib/python3.11/site-packages/boto3/session.py:467:            aws_secret_access_key=aws_secret_access_key,
./.venv-build/lib/python3.11/site-packages/boto3/session.py:468:            aws_session_token=aws_session_token,
./.venv-build/lib/python3.11/site-packages/boto3/session.py:543:        aws_secret_access_key,
./.venv-build/lib/python3.11/site-packages/boto3/session.py:548:        elif aws_access_key_id is None or aws_secret_access_key is None:
./.venv-build/lib/python3.11/site-packages/boto3/crt.py:146:        and boto3_creds.secret_key == crt_creds.secret_access_key
./.venv-build/lib/python3.11/site-packages/boto3/crt.py:147:        and boto3_creds.token == crt_creds.session_token
./.venv-build/lib/python3.11/site-packages/mypy/modulefinder.py:965:    {..., 'secrets': ((3, 6), None), 'symbol': ((2, 7), (3, 9)), ...}
./.venv-build/lib/python3.11/site-packages/mypy/stubdoc.py:13:import tokenize
./.venv-build/lib/python3.11/site-packages/mypy/stubdoc.py:188:    def add_token(self, token: tokenize.TokenInfo) -> None:
./.venv-build/lib/python3.11/site-packages/mypy/stubdoc.py:189:        """Process next token from the token stream."""
./.venv-build/lib/python3.11/site-packages/mypy/stubdoc.py:191:            token.type == tokenize.NAME
./.venv-build/lib/python3.11/site-packages/mypy/stubdoc.py:192:            and token.string == self.function_name
./.venv-build/lib/python3.11/site-packages/mypy/stubdoc.py:198:            token.type == tokenize.OP
./.venv-build/lib/python3.11/site-packages/mypy/stubdoc.py:199:            and token.string == "("
./.venv-build/lib/python3.11/site-packages/mypy/stubdoc.py:212:            token.type == tokenize.OP
./.venv-build/lib/python3.11/site-packages/mypy/stubdoc.py:213:            and token.string in ("[", "(", "{")
./.venv-build/lib/python3.11/site-packages/mypy/stubdoc.py:216:            self.accumulator += token.string
./.venv-build/lib/python3.11/site-packages/mypy/stubdoc.py:220:            token.type == tokenize.OP
./.venv-build/lib/python3.11/site-packages/mypy/stubdoc.py:221:            and token.string in ("]", ")", "}")
./.venv-build/lib/python3.11/site-packages/mypy/stubdoc.py:224:            self.accumulator += token.string
./.venv-build/lib/python3.11/site-packages/mypy/stubdoc.py:228:            token.type == tokenize.OP
./.venv-build/lib/python3.11/site-packages/mypy/stubdoc.py:229:            and token.string == ":"
./.venv-build/lib/python3.11/site-packages/mypy/stubdoc.py:237:            token.type == tokenize.OP
./.venv-build/lib/python3.11/site-packages/mypy/stubdoc.py:238:            and token.string == "="
./.venv-build/lib/python3.11/site-packages/mypy/stubdoc.py:250:            token.type == tokenize.OP
./.venv-build/lib/python3.11/site-packages/mypy/stubdoc.py:251:            and token.string in (",", ")")
./.venv-build/lib/python3.11/site-packages/mypy/stubdoc.py:273:                        token.string == ")" and self.accumulator.strip() == ""
./.venv-build/lib/python3.11/site-packages/mypy/stubdoc.py:279:            if token.string == ")":
./.venv-build/lib/python3.11/site-packages/mypy/stubdoc.py:309:            token.type == tokenize.OP
./.venv-build/lib/python3.11/site-packages/mypy/stubdoc.py:310:            and token.string == "/"
./.venv-build/lib/python3.11/site-packages/mypy/stubdoc.py:313:            if token.string == "/":
./.venv-build/lib/python3.11/site-packages/mypy/stubdoc.py:325:        elif token.type == tokenize.OP and token.string == "->" and self.state[-1] == STATE_INIT:
./.venv-build/lib/python3.11/site-packages/mypy/stubdoc.py:330:        elif token.type in (tokenize.NEWLINE, tokenize.ENDMARKER) and self.state[-1] in (
./.venv-build/lib/python3.11/site-packages/mypy/stubdoc.py:351:            self.accumulator += token.string
./.venv-build/lib/python3.11/site-packages/mypy/stubdoc.py:392:    with contextlib.suppress(tokenize.TokenError):
./.venv-build/lib/python3.11/site-packages/mypy/stubdoc.py:394:            tokens = tokenize.tokenize(io.BytesIO(docstr.encode("utf-8")).readline)
./.venv-build/lib/python3.11/site-packages/mypy/stubdoc.py:395:            for token in tokens:
./.venv-build/lib/python3.11/site-packages/mypy/stubdoc.py:396:                state.add_token(token)
./.venv-build/lib/python3.11/site-packages/mypy/report.py:12:import tokenize
./.venv-build/lib/python3.11/site-packages/mypy/report.py:143:        with tokenize.open(path) as input_file:
./.venv-build/lib/python3.11/site-packages/mypy/checker.py:1111:            # TODO not best fix, better have dedicated yield token
./.venv-build/lib/python3.11/site-packages/pygments/formatters/rtf.py:22:    Format tokens as RTF markup. This formatter automatically outputs full RTF
./.venv-build/lib/python3.11/site-packages/pygments/formatters/rtf.py:193:    def _split_tokens_on_newlines(self, tokensource):
./.venv-build/lib/python3.11/site-packages/pygments/formatters/rtf.py:195:        Split tokens containing newline characters into multiple token
./.venv-build/lib/python3.11/site-packages/pygments/formatters/rtf.py:199:        for ttype, value in tokensource:
./.venv-build/lib/python3.11/site-packages/pygments/formatters/rtf.py:280:    def format_unencoded(self, tokensource, outfile):
./.venv-build/lib/python3.11/site-packages/pygments/formatters/rtf.py:284:        tokensource = self._split_tokens_on_newlines(tokensource)
./.venv-build/lib/python3.11/site-packages/pygments/formatters/rtf.py:286:        # first pass of tokens to count lines, needed for line numbering
./.venv-build/lib/python3.11/site-packages/pygments/formatters/rtf.py:289:            tokens = []  # for copying the token source generator
./.venv-build/lib/python3.11/site-packages/pygments/formatters/rtf.py:290:            for ttype, value in tokensource:
./.venv-build/lib/python3.11/site-packages/pygments/formatters/rtf.py:291:                tokens.append((ttype, value))
./.venv-build/lib/python3.11/site-packages/pygments/formatters/rtf.py:298:            tokensource = tokens
./.venv-build/lib/python3.11/site-packages/pygments/formatters/rtf.py:303:        for ttype, value in tokensource:
./.venv-build/lib/python3.11/site-packages/pygments/formatters/rtf.py:315:            while not self.style.styles_token(ttype) and ttype.parent:
./.venv-build/lib/python3.11/site-packages/pygments/formatters/rtf.py:317:            style = self.style.style_for_token(ttype)
./.venv-build/lib/python3.11/site-packages/pygments/formatters/img.py:494:        Get the correct color for the token from the style.
./.venv-build/lib/python3.11/site-packages/pygments/formatters/img.py:504:        Get the correct background color for the token from the style.
./.venv-build/lib/python3.11/site-packages/pygments/formatters/img.py:545:    def _create_drawables(self, tokensource):
./.venv-build/lib/python3.11/site-packages/pygments/formatters/img.py:547:        Create drawables for the token content.
./.venv-build/lib/python3.11/site-packages/pygments/formatters/img.py:551:        for ttype, value in tokensource:
./.venv-build/lib/python3.11/site-packages/pygments/formatters/img.py:612:    def format(self, tokensource, outfile):
./.venv-build/lib/python3.11/site-packages/pygments/formatters/img.py:614:        Format ``tokensource``, an iterable of ``(tokentype, tokenstring)``
./.venv-build/lib/python3.11/site-packages/pygments/formatters/img.py:617:        This implementation calculates where it should draw each token on the
./.venv-build/lib/python3.11/site-packages/pygments/formatters/img.py:620:        self._create_drawables(tokensource)
./.venv-build/lib/python3.11/site-packages/pygments/formatters/groff.py:20:    Format tokens with groff escapes to change their color and font style.
./.venv-build/lib/python3.11/site-packages/pygments/formatters/groff.py:131:    def format_unencoded(self, tokensource, outfile):
./.venv-build/lib/python3.11/site-packages/pygments/formatters/groff.py:139:        for ttype, value in tokensource:
./.venv-build/lib/python3.11/site-packages/pygments/formatters/_mapping.py:10:        "Format tokens with BBcodes. These formatting codes are used by many bulletin boards, so you can highlight your sourcecode with pygments before posting it there.",
./.venv-build/lib/python3.11/site-packages/pygments/formatters/_mapping.py:31:        "Format tokens with groff escapes to change their color and font style.",
./.venv-build/lib/python3.11/site-packages/pygments/formatters/_mapping.py:38:        "Format tokens as HTML 4 ``<span>`` tags. By default, the content is enclosed in a ``<pre>`` tag, itself wrapped in a ``<div>`` tag (but see the `nowrap` option). The ``<div>``'s CSS class can be set by the `cssclass` option.",
./.venv-build/lib/python3.11/site-packages/pygments/formatters/_mapping.py:45:        "Format tokens with IRC color sequences",
./.venv-build/lib/python3.11/site-packages/pygments/formatters/_mapping.py:66:        "Format tokens as LaTeX code. This needs the `fancyvrb` and `color` standard packages.",
./.venv-build/lib/python3.11/site-packages/pygments/formatters/_mapping.py:80:        "Format tokens as Pango Markup code. It can then be rendered to an SVG.",
./.venv-build/lib/python3.11/site-packages/pygments/formatters/_mapping.py:82:    "RawTokenFormatter": (
./.venv-build/lib/python3.11/site-packages/pygments/formatters/_mapping.py:84:        "Raw tokens",
./.venv-build/lib/python3.11/site-packages/pygments/formatters/_mapping.py:85:        ("raw", "tokens"),
./.venv-build/lib/python3.11/site-packages/pygments/formatters/_mapping.py:87:        "Format tokens as a raw representation for storing token streams.",
./.venv-build/lib/python3.11/site-packages/pygments/formatters/_mapping.py:94:        "Format tokens as RTF markup. This formatter automatically outputs full RTF documents with color information and other useful stuff. Perfect for Copy and Paste into Microsoft(R) Word(R) documents.",
./.venv-build/lib/python3.11/site-packages/pygments/formatters/_mapping.py:101:        "Format tokens as an SVG graphics file.  This formatter is still experimental. Each line of code is a ``<text>`` element with explicit ``x`` and ``y`` coordinates containing ``<tspan>`` elements with the individual token styles.",
./.venv-build/lib/python3.11/site-packages/pygments/formatters/_mapping.py:108:        "Format tokens with ANSI color sequences, for output in a 256-color terminal or console.  Like in `TerminalFormatter` color sequences are terminated at newlines, so that paging the output works correctly.",
./.venv-build/lib/python3.11/site-packages/pygments/formatters/_mapping.py:115:        "Format tokens with ANSI color sequences, for output in a text console. Color sequences are terminated at newlines, so that paging the output works correctly.",
./.venv-build/lib/python3.11/site-packages/pygments/formatters/_mapping.py:122:        "Format tokens with ANSI color sequences, for output in a true-color terminal or console.  Like in `TerminalFormatter` color sequences are terminated at newlines, so that paging the output works correctly.",
./.venv-build/lib/python3.11/site-packages/pygments/formatters/_mapping.py:129:        "Format tokens as appropriate for a new testcase.",
./.venv-build/lib/python3.11/site-packages/pygments/formatters/pangomarkup.py:30:    Format tokens as Pango Markup code. It can then be rendered to an SVG.
./.venv-build/lib/python3.11/site-packages/pygments/formatters/pangomarkup.py:44:        for token, style in self.style:
./.venv-build/lib/python3.11/site-packages/pygments/formatters/pangomarkup.py:59:            self.styles[token] = (start, end)
./.venv-build/lib/python3.11/site-packages/pygments/formatters/pangomarkup.py:61:    def format_unencoded(self, tokensource, outfile):
./.venv-build/lib/python3.11/site-packages/pygments/formatters/pangomarkup.py:67:        for ttype, value in tokensource:
./.venv-build/lib/python3.11/site-packages/pygments/formatters/latex.py:15:from pygments.token import Token, STANDARD_TYPES
./.venv-build/lib/python3.11/site-packages/pygments/formatters/latex.py:65:# each token type defined in the current style.  That obviously is
./.venv-build/lib/python3.11/site-packages/pygments/formatters/latex.py:71:# specific token type, thus falling back to the parent token type if one
./.venv-build/lib/python3.11/site-packages/pygments/formatters/latex.py:73:# forms given in token.STANDARD_TYPES.
./.venv-build/lib/python3.11/site-packages/pygments/formatters/latex.py:151:    Format tokens as LaTeX code. This needs the `fancyvrb` and `color`
./.venv-build/lib/python3.11/site-packages/pygments/formatters/latex.py:179:        If set to ``True``, don't wrap the tokens at all, not even inside a
./.venv-build/lib/python3.11/site-packages/pygments/formatters/latex.py:226:        in comment tokens is not escaped so that LaTeX can render it (default:
./.venv-build/lib/python3.11/site-packages/pygments/formatters/latex.py:283:        t2n = self.ttype2name = {Token: ""}
./.venv-build/lib/python3.11/site-packages/pygments/formatters/latex.py:343:    def format_unencoded(self, tokensource, outfile):
./.venv-build/lib/python3.11/site-packages/pygments/formatters/latex.py:367:        for ttype, value in tokensource:
./.venv-build/lib/python3.11/site-packages/pygments/formatters/latex.py:368:            if ttype in Token.Comment:
./.venv-build/lib/python3.11/site-packages/pygments/formatters/latex.py:406:            elif ttype not in Token.Escape:
./.venv-build/lib/python3.11/site-packages/pygments/formatters/latex.py:409:            while ttype is not Token:
./.venv-build/lib/python3.11/site-packages/pygments/formatters/latex.py:458:    strings and comments. All other consecutive tokens are merged and
./.venv-build/lib/python3.11/site-packages/pygments/formatters/latex.py:460:    the Token.Escape type. Finally text that is not escaped is scanned
./.venv-build/lib/python3.11/site-packages/pygments/formatters/latex.py:470:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.11/site-packages/pygments/formatters/latex.py:471:        # find and remove all the escape tokens (replace with an empty string)
./.venv-build/lib/python3.11/site-packages/pygments/formatters/latex.py:472:        # this is very similar to DelegatingLexer.get_tokens_unprocessed.
./.venv-build/lib/python3.11/site-packages/pygments/formatters/latex.py:476:        for i, t, v in self._find_safe_escape_tokens(text):
./.venv-build/lib/python3.11/site-packages/pygments/formatters/latex.py:486:        return do_insertions(insertions, self.lang.get_tokens_unprocessed(buffered))
./.venv-build/lib/python3.11/site-packages/pygments/formatters/latex.py:488:    def _find_safe_escape_tokens(self, text):
./.venv-build/lib/python3.11/site-packages/pygments/formatters/latex.py:489:        """find escape tokens that are not in strings or comments"""
./.venv-build/lib/python3.11/site-packages/pygments/formatters/latex.py:491:            self.lang.get_tokens_unprocessed(text),
./.venv-build/lib/python3.11/site-packages/pygments/formatters/latex.py:492:            lambda t: t in Token.Comment or t in Token.String,
./.venv-build/lib/python3.11/site-packages/pygments/formatters/latex.py:495:                for i2, t2, v2 in self._find_escape_tokens(v):
./.venv-build/lib/python3.11/site-packages/pygments/formatters/latex.py:501:        """Keep only the tokens that match `pred`, merge the others together"""
./.venv-build/lib/python3.11/site-packages/pygments/formatters/latex.py:517:    def _find_escape_tokens(self, text):
./.venv-build/lib/python3.11/site-packages/pygments/formatters/latex.py:518:        """Find escape tokens within text, give token=None otherwise"""
./.venv-build/lib/python3.11/site-packages/pygments/formatters/latex.py:528:                    yield index + len(sep1), Token.Escape, b
./.venv-build/lib/python3.11/site-packages/pygments/formatters/latex.py:531:                    yield index, Token.Error, sep1
./.venv-build/lib/python3.11/site-packages/pygments/formatters/svg.py:12:from pygments.token import Comment
./.venv-build/lib/python3.11/site-packages/pygments/formatters/svg.py:34:    Format tokens as an SVG graphics file.  This formatter is still experimental.
./.venv-build/lib/python3.11/site-packages/pygments/formatters/svg.py:36:    coordinates containing ``<tspan>`` elements with the individual token styles.
./.venv-build/lib/python3.11/site-packages/pygments/formatters/svg.py:119:    def format_unencoded(self, tokensource, outfile):
./.venv-build/lib/python3.11/site-packages/pygments/formatters/svg.py:121:        Format ``tokensource``, an iterable of ``(tokentype, tokenstring)``
./.venv-build/lib/python3.11/site-packages/pygments/formatters/svg.py:155:        for ttype, value in tokensource:
./.venv-build/lib/python3.11/site-packages/pygments/formatters/svg.py:180:    def _get_style(self, tokentype):
./.venv-build/lib/python3.11/site-packages/pygments/formatters/svg.py:181:        if tokentype in self._stylecache:
./.venv-build/lib/python3.11/site-packages/pygments/formatters/svg.py:182:            return self._stylecache[tokentype]
./.venv-build/lib/python3.11/site-packages/pygments/formatters/svg.py:183:        otokentype = tokentype
./.venv-build/lib/python3.11/site-packages/pygments/formatters/svg.py:184:        while not self.style.styles_token(tokentype):
./.venv-build/lib/python3.11/site-packages/pygments/formatters/svg.py:185:            tokentype = tokentype.parent
./.venv-build/lib/python3.11/site-packages/pygments/formatters/svg.py:186:        value = self.style.style_for_token(tokentype)
./.venv-build/lib/python3.11/site-packages/pygments/formatters/svg.py:194:        self._stylecache[otokentype] = result
./.venv-build/lib/python3.11/site-packages/pygments/formatters/other.py:5:Other formatters: NullFormatter, RawTokenFormatter.
./.venv-build/lib/python3.11/site-packages/pygments/formatters/other.py:13:from pygments.token import Token
./.venv-build/lib/python3.11/site-packages/pygments/formatters/other.py:16:__all__ = ["NullFormatter", "RawTokenFormatter", "TestcaseFormatter"]
./.venv-build/lib/python3.11/site-packages/pygments/formatters/other.py:28:    def format(self, tokensource, outfile):
./.venv-build/lib/python3.11/site-packages/pygments/formatters/other.py:30:        for ttype, value in tokensource:
./.venv-build/lib/python3.11/site-packages/pygments/formatters/other.py:37:class RawTokenFormatter(Formatter):
./.venv-build/lib/python3.11/site-packages/pygments/formatters/other.py:39:    Format tokens as a raw representation for storing token streams.
./.venv-build/lib/python3.11/site-packages/pygments/formatters/other.py:41:    The format is ``tokentype<TAB>repr(tokenstring)\n``. The output can later
./.venv-build/lib/python3.11/site-packages/pygments/formatters/other.py:42:    be converted to a token stream with the `RawTokenLexer`, described in the
./.venv-build/lib/python3.11/site-packages/pygments/formatters/other.py:51:        If set to a color name, highlight error tokens using that color.  If
./.venv-build/lib/python3.11/site-packages/pygments/formatters/other.py:58:    name = "Raw tokens"
./.venv-build/lib/python3.11/site-packages/pygments/formatters/other.py:59:    aliases = ["raw", "tokens"]
./.venv-build/lib/python3.11/site-packages/pygments/formatters/other.py:68:        # The RawTokenFormatter outputs only ASCII. Override here.
./.venv-build/lib/python3.11/site-packages/pygments/formatters/other.py:80:    def format(self, tokensource, outfile):
./.venv-build/lib/python3.11/site-packages/pygments/formatters/other.py:84:            raise TypeError("The raw tokens formatter needs a binary " "output file")
./.venv-build/lib/python3.11/site-packages/pygments/formatters/other.py:109:            for ttype, value in tokensource:
./.venv-build/lib/python3.11/site-packages/pygments/formatters/other.py:111:                if ttype is Token.Error:
./.venv-build/lib/python3.11/site-packages/pygments/formatters/other.py:116:            for ttype, value in tokensource:
./.venv-build/lib/python3.11/site-packages/pygments/formatters/other.py:124:        tokens = [
./.venv-build/lib/python3.11/site-packages/pygments/formatters/other.py:128:        assert list(lexer.get_tokens(fragment)) == tokens
./.venv-build/lib/python3.11/site-packages/pygments/formatters/other.py:134:    Format tokens as appropriate for a new testcase.
./.venv-build/lib/python3.11/site-packages/pygments/formatters/other.py:147:    def format(self, tokensource, outfile):
./.venv-build/lib/python3.11/site-packages/pygments/formatters/other.py:151:        for ttype, value in tokensource:
./.venv-build/lib/python3.11/site-packages/pygments/formatters/terminal256.py:100:    Format tokens with ANSI color sequences, for output in a 256-color
./.venv-build/lib/python3.11/site-packages/pygments/formatters/terminal256.py:249:    def format(self, tokensource, outfile):
./.venv-build/lib/python3.11/site-packages/pygments/formatters/terminal256.py:250:        return Formatter.format(self, tokensource, outfile)
./.venv-build/lib/python3.11/site-packages/pygments/formatters/terminal256.py:252:    def format_unencoded(self, tokensource, outfile):
./.venv-build/lib/python3.11/site-packages/pygments/formatters/terminal256.py:256:        for ttype, value in tokensource:
./.venv-build/lib/python3.11/site-packages/pygments/formatters/terminal256.py:294:    Format tokens with ANSI color sequences, for output in a true-color
./.venv-build/lib/python3.11/site-packages/pygments/formatters/irc.py:12:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/formatters/irc.py:21:    Token,
./.venv-build/lib/python3.11/site-packages/pygments/formatters/irc.py:30:#: Map token types to a tuple of color values for light and dark
./.venv-build/lib/python3.11/site-packages/pygments/formatters/irc.py:33:    Token: ("", ""),
./.venv-build/lib/python3.11/site-packages/pygments/formatters/irc.py:109:    Format tokens with IRC color sequences
./.venv-build/lib/python3.11/site-packages/pygments/formatters/irc.py:121:        A dictionary mapping token types to (lightbg, darkbg) color names or
./.venv-build/lib/python3.11/site-packages/pygments/formatters/irc.py:145:    def format_unencoded(self, tokensource, outfile):
./.venv-build/lib/python3.11/site-packages/pygments/formatters/irc.py:148:        for ttype, value in tokensource:
./.venv-build/lib/python3.11/site-packages/pygments/formatters/html.py:18:from pygments.token import Token, Text, STANDARD_TYPES
./.venv-build/lib/python3.11/site-packages/pygments/formatters/html.py:126:    Format tokens as HTML 4 ``<span>`` tags. By default, the content is enclosed
./.venv-build/lib/python3.11/site-packages/pygments/formatters/html.py:180:    `get_style_defs()` method to request multiple prefixes for the tokens:
./.venv-build/lib/python3.11/site-packages/pygments/formatters/html.py:200:        around the tokens. This disables most other options (default: ``False``).
./.venv-build/lib/python3.11/site-packages/pygments/formatters/html.py:217:        If set to true, token ``<span>`` tags (as well as line number elements)
./.venv-build/lib/python3.11/site-packages/pygments/formatters/html.py:223:        Since the token types use relatively short class names, they may clash
./.venv-build/lib/python3.11/site-packages/pygments/formatters/html.py:226:        CSS class names for token types.
./.venv-build/lib/python3.11/site-packages/pygments/formatters/html.py:362:    `debug_token_types`
./.venv-build/lib/python3.11/site-packages/pygments/formatters/html.py:363:        Add ``title`` attributes to all token ``<span>`` tags that show the
./.venv-build/lib/python3.11/site-packages/pygments/formatters/html.py:364:        name of the token.
./.venv-build/lib/python3.11/site-packages/pygments/formatters/html.py:438:        self.debug_token_types = get_bool_opt(options, "debug_token_types", False)
./.venv-build/lib/python3.11/site-packages/pygments/formatters/html.py:474:        """Return the css class of this token type prefixed with
./.venv-build/lib/python3.11/site-packages/pygments/formatters/html.py:482:        """Return the CSS classes of this token type prefixed with the classprefix option."""
./.venv-build/lib/python3.11/site-packages/pygments/formatters/html.py:490:        """Return the inline CSS styles for this token type."""
./.venv-build/lib/python3.11/site-packages/pygments/formatters/html.py:498:        t2c = self.ttype2class = {Token: ""}
./.venv-build/lib/python3.11/site-packages/pygments/formatters/html.py:525:        insert before the token type classes.
./.venv-build/lib/python3.11/site-packages/pygments/formatters/html.py:531:        style_lines.extend(self.get_token_style_defs(arg))
./.venv-build/lib/python3.11/site-packages/pygments/formatters/html.py:535:    def get_token_style_defs(self, arg=None):
./.venv-build/lib/python3.11/site-packages/pygments/formatters/html.py:844:    def _format_lines(self, tokensource):
./.venv-build/lib/python3.11/site-packages/pygments/formatters/html.py:846:        Just format the tokens, without any wrapping tags.
./.venv-build/lib/python3.11/site-packages/pygments/formatters/html.py:855:        for ttype, value in tokensource:
./.venv-build/lib/python3.11/site-packages/pygments/formatters/html.py:859:                title = ' title="{}"'.format(".".join(ttype)) if self.debug_token_types else ""
./.venv-build/lib/python3.11/site-packages/pygments/formatters/html.py:877:            if tagsfile and ttype in Token.Name:
./.venv-build/lib/python3.11/site-packages/pygments/formatters/html.py:926:    def _lookup_ctag(self, token):
./.venv-build/lib/python3.11/site-packages/pygments/formatters/html.py:928:        if self._ctags.find(entry, token.encode(), 0):
./.venv-build/lib/python3.11/site-packages/pygments/formatters/html.py:933:    def _highlight_lines(self, tokensource):
./.venv-build/lib/python3.11/site-packages/pygments/formatters/html.py:936:        post-processing the token stream coming from `_format_lines`.
./.venv-build/lib/python3.11/site-packages/pygments/formatters/html.py:940:        for i, (t, value) in enumerate(tokensource):
./.venv-build/lib/python3.11/site-packages/pygments/formatters/html.py:969:    def format_unencoded(self, tokensource, outfile):
./.venv-build/lib/python3.11/site-packages/pygments/formatters/html.py:978:        is part of the original tokensource being highlighted, if it's
./.venv-build/lib/python3.11/site-packages/pygments/formatters/html.py:983:        source = self._format_lines(tokensource)
./.venv-build/lib/python3.11/site-packages/pygments/formatters/bbcode.py:19:    Format tokens with BBcodes. These formatting codes are used by many
./.venv-build/lib/python3.11/site-packages/pygments/formatters/bbcode.py:78:    def format_unencoded(self, tokensource, outfile):
./.venv-build/lib/python3.11/site-packages/pygments/formatters/bbcode.py:87:        for ttype, value in tokensource:
./.venv-build/lib/python3.11/site-packages/pygments/formatters/terminal.py:12:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/formatters/terminal.py:21:    Token,
./.venv-build/lib/python3.11/site-packages/pygments/formatters/terminal.py:31:#: Map token types to a tuple of color values for light and dark
./.venv-build/lib/python3.11/site-packages/pygments/formatters/terminal.py:34:    Token: ("", ""),
./.venv-build/lib/python3.11/site-packages/pygments/formatters/terminal.py:65:    Format tokens with ANSI color sequences, for output in a text console.
./.venv-build/lib/python3.11/site-packages/pygments/formatters/terminal.py:79:        A dictionary mapping token types to (lightbg, darkbg) color names or
./.venv-build/lib/python3.11/site-packages/pygments/formatters/terminal.py:98:    def format(self, tokensource, outfile):
./.venv-build/lib/python3.11/site-packages/pygments/formatters/terminal.py:99:        return Formatter.format(self, tokensource, outfile)
./.venv-build/lib/python3.11/site-packages/pygments/formatters/terminal.py:107:        # have to walk the tree of dots.  The base Token type must be a key,
./.venv-build/lib/python3.11/site-packages/pygments/formatters/terminal.py:115:    def format_unencoded(self, tokensource, outfile):
./.venv-build/lib/python3.11/site-packages/pygments/formatters/terminal.py:119:        for ttype, value in tokensource:
./.venv-build/lib/python3.11/site-packages/pygments/styles/sas.py:14:from pygments.token import Keyword, Name, Comment, String, Error, Number, Other, Whitespace, Generic
./.venv-build/lib/python3.11/site-packages/pygments/styles/lovelace.py:16:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/styles/default.py:12:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/styles/nord.py:13:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/styles/nord.py:25:    Token,
./.venv-build/lib/python3.11/site-packages/pygments/styles/nord.py:48:        Token: "#d8dee9",
./.venv-build/lib/python3.11/site-packages/pygments/styles/nord.py:110:        Token: "#d8dee9",
./.venv-build/lib/python3.11/site-packages/pygments/styles/native.py:12:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/styles/native.py:21:    Token,
./.venv-build/lib/python3.11/site-packages/pygments/styles/native.py:41:        Token: "#d0d0d0",
./.venv-build/lib/python3.11/site-packages/pygments/styles/manni.py:15:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/styles/dracula.py:15:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/styles/stata_dark.py:14:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/styles/stata_dark.py:15:    Token,
./.venv-build/lib/python3.11/site-packages/pygments/styles/stata_dark.py:38:        Token: "#cccccc",
./.venv-build/lib/python3.11/site-packages/pygments/styles/monokai.py:14:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/styles/monokai.py:20:    Token,
./.venv-build/lib/python3.11/site-packages/pygments/styles/monokai.py:46:        Token: "#f8f8f2",  # class:  ''
./.venv-build/lib/python3.11/site-packages/pygments/styles/perldoc.py:14:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/styles/gruvbox.py:13:from pygments.token import Token, Keyword, Name, Comment, String, Error, Number, Operator, Generic
./.venv-build/lib/python3.11/site-packages/pygments/styles/gruvbox.py:30:        Token: "#dddddd",
./.venv-build/lib/python3.11/site-packages/pygments/styles/lilypond.py:12:from pygments.token import Token
./.venv-build/lib/python3.11/site-packages/pygments/styles/lilypond.py:32:        Token.Text: "",
./.venv-build/lib/python3.11/site-packages/pygments/styles/lilypond.py:33:        Token.Keyword: "bold",
./.venv-build/lib/python3.11/site-packages/pygments/styles/lilypond.py:34:        Token.Comment: "italic #A3AAB2",
./.venv-build/lib/python3.11/site-packages/pygments/styles/lilypond.py:35:        Token.String: "#AB0909",
./.venv-build/lib/python3.11/site-packages/pygments/styles/lilypond.py:36:        Token.String.Escape: "#C46C6C",
./.venv-build/lib/python3.11/site-packages/pygments/styles/lilypond.py:37:        Token.String.Symbol: "noinherit",
./.venv-build/lib/python3.11/site-packages/pygments/styles/lilypond.py:38:        Token.Pitch: "",  # "#911520",
./.venv-build/lib/python3.11/site-packages/pygments/styles/lilypond.py:39:        Token.Number: "#976806",  # includes durations
./.venv-build/lib/python3.11/site-packages/pygments/styles/lilypond.py:42:        Token.ChordModifier: "#976806",
./.venv-build/lib/python3.11/site-packages/pygments/styles/lilypond.py:43:        Token.Name.Lvalue: "#08547A",
./.venv-build/lib/python3.11/site-packages/pygments/styles/lilypond.py:44:        Token.Name.BackslashReference: "#08547A",
./.venv-build/lib/python3.11/site-packages/pygments/styles/lilypond.py:45:        Token.Name.Builtin.MusicCommand: "bold #08547A",
./.venv-build/lib/python3.11/site-packages/pygments/styles/lilypond.py:46:        Token.Name.Builtin.PaperVariable: "bold #6C5A05",
./.venv-build/lib/python3.11/site-packages/pygments/styles/lilypond.py:47:        Token.Name.Builtin.HeaderVariable: "bold #6C5A05",
./.venv-build/lib/python3.11/site-packages/pygments/styles/lilypond.py:48:        Token.Name.Builtin.MusicFunction: "bold #08547A",
./.venv-build/lib/python3.11/site-packages/pygments/styles/lilypond.py:49:        Token.Name.Builtin.Clef: "bold #08547A",
./.venv-build/lib/python3.11/site-packages/pygments/styles/lilypond.py:50:        Token.Name.Builtin.Scale: "bold #08547A",
./.venv-build/lib/python3.11/site-packages/pygments/styles/lilypond.py:51:        Token.Name.Builtin.RepeatType: "#08547A",
./.venv-build/lib/python3.11/site-packages/pygments/styles/lilypond.py:52:        Token.Name.Builtin.Dynamic: "#68175A",
./.venv-build/lib/python3.11/site-packages/pygments/styles/lilypond.py:53:        Token.Name.Builtin.Articulation: "#68175A",
./.venv-build/lib/python3.11/site-packages/pygments/styles/lilypond.py:54:        Token.Name.Builtin.SchemeFunction: "bold #A83401",
./.venv-build/lib/python3.11/site-packages/pygments/styles/lilypond.py:55:        Token.Name.Builtin.SchemeBuiltin: "bold",
./.venv-build/lib/python3.11/site-packages/pygments/styles/lilypond.py:56:        Token.Name.Builtin.MarkupCommand: "bold #831E71",
./.venv-build/lib/python3.11/site-packages/pygments/styles/lilypond.py:57:        Token.Name.Builtin.Context: "bold #038B8B",
./.venv-build/lib/python3.11/site-packages/pygments/styles/lilypond.py:58:        Token.Name.Builtin.ContextProperty: "#038B8B",
./.venv-build/lib/python3.11/site-packages/pygments/styles/lilypond.py:59:        Token.Name.Builtin.Grob: "bold #0C7441",
./.venv-build/lib/python3.11/site-packages/pygments/styles/lilypond.py:60:        Token.Name.Builtin.GrobProperty: "#0C7441",
./.venv-build/lib/python3.11/site-packages/pygments/styles/lilypond.py:61:        Token.Name.Builtin.Translator: "bold #6200A4",
./.venv-build/lib/python3.11/site-packages/pygments/styles/onedark.py:15:from pygments.token import Comment, Keyword, Name, Number, Operator, Punctuation, String, Token
./.venv-build/lib/python3.11/site-packages/pygments/styles/onedark.py:33:        Token: "#ABB2BF",
./.venv-build/lib/python3.11/site-packages/pygments/styles/colorful.py:12:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/styles/rainbow_dash.py:14:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/styles/friendly.py:12:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/styles/vs.py:12:from pygments.token import Keyword, Name, Comment, String, Error, Operator, Generic
./.venv-build/lib/python3.11/site-packages/pygments/styles/paraiso_dark.py:16:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/styles/rrt.py:12:from pygments.token import Token, Comment, Name, Keyword, String, Number, Operator
./.venv-build/lib/python3.11/site-packages/pygments/styles/rrt.py:29:        Token: "#dddddd",
./.venv-build/lib/python3.11/site-packages/pygments/styles/algol.py:33:from pygments.token import Keyword, Name, Comment, String, Error, Operator
./.venv-build/lib/python3.11/site-packages/pygments/styles/igor.py:12:from pygments.token import Keyword, Name, Comment, String
./.venv-build/lib/python3.11/site-packages/pygments/styles/borland.py:12:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/styles/staroffice.py:12:from pygments.token import Comment, Error, Literal, Name, Token
./.venv-build/lib/python3.11/site-packages/pygments/styles/staroffice.py:26:        Token: "#000080",  # Blue
./.venv-build/lib/python3.11/site-packages/pygments/styles/abap.py:12:from pygments.token import Keyword, Name, Comment, String, Error, Number, Operator
./.venv-build/lib/python3.11/site-packages/pygments/styles/algol_nu.py:33:from pygments.token import Keyword, Name, Comment, String, Error, Operator
./.venv-build/lib/python3.11/site-packages/pygments/styles/friendly_grayscale.py:15:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/styles/xcode.py:12:from pygments.token import Keyword, Name, Comment, String, Error, Number, Operator, Literal
./.venv-build/lib/python3.11/site-packages/pygments/styles/xcode.py:37:        # In Obj-C code this token is used to colour Cocoa types
./.venv-build/lib/python3.11/site-packages/pygments/styles/gh_dark.py:13:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/styles/gh_dark.py:24:    Token,
./.venv-build/lib/python3.11/site-packages/pygments/styles/gh_dark.py:72:        Token: FG_DEFAULT,
./.venv-build/lib/python3.11/site-packages/pygments/styles/arduino.py:12:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/styles/tango.py:24:Token types, unlike most (if not all) of the styles included in the
./.venv-build/lib/python3.11/site-packages/pygments/styles/tango.py:40:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/styles/lightbulb.py:12:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/styles/lightbulb.py:23:    Token,
./.venv-build/lib/python3.11/site-packages/pygments/styles/lightbulb.py:109:        Token: COLORS["white"],
./.venv-build/lib/python3.11/site-packages/pygments/styles/coffee.py:12:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/styles/coffee.py:23:    Token,
./.venv-build/lib/python3.11/site-packages/pygments/styles/coffee.py:90:        Token: "#ddd0c0",
./.venv-build/lib/python3.11/site-packages/pygments/styles/material.py:14:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/styles/bw.py:12:from pygments.token import Keyword, Name, Comment, String, Error, Operator, Generic
./.venv-build/lib/python3.11/site-packages/pygments/styles/stata_light.py:13:from pygments.token import Keyword, Name, Comment, String, Error, Number, Operator, Whitespace, Text
./.venv-build/lib/python3.11/site-packages/pygments/styles/trac.py:12:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/styles/vim.py:12:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/styles/vim.py:22:    Token,
./.venv-build/lib/python3.11/site-packages/pygments/styles/vim.py:40:        Token: "#cccccc",
./.venv-build/lib/python3.11/site-packages/pygments/styles/fruity.py:12:from pygments.token import Token, Comment, Name, Keyword, Generic, Number, String, Whitespace
./.venv-build/lib/python3.11/site-packages/pygments/styles/fruity.py:30:        Token: "#ffffff",
./.venv-build/lib/python3.11/site-packages/pygments/styles/murphy.py:12:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/styles/autumn.py:12:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/styles/inkpot.py:12:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/styles/pastie.py:14:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/styles/solarized.py:15:from pygments.token import Comment, Error, Generic, Keyword, Name, Number, Operator, String, Token
./.venv-build/lib/python3.11/site-packages/pygments/styles/solarized.py:23:        Token: colors["base0"],
./.venv-build/lib/python3.11/site-packages/pygments/styles/zenburn.py:15:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/styles/zenburn.py:16:    Token,
./.venv-build/lib/python3.11/site-packages/pygments/styles/zenburn.py:48:        Token: "#dcdccc",
./.venv-build/lib/python3.11/site-packages/pygments/styles/emacs.py:12:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/styles/paraiso_light.py:16:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/formatter.py:27:    Converts a token stream to text.
./.venv-build/lib/python3.11/site-packages/pygments/formatter.py:58:        convert the Unicode token strings to byte strings in the
./.venv-build/lib/python3.11/site-packages/pygments/formatter.py:114:    def format(self, tokensource, outfile):
./.venv-build/lib/python3.11/site-packages/pygments/formatter.py:116:        This method must format the tokens from the `tokensource` iterable and
./.venv-build/lib/python3.11/site-packages/pygments/formatter.py:119:        Formatter options can control how exactly the tokens are converted.
./.venv-build/lib/python3.11/site-packages/pygments/formatter.py:124:        return self.format_unencoded(tokensource, outfile)
./.venv-build/lib/python3.11/site-packages/pygments/token.py:2:pygments.token
./.venv-build/lib/python3.11/site-packages/pygments/token.py:5:Basic token types and the standard tokens.
./.venv-build/lib/python3.11/site-packages/pygments/token.py:12:class _TokenType(tuple):
./.venv-build/lib/python3.11/site-packages/pygments/token.py:34:        new = _TokenType(self + (val,))
./.venv-build/lib/python3.11/site-packages/pygments/token.py:41:        return "Token" + (self and "." or "") + ".".join(self)
./.venv-build/lib/python3.11/site-packages/pygments/token.py:52:Token = _TokenType()
./.venv-build/lib/python3.11/site-packages/pygments/token.py:54:# Special token types
./.venv-build/lib/python3.11/site-packages/pygments/token.py:55:Text = Token.Text
./.venv-build/lib/python3.11/site-packages/pygments/token.py:57:Escape = Token.Escape
./.venv-build/lib/python3.11/site-packages/pygments/token.py:58:Error = Token.Error
./.venv-build/lib/python3.11/site-packages/pygments/token.py:60:Other = Token.Other
./.venv-build/lib/python3.11/site-packages/pygments/token.py:62:# Common token types for source code
./.venv-build/lib/python3.11/site-packages/pygments/token.py:63:Keyword = Token.Keyword
./.venv-build/lib/python3.11/site-packages/pygments/token.py:64:Name = Token.Name
./.venv-build/lib/python3.11/site-packages/pygments/token.py:65:Literal = Token.Literal
./.venv-build/lib/python3.11/site-packages/pygments/token.py:68:Punctuation = Token.Punctuation
./.venv-build/lib/python3.11/site-packages/pygments/token.py:69:Operator = Token.Operator
./.venv-build/lib/python3.11/site-packages/pygments/token.py:70:Comment = Token.Comment
./.venv-build/lib/python3.11/site-packages/pygments/token.py:73:Generic = Token.Generic
./.venv-build/lib/python3.11/site-packages/pygments/token.py:75:# String and some others are not direct children of Token.
./.venv-build/lib/python3.11/site-packages/pygments/token.py:77:Token.Token = Token
./.venv-build/lib/python3.11/site-packages/pygments/token.py:78:Token.String = String
./.venv-build/lib/python3.11/site-packages/pygments/token.py:79:Token.Number = Number
./.venv-build/lib/python3.11/site-packages/pygments/token.py:82:def is_token_subtype(ttype, other):
./.venv-build/lib/python3.11/site-packages/pygments/token.py:91:def string_to_tokentype(s):
./.venv-build/lib/python3.11/site-packages/pygments/token.py:93:    Convert a string into a token type::
./.venv-build/lib/python3.11/site-packages/pygments/token.py:95:        >>> string_to_token('String.Double')
./.venv-build/lib/python3.11/site-packages/pygments/token.py:96:        Token.Literal.String.Double
./.venv-build/lib/python3.11/site-packages/pygments/token.py:97:        >>> string_to_token('Token.Literal.Number')
./.venv-build/lib/python3.11/site-packages/pygments/token.py:98:        Token.Literal.Number
./.venv-build/lib/python3.11/site-packages/pygments/token.py:99:        >>> string_to_token('')
./.venv-build/lib/python3.11/site-packages/pygments/token.py:100:        Token
./.venv-build/lib/python3.11/site-packages/pygments/token.py:102:    Tokens that are already tokens are returned unchanged:
./.venv-build/lib/python3.11/site-packages/pygments/token.py:104:        >>> string_to_token(String)
./.venv-build/lib/python3.11/site-packages/pygments/token.py:105:        Token.Literal.String
./.venv-build/lib/python3.11/site-packages/pygments/token.py:107:    if isinstance(s, _TokenType):
./.venv-build/lib/python3.11/site-packages/pygments/token.py:110:        return Token
./.venv-build/lib/python3.11/site-packages/pygments/token.py:111:    node = Token
./.venv-build/lib/python3.11/site-packages/pygments/token.py:117:# Map standard token types to short names, used in CSS class naming.
./.venv-build/lib/python3.11/site-packages/pygments/token.py:121:    Token: "",
./.venv-build/lib/python3.11/site-packages/pygments/style.py:11:from pygments.token import Token, STANDARD_TYPES
./.venv-build/lib/python3.11/site-packages/pygments/style.py:62:        for token in STANDARD_TYPES:
./.venv-build/lib/python3.11/site-packages/pygments/style.py:63:            if token not in obj.styles:
./.venv-build/lib/python3.11/site-packages/pygments/style.py:64:                obj.styles[token] = ""
./.venv-build/lib/python3.11/site-packages/pygments/style.py:84:            for token in ttype.split():
./.venv-build/lib/python3.11/site-packages/pygments/style.py:85:                if token in _styles:
./.venv-build/lib/python3.11/site-packages/pygments/style.py:87:                ndef = _styles.get(token.parent, None)
./.venv-build/lib/python3.11/site-packages/pygments/style.py:88:                styledefs = obj.styles.get(token, "").split()
./.venv-build/lib/python3.11/site-packages/pygments/style.py:89:                if not ndef or token is None:
./.venv-build/lib/python3.11/site-packages/pygments/style.py:91:                elif "noinherit" in styledefs and token is not Token:
./.venv-build/lib/python3.11/site-packages/pygments/style.py:92:                    ndef = _styles[Token][:]
./.venv-build/lib/python3.11/site-packages/pygments/style.py:95:                _styles[token] = ndef
./.venv-build/lib/python3.11/site-packages/pygments/style.py:96:                for styledef in obj.styles.get(token, "").split():
./.venv-build/lib/python3.11/site-packages/pygments/style.py:126:    def style_for_token(cls, token):
./.venv-build/lib/python3.11/site-packages/pygments/style.py:127:        t = cls._styles[token]
./.venv-build/lib/python3.11/site-packages/pygments/style.py:159:    def styles_token(cls, ttype):
./.venv-build/lib/python3.11/site-packages/pygments/style.py:163:        for token in cls._styles:
./.venv-build/lib/python3.11/site-packages/pygments/style.py:164:            yield token, cls.style_for_token(token)
./.venv-build/lib/python3.11/site-packages/pygments/style.py:190:    #: Style definitions for individual token types.
./.venv-build/lib/python3.11/site-packages/pygments/lexers/pointless.py:12:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/pointless.py:92:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/verification.py:12:from pygments.token import Comment, Operator, Keyword, Name, Number, Punctuation, Text, Generic
./.venv-build/lib/python3.11/site-packages/pygments/lexers/verification.py:28:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/verification.py:93:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/hexdump.py:12:from pygments.token import Name, Number, String, Punctuation, Whitespace
./.venv-build/lib/python3.11/site-packages/pygments/lexers/hexdump.py:45:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/arturo.py:12:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/arturo.py:77:            yield from do_insertions([], lexer.get_tokens_unprocessed(code))
./.venv-build/lib/python3.11/site-packages/pygments/lexers/arturo.py:81:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/sas.py:13:from pygments.token import Comment, Keyword, Name, Number, String, Text, Other, Generic
./.venv-build/lib/python3.11/site-packages/pygments/lexers/sas.py:483:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/textfmts.py:15:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/textfmts.py:74:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/textfmts.py:129:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/textfmts.py:161:    def get_tokens_unprocessed(self, text, stack=("root",)):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/textfmts.py:164:        return RegexLexer.get_tokens_unprocessed(self, text, stack)
./.venv-build/lib/python3.11/site-packages/pygments/lexers/textfmts.py:204:                    for idx, token, value in lexer.get_tokens_unprocessed(content):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/textfmts.py:205:                        yield offset + idx, token, value
./.venv-build/lib/python3.11/site-packages/pygments/lexers/textfmts.py:209:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/textfmts.py:265:    # Aliases mapping standard token types of Todo.txt format concepts
./.venv-build/lib/python3.11/site-packages/pygments/lexers/textfmts.py:294:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/textfmts.py:324:            # Tokenize contexts and projects
./.venv-build/lib/python3.11/site-packages/pygments/lexers/textfmts.py:327:            # Tokenize non-whitespace text
./.venv-build/lib/python3.11/site-packages/pygments/lexers/textfmts.py:329:            # Tokenize whitespace not containing a newline
./.venv-build/lib/python3.11/site-packages/pygments/lexers/textfmts.py:336:            # Tokenize contexts and projects
./.venv-build/lib/python3.11/site-packages/pygments/lexers/textfmts.py:339:            # Tokenize non-whitespace text
./.venv-build/lib/python3.11/site-packages/pygments/lexers/textfmts.py:341:            # Tokenize whitespace not containing a newline
./.venv-build/lib/python3.11/site-packages/pygments/lexers/textfmts.py:374:        yield from lexer.get_tokens_unprocessed(code)
./.venv-build/lib/python3.11/site-packages/pygments/lexers/textfmts.py:376:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/textfmts.py:437:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/boa.py:12:from pygments.token import String, Comment, Keyword, Name, Number, Operator, Punctuation, Whitespace
./.venv-build/lib/python3.11/site-packages/pygments/lexers/boa.py:220:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/maple.py:12:from pygments.token import Comment, Name, String, Whitespace, Operator, Punctuation, Number, Keyword
./.venv-build/lib/python3.11/site-packages/pygments/lexers/maple.py:263:        yield from self.get_tokens_unprocessed(context=ctx)
./.venv-build/lib/python3.11/site-packages/pygments/lexers/maple.py:269:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/wren.py:14:from pygments.token import Whitespace, Punctuation, Keyword, Name, Comment, Operator, Number, String
./.venv-build/lib/python3.11/site-packages/pygments/lexers/wren.py:32:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/haskell.py:23:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/haskell.py:113:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/haskell.py:252:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/haskell.py:375:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/haskell.py:518:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/haskell.py:561:        "comment": HaskellLexer.tokens["comment"],
./.venv-build/lib/python3.11/site-packages/pygments/lexers/haskell.py:562:        "character": HaskellLexer.tokens["character"],
./.venv-build/lib/python3.11/site-packages/pygments/lexers/haskell.py:563:        "string": HaskellLexer.tokens["string"],
./.venv-build/lib/python3.11/site-packages/pygments/lexers/haskell.py:564:        "escape": HaskellLexer.tokens["escape"],
./.venv-build/lib/python3.11/site-packages/pygments/lexers/haskell.py:635:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/haskell.py:771:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/haskell.py:773:        for index, token, value in RegexLexer.get_tokens_unprocessed(self, text, stack):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/haskell.py:774:            if token is Name and value in self.EXTRA_KEYWORDS:
./.venv-build/lib/python3.11/site-packages/pygments/lexers/haskell.py:777:                yield index, token, value
./.venv-build/lib/python3.11/site-packages/pygments/lexers/haskell.py:799:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/haskell.py:834:                    insertions.append((len(code), list(lxlexer.get_tokens_unprocessed(latex))))
./.venv-build/lib/python3.11/site-packages/pygments/lexers/haskell.py:838:            insertions.append((len(code), list(lxlexer.get_tokens_unprocessed(latex))))
./.venv-build/lib/python3.11/site-packages/pygments/lexers/haskell.py:839:        yield from do_insertions(insertions, self.baselexer.get_tokens_unprocessed(code))
./.venv-build/lib/python3.11/site-packages/pygments/lexers/haskell.py:1031:    # koka token abstractions
./.venv-build/lib/python3.11/site-packages/pygments/lexers/haskell.py:1032:    tokenType = Name.Attribute
./.venv-build/lib/python3.11/site-packages/pygments/lexers/haskell.py:1033:    tokenTypeDef = Name.Class
./.venv-build/lib/python3.11/site-packages/pygments/lexers/haskell.py:1034:    tokenConstructor = Generic.Emph
./.venv-build/lib/python3.11/site-packages/pygments/lexers/haskell.py:1037:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/haskell.py:1041:            (r"::?" + sboundary, tokenType, "type"),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/haskell.py:1042:            (r"(alias)(\s+)([a-z]\w*)?", bygroups(Keyword, Whitespace, tokenTypeDef), "alias-type"),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/haskell.py:1045:                bygroups(Keyword, Whitespace, tokenTypeDef),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/haskell.py:1050:                bygroups(Keyword, Whitespace, tokenTypeDef),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/haskell.py:1053:            # special sequences of tokens (we use ?: for non-capturing group as
./.venv-build/lib/python3.11/site-packages/pygments/lexers/haskell.py:1093:            (r"((?:[a-z]\w*/)*)([A-Z]\w*)", bygroups(Name.Namespace, tokenConstructor)),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/haskell.py:1115:        "type": [(r"[(\[<]", tokenType, "type-nested"), include("type-content")],
./.venv-build/lib/python3.11/site-packages/pygments/lexers/haskell.py:1118:            (r"[)\]>]", tokenType, "#pop"),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/haskell.py:1119:            (r"[(\[<]", tokenType, "type-nested"),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/haskell.py:1120:            (r",", tokenType),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/haskell.py:1121:            (r"([a-z]\w*)(\s*)(:)(?!:)", bygroups(Name, Whitespace, tokenType)),  # parameter name
./.venv-build/lib/python3.11/site-packages/pygments/lexers/haskell.py:1135:            (r"[EPHVX]" + boundary, tokenType),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/haskell.py:1137:            (r"[a-z][0-9]*(?![\w/])", tokenType),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/haskell.py:1138:            (r"_\w*", tokenType.Variable),  # Generic.Emph
./.venv-build/lib/python3.11/site-packages/pygments/lexers/haskell.py:1139:            (r"((?:[a-z]\w*/)*)([A-Z]\w*)", bygroups(Name.Namespace, tokenType)),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/haskell.py:1140:            (r"((?:[a-z]\w*/)*)([a-z]\w+)", bygroups(Name.Namespace, tokenType)),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/haskell.py:1142:            (r"::|->|[.:|]", tokenType),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/hare.py:12:from pygments.token import Comment, Operator, Keyword, Name, String, Number, Punctuation, Whitespace
./.venv-build/lib/python3.11/site-packages/pygments/lexers/hare.py:32:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/apl.py:12:from pygments.token import Comment, Operator, Keyword, Name, String, Number, Punctuation, Whitespace
./.venv-build/lib/python3.11/site-packages/pygments/lexers/apl.py:36:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/apl.py:54:            # This token type is used for diamond and parenthesis
./.venv-build/lib/python3.11/site-packages/pygments/lexers/apl.py:60:            # Since this token type is very important in APL, it is not included in
./.venv-build/lib/python3.11/site-packages/pygments/lexers/apl.py:61:            # the punctuation token type but rather in the following one
./.venv-build/lib/python3.11/site-packages/pygments/lexers/apl.py:89:            (r"[\.\\\/âŒ¿â€Â¨â£â¨â â¤âˆ˜âŒ¸&âŒ¶@âŒºâ¥â›â¢]", Name.Attribute),  # closest token type
./.venv-build/lib/python3.11/site-packages/pygments/lexers/sgf.py:12:from pygments.token import Name, Literal, String, Punctuation, Whitespace
./.venv-build/lib/python3.11/site-packages/pygments/lexers/sgf.py:31:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/sgf.py:34:            # tokens:
./.venv-build/lib/python3.11/site-packages/pygments/lexers/elm.py:12:from pygments.token import Comment, Keyword, Name, Number, Punctuation, String, Whitespace
./.venv-build/lib/python3.11/site-packages/pygments/lexers/elm.py:89:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/tact.py:12:from pygments.token import Comment, Operator, Keyword, Name, String, Number, Whitespace, Punctuation
./.venv-build/lib/python3.11/site-packages/pygments/lexers/tact.py:26:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_mapping.py:2244:    "RawTokenLexer": (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_mapping.py:2246:        "Raw token data",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_mapping.py:2249:        ("application/x-pygments-tokens",),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_usd_builtins.py:99:    "token",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/dylan.py:14:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/dylan.py:300:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/dylan.py:301:        for index, token, value in RegexLexer.get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/dylan.py:302:            if token is Name:
./.venv-build/lib/python3.11/site-packages/pygments/lexers/dylan.py:316:            yield index, token, value
./.venv-build/lib/python3.11/site-packages/pygments/lexers/dylan.py:318:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/dylan.py:353:                r"(\?" + valid_name + ")(:)" r"(token|name|variable|expression|body|case-body|\*)",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/dylan.py:357:                r"(\?)(:)(token|name|variable|expression|body|case-body|\*)",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/dylan.py:416:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/dylan.py:448:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/dylan.py:462:                    yield from do_insertions(insertions, dylexer.get_tokens_unprocessed(curcode))
./.venv-build/lib/python3.11/site-packages/pygments/lexers/dylan.py:467:            yield from do_insertions(insertions, dylexer.get_tokens_unprocessed(curcode))
./.venv-build/lib/python3.11/site-packages/pygments/lexers/scripting.py:14:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/scripting.py:87:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/scripting.py:198:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/scripting.py:199:        for index, token, value in RegexLexer.get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/scripting.py:200:            if token is Name.Builtin and value not in self._functions:
./.venv-build/lib/python3.11/site-packages/pygments/lexers/scripting.py:209:            yield index, token, value
./.venv-build/lib/python3.11/site-packages/pygments/lexers/scripting.py:271:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/scripting.py:475:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/scripting.py:476:        for index, token, value in RegexLexer.get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/scripting.py:477:            if token is Name or token is Name.Other:
./.venv-build/lib/python3.11/site-packages/pygments/lexers/scripting.py:495:                        if token is Name:
./.venv-build/lib/python3.11/site-packages/pygments/lexers/scripting.py:506:            yield index, token, value
./.venv-build/lib/python3.11/site-packages/pygments/lexers/scripting.py:521:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/scripting.py:587:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/scripting.py:589:        for index, token, value in LuaLexer.get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/scripting.py:590:            if token == Punctuation and value == ".":
./.venv-build/lib/python3.11/site-packages/pygments/lexers/scripting.py:591:                token = Operator
./.venv-build/lib/python3.11/site-packages/pygments/lexers/scripting.py:592:            yield index, token, value
./.venv-build/lib/python3.11/site-packages/pygments/lexers/scripting.py:609:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/scripting.py:696:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/scripting.py:1521:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/scripting.py:1585:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/scripting.py:1770:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/scripting.py:1818:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/scripting.py:2309:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/scripting.py:2464:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/scripting.py:2553:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/ada.py:14:from pygments.token import Text, Comment, Operator, Keyword, Name, String, Number, Punctuation
./.venv-build/lib/python3.11/site-packages/pygments/lexers/ada.py:34:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/nimrod.py:14:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/nimrod.py:135:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/objective.py:14:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/objective.py:56:        tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/objective.py:262:        def get_tokens_unprocessed(self, text, stack=("root",)):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/objective.py:269:            for index, token, value in baselexer.get_tokens_unprocessed(self, text, stack):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/objective.py:270:                if token is Name or token is Name.Class:
./.venv-build/lib/python3.11/site-packages/pygments/lexers/objective.py:276:                        token = Name.Builtin.Pseudo
./.venv-build/lib/python3.11/site-packages/pygments/lexers/objective.py:278:                yield index, token, value
./.venv-build/lib/python3.11/site-packages/pygments/lexers/objective.py:322:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/objective.py:387:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/objective.py:894:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/objective.py:901:        for index, token, value in RegexLexer.get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/objective.py:902:            if token is Name or token is Name.Class:
./.venv-build/lib/python3.11/site-packages/pygments/lexers/objective.py:908:                    token = Name.Builtin.Pseudo
./.venv-build/lib/python3.11/site-packages/pygments/lexers/objective.py:910:            yield index, token, value
./.venv-build/lib/python3.11/site-packages/pygments/lexers/numbair.py:12:from pygments.token import Whitespace, Name, String, Punctuation, Keyword, Operator, Number
./.venv-build/lib/python3.11/site-packages/pygments/lexers/numbair.py:32:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/php.py:25:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/php.py:61:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/php.py:140:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/php.py:154:                    yield from do_insertions(insertions, phplexer.get_tokens_unprocessed(curcode))
./.venv-build/lib/python3.11/site-packages/pygments/lexers/php.py:159:            yield from do_insertions(insertions, phplexer.get_tokens_unprocessed(curcode))
./.venv-build/lib/python3.11/site-packages/pygments/lexers/php.py:209:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/php.py:362:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/php.py:366:        for index, token, value in RegexLexer.get_tokens_unprocessed(self, text, stack):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/php.py:367:            if token is Name.Other:
./.venv-build/lib/python3.11/site-packages/pygments/lexers/php.py:371:            yield index, token, value
./.venv-build/lib/python3.11/site-packages/pygments/lexers/ruby.py:25:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/ruby.py:113:            yield from self.get_tokens_unprocessed(context=ctx)
./.venv-build/lib/python3.11/site-packages/pygments/lexers/ruby.py:146:            for i, t, v in self.get_tokens_unprocessed(context=nctx):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/ruby.py:154:            for i, t, v in self.get_tokens_unprocessed(context=nctx):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/ruby.py:242:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/ruby.py:599:    tokens.update(gen_rubystrings_rules())
./.venv-build/lib/python3.11/site-packages/pygments/lexers/ruby.py:619:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/ruby.py:633:                    yield from do_insertions(insertions, rblexer.get_tokens_unprocessed(curcode))
./.venv-build/lib/python3.11/site-packages/pygments/lexers/ruby.py:638:            yield from do_insertions(insertions, rblexer.get_tokens_unprocessed(curcode))
./.venv-build/lib/python3.11/site-packages/pygments/lexers/ruby.py:657:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/wgsl.py:12:from pygments.token import Comment, Operator, Keyword, Name, Number, Punctuation, Whitespace
./.venv-build/lib/python3.11/site-packages/pygments/lexers/wgsl.py:28:# https://www.w3.org/TR/WGSL/#syntax-ident_pattern_token
./.venv-build/lib/python3.11/site-packages/pygments/lexers/wgsl.py:29:ident_pattern_token = f"([{uni.xid_start}][{uni.xid_continue}]+)|[{uni.xid_start}]"
./.venv-build/lib/python3.11/site-packages/pygments/lexers/wgsl.py:359:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/wgsl.py:374:            (ident_pattern_token, Name.Decorator, "#pop"),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/wgsl.py:427:            (ident_pattern_token, Name),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/wgsl.py:428:            # TODO: templates start and end tokens.
./.venv-build/lib/python3.11/site-packages/pygments/lexers/roboconf.py:12:from pygments.token import Text, Operator, Keyword, Name, Comment
./.venv-build/lib/python3.11/site-packages/pygments/lexers/roboconf.py:29:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/roboconf.py:66:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/tcl.py:12:from pygments.token import Text, Comment, Operator, Keyword, Name, String, Number, Whitespace
./.venv-build/lib/python3.11/site-packages/pygments/lexers/tcl.py:157:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/make.py:14:from pygments.token import Text, Comment, Operator, Keyword, Name, String, Punctuation, Whitespace
./.venv-build/lib/python3.11/site-packages/pygments/lexers/make.py:46:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/make.py:60:        yield from do_insertions(ins, lex.get_tokens_unprocessed(done))
./.venv-build/lib/python3.11/site-packages/pygments/lexers/make.py:80:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/make.py:143:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/verifpal.py:12:from pygments.token import Comment, Keyword, Name, String, Punctuation, Whitespace
./.venv-build/lib/python3.11/site-packages/pygments/lexers/verifpal.py:29:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/verifpal.py:75:            (words(("password",), suffix=r"\b"), Keyword.Constant),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/automation.py:12:from pygments.token import Text, Comment, Operator, Name, String, Number, Punctuation, Generic
./.venv-build/lib/python3.11/site-packages/pygments/lexers/automation.py:29:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/automation.py:320:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/json5.py:12:from pygments.token import Comment, Keyword, Name, Number, Punctuation, String, Whitespace
./.venv-build/lib/python3.11/site-packages/pygments/lexers/json5.py:38:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/fantom.py:14:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/fantom.py:55:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/teal.py:12:from pygments.token import Comment, Name, Number, String, Text, Keyword, Whitespace
./.venv-build/lib/python3.11/site-packages/pygments/lexers/teal.py:115:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/textedit.py:16:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/textedit.py:43:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/textedit.py:107:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/textedit.py:175:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/textedit.py:225:        we match `\b\w+\b` and then call is_in() on those tokens.  See
./.venv-build/lib/python3.11/site-packages/pygments/lexers/textedit.py:239:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/textedit.py:240:        # TODO: builtins are only subsequent tokens on lines
./.venv-build/lib/python3.11/site-packages/pygments/lexers/textedit.py:243:        for index, token, value in RegexLexer.get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/textedit.py:244:            if token is Name.Other:
./.venv-build/lib/python3.11/site-packages/pygments/lexers/textedit.py:252:                yield index, token, value
./.venv-build/lib/python3.11/site-packages/pygments/lexers/algebra.py:14:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/algebra.py:41:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/algebra.py:127:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/algebra.py:146:                    yield from do_insertions(insertions, gaplexer.get_tokens_unprocessed(curcode))
./.venv-build/lib/python3.11/site-packages/pygments/lexers/algebra.py:158:            yield from do_insertions(insertions, gaplexer.get_tokens_unprocessed(curcode))
./.venv-build/lib/python3.11/site-packages/pygments/lexers/algebra.py:230:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/algebra.py:260:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/algebra.py:347:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/go.py:12:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/go.py:39:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/go.py:180:            # Tokens
./.venv-build/lib/python3.11/site-packages/pygments/lexers/tnt.py:14:from pygments.token import Text, Comment, Operator, Keyword, Name, Number, Punctuation, Error
./.venv-build/lib/python3.11/site-packages/pygments/lexers/tnt.py:61:        """Tokenize whitespace."""
./.venv-build/lib/python3.11/site-packages/pygments/lexers/tnt.py:75:        """Tokenize a variable."""
./.venv-build/lib/python3.11/site-packages/pygments/lexers/tnt.py:85:        """Tokenize a term."""
./.venv-build/lib/python3.11/site-packages/pygments/lexers/tnt.py:111:        """Tokenize a formula."""
./.venv-build/lib/python3.11/site-packages/pygments/lexers/tnt.py:145:        """Tokenize a rule."""
./.venv-build/lib/python3.11/site-packages/pygments/lexers/tnt.py:162:        """Tokenize a line referral."""
./.venv-build/lib/python3.11/site-packages/pygments/lexers/tnt.py:191:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/tnt.py:192:        """Returns a list of TNT tokens."""
./.venv-build/lib/python3.11/site-packages/pygments/lexers/dsls.py:24:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/dsls.py:63:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/dsls.py:160:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/dsls.py:399:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/dsls.py:612:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/dsls.py:790:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/dsls.py:926:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/dsls.py:983:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/dsls.py:1024:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/dsls.py:1086:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/dsls.py:1262:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/dsls.py:1453:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/dsls.py:1538:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/dsls.py:1628:    def get_tokens_unprocessed(self, text=None, context=None):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/dsls.py:1630:        return ExtendedRegexLexer.get_tokens_unprocessed(self, text, context)
./.venv-build/lib/python3.11/site-packages/pygments/lexers/tal.py:14:from pygments.token import Comment, Keyword, Name, String, Number, Punctuation, Whitespace, Literal
./.venv-build/lib/python3.11/site-packages/pygments/lexers/tal.py:67:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/srcinfo.py:15:from pygments.token import Text, Comment, Keyword, Name, Operator, Whitespace
./.venv-build/lib/python3.11/site-packages/pygments/lexers/srcinfo.py:65:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/carbon.py:14:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/carbon.py:43:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/carbon.py:114:            # tokens
./.venv-build/lib/python3.11/site-packages/pygments/lexers/ampl.py:12:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/ampl.py:38:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/iolang.py:12:from pygments.token import Comment, Operator, Keyword, Name, String, Number, Whitespace
./.venv-build/lib/python3.11/site-packages/pygments/lexers/iolang.py:28:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/special.py:14:from pygments.token import Token, Error, Text, Generic
./.venv-build/lib/python3.11/site-packages/pygments/lexers/special.py:18:__all__ = ["TextLexer", "OutputLexer", "RawTokenLexer"]
./.venv-build/lib/python3.11/site-packages/pygments/lexers/special.py:35:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/special.py:44:    Simple lexer that highlights everything as ``Token.Generic.Output``.
./.venv-build/lib/python3.11/site-packages/pygments/lexers/special.py:53:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/special.py:60:class RawTokenLexer(Lexer):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/special.py:62:    Recreate a token stream formatted with the `RawTokenFormatter`.
./.venv-build/lib/python3.11/site-packages/pygments/lexers/special.py:67:        If set to ``"gz"`` or ``"bz2"``, decompress the token stream with
./.venv-build/lib/python3.11/site-packages/pygments/lexers/special.py:71:    name = "Raw token data"
./.venv-build/lib/python3.11/site-packages/pygments/lexers/special.py:74:    mimetypes = ["application/x-pygments-tokens"]
./.venv-build/lib/python3.11/site-packages/pygments/lexers/special.py:75:    url = "https://pygments.org/docs/formatters/#RawTokenFormatter"
./.venv-build/lib/python3.11/site-packages/pygments/lexers/special.py:82:    def get_tokens(self, text):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/special.py:100:        # do not call Lexer.get_tokens() because stripping is not optional.
./.venv-build/lib/python3.11/site-packages/pygments/lexers/special.py:102:        for i, t, v in self.get_tokens_unprocessed(text):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/special.py:105:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/special.py:112:                    ttype = Token
./.venv-build/lib/python3.11/site-packages/pygments/lexers/special.py:116:                            raise ValueError("malformed token name")
./.venv-build/lib/python3.11/site-packages/pygments/lexers/modula2.py:15:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/modula2.py:103:    No whitespace is permitted between the tokens of a dialect tag.
./.venv-build/lib/python3.11/site-packages/pygments/lexers/modula2.py:178:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/modula2.py:419:    # Lexemes to Mark as Error Tokens for PIM Modula-2
./.venv-build/lib/python3.11/site-packages/pygments/lexers/modula2.py:482:    # Lexemes to Mark as Error Tokens for ISO Modula-2
./.venv-build/lib/python3.11/site-packages/pygments/lexers/modula2.py:633:    # Lexemes to Mark as Error Tokens for Modula-2 R10
./.venv-build/lib/python3.11/site-packages/pygments/lexers/modula2.py:782:    # Lexemes to Mark as Error Tokens for Objective Modula-2
./.venv-build/lib/python3.11/site-packages/pygments/lexers/modula2.py:1813:    # intercept the token stream, modify token attributes and return them
./.venv-build/lib/python3.11/site-packages/pygments/lexers/modula2.py:1814:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/modula2.py:1815:        for index, token, value in RegexLexer.get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/modula2.py:1818:            if not self.dialect_set_by_tag and token == Comment.Special:
./.venv-build/lib/python3.11/site-packages/pygments/lexers/modula2.py:1821:                    # token is a dialect indicator
./.venv-build/lib/python3.11/site-packages/pygments/lexers/modula2.py:1827:            if token is Name:
./.venv-build/lib/python3.11/site-packages/pygments/lexers/modula2.py:1829:                    token = Keyword.Reserved
./.venv-build/lib/python3.11/site-packages/pygments/lexers/modula2.py:1834:                    token = Name.Builtin
./.venv-build/lib/python3.11/site-packages/pygments/lexers/modula2.py:1839:                    token = Name.Builtin.Pseudo
./.venv-build/lib/python3.11/site-packages/pygments/lexers/modula2.py:1845:                        token = Name.Namespace
./.venv-build/lib/python3.11/site-packages/pygments/lexers/modula2.py:1847:                        token = Name.Builtin.Pseudo
./.venv-build/lib/python3.11/site-packages/pygments/lexers/modula2.py:1852:                    token = Name.Namespace
./.venv-build/lib/python3.11/site-packages/pygments/lexers/modula2.py:1855:                    token = Name.Class
./.venv-build/lib/python3.11/site-packages/pygments/lexers/modula2.py:1858:                    token = Name.Function
./.venv-build/lib/python3.11/site-packages/pygments/lexers/modula2.py:1861:                    token = Name.Variable
./.venv-build/lib/python3.11/site-packages/pygments/lexers/modula2.py:1864:                    token = Name.Constant
./.venv-build/lib/python3.11/site-packages/pygments/lexers/modula2.py:1866:            elif token in Number:
./.venv-build/lib/python3.11/site-packages/pygments/lexers/modula2.py:1871:                        token = Error
./.venv-build/lib/python3.11/site-packages/pygments/lexers/modula2.py:1875:                    if token is Number.Oct:
./.venv-build/lib/python3.11/site-packages/pygments/lexers/modula2.py:1876:                        token = Error
./.venv-build/lib/python3.11/site-packages/pygments/lexers/modula2.py:1878:                    elif token is Number.Hex and "H" in value:
./.venv-build/lib/python3.11/site-packages/pygments/lexers/modula2.py:1879:                        token = Error
./.venv-build/lib/python3.11/site-packages/pygments/lexers/modula2.py:1881:                    elif token is Number.Float and "E" in value:
./.venv-build/lib/python3.11/site-packages/pygments/lexers/modula2.py:1882:                        token = Error
./.venv-build/lib/python3.11/site-packages/pygments/lexers/modula2.py:1884:            elif token in Comment:
./.venv-build/lib/python3.11/site-packages/pygments/lexers/modula2.py:1887:                if token is Comment.Single:
./.venv-build/lib/python3.11/site-packages/pygments/lexers/modula2.py:1889:                        token = Error
./.venv-build/lib/python3.11/site-packages/pygments/lexers/modula2.py:1891:                if token is Comment.Preproc:
./.venv-build/lib/python3.11/site-packages/pygments/lexers/modula2.py:1894:                        token = Error
./.venv-build/lib/python3.11/site-packages/pygments/lexers/modula2.py:1901:                        token = Comment.Multiline
./.venv-build/lib/python3.11/site-packages/pygments/lexers/modula2.py:1903:            else:  # token is neither Name nor Comment
./.venv-build/lib/python3.11/site-packages/pygments/lexers/modula2.py:1905:                # mark lexemes matching the dialect's error token set as errors
./.venv-build/lib/python3.11/site-packages/pygments/lexers/modula2.py:1907:                    token = Error
./.venv-build/lib/python3.11/site-packages/pygments/lexers/modula2.py:1923:            yield index, token, value
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:14:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:72:    token_end = r"""
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:83:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:84:        for index, token, value in super().get_tokens_unprocessed(text):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:85:            if token is Name.Function or token is Name.Variable:
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:91:                    yield index, token, value
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:93:                yield index, token, value
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:186:          # Need to ensure we have a full token. 1+ is not a
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:189:          {token_end}
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:200:            token_type = Number.Float  # includes [+-](inf|nan).0
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:202:            token_type = Number.Integer
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:203:        yield match.start(), token_type, match.group()
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:218:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:285:            (rf"(?x).*?{token_end}", Comment, "#pop"),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:328:    # symbol token, reverse-engineered from hyperspec
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:352:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:354:        for index, token, value in RegexLexer.get_tokens_unprocessed(self, text, stack):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:355:            if token is Name.Variable:
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:377:            yield index, token, value
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:379:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:572:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:617:        "py-keywords": PythonLexer.tokens["keywords"],
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:618:        "py-builtins": PythonLexer.tokens["builtins"],
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:3078:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:3088:            # onto Pygments token types; some judgment calls here.
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:3599:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:3641:    An ELisp lexer, parsing a stream and outputting the tokens
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:3661:    # symbol token, reverse-engineered from hyperspec
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:5198:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:5200:        for index, token, value in RegexLexer.get_tokens_unprocessed(self, text, stack):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:5201:            if token is Name.Variable:
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:5220:            yield index, token, value
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:5222:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:5515:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:5540:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:5541:        tokens = RegexLexer.get_tokens_unprocessed(self, text)
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:5542:        tokens = self._process_symbols(tokens)
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:5543:        tokens = self._process_declarations(tokens)
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:5544:        return tokens
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:5546:    def _relevant(self, token):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:5547:        return token not in (Text, Whitespace, Comment.Single, Comment.Multiline)
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:5549:    def _process_declarations(self, tokens):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:5551:        for index, token, value in tokens:
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:5552:            yield index, token, value
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:5553:            if self._relevant(token):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:5554:                if opening_paren and token == Keyword and value in self.DECLARATIONS:
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:5556:                    yield from self._process_declaration(declaration, tokens)
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:5557:                opening_paren = value == "(" and token == Punctuation
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:5559:    def _process_symbols(self, tokens):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:5561:        for index, token, value in tokens:
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:5562:            if opening_paren and token in (Literal, Name.Variable):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:5563:                token = self.MAPPINGS.get(value, Name.Function)
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:5564:            elif token == Literal and value in self.BUILTINS_ANYWHERE:
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:5565:                token = Name.Builtin
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:5566:            opening_paren = value == "(" and token == Punctuation
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:5567:            yield index, token, value
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:5569:    def _process_declaration(self, declaration, tokens):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:5570:        for index, token, value in tokens:
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:5571:            if self._relevant(token):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:5573:            yield index, token, value
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:5577:            token = Keyword.Type if token == Literal else token
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:5578:            yield index, token, value
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:5579:            for index, token, value in tokens:
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:5580:                if prev_was_colon and token == Literal:
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:5581:                    token = Keyword.Type
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:5582:                yield index, token, value
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:5583:                if self._relevant(token):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:5584:                    prev_was_colon = token == Literal and value == ":"
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:5586:            token = Name.Namespace if token == Literal else token
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:5587:            yield index, token, value
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:5589:            token = Name.Function if token == Literal else token
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:5590:            yield index, token, value
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:5591:            for index, token, value in tokens:
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:5592:                if self._relevant(token):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:5594:                yield index, token, value
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:5595:            if value == "{" and token == Literal:
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:5597:                for index, token, value in self._process_signature(tokens):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:5598:                    yield index, token, value
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:5600:                yield index, token, value
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:5602:            token = Name.Function if token == Literal else token
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:5603:            yield index, token, value
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:5607:    def _process_signature(self, tokens):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:5608:        for index, token, value in tokens:
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:5609:            if token == Literal and value == "}":
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:5612:            elif token in (Literal, Name.Function):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:5613:                token = Name.Variable if value.istitle() else Keyword.Type
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:5614:            yield index, token, value
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:5672:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:6100:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:6310:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:7024:    # _token_end = r'''
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:7034:    _token_end = r"(?=\s|#|[)\]]|$)"
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:7056:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:7099:            (words(constants, suffix=_token_end), Keyword.Constants),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:7103:            (words(builtin_variables, suffix=_token_end), Name.Variable.Global),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:7104:            (words(special_forms, prefix=r"(?<=\()", suffix=_token_end), Keyword.Reserved),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:7105:            (words(builtin_macros, prefix=r"(?<=\()", suffix=_token_end), Name.Builtin),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lisp.py:7106:            (words(builtin_functions, prefix=r"(?<=\()", suffix=_token_end), Name.Function),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/sieve.py:21:from pygments.token import Comment, Name, Literal, String, Text, Punctuation, Keyword
./.venv-build/lib/python3.11/site-packages/pygments/lexers/sieve.py:37:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/sieve.py:52:            # tokens:
./.venv-build/lib/python3.11/site-packages/pygments/lexers/factor.py:12:from pygments.token import Text, Comment, Keyword, Name, String, Number, Whitespace, Punctuation
./.venv-build/lib/python3.11/site-packages/pygments/lexers/factor.py:792:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/meson.py:12:from pygments.token import Comment, Name, Number, Punctuation, Operator, Keyword, String, Whitespace
./.venv-build/lib/python3.11/site-packages/pygments/lexers/meson.py:36:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_cocoa_builtins.py:93:    "ASAccountAuthenticationModificationReplacePasswordWithSignInWithAppleRequest",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_cocoa_builtins.py:95:    "ASAccountAuthenticationModificationUpgradePasswordToStrongPasswordRequest",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_cocoa_builtins.py:104:    "ASAuthorizationPasswordProvider",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_cocoa_builtins.py:105:    "ASAuthorizationPasswordRequest",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_cocoa_builtins.py:117:    "ASPasswordCredential",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_cocoa_builtins.py:118:    "ASPasswordCredentialIdentity",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_cocoa_builtins.py:409:    "CKFetchWebAuthTokenOperation",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_cocoa_builtins.py:434:    "CKServerChangeToken",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_cocoa_builtins.py:951:    "HMAccessoryOwnershipToken",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_cocoa_builtins.py:1840:    "NIDiscoveryToken",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_cocoa_builtins.py:1853:    "NLTokenizer",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_cocoa_builtins.py:2031:    "NSPersistentHistoryToken",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_cocoa_builtins.py:2053:    "NSQueryGenerationToken",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_cocoa_builtins.py:2264:    "PKPaymentToken",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_cocoa_builtins.py:2488:    "TKSmartCardToken",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_cocoa_builtins.py:2489:    "TKSmartCardTokenDriver",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_cocoa_builtins.py:2490:    "TKSmartCardTokenSession",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_cocoa_builtins.py:2496:    "TKToken",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_cocoa_builtins.py:2497:    "TKTokenAuthOperation",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_cocoa_builtins.py:2498:    "TKTokenConfiguration",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_cocoa_builtins.py:2499:    "TKTokenDriver",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_cocoa_builtins.py:2500:    "TKTokenDriverConfiguration",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_cocoa_builtins.py:2501:    "TKTokenKeyAlgorithm",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_cocoa_builtins.py:2502:    "TKTokenKeyExchangeParameters",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_cocoa_builtins.py:2503:    "TKTokenKeychainCertificate",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_cocoa_builtins.py:2504:    "TKTokenKeychainContents",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_cocoa_builtins.py:2505:    "TKTokenKeychainItem",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_cocoa_builtins.py:2506:    "TKTokenKeychainKey",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_cocoa_builtins.py:2507:    "TKTokenPasswordAuthOperation",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_cocoa_builtins.py:2508:    "TKTokenSession",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_cocoa_builtins.py:2509:    "TKTokenSmartCardPINAuthOperation",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_cocoa_builtins.py:2510:    "TKTokenWatcher",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_cocoa_builtins.py:2751:    "UISearchToken",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_cocoa_builtins.py:2796:    "UITextInputPasswordRules",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_cocoa_builtins.py:2797:    "UITextInputStringTokenizer",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_cocoa_builtins.py:3591:    "TKSmartCardTokenDriverDelegate",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_cocoa_builtins.py:3593:    "TKTokenDelegate",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_cocoa_builtins.py:3594:    "TKTokenDriverDelegate",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_cocoa_builtins.py:3595:    "TKTokenSessionDelegate",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_cocoa_builtins.py:3726:    "UITextInputTokenizer",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_cocoa_builtins.py:4124:    "opaqueCMBufferQueueTriggerToken",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/codeql.py:19:from pygments.token import Comment, Operator, Keyword, Name, String, Number, Punctuation, Whitespace
./.venv-build/lib/python3.11/site-packages/pygments/lexers/codeql.py:32:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/macaulay2.py:12:from pygments.token import Comment, Keyword, Name, String, Text
./.venv-build/lib/python3.11/site-packages/pygments/lexers/macaulay2.py:1786:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/fift.py:12:from pygments.token import Literal, Comment, Name, String, Number, Whitespace
./.venv-build/lib/python3.11/site-packages/pygments/lexers/fift.py:28:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/pony.py:12:from pygments.token import Text, Comment, Operator, Keyword, Name, String, Number, Punctuation
./.venv-build/lib/python3.11/site-packages/pygments/lexers/pony.py:30:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/parsers.py:14:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/parsers.py:72:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/parsers.py:167:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/parsers.py:367:    _TOKEN_REF = r"[A-Z]\w*"
./.venv-build/lib/python3.11/site-packages/pygments/lexers/parsers.py:372:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/parsers.py:389:            # tokensSpec
./.venv-build/lib/python3.11/site-packages/pygments/lexers/parsers.py:390:            (r"tokens\b", Keyword, "tokens"),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/parsers.py:465:            # Tokens start with capital letter.
./.venv-build/lib/python3.11/site-packages/pygments/lexers/parsers.py:476:        "tokens": [
./.venv-build/lib/python3.11/site-packages/pygments/lexers/parsers.py:481:                r"(" + _TOKEN_REF + r")(\s*)(=)?(\s*)(" + _STRING_LITERAL + r")?(\s*)(;)",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/parsers.py:746:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/parsers.py:835:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/css.py:23:from pygments.token import Comment, Operator, Keyword, Name, String, Number, Punctuation, Whitespace
./.venv-build/lib/python3.11/site-packages/pygments/lexers/css.py:689:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/css.py:823:common_sass_tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/css.py:1127:def _starts_block(token, state):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/css.py:1129:        yield match.start(), token, match.group(0)
./.venv-build/lib/python3.11/site-packages/pygments/lexers/css.py:1156:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/css.py:1208:    for group, common in common_sass_tokens.items():
./.venv-build/lib/python3.11/site-packages/pygments/lexers/css.py:1209:        tokens[group] = copy.copy(common)
./.venv-build/lib/python3.11/site-packages/pygments/lexers/css.py:1210:    tokens["value"].append((r"\n", Whitespace, "root"))
./.venv-build/lib/python3.11/site-packages/pygments/lexers/css.py:1211:    tokens["selector"].append((r"\n", Whitespace, "root"))
./.venv-build/lib/python3.11/site-packages/pygments/lexers/css.py:1227:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/css.py:1258:    for group, common in common_sass_tokens.items():
./.venv-build/lib/python3.11/site-packages/pygments/lexers/css.py:1259:        tokens[group] = copy.copy(common)
./.venv-build/lib/python3.11/site-packages/pygments/lexers/css.py:1260:    tokens["value"].extend([(r"\n", Whitespace), (r"[;{}]", Punctuation, "#pop")])
./.venv-build/lib/python3.11/site-packages/pygments/lexers/css.py:1261:    tokens["selector"].extend([(r"\n", Whitespace), (r"[;{}]", Punctuation, "#pop")])
./.venv-build/lib/python3.11/site-packages/pygments/lexers/css.py:1276:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_sourcemod_builtins.py:403:    "SetAdminPassword",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_sourcemod_builtins.py:404:    "GetAdminPassword",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/stata.py:13:from pygments.token import Comment, Keyword, Name, Number, String, Text, Operator
./.venv-build/lib/python3.11/site-packages/pygments/lexers/stata.py:38:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/foxpro.py:14:from pygments.token import Punctuation, Text, Comment, Operator, Keyword, Name, String
./.venv-build/lib/python3.11/site-packages/pygments/lexers/foxpro.py:35:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/foxpro.py:228:                r"Parent|Partition|PasswordChar|PictureMargin|"
./.venv-build/lib/python3.11/site-packages/pygments/lexers/crystal.py:14:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/crystal.py:94:            yield from self.get_tokens_unprocessed(context=ctx)
./.venv-build/lib/python3.11/site-packages/pygments/lexers/crystal.py:210:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/crystal.py:474:    tokens.update(gen_crystalstrings_rules())
./.venv-build/lib/python3.11/site-packages/pygments/lexers/gdscript.py:17:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/gdscript.py:59:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/soong.py:12:from pygments.token import Comment, Name, Number, Operator, Punctuation, String, Whitespace
./.venv-build/lib/python3.11/site-packages/pygments/lexers/soong.py:24:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/asc.py:14:from pygments.token import Comment, Generic, Name, Operator, String, Whitespace
./.venv-build/lib/python3.11/site-packages/pygments/lexers/asc.py:48:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/rust.py:12:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/rust.py:169:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/markup.py:30:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/markup.py:75:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/markup.py:109:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/markup.py:203:        yield from do_insertions(ins, lexer.get_tokens_unprocessed(code))
./.venv-build/lib/python3.11/site-packages/pygments/lexers/markup.py:212:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/markup.py:363:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/markup.py:423:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/markup.py:480:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/markup.py:541:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/markup.py:636:            yield from do_insertions([], lexer.get_tokens_unprocessed(code))
./.venv-build/lib/python3.11/site-packages/pygments/lexers/markup.py:640:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/markup.py:745:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/markup.py:811:            (_inline(r"=", r"="), String),  # TODO token
./.venv-build/lib/python3.11/site-packages/pygments/lexers/markup.py:878:        yield from do_insertions([], lexer.get_tokens_unprocessed(code))
./.venv-build/lib/python3.11/site-packages/pygments/lexers/markup.py:905:        yield from do_insertions([], lexer.get_tokens_unprocessed(code))
./.venv-build/lib/python3.11/site-packages/pygments/lexers/markup.py:909:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/markup.py:1059:    def text_rules(token):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/markup.py:1061:            (r"\w+", token),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/markup.py:1062:            (r"[^\S\n]+", token),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/markup.py:1063:            (r"(?s).", token),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/markup.py:1081:            yield from self.get_tokens_unprocessed(attr_content, stack=["root", "attr"])
./.venv-build/lib/python3.11/site-packages/pygments/lexers/markup.py:1084:        yield from self.get_tokens_unprocessed(attr, stack=["root", "attr"])
./.venv-build/lib/python3.11/site-packages/pygments/lexers/markup.py:1102:            yield from lexer.get_tokens_unprocessed(content)
./.venv-build/lib/python3.11/site-packages/pygments/lexers/markup.py:1117:            yield from self.get_tokens_unprocessed(attr_content, stack=["root", "attr"])
./.venv-build/lib/python3.11/site-packages/pygments/lexers/markup.py:1121:        yield from self.get_tokens_unprocessed(attr, stack=["root", "attr"])
./.venv-build/lib/python3.11/site-packages/pygments/lexers/markup.py:1129:            yield from LilyPondLexer().get_tokens_unprocessed(content)
./.venv-build/lib/python3.11/site-packages/pygments/lexers/markup.py:1530:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/rego.py:12:from pygments.token import Comment, Operator, Keyword, Name, String, Number, Punctuation, Whitespace
./.venv-build/lib/python3.11/site-packages/pygments/lexers/rego.py:52:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/prolog.py:14:from pygments.token import Text, Comment, Operator, Keyword, Name, String, Number, Punctuation
./.venv-build/lib/python3.11/site-packages/pygments/lexers/prolog.py:31:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/prolog.py:110:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/forth.py:14:from pygments.token import Text, Comment, Keyword, Name, String, Number, Whitespace
./.venv-build/lib/python3.11/site-packages/pygments/lexers/forth.py:34:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/dalvik.py:14:from pygments.token import Keyword, Text, Comment, Name, String, Number, Punctuation, Whitespace
./.venv-build/lib/python3.11/site-packages/pygments/lexers/dalvik.py:32:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/console.py:12:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/console.py:40:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/console.py:67:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/console.py:84:            (r"(None|descr|ConstClass|ConstPtr|TargetToken)", Name),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/console.py:97:                r"force_token|quasiimmut_field|same_as|virtual_ref_finish|"
./.venv-build/lib/python3.11/site-packages/pygments/lexers/urbi.py:14:from pygments.token import Text, Comment, Operator, Keyword, Name, String, Number, Punctuation
./.venv-build/lib/python3.11/site-packages/pygments/lexers/urbi.py:34:    # - handle Experimental and deprecated tags with specific tokens
./.venv-build/lib/python3.11/site-packages/pygments/lexers/urbi.py:35:    # - handle Angles and Durations with specific tokens
./.venv-build/lib/python3.11/site-packages/pygments/lexers/urbi.py:58:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/urbi.py:146:            # deprecated keywords, use a meaningful token when available
./.venv-build/lib/python3.11/site-packages/pygments/lexers/urbi.py:148:            # ignored keywords, use a meaningful token when available
./.venv-build/lib/python3.11/site-packages/pygments/lexers/kusto.py:12:from pygments.token import Comment, Keyword, Name, Number, Punctuation, String, Whitespace
./.venv-build/lib/python3.11/site-packages/pygments/lexers/kusto.py:138:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/typoscript.py:23:from pygments.token import Text, Comment, Name, String, Number, Operator, Punctuation
./.venv-build/lib/python3.11/site-packages/pygments/lexers/typoscript.py:38:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/typoscript.py:82:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/typoscript.py:131:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/unicon.py:14:from pygments.token import Text, Comment, Operator, Keyword, Name, String, Number, Punctuation
./.venv-build/lib/python3.11/site-packages/pygments/lexers/unicon.py:33:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/unicon.py:458:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/unicon.py:834:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/yang.py:12:from pygments.token import Text, Token, Name, String, Comment, Number
./.venv-build/lib/python3.11/site-packages/pygments/lexers/yang.py:136:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/yang.py:145:            (r"[{};]+", Token.Punctuation),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/yang.py:146:            (r"(?<![\-\w])(and|or|not|\+|\.)(?![\-\w])", Token.Operator),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/yang.py:154:                bygroups(Name.Namespace, Token.Punctuation, Name.Variable),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/yang.py:160:            (words(TOP_STMTS_KEYWORDS, suffix=suffix_re_pattern), Token.Keyword),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/yang.py:161:            (words(MODULE_HEADER_STMT_KEYWORDS, suffix=suffix_re_pattern), Token.Keyword),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/yang.py:162:            (words(META_STMT_KEYWORDS, suffix=suffix_re_pattern), Token.Keyword),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/yang.py:163:            (words(LINKAGE_STMTS_KEYWORDS, suffix=suffix_re_pattern), Token.Keyword),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/yang.py:164:            (words(BODY_STMT_KEYWORDS, suffix=suffix_re_pattern), Token.Keyword),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/yang.py:165:            (words(DATA_DEF_STMT_KEYWORDS, suffix=suffix_re_pattern), Token.Keyword),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/yang.py:166:            (words(TYPE_STMT_KEYWORDS, suffix=suffix_re_pattern), Token.Keyword),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/yang.py:167:            (words(LIST_STMT_KEYWORDS, suffix=suffix_re_pattern), Token.Keyword),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lilypond.py:38:from pygments.token import Token
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lilypond.py:42:# In LilyPond, (unquoted) name tokens only contain letters, hyphens,
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lilypond.py:44:# a name token.
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lilypond.py:93:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lilypond.py:95:        for index, token, value in super().get_tokens_unprocessed(text):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lilypond.py:96:            if token is Token.Name.Function or token is Token.Name.Variable:
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lilypond.py:98:                    token = Token.Name.Builtin.SchemeFunction
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lilypond.py:99:            elif token is Token.Name.Builtin:
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lilypond.py:100:                token = Token.Name.Builtin.SchemeBuiltin
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lilypond.py:101:            yield index, token, value
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lilypond.py:103:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lilypond.py:106:            (r"\s+", Token.Text.Whitespace),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lilypond.py:108:            (r"%\{.*?%\}", Token.Comment.Multiline),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lilypond.py:110:            (r"%.*?$", Token.Comment.Single),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lilypond.py:112:            (r"#\}", Token.Punctuation, "#pop"),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lilypond.py:116:            (r"[#$]@?", Token.Punctuation, "value"),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lilypond.py:136:                Token.Punctuation,
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lilypond.py:140:            (words(pitches, suffix=r"=?[',]*!?\??" + NAME_END_RE), Token.Pitch),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lilypond.py:142:            (r'[\-_^]?"', Token.String, "string"),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lilypond.py:144:            (r"-?\d+\.\d+", Token.Number.Float),  # 5. and .5 are not allowed
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lilypond.py:145:            (r"-?\d+/\d+", Token.Number.Fraction),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lilypond.py:160:                Token.Number,
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lilypond.py:163:            (r"\*", Token.Number),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lilypond.py:165:            (r"[~()[\]]", Token.Name.Builtin.Articulation),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lilypond.py:168:            (r"[\-_^][>^_!.\-+]", Token.Name.Builtin.Articulation),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lilypond.py:170:            (r"[\-_^]?\\?\d+", Token.Name.Builtin.Articulation),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lilypond.py:172:            (builtin_words(keywords, "mandatory"), Token.Keyword),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lilypond.py:173:            (builtin_words(pitch_language_names, "disallowed"), Token.Name.PitchLanguage),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lilypond.py:174:            (builtin_words(clefs, "disallowed"), Token.Name.Builtin.Clef),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lilypond.py:175:            (builtin_words(scales, "mandatory"), Token.Name.Builtin.Scale),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lilypond.py:176:            (builtin_words(repeat_types, "disallowed"), Token.Name.Builtin.RepeatType),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lilypond.py:177:            (builtin_words(units, "mandatory"), Token.Number),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lilypond.py:178:            (builtin_words(chord_modifiers, "disallowed"), Token.ChordModifier),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lilypond.py:179:            (builtin_words(music_functions, "mandatory"), Token.Name.Builtin.MusicFunction),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lilypond.py:180:            (builtin_words(dynamics, "mandatory"), Token.Name.Builtin.Dynamic),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lilypond.py:182:            (builtin_words(articulations, "mandatory"), Token.Name.Builtin.Articulation),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lilypond.py:183:            (builtin_words(music_commands, "mandatory"), Token.Name.Builtin.MusicCommand),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lilypond.py:184:            (builtin_words(markup_commands, "mandatory"), Token.Name.Builtin.MarkupCommand),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lilypond.py:185:            (builtin_words(grobs, "disallowed"), Token.Name.Builtin.Grob),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lilypond.py:186:            (builtin_words(translators, "disallowed"), Token.Name.Builtin.Translator),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lilypond.py:188:            (builtin_words(contexts, "optional"), Token.Name.Builtin.Context),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lilypond.py:189:            (builtin_words(context_properties, "disallowed"), Token.Name.Builtin.ContextProperty),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lilypond.py:192:                Token.Name.Builtin.GrobProperty,
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lilypond.py:198:            (builtin_words(paper_variables, "optional"), Token.Name.Builtin.PaperVariable),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lilypond.py:199:            (builtin_words(header_variables, "optional"), Token.Name.Builtin.HeaderVariable),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lilypond.py:202:            (r"[\-_^]?\\.+?" + NAME_END_RE, Token.Name.BackslashReference),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lilypond.py:210:                Token.Name.Lvalue,
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lilypond.py:215:            (r"([^\W\d]|-)+?" + NAME_END_RE, Token.Text),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lilypond.py:216:            (r".", Token.Text),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lilypond.py:219:            (r'"', Token.String, "#pop"),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lilypond.py:220:            (r"\\.", Token.String.Escape),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lilypond.py:221:            (r'[^\\"]+', Token.String),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lilypond.py:226:            (r"#\{", Token.Punctuation, ("#pop", "root")),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lilypond.py:234:            (r"\s+", Token.Text.Whitespace),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lilypond.py:237:                bygroups(Token.Punctuation, Token.Name.Builtin.GrobProperty),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/modeling.py:14:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/modeling.py:48:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/modeling.py:240:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/modeling.py:265:            # SLexer makes these tokens Operators.
./.venv-build/lib/python3.11/site-packages/pygments/lexers/modeling.py:389:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/modeling.py:459:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/vyper.py:12:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/vyper.py:36:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/clean.py:12:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/clean.py:77:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/gleam.py:12:from pygments.token import Comment, Operator, Keyword, Name, String, Number, Punctuation, Whitespace
./.venv-build/lib/python3.11/site-packages/pygments/lexers/gleam.py:57:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/jsonnet.py:12:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/jsonnet.py:26:jsonnet_token = r"[^\W\d]\w*"
./.venv-build/lib/python3.11/site-packages/pygments/lexers/jsonnet.py:27:jsonnet_function_token = jsonnet_token + r"(?=\()"
./.venv-build/lib/python3.11/site-packages/pygments/lexers/jsonnet.py:50:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/jsonnet.py:98:            (r"std\." + jsonnet_function_token, Name.Builtin, "function_args"),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/jsonnet.py:99:            (jsonnet_function_token, Name.Function, "function_args"),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/jsonnet.py:100:            (jsonnet_token, Name.Variable),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/jsonnet.py:111:            (jsonnet_function_token, Name.Function, "function_params"),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/jsonnet.py:112:            (jsonnet_token, Name.Variable),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/jsonnet.py:127:            (jsonnet_token, Name.Variable),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/jsonnet.py:146:            (rf"(?={jsonnet_token})", Text, "field_name"),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/jsonnet.py:153:            (jsonnet_function_token, Name.Function, ("field_separator", "function_params")),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/jsonnet.py:154:            (jsonnet_token, Name.Variable, "field_separator"),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/jsonnet.py:182:            (jsonnet_token, Name.Variable, ("#pop", "object_local_value")),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/hdl.py:14:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/hdl.py:44:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/hdl.py:368:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/hdl.py:954:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/j.py:12:from pygments.token import Comment, Keyword, Name, Number, Operator, Punctuation, String, Whitespace
./.venv-build/lib/python3.11/site-packages/pygments/lexers/j.py:31:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/mips.py:12:from pygments.token import Whitespace, Comment, String, Keyword, Name, Text
./.venv-build/lib/python3.11/site-packages/pygments/lexers/mips.py:278:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/capnproto.py:12:from pygments.token import Text, Comment, Keyword, Name, Literal, Whitespace
./.venv-build/lib/python3.11/site-packages/pygments/lexers/capnproto.py:28:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/qvt.py:12:from pygments.token import Text, Comment, Operator, Keyword, Punctuation, Name, String, Number
./.venv-build/lib/python3.11/site-packages/pygments/lexers/qvt.py:26:    Notable tokens assignments:
./.venv-build/lib/python3.11/site-packages/pygments/lexers/qvt.py:46:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/mime.py:15:from pygments.token import Text, Name, String, Operator, Comment, Other
./.venv-build/lib/python3.11/site-packages/pygments/lexers/mime.py:64:    def get_header_tokens(self, match):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/mime.py:73:            for i, t, v in self.get_tokens_unprocessed(body, ("root", field.lower())):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/mime.py:79:    def get_body_tokens(self, match):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/mime.py:91:            for i, t, v in self.get_bodypart_tokens(entire_body):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/mime.py:110:        # process tokens of each body part
./.venv-build/lib/python3.11/site-packages/pygments/lexers/mime.py:116:            for i, t, v in self.get_bodypart_tokens(part):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/mime.py:128:    def get_bodypart_tokens(self, text):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/mime.py:153:        return lexer.get_tokens_unprocessed(text)
./.venv-build/lib/python3.11/site-packages/pygments/lexers/mime.py:164:    def get_content_type_subtokens(self, match):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/mime.py:183:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/mime.py:185:            (r"^([\w-]+):( *)([\s\S]*?\n)(?![ \t])", get_header_tokens),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/mime.py:186:            (r"^$[\s\S]+", get_body_tokens),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/mime.py:202:                get_content_type_subtokens,
./.venv-build/lib/python3.11/site-packages/pygments/lexers/asm.py:16:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/asm.py:66:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/asm.py:138:def _objdump_lexer_tokens(asm_lexer):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/asm.py:140:    Common objdump lexer tokens to wrap an ASM lexer.
./.venv-build/lib/python3.11/site-packages/pygments/lexers/asm.py:232:    tokens = _objdump_lexer_tokens(GasLexer)
./.venv-build/lib/python3.11/site-packages/pygments/lexers/asm.py:325:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/asm.py:644:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/asm.py:988:                        "token",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/asm.py:1082:                        "token",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/asm.py:1106:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/asm.py:1279:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/asm.py:1287:            # Consume everything else in one token for efficiency
./.venv-build/lib/python3.11/site-packages/pygments/lexers/asm.py:1405:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/asm.py:1471:    tokens = _objdump_lexer_tokens(NasmLexer)
./.venv-build/lib/python3.11/site-packages/pygments/lexers/asm.py:1512:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/asm.py:1581:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/asm.py:1678:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/xorg.py:12:from pygments.token import Comment, String, Name, Text
./.venv-build/lib/python3.11/site-packages/pygments/lexers/xorg.py:27:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/elpi.py:12:from pygments.token import Text, Comment, Operator, Keyword, Name, String, Number, Punctuation
./.venv-build/lib/python3.11/site-packages/pygments/lexers/elpi.py:44:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/q.py:12:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/q.py:38:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/q.py:225:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:28:from pygments.token import Token
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:33:HEADING = Token.Generic.Heading
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:34:SETTING = Token.Keyword.Namespace
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:35:IMPORT = Token.Name.Namespace
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:36:TC_KW_NAME = Token.Generic.Subheading
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:37:KEYWORD = Token.Name.Function
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:38:ARGUMENT = Token.String
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:39:VARIABLE = Token.Name.Variable
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:40:COMMENT = Token.Comment
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:41:SEPARATOR = Token.Punctuation
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:42:SYNTAX = Token.Punctuation
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:43:GHERKIN = Token.Generic.Emph
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:44:ERROR = Token.Error
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:74:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:75:        row_tokenizer = RowTokenizer()
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:76:        var_tokenizer = VariableTokenizer()
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:79:            for value, token in row_tokenizer.tokenize(row):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:80:                for value, token in var_tokenizer.tokenize(value, token):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:82:                        yield index, token, str(value)
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:86:class VariableTokenizer:
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:88:    def tokenize(self, string, token):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:90:        if var.start < 0 or token in (COMMENT, ERROR):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:91:            yield string, token
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:93:        for value, token in self._tokenize(var, string, token):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:95:                yield value, token
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:97:    def _tokenize(self, var, string, orig_token):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:99:        yield before, orig_token
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:101:        yield from self.tokenize(var.base, VARIABLE)
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:105:            yield from self.tokenize(var.index, VARIABLE)
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:107:        yield from self.tokenize(string[var.end :], orig_token)
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:110:class RowTokenizer:
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:135:    def tokenize(self, row):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:146:            yield from self._tokenize(value, index, commented, separator, heading)
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:153:    def _tokenize(self, value, index, commented, separator, heading):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:161:            yield from self._table.tokenize(value, index)
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:187:class Tokenizer:
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:188:    _tokens = None
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:193:    def tokenize(self, value):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:194:        values_and_tokens = self._tokenize(value, self._index)
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:196:        if isinstance(values_and_tokens, type(Token)):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:197:            values_and_tokens = [(value, values_and_tokens)]
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:198:        return values_and_tokens
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:200:    def _tokenize(self, value, index):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:201:        index = min(index, len(self._tokens) - 1)
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:202:        return self._tokens[index]
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:211:class Comment(Tokenizer):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:212:    _tokens = (COMMENT,)
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:215:class Setting(Tokenizer):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:216:    _tokens = (SETTING, ARGUMENT)
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:240:    _custom_tokenizer = None
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:243:        Tokenizer.__init__(self)
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:246:    def _tokenize(self, value, index):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:252:                self._custom_tokenizer = KeywordCall(support_assign=False)
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:254:                self._custom_tokenizer = ImportSetting()
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:257:        elif self._custom_tokenizer:
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:258:            return self._custom_tokenizer.tokenize(value)
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:259:        return Tokenizer._tokenize(self, value, index)
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:262:class ImportSetting(Tokenizer):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:263:    _tokens = (IMPORT, ARGUMENT)
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:271:    def _tokenize(self, value, index):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:273:            type = Setting._tokenize(self, value[1:-1], index)
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:275:        return Setting._tokenize(self, value, index)
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:283:class Variable(Tokenizer):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:284:    _tokens = (SYNTAX, ARGUMENT)
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:286:    def _tokenize(self, value, index):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:289:        return Tokenizer._tokenize(self, value, index)
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:292:class KeywordCall(Tokenizer):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:293:    _tokens = (KEYWORD, ARGUMENT)
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:296:        Tokenizer.__init__(self)
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:300:    def _tokenize(self, value, index):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:303:            return SYNTAX  # VariableTokenizer tokenizes this later.
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:305:            return Tokenizer._tokenize(self, value, index - self._assigns)
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:307:        return GherkinTokenizer().tokenize(value, KEYWORD)
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:310:class GherkinTokenizer:
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:313:    def tokenize(self, value, token):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:316:            return [(value, token)]
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:318:        return [(value[:end], GHERKIN), (value[end:], token)]
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:321:class TemplatedKeywordCall(Tokenizer):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:322:    _tokens = (ARGUMENT,)
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:325:class ForLoop(Tokenizer):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:328:        Tokenizer.__init__(self)
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:331:    def _tokenize(self, value, index):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:332:        token = self._in_arguments and ARGUMENT or SYNTAX
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:335:        return token
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:339:    _tokenizer_class = None
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:341:    def __init__(self, prev_tokenizer=None):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:342:        self._tokenizer = self._tokenizer_class()
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:343:        self._prev_tokenizer = prev_tokenizer
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:346:    def tokenize(self, value, index):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:348:            self._tokenizer = self._prev_tokenizer
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:351:            yield from self._tokenize(value, index)
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:360:    def _tokenize(self, value, index):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:361:        return self._tokenizer.tokenize(value)
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:364:        self.__init__(prev_tokenizer=self._tokenizer)
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:368:    _tokenizer_class = Comment
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:375:    _tokenizer_class = Variable
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:379:    _tokenizer_class = Setting
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:381:    def __init__(self, template_setter, prev_tokenizer=None):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:382:        _Table.__init__(self, prev_tokenizer)
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:385:    def _tokenize(self, value, index):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:387:            self._tokenizer = Setting(self._template_setter)
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:388:        return _Table._tokenize(self, value, index)
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:391:        self.__init__(self._template_setter, prev_tokenizer=self._tokenizer)
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:400:    def _tokenizer_class(self):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:408:    def _tokenize(self, value, index):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:412:            return GherkinTokenizer().tokenize(value, TC_KW_NAME)
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:416:                self._tokenizer = self._setting_class(self.set_test_template)
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:418:                self._tokenizer = self._setting_class()
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:420:            self._tokenizer = ForLoop()
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:423:        return _Table._tokenize(self, value, index)
./.venv-build/lib/python3.11/site-packages/pygments/lexers/robotframework.py:445:    _tokenizer_class = KeywordCall
./.venv-build/lib/python3.11/site-packages/pygments/lexers/prql.py:12:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/prql.py:70:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/apdlexer.py:14:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/apdlexer.py:2161:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/ptx.py:12:from pygments.token import Comment, Keyword, Name, String, Number, Punctuation, Whitespace, Operator
./.venv-build/lib/python3.11/site-packages/pygments/lexers/ptx.py:36:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/int_fiction.py:14:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/int_fiction.py:53:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/int_fiction.py:567:                        "tokens",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/int_fiction.py:660:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/int_fiction.py:662:        # If the token two tokens after 'in' is ')', 'in' is a keyword:
./.venv-build/lib/python3.11/site-packages/pygments/lexers/int_fiction.py:667:        objectloop_token_count = -1
./.venv-build/lib/python3.11/site-packages/pygments/lexers/int_fiction.py:668:        previous_token = None
./.venv-build/lib/python3.11/site-packages/pygments/lexers/int_fiction.py:669:        for index, token, value in RegexLexer.get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/int_fiction.py:670:            if previous_token is Name.Variable and value == "in":
./.venv-build/lib/python3.11/site-packages/pygments/lexers/int_fiction.py:671:                objectloop_queue = [[index, token, value]]
./.venv-build/lib/python3.11/site-packages/pygments/lexers/int_fiction.py:672:                objectloop_token_count = 2
./.venv-build/lib/python3.11/site-packages/pygments/lexers/int_fiction.py:673:            elif objectloop_token_count > 0:
./.venv-build/lib/python3.11/site-packages/pygments/lexers/int_fiction.py:674:                if token not in Comment and token not in Text:
./.venv-build/lib/python3.11/site-packages/pygments/lexers/int_fiction.py:675:                    objectloop_token_count -= 1
./.venv-build/lib/python3.11/site-packages/pygments/lexers/int_fiction.py:676:                objectloop_queue.append((index, token, value))
./.venv-build/lib/python3.11/site-packages/pygments/lexers/int_fiction.py:678:                if objectloop_token_count == 0:
./.venv-build/lib/python3.11/site-packages/pygments/lexers/int_fiction.py:683:                    objectloop_token_count = -1
./.venv-build/lib/python3.11/site-packages/pygments/lexers/int_fiction.py:684:                yield index, token, value
./.venv-build/lib/python3.11/site-packages/pygments/lexers/int_fiction.py:685:            if token not in Comment and token not in Text:
./.venv-build/lib/python3.11/site-packages/pygments/lexers/int_fiction.py:686:                previous_token = token
./.venv-build/lib/python3.11/site-packages/pygments/lexers/int_fiction.py:721:    # and use options, tokens in braces are treated as I7. Use options
./.venv-build/lib/python3.11/site-packages/pygments/lexers/int_fiction.py:723:    tokens = {}
./.venv-build/lib/python3.11/site-packages/pygments/lexers/int_fiction.py:724:    token_variants = ["+i6t-not-inline", "+i6t-inline", "+i6t-use-option"]
./.venv-build/lib/python3.11/site-packages/pygments/lexers/int_fiction.py:726:    for level in token_variants:
./.venv-build/lib/python3.11/site-packages/pygments/lexers/int_fiction.py:727:        tokens[level] = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/int_fiction.py:728:            "+i6-root": list(Inform6Lexer.tokens["root"]),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/int_fiction.py:906:        for token in Inform6Lexer.tokens:
./.venv-build/lib/python3.11/site-packages/pygments/lexers/int_fiction.py:907:            if token == "root":
./.venv-build/lib/python3.11/site-packages/pygments/lexers/int_fiction.py:909:            tokens[level][token] = list(Inform6Lexer.tokens[token])
./.venv-build/lib/python3.11/site-packages/pygments/lexers/int_fiction.py:910:            if not token.startswith("_"):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/int_fiction.py:911:                tokens[level][token][:0] = [include("+i6t"), include(level)]
./.venv-build/lib/python3.11/site-packages/pygments/lexers/int_fiction.py:915:        if level not in self._all_tokens:
./.venv-build/lib/python3.11/site-packages/pygments/lexers/int_fiction.py:916:            self._tokens = self.__class__.process_tokendef(level)
./.venv-build/lib/python3.11/site-packages/pygments/lexers/int_fiction.py:918:            self._tokens = self._all_tokens[level]
./.venv-build/lib/python3.11/site-packages/pygments/lexers/int_fiction.py:932:    def get_tokens_unprocessed(self, text, stack=("+i6t-root",)):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/int_fiction.py:933:        return Inform7Lexer.get_tokens_unprocessed(self, text, stack)
./.venv-build/lib/python3.11/site-packages/pygments/lexers/int_fiction.py:964:        token = String.Double if double else String.Single
./.venv-build/lib/python3.11/site-packages/pygments/lexers/int_fiction.py:971:                (rf"{char}{{3,}}", token, "#pop"),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/int_fiction.py:973:                (char, token),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/int_fiction.py:976:            state.append((char, token, "#pop"))
./.venv-build/lib/python3.11/site-packages/pygments/lexers/int_fiction.py:977:        state += [include("s/verbatim"), (rf"[^\\<&{{}}{char}]+", token)]
./.venv-build/lib/python3.11/site-packages/pygments/lexers/int_fiction.py:1017:            (r"[\\&{}<]", token),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/int_fiction.py:1025:        token = String.Double if double else String.Single
./.venv-build/lib/python3.11/site-packages/pygments/lexers/int_fiction.py:1028:            (rf"{char}{quantifier}", token, "#pop:2"),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/int_fiction.py:1046:        token = (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/int_fiction.py:1053:        host_token = String.Double if host_double else String.Single
./.venv-build/lib/python3.11/site-packages/pygments/lexers/int_fiction.py:1056:            (rf"{host_char}{host_quantifier}", host_token, "#pop:3"),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/int_fiction.py:1057:            (r"{}{}".format(r"" if token is String.Other else r"\\?", terminator), token, "#pop"),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/int_fiction.py:1064:            (r'([^\s"\'<%s{}\\&])+' % (r">" if token is String.Other else r""), token),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/int_fiction.py:1066:            (r'["\'\s&{<}\\]', token),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/int_fiction.py:1069:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/int_fiction.py:1185:            # Two-token keywords
./.venv-build/lib/python3.11/site-packages/pygments/lexers/int_fiction.py:1339:            (r"token\b", Keyword, ("#pop", "constants")),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/int_fiction.py:1513:    def get_tokens_unprocessed(self, text, **kwargs):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/int_fiction.py:1516:        for index, token, value in RegexLexer.get_tokens_unprocessed(self, text, **kwargs):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/int_fiction.py:1518:                if token is Comment.Preproc and re.match(
./.venv-build/lib/python3.11/site-packages/pygments/lexers/int_fiction.py:1523:                if token is Comment.Preproc:
./.venv-build/lib/python3.11/site-packages/pygments/lexers/int_fiction.py:1531:                    token = Comment
./.venv-build/lib/python3.11/site-packages/pygments/lexers/int_fiction.py:1532:            yield index, token, value
./.venv-build/lib/python3.11/site-packages/pygments/lexers/diff.py:14:from pygments.token import Text, Comment, Operator, Keyword, Name, Generic, Literal, Whitespace
./.venv-build/lib/python3.11/site-packages/pygments/lexers/diff.py:31:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/diff.py:71:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/diff.py:159:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/webmisc.py:14:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/webmisc.py:49:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/webmisc.py:69:    An XQuery lexer, parsing a stream and outputting the tokens needed to
./.venv-build/lib/python3.11/site-packages/pygments/lexers/webmisc.py:329:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/webmisc.py:878:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/webmisc.py:957:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/webmisc.py:1006:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_php_builtins.py:1743:        "oci_password_change",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_php_builtins.py:1896:        "openssl_x509_check_private_key",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_php_builtins.py:2137:    "Password Hashing": (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_php_builtins.py:2138:        "password_algos",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_php_builtins.py:2139:        "password_get_info",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_php_builtins.py:2140:        "password_hash",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_php_builtins.py:2141:        "password_needs_rehash",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_php_builtins.py:2142:        "password_verify",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_php_builtins.py:2312:        "radius_server_secret",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_php_builtins.py:2409:        "ssh2_auth_password",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_php_builtins.py:2611:        "sodium_crypto_box_keypair_from_secretkey_and_publickey",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_php_builtins.py:2614:        "sodium_crypto_box_publickey_from_secretkey",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_php_builtins.py:2618:        "sodium_crypto_box_secretkey",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_php_builtins.py:2631:        "sodium_crypto_kx_secretkey",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_php_builtins.py:2643:        "sodium_crypto_secretbox_keygen",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_php_builtins.py:2644:        "sodium_crypto_secretbox_open",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_php_builtins.py:2645:        "sodium_crypto_secretbox",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_php_builtins.py:2646:        "sodium_crypto_secretstream_xchacha20poly1305_init_pull",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_php_builtins.py:2647:        "sodium_crypto_secretstream_xchacha20poly1305_init_push",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_php_builtins.py:2648:        "sodium_crypto_secretstream_xchacha20poly1305_keygen",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_php_builtins.py:2649:        "sodium_crypto_secretstream_xchacha20poly1305_pull",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_php_builtins.py:2650:        "sodium_crypto_secretstream_xchacha20poly1305_push",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_php_builtins.py:2651:        "sodium_crypto_secretstream_xchacha20poly1305_rekey",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_php_builtins.py:2657:        "sodium_crypto_sign_keypair_from_secretkey_and_publickey",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_php_builtins.py:2660:        "sodium_crypto_sign_publickey_from_secretkey",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_php_builtins.py:2662:        "sodium_crypto_sign_secretkey",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_php_builtins.py:2869:    "Tokenizer": ("token_get_all", "token_name"),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/maxima.py:16:from pygments.token import Text, Comment, Operator, Keyword, Name, String, Number, Punctuation
./.venv-build/lib/python3.11/site-packages/pygments/lexers/maxima.py:87:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/graphviz.py:12:from pygments.token import Comment, Keyword, Operator, Name, String, Number, Punctuation, Whitespace
./.venv-build/lib/python3.11/site-packages/pygments/lexers/graphviz.py:29:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/tls.py:14:from pygments.token import Comment, Operator, Keyword, Name, String, Number, Punctuation, Whitespace
./.venv-build/lib/python3.11/site-packages/pygments/lexers/tls.py:33:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/tls.py:49:            # tokens
./.venv-build/lib/python3.11/site-packages/pygments/lexers/graph.py:14:from pygments.token import Keyword, Punctuation, Comment, Operator, Name, String, Number, Whitespace
./.venv-build/lib/python3.11/site-packages/pygments/lexers/graph.py:35:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/haxe.py:14:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/haxe.py:97:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/haxe.py:258:        # same as 'ident' but set token as Name.Decorator instead of Name
./.venv-build/lib/python3.11/site-packages/pygments/lexers/haxe.py:834:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/gsql.py:14:from pygments.token import Keyword, Punctuation, Comment, Operator, Name, String, Number, Whitespace
./.venv-build/lib/python3.11/site-packages/pygments/lexers/gsql.py:32:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/igor.py:14:from pygments.token import Text, Comment, Keyword, Name, String, Whitespace
./.venv-build/lib/python3.11/site-packages/pygments/lexers/igor.py:1618:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/floscript.py:12:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/floscript.py:53:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/rebol.py:14:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/rebol.py:136:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/rebol.py:349:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/ncl.py:14:from pygments.token import Text, Comment, Operator, Keyword, Name, String, Number, Punctuation
./.venv-build/lib/python3.11/site-packages/pygments/lexers/ncl.py:33:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/templates.py:33:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/templates.py:45:    Token,
./.venv-build/lib/python3.11/site-packages/pygments/lexers/templates.py:141:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/templates.py:147:        tokens = self._block_re.split(text)
./.venv-build/lib/python3.11/site-packages/pygments/lexers/templates.py:148:        tokens.reverse()
./.venv-build/lib/python3.11/site-packages/pygments/lexers/templates.py:154:                    val = tokens.pop()
./.venv-build/lib/python3.11/site-packages/pygments/lexers/templates.py:160:                    tag = tokens.pop()
./.venv-build/lib/python3.11/site-packages/pygments/lexers/templates.py:169:                        val = tokens.pop()
./.venv-build/lib/python3.11/site-packages/pygments/lexers/templates.py:177:                        data = tokens.pop()
./.venv-build/lib/python3.11/site-packages/pygments/lexers/templates.py:179:                        for r_idx, r_token, r_value in self.ruby_lexer.get_tokens_unprocessed(data):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/templates.py:180:                            yield r_idx + idx, r_token, r_value
./.venv-build/lib/python3.11/site-packages/pygments/lexers/templates.py:191:                        for r_idx, r_token, r_value in self.ruby_lexer.get_tokens_unprocessed(
./.venv-build/lib/python3.11/site-packages/pygments/lexers/templates.py:194:                            yield idx + 1 + r_idx, r_token, r_value
./.venv-build/lib/python3.11/site-packages/pygments/lexers/templates.py:199:                    tag = tokens.pop()
./.venv-build/lib/python3.11/site-packages/pygments/lexers/templates.py:231:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/templates.py:292:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/templates.py:412:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/templates.py:508:    markup is yielded as `Token.Other`.
./.venv-build/lib/python3.11/site-packages/pygments/lexers/templates.py:518:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/templates.py:639:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/templates.py:691:    markup is yielded as `Token.Other`.
./.venv-build/lib/python3.11/site-packages/pygments/lexers/templates.py:701:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/templates.py:827:    Lexer for handling Cheetah's special $ tokens in Python syntax.
./.venv-build/lib/python3.11/site-packages/pygments/lexers/templates.py:830:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/templates.py:832:        for pos, type_, value in pylexer.get_tokens_unprocessed(text):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/templates.py:833:            if type_ == Token.Error and value == "$":
./.venv-build/lib/python3.11/site-packages/pygments/lexers/templates.py:841:    markup is yielded as `Token.Other`.  This also works for
./.venv-build/lib/python3.11/site-packages/pygments/lexers/templates.py:854:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/templates.py:952:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/templates.py:989:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/templates.py:1511:    Base for the `JspLexer`. Yields `Token.Other` for area outside of
./.venv-build/lib/python3.11/site-packages/pygments/lexers/templates.py:1517:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/templates.py:1571:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/templates.py:1685:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/templates.py:1738:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/templates.py:1838:    Base for the `TeaTemplateLexer`. Yields `Token.Other` for area outside of
./.venv-build/lib/python3.11/site-packages/pygments/lexers/templates.py:1844:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/templates.py:2004:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/templates.py:2103:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/templates.py:2304:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/templates.py:2420:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/eiffel.py:12:from pygments.token import Comment, Operator, Keyword, Name, String, Number, Punctuation, Whitespace
./.venv-build/lib/python3.11/site-packages/pygments/lexers/eiffel.py:29:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/ldap.py:15:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/ldap.py:43:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/ldap.py:165:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_mysql_builtins.py:479:    "validate_password_strength",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_mysql_builtins.py:840:    "master_password",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_mysql_builtins.py:937:    "password",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_mysql_builtins.py:938:    "password_lock_time",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_mysql_builtins.py:1082:    "source_password",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/pddl.py:12:from pygments.token import Punctuation, Keyword, Whitespace, Name, Comment, Operator, Number
./.venv-build/lib/python3.11/site-packages/pygments/lexers/pddl.py:32:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/typst.py:12:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/typst.py:93:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/typst.py:236:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/typst.py:241:        yield from RegexLexer.get_tokens_unprocessed(self, text, stack)
./.venv-build/lib/python3.11/site-packages/pygments/lexers/csound.py:14:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/csound.py:39:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/csound.py:141:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/csound.py:200:        type_annotation_token = Keyword.Type
./.venv-build/lib/python3.11/site-packages/pygments/lexers/csound.py:208:            type_annotation_token = Name
./.venv-build/lib/python3.11/site-packages/pygments/lexers/csound.py:218:            yield match.start(3), type_annotation_token, match.group(3)
./.venv-build/lib/python3.11/site-packages/pygments/lexers/csound.py:220:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/csound.py:405:    # These tokens are based on those in XmlLexer in pygments/lexers/html.py. Making
./.venv-build/lib/python3.11/site-packages/pygments/lexers/csound.py:412:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/dotnet.py:23:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/dotnet.py:108:    tokens = {}
./.venv-build/lib/python3.11/site-packages/pygments/lexers/dotnet.py:109:    token_variants = True
./.venv-build/lib/python3.11/site-packages/pygments/lexers/dotnet.py:112:        tokens[levelname] = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/dotnet.py:340:        level = get_choice_opt(options, "unicodelevel", list(self.tokens), "basic")
./.venv-build/lib/python3.11/site-packages/pygments/lexers/dotnet.py:341:        if level not in self._all_tokens:
./.venv-build/lib/python3.11/site-packages/pygments/lexers/dotnet.py:343:            self._tokens = self.__class__.process_tokendef(level)
./.venv-build/lib/python3.11/site-packages/pygments/lexers/dotnet.py:345:            self._tokens = self._all_tokens[level]
./.venv-build/lib/python3.11/site-packages/pygments/lexers/dotnet.py:403:    tokens = {}
./.venv-build/lib/python3.11/site-packages/pygments/lexers/dotnet.py:404:    token_variants = True
./.venv-build/lib/python3.11/site-packages/pygments/lexers/dotnet.py:407:        tokens[levelname] = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/dotnet.py:513:        level = get_choice_opt(options, "unicodelevel", list(self.tokens), "basic")
./.venv-build/lib/python3.11/site-packages/pygments/lexers/dotnet.py:514:        if level not in self._all_tokens:
./.venv-build/lib/python3.11/site-packages/pygments/lexers/dotnet.py:516:            self._tokens = self.__class__.process_tokendef(level)
./.venv-build/lib/python3.11/site-packages/pygments/lexers/dotnet.py:518:            self._tokens = self._all_tokens[level]
./.venv-build/lib/python3.11/site-packages/pygments/lexers/dotnet.py:545:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/dotnet.py:625:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/dotnet.py:839:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/dotnet.py:1085:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/dotnet.py:1560:    tokens = {}
./.venv-build/lib/python3.11/site-packages/pygments/lexers/dotnet.py:1562:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/futhark.py:12:from pygments.token import Comment, Operator, Keyword, Name, String, Number, Punctuation, Whitespace
./.venv-build/lib/python3.11/site-packages/pygments/lexers/futhark.py:97:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/comal.py:14:from pygments.token import Comment, Whitespace, Operator, Keyword, String, Number, Name, Punctuation
./.venv-build/lib/python3.11/site-packages/pygments/lexers/comal.py:38:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_openedge_builtins.py:76:    "APPSERVER-PASSWORD",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_openedge_builtins.py:1780:    "PASSWORD-FIELD",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_openedge_builtins.py:1892:    "PROXY-PASSWORD",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_openedge_builtins.py:2434:    "URL-PASSWORD",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/archetype.py:17:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/archetype.py:41:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/archetype.py:171:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/archetype.py:220:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/archetype.py:294:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/ride.py:12:from pygments.token import Comment, Keyword, Name, Number, Punctuation, String, Text
./.venv-build/lib/python3.11/site-packages/pygments/lexers/ride.py:225:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/cplint.py:13:from pygments.token import Operator, Keyword, Name, String, Punctuation
./.venv-build/lib/python3.11/site-packages/pygments/lexers/cplint.py:31:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/bare.py:12:from pygments.token import Text, Comment, Keyword, Name, Literal, Whitespace
./.venv-build/lib/python3.11/site-packages/pygments/lexers/bare.py:51:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/usd.py:20:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/usd.py:55:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/usd.py:60:                    Keyword.Token,
./.venv-build/lib/python3.11/site-packages/pygments/lexers/usd.py:62:                    Keyword.Token,
./.venv-build/lib/python3.11/site-packages/pygments/lexers/usd.py:68:                    Name.Keyword.Tokens,
./.venv-build/lib/python3.11/site-packages/pygments/lexers/usd.py:76:                    Keyword.Token,
./.venv-build/lib/python3.11/site-packages/pygments/lexers/usd.py:82:                    Name.Keyword.Tokens,
./.venv-build/lib/python3.11/site-packages/pygments/lexers/usd.py:90:                    Keyword.Token,
./.venv-build/lib/python3.11/site-packages/pygments/lexers/usd.py:96:                    Name.Keyword.Tokens,
./.venv-build/lib/python3.11/site-packages/pygments/lexers/usd.py:108:                    Name.Keyword.Tokens,
./.venv-build/lib/python3.11/site-packages/pygments/lexers/usd.py:114:        + _keywords(KEYWORDS, Keyword.Tokens)
./.venv-build/lib/python3.11/site-packages/pygments/lexers/rnc.py:12:from pygments.token import Text, Comment, Operator, Keyword, Name, String, Punctuation
./.venv-build/lib/python3.11/site-packages/pygments/lexers/rnc.py:28:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/vip.py:14:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/vip.py:54:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/vip.py:187:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/vip.py:230:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/berry.py:12:from pygments.token import Comment, Whitespace, Operator, Keyword, Name, String, Number, Punctuation
./.venv-build/lib/python3.11/site-packages/pygments/lexers/berry.py:31:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/snobol.py:12:from pygments.token import Text, Comment, Operator, Keyword, Name, String, Number, Punctuation
./.venv-build/lib/python3.11/site-packages/pygments/lexers/snobol.py:32:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/webassembly.py:16:from pygments.token import Text, Comment, Operator, Keyword, String, Number, Punctuation, Name
./.venv-build/lib/python3.11/site-packages/pygments/lexers/webassembly.py:230:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/inferno.py:14:from pygments.token import Punctuation, Comment, Operator, Keyword, Name, String, Number, Whitespace
./.venv-build/lib/python3.11/site-packages/pygments/lexers/inferno.py:35:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/shell.py:25:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/shell.py:84:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/shell.py:188:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/shell.py:189:        for index, token, value in BashLexer.get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/shell.py:190:            if token is Text and value in self.EXTRA_KEYWORDS:
./.venv-build/lib/python3.11/site-packages/pygments/lexers/shell.py:192:            elif token is Comment.Single and "SBATCH" in value:
./.venv-build/lib/python3.11/site-packages/pygments/lexers/shell.py:195:                yield index, token, value
./.venv-build/lib/python3.11/site-packages/pygments/lexers/shell.py:208:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/shell.py:251:                    toks = innerlexer.get_tokens_unprocessed(curcode)
./.venv-build/lib/python3.11/site-packages/pygments/lexers/shell.py:258:            for i, t, v in do_insertions(insertions, innerlexer.get_tokens_unprocessed(curcode)):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/shell.py:304:    _token_terminator = rf"(?=\^?[{_ws}]|[{_punct}{_nl}])"
./.venv-build/lib/python3.11/site-packages/pygments/lexers/shell.py:308:    _number = rf"(?:-?(?:0[0-7]+|0x[\da-f]+|\d+){_token_terminator})"
./.venv-build/lib/python3.11/site-packages/pygments/lexers/shell.py:318:    _core_token = rf'(?:(?:(?:\^[{_nl}]?)?[^"{_nlws}{_punct}])+)'
./.venv-build/lib/python3.11/site-packages/pygments/lexers/shell.py:319:    _core_token_compound = rf'(?:(?:(?:\^[{_nl}]?)?[^"{_nlws}{_punct})])+)'
./.venv-build/lib/python3.11/site-packages/pygments/lexers/shell.py:320:    _token = rf"(?:[{_punct}]+|{_core_token})"
./.venv-build/lib/python3.11/site-packages/pygments/lexers/shell.py:321:    _token_compound = rf"(?:[{_punct}]+|{_core_token_compound})"
./.venv-build/lib/python3.11/site-packages/pygments/lexers/shell.py:322:    _stoken = rf"(?:[{_punct}]+|(?:{_string}|{_variable}|{_core_token})+)"
./.venv-build/lib/python3.11/site-packages/pygments/lexers/shell.py:326:        _core_token=_core_token,
./.venv-build/lib/python3.11/site-packages/pygments/lexers/shell.py:327:        _core_token_compound=_core_token_compound,
./.venv-build/lib/python3.11/site-packages/pygments/lexers/shell.py:334:        _stoken=_stoken,
./.venv-build/lib/python3.11/site-packages/pygments/lexers/shell.py:335:        _token_terminator=_token_terminator,
./.venv-build/lib/python3.11/site-packages/pygments/lexers/shell.py:348:            _token_terminator = rf"(?:(?=\))|{_token_terminator})"
./.venv-build/lib/python3.11/site-packages/pygments/lexers/shell.py:354:                else (rf"\)((?=\()|{_token_terminator}){rest_of_line}", Comment.Single)
./.venv-build/lib/python3.11/site-packages/pygments/lexers/shell.py:364:                rf"(?<=m))(?:(?=\()|{_token_terminator})))({_space}?{_core_token_compound if compound else _core_token}?(?:\^[{_nl}]?)?/(?:\^[{_nl}]?)?\?)",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/shell.py:428:                rf"(for{_token_terminator}(?!\^))({_space})(/f{_token_terminator})",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/shell.py:433:                rf"(for{_token_terminator}(?!\^))({_space})(/l{_token_terminator})",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/shell.py:437:            (rf"for{_token_terminator}(?!\^)", Keyword, ("for2", "for")),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/shell.py:444:                rf"(if(?:(?=\()|{_token_terminator})(?!\^))({_space}?)((?:/i{_token_terminator})?)({_space}?)((?:not{_token_terminator})?)({_space}?)",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/shell.py:456:                rf"rem(((?=\()|{_token_terminator}){_space}?{_stoken}?.*|{_keyword_terminator}{rest_of_line_compound if compound else rest_of_line})",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/shell.py:498:        _token=_token,
./.venv-build/lib/python3.11/site-packages/pygments/lexers/shell.py:499:        _token_compound=_token_compound,
./.venv-build/lib/python3.11/site-packages/pygments/lexers/shell.py:594:        _core_token_compound=_core_token_compound,
./.venv-build/lib/python3.11/site-packages/pygments/lexers/shell.py:597:        _stoken=_stoken,
./.venv-build/lib/python3.11/site-packages/pygments/lexers/shell.py:603:        stoken_compound = rf"(?:[{_punct}]+|(?:{_string}|{_variable}|{_core_token_compound})+)"
./.venv-build/lib/python3.11/site-packages/pygments/lexers/shell.py:610:                rf"((?:(?<=[{_nlws}])(?<!\^[{_nl}])\d)?)(>>?|<)({_space}?{stoken_compound if compound else _stoken})",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/shell.py:615:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/shell.py:664:                rf"({_space})(do{_token_terminator})",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/shell.py:690:                rf"((?:cmdextversion|errorlevel){_token_terminator})({_space})(\d+)",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/shell.py:695:                rf"(defined{_token_terminator})({_space})({_stoken})",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/shell.py:700:                rf"(exist{_token_terminator})({_space}{_stoken})",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/shell.py:711:            (_stoken, using(this, state="text"), ("#pop", "if2")),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/shell.py:715:                rf"({_space}?)(==)({_space}?{_stoken})",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/shell.py:720:                rf"({_space})({_opword})({_space}{_stoken})",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/shell.py:732:            (rf"else{_token_terminator}", Keyword, "#pop"),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/shell.py:769:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/shell.py:914:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/shell.py:1001:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/shell.py:1077:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/arrow.py:12:from pygments.token import Text, Operator, Keyword, Punctuation, Name, String, Number, Whitespace
./.venv-build/lib/python3.11/site-packages/pygments/lexers/arrow.py:32:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/supercollider.py:14:from pygments.token import Text, Comment, Operator, Keyword, Name, String, Number, Punctuation
./.venv-build/lib/python3.11/site-packages/pygments/lexers/supercollider.py:32:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/qlik.py:14:from pygments.token import Comment, Keyword, Name, Number, Operator, Punctuation, String, Text
./.venv-build/lib/python3.11/site-packages/pygments/lexers/qlik.py:38:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/praat.py:12:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/praat.py:421:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/sophia.py:14:from pygments.token import Comment, Keyword, Name, Number, Operator, Punctuation, String, Text
./.venv-build/lib/python3.11/site-packages/pygments/lexers/sophia.py:81:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lean.py:14:from pygments.token import Comment, Operator, Keyword, Name, String, Number, Generic, Whitespace
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lean.py:38:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/lean.py:387:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/tablegen.py:13:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/tablegen.py:127:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/tablegen.py:149:            # numbers, and we want to parse 1X as one name token as opposed to
./.venv-build/lib/python3.11/site-packages/pygments/lexers/erlang.py:23:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/erlang.py:235:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/erlang.py:315:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/erlang.py:329:                    yield from do_insertions(insertions, erlexer.get_tokens_unprocessed(curcode))
./.venv-build/lib/python3.11/site-packages/pygments/lexers/erlang.py:337:            yield from do_insertions(insertions, erlexer.get_tokens_unprocessed(curcode))
./.venv-build/lib/python3.11/site-packages/pygments/lexers/erlang.py:340:def gen_elixir_string_rules(name, symbol, token):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/erlang.py:343:        (rf"[^#{symbol}\\]+", token),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/erlang.py:345:        (r"\\.", token),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/erlang.py:346:        (rf"({symbol})", bygroups(token), "#pop"),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/erlang.py:352:def gen_elixir_sigstr_rules(term, term_class, token, interpol=True):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/erlang.py:355:            (rf"[^#{term_class}\\]+", token),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/erlang.py:357:            (r"\\.", token),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/erlang.py:358:            (rf"{term}[a-zA-Z]*", token, "#pop"),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/erlang.py:363:            (rf"[^{term_class}\\]+", token),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/erlang.py:364:            (r"\\.", token),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/erlang.py:365:            (rf"{term}[a-zA-Z]*", token, "#pop"),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/erlang.py:467:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/erlang.py:468:        for index, token, value in RegexLexer.get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/erlang.py:469:            if token is Name:
./.venv-build/lib/python3.11/site-packages/pygments/lexers/erlang.py:485:                    yield index, token, value
./.venv-build/lib/python3.11/site-packages/pygments/lexers/erlang.py:487:                yield index, token, value
./.venv-build/lib/python3.11/site-packages/pygments/lexers/erlang.py:505:        token = String.Other
./.venv-build/lib/python3.11/site-packages/pygments/lexers/erlang.py:512:                    bygroups(token, String.Heredoc),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/erlang.py:517:                    bygroups(token, String.Heredoc),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/erlang.py:523:                (r"[a-zA-Z]+", token, "#pop"),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/erlang.py:537:                (r"~[a-z]" + lterm, token, name + "-intp"),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/erlang.py:538:                (r"~[A-Z]" + lterm, token, name + "-no-intp"),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/erlang.py:540:            states[name + "-intp"] = gen_elixir_sigstr_rules(rterm, rterm_class, token)
./.venv-build/lib/python3.11/site-packages/pygments/lexers/erlang.py:542:                rterm, rterm_class, token, interpol=False
./.venv-build/lib/python3.11/site-packages/pygments/lexers/erlang.py:562:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/erlang.py:658:    tokens.update(gen_elixir_string_rules("double", '"', String.Double))
./.venv-build/lib/python3.11/site-packages/pygments/lexers/erlang.py:659:    tokens.update(gen_elixir_string_rules("single", "'", String.Single))
./.venv-build/lib/python3.11/site-packages/pygments/lexers/erlang.py:660:    tokens.update(gen_elixir_string_rules("double_atom", '"', String.Symbol))
./.venv-build/lib/python3.11/site-packages/pygments/lexers/erlang.py:661:    tokens.update(gen_elixir_string_rules("single_atom", "'", String.Symbol))
./.venv-build/lib/python3.11/site-packages/pygments/lexers/erlang.py:662:    tokens.update(gen_elixir_sigil_rules())
./.venv-build/lib/python3.11/site-packages/pygments/lexers/erlang.py:691:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/erlang.py:713:                            insertions, exlexer.get_tokens_unprocessed(curcode)
./.venv-build/lib/python3.11/site-packages/pygments/lexers/erlang.py:717:                    token = Generic.Error if in_error else Generic.Output
./.venv-build/lib/python3.11/site-packages/pygments/lexers/erlang.py:718:                    yield match.start(), token, line
./.venv-build/lib/python3.11/site-packages/pygments/lexers/erlang.py:720:            yield from do_insertions(insertions, exlexer.get_tokens_unprocessed(curcode))
./.venv-build/lib/python3.11/site-packages/pygments/lexers/jvm.py:24:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/jvm.py:72:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/jvm.py:209:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/jvm.py:210:        for index, token, value in JavaLexer.get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/jvm.py:211:            if token is Name and value in self.aj_keywords:
./.venv-build/lib/python3.11/site-packages/pygments/lexers/jvm.py:213:            elif token is Name.Label and value in self.aj_inter_type:
./.venv-build/lib/python3.11/site-packages/pygments/lexers/jvm.py:216:            elif token is Name.Decorator and value in self.aj_inter_type_annotation:
./.venv-build/lib/python3.11/site-packages/pygments/lexers/jvm.py:219:                yield index, token, value
./.venv-build/lib/python3.11/site-packages/pygments/lexers/jvm.py:319:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/jvm.py:589:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/jvm.py:667:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/jvm.py:670:        yield from lexer.get_tokens_unprocessed(text, stack)
./.venv-build/lib/python3.11/site-packages/pygments/lexers/jvm.py:687:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/jvm.py:763:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/jvm.py:1285:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/jvm.py:1353:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/jvm.py:1404:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/jvm.py:1508:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/jvm.py:1679:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/jvm.py:1741:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/jvm.py:1780:                r"TOKENIZE)\b",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/jvm.py:1813:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/jvm.py:1908:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/jvm.py:2382:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/freefem.py:11:from pygments.token import Comment, Operator, Keyword, Name
./.venv-build/lib/python3.11/site-packages/pygments/lexers/freefem.py:936:    def get_tokens_unprocessed(self, text, stack=("root",)):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/freefem.py:937:        for index, token, value in CppLexer.get_tokens_unprocessed(self, text, stack):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/freefem.py:955:                yield index, token, value
./.venv-build/lib/python3.11/site-packages/pygments/lexers/ml.py:14:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/ml.py:123:    # Callbacks for distinguishing tokens and reserved words
./.venv-build/lib/python3.11/site-packages/pygments/lexers/ml.py:126:            token = Error
./.venv-build/lib/python3.11/site-packages/pygments/lexers/ml.py:128:            token = Name.Namespace
./.venv-build/lib/python3.11/site-packages/pygments/lexers/ml.py:129:        yield match.start(1), token, match.group(1)
./.venv-build/lib/python3.11/site-packages/pygments/lexers/ml.py:134:            token = Error
./.venv-build/lib/python3.11/site-packages/pygments/lexers/ml.py:136:            token = Error
./.venv-build/lib/python3.11/site-packages/pygments/lexers/ml.py:138:            token = Name
./.venv-build/lib/python3.11/site-packages/pygments/lexers/ml.py:139:        yield match.start(1), token, match.group(1)
./.venv-build/lib/python3.11/site-packages/pygments/lexers/ml.py:144:            token = Keyword.Reserved
./.venv-build/lib/python3.11/site-packages/pygments/lexers/ml.py:146:            token = Punctuation
./.venv-build/lib/python3.11/site-packages/pygments/lexers/ml.py:148:            token = Name
./.venv-build/lib/python3.11/site-packages/pygments/lexers/ml.py:149:        yield match.start(1), token, str
./.venv-build/lib/python3.11/site-packages/pygments/lexers/ml.py:151:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/ml.py:464:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/ml.py:569:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/ml.py:948:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/ml.py:1125:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/minecraft.py:23:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/minecraft.py:49:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/minecraft.py:119:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/minecraft.py:314:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/smalltalk.py:12:from pygments.token import Text, Comment, Operator, Keyword, Name, String, Number, Punctuation
./.venv-build/lib/python3.11/site-packages/pygments/lexers/smalltalk.py:31:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/smalltalk.py:170:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/jsx.py:15:from pygments.token import Name, Operator, Punctuation, String, Text, Whitespace
./.venv-build/lib/python3.11/site-packages/pygments/lexers/jsx.py:74:    # Use same tokens as `JavascriptLexer`, but with tags and attributes support
./.venv-build/lib/python3.11/site-packages/pygments/lexers/jsx.py:75:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/jsx.py:96:    # Use same tokens as `TypescriptLexer`, but with tags and attributes support
./.venv-build/lib/python3.11/site-packages/pygments/lexers/jsx.py:97:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/esoteric.py:12:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/esoteric.py:46:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/esoteric.py:104:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/esoteric.py:133:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/esoteric.py:266:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/esoteric.py:395:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/esoteric.py:426:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/smv.py:12:from pygments.token import Comment, Keyword, Name, Number, Operator, Punctuation, Text
./.venv-build/lib/python3.11/site-packages/pygments/lexers/smv.py:29:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/x10.py:12:from pygments.token import Text, Comment, Keyword, String
./.venv-build/lib/python3.11/site-packages/pygments/lexers/x10.py:95:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/ambient.py:14:from pygments.token import Comment, Operator, Keyword, Name, String, Number, Punctuation, Whitespace
./.venv-build/lib/python3.11/site-packages/pygments/lexers/ambient.py:55:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/thingsdb.py:12:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/thingsdb.py:38:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/thingsdb.py:107:                r"collections_info|del_collection|del_expired|del_node|del_token|"
./.venv-build/lib/python3.11/site-packages/pygments/lexers/thingsdb.py:108:                r"del_user|grant|has_collection|has_node|has_token|has_user|"
./.venv-build/lib/python3.11/site-packages/pygments/lexers/thingsdb.py:109:                r"new_collection|new_node|new_token|new_user|rename_collection|"
./.venv-build/lib/python3.11/site-packages/pygments/lexers/thingsdb.py:110:                r"rename_user|restore|revoke|set_password|set_time_zone|"
./.venv-build/lib/python3.11/site-packages/pygments/lexers/oberon.py:14:from pygments.token import Text, Comment, Operator, Keyword, Name, String, Number, Punctuation
./.venv-build/lib/python3.11/site-packages/pygments/lexers/oberon.py:33:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/rdf.py:14:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/rdf.py:116:    # Lexer token definitions ::
./.venv-build/lib/python3.11/site-packages/pygments/lexers/rdf.py:118:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/rdf.py:282:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/rdf.py:451:    # Lexer token definitions ::
./.venv-build/lib/python3.11/site-packages/pygments/lexers/rdf.py:453:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/phix.py:14:from pygments.token import Text, Comment, Operator, Keyword, Name, String, Whitespace
./.venv-build/lib/python3.11/site-packages/pygments/lexers/phix.py:1160:        "CURLOPT_PASSWORD",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/phix.py:1395:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/d.py:12:from pygments.token import Comment, Keyword, Name, String, Number, Punctuation, Whitespace
./.venv-build/lib/python3.11/site-packages/pygments/lexers/d.py:29:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/d.py:238:            # -- TokenString
./.venv-build/lib/python3.11/site-packages/pygments/lexers/d.py:239:            (r"q\{", String, "token_string"),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/d.py:242:            # Tokens
./.venv-build/lib/python3.11/site-packages/pygments/lexers/d.py:263:        "token_string": [
./.venv-build/lib/python3.11/site-packages/pygments/lexers/d.py:264:            (r"\{", Punctuation, "token_string_nest"),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/d.py:268:        "token_string_nest": [
./.venv-build/lib/python3.11/site-packages/pygments/lexers/d.py:328:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/d.py:400:            # Tokens
./.venv-build/lib/python3.11/site-packages/pygments/lexers/varnish.py:12:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/varnish.py:51:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/varnish.py:272:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/asn1.py:13:from pygments.token import Comment, Operator, Keyword, Name, String, Number, Punctuation, Whitespace
./.venv-build/lib/python3.11/site-packages/pygments/lexers/asn1.py:112:def word_sequences(tokens):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/asn1.py:113:    return "(" + "|".join(token.replace(" ", r"\s+") for token in tokens) + r")\b"
./.venv-build/lib/python3.11/site-packages/pygments/lexers/asn1.py:129:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/pascal.py:15:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/pascal.py:48:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/pascal.py:49:        return self.lexer.get_tokens_unprocessed(text)
./.venv-build/lib/python3.11/site-packages/pygments/lexers/pascal.py:1298:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/pascal.py:1304:        next_token_is_function = False
./.venv-build/lib/python3.11/site-packages/pygments/lexers/pascal.py:1305:        next_token_is_property = False
./.venv-build/lib/python3.11/site-packages/pygments/lexers/pascal.py:1311:            token = Error
./.venv-build/lib/python3.11/site-packages/pygments/lexers/pascal.py:1315:                    token = Whitespace
./.venv-build/lib/python3.11/site-packages/pygments/lexers/pascal.py:1318:                        token = Comment.Preproc
./.venv-build/lib/python3.11/site-packages/pygments/lexers/pascal.py:1320:                        token = Comment.Multiline
./.venv-build/lib/python3.11/site-packages/pygments/lexers/pascal.py:1322:                    token = Comment.Single
./.venv-build/lib/python3.11/site-packages/pygments/lexers/pascal.py:1326:                    token = Operator
./.venv-build/lib/python3.11/site-packages/pygments/lexers/pascal.py:1328:                    token = Operator
./.venv-build/lib/python3.11/site-packages/pygments/lexers/pascal.py:1333:                    token = Punctuation
./.venv-build/lib/python3.11/site-packages/pygments/lexers/pascal.py:1335:                    next_token_is_function = False
./.venv-build/lib/python3.11/site-packages/pygments/lexers/pascal.py:1351:                        token = Name.Builtin.Pseudo
./.venv-build/lib/python3.11/site-packages/pygments/lexers/pascal.py:1353:                        token = Keyword
./.venv-build/lib/python3.11/site-packages/pygments/lexers/pascal.py:1360:                                next_token_is_function = True
./.venv-build/lib/python3.11/site-packages/pygments/lexers/pascal.py:1378:                                next_token_is_property = True
./.venv-build/lib/python3.11/site-packages/pygments/lexers/pascal.py:1387:                                next_token_is_function = True
./.venv-build/lib/python3.11/site-packages/pygments/lexers/pascal.py:1396:                        token = Keyword.Pseudo
./.venv-build/lib/python3.11/site-packages/pygments/lexers/pascal.py:1404:                        token = Keyword.Pseudo
./.venv-build/lib/python3.11/site-packages/pygments/lexers/pascal.py:1405:                        next_token_is_function = True
./.venv-build/lib/python3.11/site-packages/pygments/lexers/pascal.py:1406:                    # if the last iteration set next_token_is_function
./.venv-build/lib/python3.11/site-packages/pygments/lexers/pascal.py:1409:                    elif next_token_is_function:
./.venv-build/lib/python3.11/site-packages/pygments/lexers/pascal.py:1410:                        # Look if the next token is a dot. If yes it's
./.venv-build/lib/python3.11/site-packages/pygments/lexers/pascal.py:1414:                            token = Name.Class
./.venv-build/lib/python3.11/site-packages/pygments/lexers/pascal.py:1417:                            token = Name.Function
./.venv-build/lib/python3.11/site-packages/pygments/lexers/pascal.py:1418:                            next_token_is_function = False
./.venv-build/lib/python3.11/site-packages/pygments/lexers/pascal.py:1424:                    elif not self.is_portugol and next_token_is_property:
./.venv-build/lib/python3.11/site-packages/pygments/lexers/pascal.py:1425:                        token = Name.Property
./.venv-build/lib/python3.11/site-packages/pygments/lexers/pascal.py:1426:                        next_token_is_property = False
./.venv-build/lib/python3.11/site-packages/pygments/lexers/pascal.py:1427:                    # Highlight this token as label and add it
./.venv-build/lib/python3.11/site-packages/pygments/lexers/pascal.py:1430:                        token = Name.Label
./.venv-build/lib/python3.11/site-packages/pygments/lexers/pascal.py:1434:                        token = Name.Label
./.venv-build/lib/python3.11/site-packages/pygments/lexers/pascal.py:1436:                        token = Keyword.Type
./.venv-build/lib/python3.11/site-packages/pygments/lexers/pascal.py:1438:                        token = Keyword.Type
./.venv-build/lib/python3.11/site-packages/pygments/lexers/pascal.py:1440:                        token = Keyword.Pseudo
./.venv-build/lib/python3.11/site-packages/pygments/lexers/pascal.py:1441:                    # builtins are just builtins if the token
./.venv-build/lib/python3.11/site-packages/pygments/lexers/pascal.py:1444:                        token = Name.Builtin
./.venv-build/lib/python3.11/site-packages/pygments/lexers/pascal.py:1446:                        token = Name
./.venv-build/lib/python3.11/site-packages/pygments/lexers/pascal.py:1448:                    token = String
./.venv-build/lib/python3.11/site-packages/pygments/lexers/pascal.py:1451:                    token = String
./.venv-build/lib/python3.11/site-packages/pygments/lexers/pascal.py:1454:                    token = String.Char
./.venv-build/lib/python3.11/site-packages/pygments/lexers/pascal.py:1456:                    token = Number.Hex
./.venv-build/lib/python3.11/site-packages/pygments/lexers/pascal.py:1458:                    token = Number.Integer
./.venv-build/lib/python3.11/site-packages/pygments/lexers/pascal.py:1460:                    token = Number.Float
./.venv-build/lib/python3.11/site-packages/pygments/lexers/pascal.py:1470:                        token = String.Escape
./.venv-build/lib/python3.11/site-packages/pygments/lexers/pascal.py:1472:                        token = String
./.venv-build/lib/python3.11/site-packages/pygments/lexers/pascal.py:1475:                        token = String
./.venv-build/lib/python3.11/site-packages/pygments/lexers/pascal.py:1481:                        token = String.Escape
./.venv-build/lib/python3.11/site-packages/pygments/lexers/pascal.py:1483:                        token = String
./.venv-build/lib/python3.11/site-packages/pygments/lexers/pascal.py:1486:                        token = String
./.venv-build/lib/python3.11/site-packages/pygments/lexers/pascal.py:1492:                    token = Whitespace
./.venv-build/lib/python3.11/site-packages/pygments/lexers/pascal.py:1494:                    token = Keyword
./.venv-build/lib/python3.11/site-packages/pygments/lexers/pascal.py:1498:                        token = Comment.Preproc
./.venv-build/lib/python3.11/site-packages/pygments/lexers/pascal.py:1500:                        token = Comment.Multiline
./.venv-build/lib/python3.11/site-packages/pygments/lexers/pascal.py:1502:                    token = Comment.Single
./.venv-build/lib/python3.11/site-packages/pygments/lexers/pascal.py:1504:                    token = String
./.venv-build/lib/python3.11/site-packages/pygments/lexers/pascal.py:1507:                    token = Name.Label
./.venv-build/lib/python3.11/site-packages/pygments/lexers/pascal.py:1511:                        token = Keyword
./.venv-build/lib/python3.11/site-packages/pygments/lexers/pascal.py:1513:                        token = Name.Builtin
./.venv-build/lib/python3.11/site-packages/pygments/lexers/pascal.py:1515:                        token = Name
./.venv-build/lib/python3.11/site-packages/pygments/lexers/pascal.py:1517:                    token = Operator
./.venv-build/lib/python3.11/site-packages/pygments/lexers/pascal.py:1519:                    token = Punctuation
./.venv-build/lib/python3.11/site-packages/pygments/lexers/pascal.py:1521:                    token = Number.Hex
./.venv-build/lib/python3.11/site-packages/pygments/lexers/pascal.py:1523:                    token = Number.Integer
./.venv-build/lib/python3.11/site-packages/pygments/lexers/pascal.py:1525:                    token = Number.Float
./.venv-build/lib/python3.11/site-packages/pygments/lexers/pascal.py:1534:            yield scanner.start_pos, token, scanner.match or ""
./.venv-build/lib/python3.11/site-packages/pygments/lexers/graphics.py:12:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/graphics.py:46:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/graphics.py:343:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/graphics.py:832:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/graphics.py:1015:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/graphics.py:1118:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/graphics.py:1121:        for index, token, value in RegexLexer.get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/graphics.py:1122:            if token is Name and value in ASYFUNCNAME:
./.venv-build/lib/python3.11/site-packages/pygments/lexers/graphics.py:1123:                token = Name.Function
./.venv-build/lib/python3.11/site-packages/pygments/lexers/graphics.py:1124:            elif token is Name and value in ASYVARNAME:
./.venv-build/lib/python3.11/site-packages/pygments/lexers/graphics.py:1125:                token = Name.Variable
./.venv-build/lib/python3.11/site-packages/pygments/lexers/graphics.py:1126:            yield index, token, value
./.venv-build/lib/python3.11/site-packages/pygments/lexers/graphics.py:1150:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/graphics.py:1210:            # don't add the newline to the Comment token
./.venv-build/lib/python3.11/site-packages/pygments/lexers/graphics.py:1474:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/savi.py:12:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/savi.py:56:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/rita.py:12:from pygments.token import Comment, Operator, Keyword, Name, Literal, Punctuation, Whitespace
./.venv-build/lib/python3.11/site-packages/pygments/lexers/rita.py:29:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/bibtex.py:14:from pygments.token import Name, Comment, String, Error, Number, Keyword, Punctuation, Whitespace
./.venv-build/lib/python3.11/site-packages/pygments/lexers/bibtex.py:55:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/bibtex.py:127:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/mojo.py:24:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/mojo.py:99:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/ul4.py:14:from pygments.token import Comment, Text, Keyword, String, Number, Literal, Name, Other, Operator
./.venv-build/lib/python3.11/site-packages/pygments/lexers/ul4.py:41:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/nix.py:14:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/nix.py:77:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_lasso_builtins.py:29:        "curltoken",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_lasso_builtins.py:433:        "client_password",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_lasso_builtins.py:502:        "curle_bad_password_entered",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_lasso_builtins.py:520:        "curle_ftp_user_password_incorrect",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_lasso_builtins.py:760:        "email_token",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_lasso_builtins.py:1093:        "json_consume_token",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_lasso_builtins.py:1604:        "token_value",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_lasso_builtins.py:2079:        "client_password",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_lasso_builtins.py:2232:        "email_token",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_lasso_builtins.py:2298:        "error_code_invalidpassword",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_lasso_builtins.py:2339:        "error_invalidpassword",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_lasso_builtins.py:2380:        "error_msg_invalidpassword",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_lasso_builtins.py:2981:        "token_value",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_lasso_builtins.py:3149:        "addpasswordfield",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_lasso_builtins.py:3568:        "encodepassword",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_lasso_builtins.py:3949:        "hostpassword",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_lasso_builtins.py:4363:        "pop_token",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_lasso_builtins.py:4780:        "token",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_lasso_builtins.py:4891:        "addpasswordfield",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/resource.py:14:from pygments.token import Comment, String, Number, Operator, Text, Keyword, Name
./.venv-build/lib/python3.11/site-packages/pygments/lexers/resource.py:31:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/func.py:12:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/func.py:44:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_googlesql_builtins.py:914:    "TOKENLIST",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/yara.py:12:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/yara.py:39:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/graphql.py:16:from pygments.token import Comment, Keyword, Name, Number, Punctuation, String, Whitespace
./.venv-build/lib/python3.11/site-packages/pygments/lexers/graphql.py:67:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/graphql.py:68:        "ignored_tokens": [
./.venv-build/lib/python3.11/site-packages/pygments/lexers/graphql.py:74:            include("ignored_tokens"),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/graphql.py:89:            include("ignored_tokens"),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/graphql.py:94:            include("ignored_tokens"),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/graphql.py:105:            include("ignored_tokens"),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/graphql.py:112:            include("ignored_tokens"),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/graphql.py:118:            include("ignored_tokens"),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/graphql.py:126:            include("ignored_tokens"),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/graphql.py:132:            include("ignored_tokens"),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/graphql.py:147:            include("ignored_tokens"),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/graphql.py:151:            include("ignored_tokens"),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/graphql.py:158:            include("ignored_tokens"),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/graphql.py:166:            include("ignored_tokens"),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/graphql.py:171:            include("ignored_tokens"),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/chapel.py:12:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/chapel.py:131:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/chapel.py:172:            # tokens
./.venv-build/lib/python3.11/site-packages/pygments/lexers/trafficscript.py:12:from pygments.token import String, Number, Name, Keyword, Operator, Text, Comment
./.venv-build/lib/python3.11/site-packages/pygments/lexers/trafficscript.py:28:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/blueprint.py:14:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/blueprint.py:41:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_scilab_builtins.py:953:    "tokens",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_scilab_builtins.py:2891:    "tokenpos",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/bqn.py:12:from pygments.token import Comment, Operator, Keyword, Name, String, Number, Punctuation, Whitespace
./.venv-build/lib/python3.11/site-packages/pygments/lexers/bqn.py:33:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/bqn.py:56:            # This token type is used for diamond, commas
./.venv-build/lib/python3.11/site-packages/pygments/lexers/bqn.py:62:            # Since this token type is important in BQN, it is not included in
./.venv-build/lib/python3.11/site-packages/pygments/lexers/bqn.py:63:            # the punctuation token type but rather in the following one
./.venv-build/lib/python3.11/site-packages/pygments/lexers/openscad.py:12:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/openscad.py:38:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/pawn.py:12:from pygments.token import Text, Comment, Operator, Keyword, Name, String, Number, Punctuation
./.venv-build/lib/python3.11/site-packages/pygments/lexers/pawn.py:35:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/pawn.py:181:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/pawn.py:182:        for index, token, value in RegexLexer.get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/pawn.py:183:            if token is Name:
./.venv-build/lib/python3.11/site-packages/pygments/lexers/pawn.py:186:                        token = Keyword.Type
./.venv-build/lib/python3.11/site-packages/pygments/lexers/pawn.py:188:                        token = Name.Builtin
./.venv-build/lib/python3.11/site-packages/pygments/lexers/pawn.py:189:            yield index, token, value
./.venv-build/lib/python3.11/site-packages/pygments/lexers/pawn.py:209:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/basic.py:14:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/basic.py:63:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/basic.py:255:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/basic.py:432:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/basic.py:563:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/basic.py:871:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/basic.py:972:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/basic.py:1234:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/html.py:23:from pygments.token import Text, Comment, Operator, Keyword, Name, String, Punctuation, Whitespace
./.venv-build/lib/python3.11/site-packages/pygments/lexers/html.py:58:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/html.py:144:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/html.py:180:            (r"CDATA|IDREFS|IDREF|ID|NMTOKENS|NMTOKEN|ENTITIES|ENTITY|NOTATION", Keyword.Constant),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/html.py:227:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/html.py:307:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/html.py:308:        for index, token, value in XmlLexer.get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/html.py:311:            if token is Name.Tag and m and m.group(1) in self.EXTRA_KEYWORDS:
./.venv-build/lib/python3.11/site-packages/pygments/lexers/html.py:314:                yield index, token, value
./.venv-build/lib/python3.11/site-packages/pygments/lexers/html.py:342:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/html.py:444:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/html.py:545:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/html.py:643:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/html.py:663:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/perl.py:23:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/perl.py:53:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/perl.py:538:        "token",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/perl.py:1842:    def brackets_callback(token_class):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/perl.py:1894:            yield match.start(), token_class, text[match.start() : end_pos + n_chars]
./.venv-build/lib/python3.11/site-packages/pygments/lexers/perl.py:1906:        # below a token state, it means we need to increment
./.venv-build/lib/python3.11/site-packages/pygments/lexers/perl.py:1908:        # we should return to the token rules.
./.venv-build/lib/python3.11/site-packages/pygments/lexers/perl.py:1909:        if len(stack) > 2 and stack[-2] == "token":
./.venv-build/lib/python3.11/site-packages/pygments/lexers/perl.py:1910:            context.perl6_token_nesting_level += 1
./.venv-build/lib/python3.11/site-packages/pygments/lexers/perl.py:1919:        # below a token state, it means we need to check the nesting
./.venv-build/lib/python3.11/site-packages/pygments/lexers/perl.py:1920:        # level to see if we need to return to the token state.
./.venv-build/lib/python3.11/site-packages/pygments/lexers/perl.py:1921:        if len(stack) > 2 and stack[-2] == "token":
./.venv-build/lib/python3.11/site-packages/pygments/lexers/perl.py:1922:            context.perl6_token_nesting_level -= 1
./.venv-build/lib/python3.11/site-packages/pygments/lexers/perl.py:1923:            if context.perl6_token_nesting_level == 0:
./.venv-build/lib/python3.11/site-packages/pygments/lexers/perl.py:1927:        context.perl6_token_nesting_level = 1
./.venv-build/lib/python3.11/site-packages/pygments/lexers/perl.py:1936:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/perl.py:1949:                r"(regex|token|rule)(\s*" + PERL6_IDENTIFIER_RANGE + "+:sym)",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/perl.py:1951:                "token-sym-brackets",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/perl.py:1954:                r"(regex|token|rule)(?!"
./.venv-build/lib/python3.11/site-packages/pygments/lexers/perl.py:1960:                "pre-token",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/perl.py:2017:        "pre-token": [
./.venv-build/lib/python3.11/site-packages/pygments/lexers/perl.py:2019:            (r"\{", Text, ("#pop", "token")),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/perl.py:2022:        "token-sym-brackets": [
./.venv-build/lib/python3.11/site-packages/pygments/lexers/perl.py:2026:                ("#pop", "pre-token"),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/perl.py:2028:            default(("#pop", "pre-token")),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/perl.py:2030:        "token": [
./.venv-build/lib/python3.11/site-packages/pygments/lexers/theorem.py:14:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/theorem.py:354:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/theorem.py:410:            # Consume comments like ***** as one token
./.venv-build/lib/python3.11/site-packages/pygments/lexers/theorem.py:838:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/sql.py:53:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/sql.py:126:        yield from lx.get_tokens_unprocessed(match.group(4))
./.venv-build/lib/python3.11/site-packages/pygments/lexers/sql.py:140:    had, _tokens could be created on this ancestor and not updated for the
./.venv-build/lib/python3.11/site-packages/pygments/lexers/sql.py:145:    def get_tokens_unprocessed(self, text, *args):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/sql.py:148:        yield from super().get_tokens_unprocessed(text, *args)
./.venv-build/lib/python3.11/site-packages/pygments/lexers/sql.py:185:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/sql.py:246:    tokens = {name: state[:] for (name, state) in PostgresLexer.tokens.items()}
./.venv-build/lib/python3.11/site-packages/pygments/lexers/sql.py:249:    for i, pattern in enumerate(tokens["root"]):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/sql.py:251:            tokens["root"][i] = (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/sql.py:263:    tokens["root"][:0] = [
./.venv-build/lib/python3.11/site-packages/pygments/lexers/sql.py:283:    tokens = {name: state[:] for (name, state) in PostgresLexer.tokens.items()}
./.venv-build/lib/python3.11/site-packages/pygments/lexers/sql.py:285:    tokens["root"].append((r"\\[^\s]+", Keyword.Pseudo, "psql-command"))
./.venv-build/lib/python3.11/site-packages/pygments/lexers/sql.py:286:    tokens["psql-command"] = [
./.venv-build/lib/python3.11/site-packages/pygments/lexers/sql.py:343:    def get_tokens_unprocessed(self, data):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/sql.py:359:                    yield from lexer.get_tokens_unprocessed(line)
./.venv-build/lib/python3.11/site-packages/pygments/lexers/sql.py:377:            yield from do_insertions(insertions, sql.get_tokens_unprocessed(curcode))
./.venv-build/lib/python3.11/site-packages/pygments/lexers/sql.py:380:            out_token = Generic.Output
./.venv-build/lib/python3.11/site-packages/pygments/lexers/sql.py:391:                        out_token = Generic.Error
./.venv-build/lib/python3.11/site-packages/pygments/lexers/sql.py:393:                    yield (mmsg.start(2), out_token, mmsg.group(2))
./.venv-build/lib/python3.11/site-packages/pygments/lexers/sql.py:395:                    yield (0, out_token, line)
./.venv-build/lib/python3.11/site-packages/pygments/lexers/sql.py:412:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/sql.py:662:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/sql.py:706:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/sql.py:730:            # tokens starting with a digit have already been recognized
./.venv-build/lib/python3.11/site-packages/pygments/lexers/sql.py:791:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/sql.py:855:            # Exceptions; these words tokenize differently in different contexts.
./.venv-build/lib/python3.11/site-packages/pygments/lexers/sql.py:858:            # In all other known cases, "SET" is tokenized by MYSQL_DATATYPES.
./.venv-build/lib/python3.11/site-packages/pygments/lexers/sql.py:927:        # additional styles based on the token name. This gives users
./.venv-build/lib/python3.11/site-packages/pygments/lexers/sql.py:971:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/sql.py:1029:            # Exceptions; these words tokenize differently in different contexts.
./.venv-build/lib/python3.11/site-packages/pygments/lexers/sql.py:1095:        # additional styles based on the token name. This gives users
./.venv-build/lib/python3.11/site-packages/pygments/lexers/sql.py:1106:        tokens = collections.Counter(text.split())
./.venv-build/lib/python3.11/site-packages/pygments/lexers/sql.py:1107:        return 0.001 * sum(count for t, count in tokens.items() if t in googlesql_identifiers)
./.venv-build/lib/python3.11/site-packages/pygments/lexers/sql.py:1123:    def get_tokens_unprocessed(self, data):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/sql.py:1137:                    yield from do_insertions(insertions, sql.get_tokens_unprocessed(curcode))
./.venv-build/lib/python3.11/site-packages/pygments/lexers/sql.py:1145:            yield from do_insertions(insertions, sql.get_tokens_unprocessed(curcode))
./.venv-build/lib/python3.11/site-packages/pygments/lexers/sql.py:1161:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/zig.py:12:from pygments.token import Comment, Operator, Keyword, Name, String, Number, Punctuation, Whitespace
./.venv-build/lib/python3.11/site-packages/pygments/lexers/zig.py:139:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/procfile.py:12:from pygments.token import Name, Number, String, Text, Punctuation
./.venv-build/lib/python3.11/site-packages/pygments/lexers/procfile.py:31:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/felix.py:12:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/felix.py:242:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/kuin.py:12:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/kuin.py:38:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/nit.py:12:from pygments.token import Text, Comment, Operator, Keyword, Name, String, Number, Punctuation
./.venv-build/lib/python3.11/site-packages/pygments/lexers/nit.py:27:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/testing.py:12:from pygments.token import Comment, Keyword, Name, String, Number, Generic, Text
./.venv-build/lib/python3.11/site-packages/pygments/lexers/testing.py:34:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/testing.py:152:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/tlb.py:12:from pygments.token import Operator, Name, Number, Whitespace, Punctuation, Comment
./.venv-build/lib/python3.11/site-packages/pygments/lexers/tlb.py:28:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/parasail.py:14:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/parasail.py:43:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/mosel.py:13:from pygments.token import Text, Comment, Operator, Keyword, Name, String, Number, Punctuation
./.venv-build/lib/python3.11/site-packages/pygments/lexers/mosel.py:398:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/grammar_notation.py:12:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/grammar_notation.py:61:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/grammar_notation.py:106:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/grammar_notation.py:155:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/grammar_notation.py:251:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_postgres_builtins.py:302:    "PASSWORD",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/webidl.py:12:from pygments.token import Comment, Keyword, Name, Number, Punctuation, String, Text
./.venv-build/lib/python3.11/site-packages/pygments/lexers/webidl.py:65:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/installers.py:14:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/installers.py:50:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/installers.py:193:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/installers.py:264:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/installers.py:291:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/installers.py:337:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/cddl.py:16:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/cddl.py:111:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/cddl.py:138:            # Token type is String as barewords are always interpreted as such.
./.venv-build/lib/python3.11/site-packages/pygments/lexers/cddl.py:161:            # (r";.+$", Token.Other),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/smithy.py:12:from pygments.token import Text, Comment, Keyword, Name, String, Number, Whitespace, Punctuation
./.venv-build/lib/python3.11/site-packages/pygments/lexers/smithy.py:61:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/c_like.py:14:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/c_like.py:56:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/c_like.py:157:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/c_like.py:225:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/c_like.py:281:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/c_like.py:360:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/c_like.py:608:    def get_tokens_unprocessed(self, text, stack=("root",)):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/c_like.py:609:        for index, token, value in CLexer.get_tokens_unprocessed(self, text, stack):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/c_like.py:610:            if token is Name:
./.venv-build/lib/python3.11/site-packages/pygments/lexers/c_like.py:612:                    token = Keyword.Type
./.venv-build/lib/python3.11/site-packages/pygments/lexers/c_like.py:614:                    token = Keyword.Type
./.venv-build/lib/python3.11/site-packages/pygments/lexers/c_like.py:616:                    token = Name.Builtin
./.venv-build/lib/python3.11/site-packages/pygments/lexers/c_like.py:618:                    token = Keyword.Pseudo
./.venv-build/lib/python3.11/site-packages/pygments/lexers/c_like.py:620:                    token = Keyword.Reserved
./.venv-build/lib/python3.11/site-packages/pygments/lexers/c_like.py:622:                    token = Name.Function
./.venv-build/lib/python3.11/site-packages/pygments/lexers/c_like.py:623:            yield index, token, value
./.venv-build/lib/python3.11/site-packages/pygments/lexers/c_like.py:638:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/c_like.py:770:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/c_like.py:1280:    def get_tokens_unprocessed(self, text, stack=("root",)):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/c_like.py:1281:        for index, token, value in CppLexer.get_tokens_unprocessed(self, text, stack):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/c_like.py:1293:                yield index, token, value
./.venv-build/lib/python3.11/site-packages/pygments/lexers/c_like.py:1308:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/c_like.py:1383:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/c_like.py:1572:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/data.py:12:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/data.py:51:    def something(token_class):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/data.py:52:        """Do not produce empty tokens."""
./.venv-build/lib/python3.11/site-packages/pygments/lexers/data.py:58:            yield match.start(), token_class, text
./.venv-build/lib/python3.11/site-packages/pygments/lexers/data.py:63:    def reset_indent(token_class):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/data.py:72:            yield match.start(), token_class, text
./.venv-build/lib/python3.11/site-packages/pygments/lexers/data.py:77:    def save_indent(token_class, start=False):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/data.py:94:                yield match.start(), token_class, text
./.venv-build/lib/python3.11/site-packages/pygments/lexers/data.py:96:                yield match.start() + len(text), token_class.Error, extra
./.venv-build/lib/python3.11/site-packages/pygments/lexers/data.py:101:    def set_indent(token_class, implicit=False):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/data.py:111:            yield match.start(), token_class, text
./.venv-build/lib/python3.11/site-packages/pygments/lexers/data.py:116:    def set_block_scalar_indent(token_class):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/data.py:130:                yield match.start(), token_class, text
./.venv-build/lib/python3.11/site-packages/pygments/lexers/data.py:135:    def parse_block_scalar_empty_line(indent_token_class, content_token_class):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/data.py:142:                    yield match.start(), indent_token_class, text
./.venv-build/lib/python3.11/site-packages/pygments/lexers/data.py:146:                yield match.start(), indent_token_class, indentation
./.venv-build/lib/python3.11/site-packages/pygments/lexers/data.py:147:                yield (match.start() + context.block_scalar_indent, content_token_class, content)
./.venv-build/lib/python3.11/site-packages/pygments/lexers/data.py:152:    def parse_block_scalar_indent(token_class):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/data.py:169:                yield match.start(), token_class, text
./.venv-build/lib/python3.11/site-packages/pygments/lexers/data.py:174:    def parse_plain_scalar_indent(token_class):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/data.py:184:                yield match.start(), token_class, text
./.venv-build/lib/python3.11/site-packages/pygments/lexers/data.py:189:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/data.py:245:            # whitespaces separating tokens
./.venv-build/lib/python3.11/site-packages/pygments/lexers/data.py:435:    def get_tokens_unprocessed(self, text=None, context=None):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/data.py:438:        return super().get_tokens_unprocessed(text, context)
./.venv-build/lib/python3.11/site-packages/pygments/lexers/data.py:467:    # sets, the token will be considered valid. For example,
./.venv-build/lib/python3.11/site-packages/pygments/lexers/data.py:480:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/data.py:498:        # The queue is used to store data that may need to be tokenized
./.venv-build/lib/python3.11/site-packages/pygments/lexers/data.py:500:        # keys are tokenized differently than string values, but cannot
./.venv-build/lib/python3.11/site-packages/pygments/lexers/data.py:510:        #     (start_index, token_type, text)
./.venv-build/lib/python3.11/site-packages/pygments/lexers/data.py:512:        # By default the token type of text in double quotes is
./.venv-build/lib/python3.11/site-packages/pygments/lexers/data.py:513:        # String.Double. The token type will be replaced if a colon
./.venv-build/lib/python3.11/site-packages/pygments/lexers/data.py:624:                # Exhaust the queue. Accept the existing token types.
./.venv-build/lib/python3.11/site-packages/pygments/lexers/data.py:640:                # Exhaust the queue. Accept the existing token types.
./.venv-build/lib/python3.11/site-packages/pygments/lexers/data.py:647:                # Exhaust the queue. Accept the existing token types.
./.venv-build/lib/python3.11/site-packages/pygments/lexers/data.py:654:                # Yield from the queue. Replace string token types.
./.venv-build/lib/python3.11/site-packages/pygments/lexers/data.py:655:                for _start, _token, _text in queue:
./.venv-build/lib/python3.11/site-packages/pygments/lexers/data.py:656:                    # There can be only three types of tokens before a ':':
./.venv-build/lib/python3.11/site-packages/pygments/lexers/data.py:660:                    # Otherwise, we yield the original token.
./.venv-build/lib/python3.11/site-packages/pygments/lexers/data.py:664:                    if _token is String.Double:
./.venv-build/lib/python3.11/site-packages/pygments/lexers/data.py:667:                        yield _start, _token, _text
./.venv-build/lib/python3.11/site-packages/pygments/lexers/data.py:673:                # Exhaust the queue. Accept the existing token types.
./.venv-build/lib/python3.11/site-packages/pygments/lexers/data.py:684:                # Exhaust the queue. Accept the existing token types.
./.venv-build/lib/python3.11/site-packages/pygments/lexers/data.py:769:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/data.py:770:        for start, token, value in super().get_tokens_unprocessed(text):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/data.py:771:            if token is Name.Tag and value in self.json_ld_keywords:
./.venv-build/lib/python3.11/site-packages/pygments/lexers/data.py:774:                yield start, token, value
./.venv-build/lib/python3.11/site-packages/pygments/lexers/javascript.py:27:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/javascript.py:88:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/javascript.py:200:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/javascript.py:244:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/javascript.py:440:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/javascript.py:565:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/javascript.py:686:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/javascript.py:770:                r"Error_InvalidDatabase|Error_InvalidPassword|"
./.venv-build/lib/python3.11/site-packages/pygments/lexers/javascript.py:939:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/javascript.py:943:        for index, token, value in RegexLexer.get_tokens_unprocessed(self, text, stack):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/javascript.py:945:                token is Name.Other
./.venv-build/lib/python3.11/site-packages/pygments/lexers/javascript.py:947:                or token is Name.Other.Member
./.venv-build/lib/python3.11/site-packages/pygments/lexers/javascript.py:952:            yield index, token, value
./.venv-build/lib/python3.11/site-packages/pygments/lexers/javascript.py:982:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/javascript.py:1214:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/javascript.py:1333:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/javascript.py:1434:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/javascript.py:1742:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/javascript.py:1832:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/javascript.py:1856:                    yield from do_insertions(insertions, jslexer.get_tokens_unprocessed(curcode))
./.venv-build/lib/python3.11/site-packages/pygments/lexers/javascript.py:1861:                yield from do_insertions([], jslexer.get_tokens_unprocessed(line))
./.venv-build/lib/python3.11/site-packages/pygments/lexers/javascript.py:1864:            yield from do_insertions(insertions, jslexer.get_tokens_unprocessed(curcode))
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_scheme_builtins.py:1408:    "string-tokenize",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/jslt.py:12:from pygments.token import Comment, Keyword, Name, Number, Operator, Punctuation, String, Whitespace
./.venv-build/lib/python3.11/site-packages/pygments/lexers/jslt.py:33:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/business.py:14:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/business.py:62:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/business.py:675:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/business.py:696:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/business.py:920:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/business.py:983:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/business.py:1027:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/idl.py:14:from pygments.token import Text, Comment, Operator, Keyword, Name, Number, String, Whitespace
./.venv-build/lib/python3.11/site-packages/pygments/lexers/idl.py:937:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/ecl.py:14:from pygments.token import Comment, Operator, Keyword, Name, String, Number, Punctuation, Whitespace
./.venv-build/lib/python3.11/site-packages/pygments/lexers/ecl.py:33:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/ecl.py:66:                r"QSTRING|REAL|RECORD|RULE|SET OF|STRING|TOKEN|UDECIMAL|UNICODE|"
./.venv-build/lib/python3.11/site-packages/pygments/lexers/ecl.py:200:                        "TOKEN",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/promql.py:12:from pygments.token import Comment, Keyword, Name, Number, Operator, Punctuation, String, Whitespace
./.venv-build/lib/python3.11/site-packages/pygments/lexers/promql.py:127:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/scdoc.py:14:from pygments.token import Text, Comment, Keyword, String, Generic
./.venv-build/lib/python3.11/site-packages/pygments/lexers/scdoc.py:31:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/slash.py:12:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/slash.py:40:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/spice.py:12:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/spice.py:39:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/spice.py:108:            # tokens
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_stata_builtins.py:468:    "gettoken",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_stata_builtins.py:1489:    "token",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_stata_builtins.py:1490:    "tokeni",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_stata_builtins.py:1491:    "tokeniz",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/_stata_builtins.py:1492:    "tokenize",
./.venv-build/lib/python3.11/site-packages/pygments/lexers/matlab.py:14:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/matlab.py:46:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/matlab.py:2797:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/matlab.py:2821:                token = (0, Generic.Traceback, line)
./.venv-build/lib/python3.11/site-packages/pygments/lexers/matlab.py:2822:                insertions.append((idx, [token]))
./.venv-build/lib/python3.11/site-packages/pygments/lexers/matlab.py:2835:                    yield from do_insertions(insertions, mlexer.get_tokens_unprocessed(curcode))
./.venv-build/lib/python3.11/site-packages/pygments/lexers/matlab.py:2849:            yield from do_insertions(insertions, mlexer.get_tokens_unprocessed(curcode))
./.venv-build/lib/python3.11/site-packages/pygments/lexers/matlab.py:4074:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/matlab.py:4212:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/monte.py:11:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/monte.py:202:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/c_cpp.py:15:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/c_cpp.py:64:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/c_cpp.py:482:    def get_tokens_unprocessed(self, text, stack=("root",)):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/c_cpp.py:483:        for index, token, value in RegexLexer.get_tokens_unprocessed(self, text, stack):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/c_cpp.py:484:            if token is Name:
./.venv-build/lib/python3.11/site-packages/pygments/lexers/c_cpp.py:486:                    token = Keyword.Type
./.venv-build/lib/python3.11/site-packages/pygments/lexers/c_cpp.py:488:                    token = Keyword.Type
./.venv-build/lib/python3.11/site-packages/pygments/lexers/c_cpp.py:490:                    token = Keyword.Type
./.venv-build/lib/python3.11/site-packages/pygments/lexers/c_cpp.py:492:                    token = Keyword.Type
./.venv-build/lib/python3.11/site-packages/pygments/lexers/c_cpp.py:493:            yield index, token, value
./.venv-build/lib/python3.11/site-packages/pygments/lexers/c_cpp.py:528:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/c_cpp.py:606:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/solidity.py:12:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/solidity.py:45:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/actionscript.py:14:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/actionscript.py:42:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/actionscript.py:378:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/actionscript.py:499:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/amdgpu.py:12:from pygments.token import Name, Text, Keyword, Whitespace, Number, Comment
./.venv-build/lib/python3.11/site-packages/pygments/lexers/amdgpu.py:32:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/dax.py:12:from pygments.token import Comment, Punctuation, Whitespace, Name, Operator, String, Number, Text
./.venv-build/lib/python3.11/site-packages/pygments/lexers/dax.py:30:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/python.py:25:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/python.py:130:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/python.py:746:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/python.py:1200:    Code tokens are output as ``Token.Other.Code``, traceback tokens as
./.venv-build/lib/python3.11/site-packages/pygments/lexers/python.py:1201:    ``Token.Other.Traceback``.
./.venv-build/lib/python3.11/site-packages/pygments/lexers/python.py:1203:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/python.py:1270:        # different tokens.  TODO: DelegatingLexer should support this
./.venv-build/lib/python3.11/site-packages/pygments/lexers/python.py:1272:        # distinguishing tokens. Then we wouldn't need this intermediary
./.venv-build/lib/python3.11/site-packages/pygments/lexers/python.py:1297:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/python.py:1359:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/python.py:1402:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/python.py:1727:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/python.py:2298:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/python.py:2299:        for index, token, value in PythonLexer.get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/python.py:2300:            if token is Name and value in self.EXTRA_KEYWORDS:
./.venv-build/lib/python3.11/site-packages/pygments/lexers/python.py:2303:                yield index, token, value
./.venv-build/lib/python3.11/site-packages/pygments/lexers/devicetree.py:12:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/devicetree.py:42:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/whiley.py:12:from pygments.token import Comment, Keyword, Name, Number, Operator, Punctuation, String, Text
./.venv-build/lib/python3.11/site-packages/pygments/lexers/whiley.py:32:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/wowtoc.py:16:from pygments.token import Comment, Name, Text, Punctuation, String, Keyword
./.venv-build/lib/python3.11/site-packages/pygments/lexers/wowtoc.py:30:def _create_tag_line_token(inner_pattern, inner_token, ignore_case=False):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/wowtoc.py:32:    # have a different pattern and different token. otherwise, everything about a tag
./.venv-build/lib/python3.11/site-packages/pygments/lexers/wowtoc.py:39:            inner_token,
./.venv-build/lib/python3.11/site-packages/pygments/lexers/wowtoc.py:60:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/wowtoc.py:64:            _create_tag_line_token(
./.venv-build/lib/python3.11/site-packages/pygments/lexers/wowtoc.py:70:            _create_tag_line_token(
./.venv-build/lib/python3.11/site-packages/pygments/lexers/wowtoc.py:78:            _create_tag_line_token(
./.venv-build/lib/python3.11/site-packages/pygments/lexers/wowtoc.py:84:            _create_tag_line_token(
./.venv-build/lib/python3.11/site-packages/pygments/lexers/teraterm.py:14:from pygments.token import Text, Comment, Operator, Name, String, Number, Keyword, Error
./.venv-build/lib/python3.11/site-packages/pygments/lexers/teraterm.py:31:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/teraterm.py:86:                r"delpassword|"
./.venv-build/lib/python3.11/site-packages/pygments/lexers/teraterm.py:141:                r"getpassword|"
./.venv-build/lib/python3.11/site-packages/pygments/lexers/teraterm.py:154:                r"ispassword|"
./.venv-build/lib/python3.11/site-packages/pygments/lexers/teraterm.py:174:                r"passwordbox|"
./.venv-build/lib/python3.11/site-packages/pygments/lexers/teraterm.py:208:                r"setpassword|"
./.venv-build/lib/python3.11/site-packages/pygments/lexers/teraterm.py:326:        if re.search(TeraTermLexer.tokens["commands"][0][0], text):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/fortran.py:14:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/fortran.py:49:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/fortran.py:600:        for index, token, value in lexer.get_tokens_unprocessed(text):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/fortran.py:603:                yield index, token, value
./.venv-build/lib/python3.11/site-packages/pygments/lexers/fortran.py:605:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/jmespath.py:12:from pygments.token import String, Punctuation, Whitespace, Name, Operator, Number, Literal, Keyword
./.venv-build/lib/python3.11/site-packages/pygments/lexers/jmespath.py:28:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/email.py:13:from pygments.token import Text, Keyword, Name, String, Number, Comment
./.venv-build/lib/python3.11/site-packages/pygments/lexers/email.py:30:    def get_x_header_tokens(self, match):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/email.py:36:            default_actions = self.get_tokens_unprocessed(match.group(2), stack=("root", "header"))
./.venv-build/lib/python3.11/site-packages/pygments/lexers/email.py:43:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/email.py:46:            (r"^(X-(?:\w[\w\-]*:))([\s\S]*?\n)(?![ \t])", get_x_header_tokens),
./.venv-build/lib/python3.11/site-packages/pygments/lexers/ooc.py:12:from pygments.token import Text, Comment, Operator, Keyword, Name, String, Number, Punctuation
./.venv-build/lib/python3.11/site-packages/pygments/lexers/ooc.py:29:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/ezhil.py:14:from pygments.token import Keyword, Comment, Name, String, Number, Punctuation, Operator, Whitespace
./.venv-build/lib/python3.11/site-packages/pygments/lexers/ezhil.py:33:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/r.py:14:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/r.py:45:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/r.py:64:                        insertions, slexer.get_tokens_unprocessed(current_code_block)
./.venv-build/lib/python3.11/site-packages/pygments/lexers/r.py:76:            yield from do_insertions(insertions, slexer.get_tokens_unprocessed(current_code_block))
./.venv-build/lib/python3.11/site-packages/pygments/lexers/r.py:100:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/r.py:197:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/julia.py:12:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/julia.py:53:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/julia.py:282:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.11/site-packages/pygments/lexers/julia.py:306:                    yield from do_insertions(insertions, jllexer.get_tokens_unprocessed(curcode))
./.venv-build/lib/python3.11/site-packages/pygments/lexers/julia.py:318:            yield from do_insertions(insertions, jllexer.get_tokens_unprocessed(curcode))
./.venv-build/lib/python3.11/site-packages/pygments/lexers/gcodelexer.py:12:from pygments.token import Comment, Name, Text, Keyword, Number
./.venv-build/lib/python3.11/site-packages/pygments/lexers/gcodelexer.py:28:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/bdd.py:12:from pygments.token import Comment, Keyword, Name, String, Number, Text, Punctuation, Whitespace
./.venv-build/lib/python3.11/site-packages/pygments/lexers/bdd.py:34:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/dns.py:13:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/dns.py:53:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/configs.py:23:from pygments.token import (
./.venv-build/lib/python3.11/site-packages/pygments/lexers/configs.py:83:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/configs.py:140:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/configs.py:191:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/configs.py:236:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/configs.py:286:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/configs.py:375:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/configs.py:417:            # Skip blank lines after help token, if any
./.venv-build/lib/python3.11/site-packages/pygments/lexers/configs.py:454:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/configs.py:532:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/configs.py:849:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/configs.py:884:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/configs.py:930:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/configs.py:963:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/configs.py:1184:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/configs.py:1280:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/configs.py:1328:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/configs.py:1370:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/configs.py:1411:    but it yield error token. It is because pacman.conf has
./.venv-build/lib/python3.11/site-packages/pygments/lexers/configs.py:1430:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/configs.py:1472:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/configs.py:1538:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/configs.py:1674:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/configs.py:1764:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexers/configs.py:1807:    tokens = {
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:17:from pygments.token import Error, Text, Other, Whitespace, _TokenType
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:274:    def get_tokens(self, text, unfiltered=False):
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:278:        iterable of ``(tokentype, value)`` pairs from `text`.
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:282:        (`stripnl`, `stripall` and so on), and then yields all tokens
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:283:        from `get_tokens_unprocessed()`, with the ``index`` dropped.
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:291:            for _, t, v in self.get_tokens_unprocessed(text):
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:299:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:302:        ``(index, tokentype, value)`` tuples where ``index`` is the starting
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:303:        position of the token within the input text.
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:315:    lexer, afterwards all ``Other`` tokens are lexed using the root
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:327:    def get_tokens_unprocessed(self, text):
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:331:        for i, t, v in self.language_lexer.get_tokens_unprocessed(text):
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:341:        return do_insertions(insertions, self.root_lexer.get_tokens_unprocessed(buffered))
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:418:            elif type(action) is _TokenType:
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:479:            for i, t, v in lx.get_tokens_unprocessed(match.group(), **gt_kwargs):
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:492:            for i, t, v in lx.get_tokens_unprocessed(match.group(), **gt_kwargs):
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:503:    For example default('#pop') is equivalent to ('', Token, '#pop')
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:532:    Metaclass for RegexLexer, creates the self._tokens attribute from
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:533:    self.tokens on the first instantiation.
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:537:        """Preprocess the regular expression component of a token definition."""
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:542:    def _process_token(cls, token):
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:543:        """Preprocess the token component of a token definition."""
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:544:        assert type(token) is _TokenType or callable(
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:545:            token
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:546:        ), f"token type must be simple type or callable, not {token!r}"
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:547:        return token
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:550:        """Preprocess the state transition action of a token definition."""
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:567:            itokens = []
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:570:                itokens.extend(cls._process_state(unprocessed, processed, istate))
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:571:            processed[tmp_state] = itokens
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:589:        tokens = processed[state] = []
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:595:                tokens.extend(cls._process_state(unprocessed, processed, str(tdef)))
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:604:                tokens.append((re.compile("").match, None, new_state))
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:616:            token = cls._process_token(tdef[1])
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:623:            tokens.append((rex, token, new_state))
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:624:        return tokens
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:626:    def process_tokendef(cls, name, tokendefs=None):
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:627:        """Preprocess a dictionary of token definitions."""
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:628:        processed = cls._all_tokens[name] = {}
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:629:        tokendefs = tokendefs or cls.tokens[name]
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:630:        for state in list(tokendefs):
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:631:            cls._process_state(tokendefs, processed, state)
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:634:    def get_tokendefs(cls):
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:636:        Merge tokens from superclasses in MRO order, returning a single tokendef
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:646:        tokens = {}
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:649:            toks = c.__dict__.get("tokens", {})
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:652:                curitems = tokens.get(state)
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:658:                    tokens[state] = items
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:681:        return tokens
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:684:        """Instantiate cls after preprocessing its token definitions."""
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:685:        if "_tokens" not in cls.__dict__:
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:686:            cls._all_tokens = {}
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:688:            if hasattr(cls, "token_variants") and cls.token_variants:
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:692:                cls._tokens = cls.process_tokendef("", cls.get_tokendefs())
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:711:    #: Dict of ``{'state': [(regex, tokentype, new_state), ...], ...}``
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:730:    tokens = {}
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:732:    def get_tokens_unprocessed(self, text, stack=("root",)):
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:734:        Split ``text`` into (tokentype, text) pairs.
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:739:        tokendefs = self._tokens
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:741:        statetokens = tokendefs[statestack[-1]]
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:743:            for rexmatch, action, new_state in statetokens:
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:747:                        if type(action) is _TokenType:
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:775:                        statetokens = tokendefs[statestack[-1]]
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:778:                # We are here only if all state tokens have been considered
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:784:                        statetokens = tokendefs["root"]
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:814:    def get_tokens_unprocessed(self, text=None, context=None):
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:816:        Split ``text`` into (tokentype, text) pairs.
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:819:        tokendefs = self._tokens
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:822:            statetokens = tokendefs["root"]
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:825:            statetokens = tokendefs[ctx.stack[-1]]
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:828:            for rexmatch, action, new_state in statetokens:
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:832:                        if type(action) is _TokenType:
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:839:                                statetokens = tokendefs[ctx.stack[-1]]
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:862:                        statetokens = tokendefs[ctx.stack[-1]]
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:871:                        statetokens = tokendefs["root"]
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:881:def do_insertions(insertions, tokens):
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:886:    ``insertions`` is a list of ``(index, itokens)`` pairs.
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:887:    Each ``itokens`` iterable should be inserted at position
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:888:    ``index`` into the token stream given by the ``tokens``
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:891:    The result is a combined token stream.
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:897:        index, itokens = next(insertions)
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:900:        yield from tokens
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:906:    # iterate over the token stream where we want to insert
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:907:    # the tokens from the insertion list.
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:908:    for i, t, v in tokens:
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:918:            for it_index, it_token, it_value in itokens:
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:919:                yield realpos, it_token, it_value
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:923:                index, itokens = next(insertions)
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:931:    # leftover tokens
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:933:        # no normal tokens, set realpos to zero
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:935:        for p, t, v in itokens:
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:939:            index, itokens = next(insertions)
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:973:    def get_tokens_unprocessed(self, text, stack=("root",)):
./.venv-build/lib/python3.11/site-packages/pygments/lexer.py:976:        yield from RegexLexer.get_tokens_unprocessed(self, text, stack)
./.venv-build/lib/python3.11/site-packages/pygments/cmdline.py:569:        help="Add a filter to the token stream.  (Query names with -L.) "
./.venv-build/lib/python3.11/site-packages/pygments/__init__.py:39:    and return an iterable of tokens. Currently, this only calls
./.venv-build/lib/python3.11/site-packages/pygments/__init__.py:40:    `lexer.get_tokens()`.
./.venv-build/lib/python3.11/site-packages/pygments/__init__.py:43:        return lexer.get_tokens(code)
./.venv-build/lib/python3.11/site-packages/pygments/__init__.py:53:def format(tokens, formatter, outfile=None):  # pylint: disable=redefined-builtin
./.venv-build/lib/python3.11/site-packages/pygments/__init__.py:55:    Format ``tokens`` (an iterable of tokens) with the formatter ``formatter``
./.venv-build/lib/python3.11/site-packages/pygments/__init__.py:65:            formatter.format(tokens, realoutfile)
./.venv-build/lib/python3.11/site-packages/pygments/__init__.py:68:            formatter.format(tokens, outfile)
./.venv-build/lib/python3.11/site-packages/pygments/filters/__init__.py:14:from pygments.token import String, Comment, Keyword, Name, Error, Whitespace, string_to_tokentype
./.venv-build/lib/python3.11/site-packages/pygments/filters/__init__.py:715:    """Highlight a normal Name (and Name.*) token with a different token type.
./.venv-build/lib/python3.11/site-packages/pygments/filters/__init__.py:721:            tokentype=Name.Function,
./.venv-build/lib/python3.11/site-packages/pygments/filters/__init__.py:725:    as functions. `Name.Function` is the default token type.
./.venv-build/lib/python3.11/site-packages/pygments/filters/__init__.py:730:      A list of names that should be given the different token type.
./.venv-build/lib/python3.11/site-packages/pygments/filters/__init__.py:732:    `tokentype` : TokenType or string
./.venv-build/lib/python3.11/site-packages/pygments/filters/__init__.py:733:      A token type or a string containing a token type name that is
./.venv-build/lib/python3.11/site-packages/pygments/filters/__init__.py:741:        tokentype = options.get("tokentype")
./.venv-build/lib/python3.11/site-packages/pygments/filters/__init__.py:742:        if tokentype:
./.venv-build/lib/python3.11/site-packages/pygments/filters/__init__.py:743:            self.tokentype = string_to_tokentype(tokentype)
./.venv-build/lib/python3.11/site-packages/pygments/filters/__init__.py:745:            self.tokentype = Name.Function
./.venv-build/lib/python3.11/site-packages/pygments/filters/__init__.py:750:                yield self.tokentype, value
./.venv-build/lib/python3.11/site-packages/pygments/filters/__init__.py:755:class ErrorToken(Exception):
./.venv-build/lib/python3.11/site-packages/pygments/filters/__init__.py:759:class RaiseOnErrorTokenFilter(Filter):
./.venv-build/lib/python3.11/site-packages/pygments/filters/__init__.py:760:    """Raise an exception when the lexer generates an error token.
./.venv-build/lib/python3.11/site-packages/pygments/filters/__init__.py:766:      The default is `pygments.filters.ErrorToken`.
./.venv-build/lib/python3.11/site-packages/pygments/filters/__init__.py:773:        self.exception = options.get("excclass", ErrorToken)
./.venv-build/lib/python3.11/site-packages/pygments/filters/__init__.py:810:    `wstokentype` : bool
./.venv-build/lib/python3.11/site-packages/pygments/filters/__init__.py:811:      If true, give whitespace the special `Whitespace` token type.  This allows
./.venv-build/lib/python3.11/site-packages/pygments/filters/__init__.py:831:        self.wstt = get_bool_opt(options, "wstokentype", True)
./.venv-build/lib/python3.11/site-packages/pygments/filters/__init__.py:893:            # Remove ``left`` tokens from first line, ``n`` from all others.
./.venv-build/lib/python3.11/site-packages/pygments/filters/__init__.py:904:class TokenMergeFilter(Filter):
./.venv-build/lib/python3.11/site-packages/pygments/filters/__init__.py:905:    """Merges consecutive tokens with the same token type in the output
./.venv-build/lib/python3.11/site-packages/pygments/filters/__init__.py:933:    "raiseonerror": RaiseOnErrorTokenFilter,
./.venv-build/lib/python3.11/site-packages/pygments/filters/__init__.py:936:    "tokenmerge": TokenMergeFilter,
./.venv-build/lib/python3.11/site-packages/s3transfer/utils.py:626:        :returns: A token (can be None) to use when releasing the semaphore
./.venv-build/lib/python3.11/site-packages/s3transfer/utils.py:632:    def release(self, tag, acquire_token):
./.venv-build/lib/python3.11/site-packages/s3transfer/utils.py:636:        :param acquire_token:  The token returned from when the semaphore was
./.venv-build/lib/python3.11/site-packages/s3transfer/utils.py:641:        logger.debug(f"Releasing acquire {tag}/{acquire_token}")
./.venv-build/lib/python3.11/site-packages/s3transfer/utils.py:704:    def release(self, tag, acquire_token):
./.venv-build/lib/python3.11/site-packages/s3transfer/utils.py:705:        sequence_number = acquire_token
./.venv-build/lib/python3.11/site-packages/s3transfer/bandwidth.py:37:class RequestToken:
./.venv-build/lib/python3.11/site-packages/s3transfer/bandwidth.py:38:    """A token to pass as an identifier when consuming from the LeakyBucket"""
./.venv-build/lib/python3.11/site-packages/s3transfer/bandwidth.py:129:        self._request_token = RequestToken()
./.venv-build/lib/python3.11/site-packages/s3transfer/bandwidth.py:169:                self._leaky_bucket.consume(self._bytes_seen, self._request_token)
./.venv-build/lib/python3.11/site-packages/s3transfer/bandwidth.py:244:    def consume(self, amt, request_token):
./.venv-build/lib/python3.11/site-packages/s3transfer/bandwidth.py:250:        :type request_token: RequestToken
./.venv-build/lib/python3.11/site-packages/s3transfer/bandwidth.py:251:        :param request_token: The token associated to the consumption
./.venv-build/lib/python3.11/site-packages/s3transfer/bandwidth.py:253:            RequestExceededException is raised the token should be used
./.venv-build/lib/python3.11/site-packages/s3transfer/bandwidth.py:264:            if self._consumption_scheduler.is_scheduled(request_token):
./.venv-build/lib/python3.11/site-packages/s3transfer/bandwidth.py:266:                    amt, request_token, time_now
./.venv-build/lib/python3.11/site-packages/s3transfer/bandwidth.py:269:                self._raise_request_exceeded_exception(amt, request_token, time_now)
./.venv-build/lib/python3.11/site-packages/s3transfer/bandwidth.py:277:    def _release_requested_amt_for_scheduled_request(self, amt, request_token, time_now):
./.venv-build/lib/python3.11/site-packages/s3transfer/bandwidth.py:278:        self._consumption_scheduler.process_scheduled_consumption(request_token)
./.venv-build/lib/python3.11/site-packages/s3transfer/bandwidth.py:281:    def _raise_request_exceeded_exception(self, amt, request_token, time_now):
./.venv-build/lib/python3.11/site-packages/s3transfer/bandwidth.py:284:            amt, request_token, allocated_time
./.venv-build/lib/python3.11/site-packages/s3transfer/bandwidth.py:296:        self._tokens_to_scheduled_consumption = {}
./.venv-build/lib/python3.11/site-packages/s3transfer/bandwidth.py:299:    def is_scheduled(self, token):
./.venv-build/lib/python3.11/site-packages/s3transfer/bandwidth.py:302:        :type token: RequestToken
./.venv-build/lib/python3.11/site-packages/s3transfer/bandwidth.py:303:        :param token: The token associated to the consumption
./.venv-build/lib/python3.11/site-packages/s3transfer/bandwidth.py:306:        return token in self._tokens_to_scheduled_consumption
./.venv-build/lib/python3.11/site-packages/s3transfer/bandwidth.py:308:    def schedule_consumption(self, amt, token, time_to_consume):
./.venv-build/lib/python3.11/site-packages/s3transfer/bandwidth.py:314:        :type token: RequestToken
./.venv-build/lib/python3.11/site-packages/s3transfer/bandwidth.py:315:        :param token: The token associated to the consumption
./.venv-build/lib/python3.11/site-packages/s3transfer/bandwidth.py:328:        self._tokens_to_scheduled_consumption[token] = {
./.venv-build/lib/python3.11/site-packages/s3transfer/bandwidth.py:334:    def process_scheduled_consumption(self, token):
./.venv-build/lib/python3.11/site-packages/s3transfer/bandwidth.py:337:        :type token: RequestToken
./.venv-build/lib/python3.11/site-packages/s3transfer/bandwidth.py:338:        :param token: The token associated to the consumption
./.venv-build/lib/python3.11/site-packages/s3transfer/bandwidth.py:341:        scheduled_retry = self._tokens_to_scheduled_consumption.pop(token)
./.venv-build/lib/python3.11/site-packages/s3transfer/crt.py:622:        return AwsCredentials(credentials.access_key, credentials.secret_key, credentials.token)
./.venv-build/lib/python3.11/site-packages/s3transfer/futures.py:477:        acquire_token = semaphore.acquire(task.transfer_id, block)
./.venv-build/lib/python3.11/site-packages/s3transfer/futures.py:480:        release_callback = FunctionContainer(semaphore.release, task.transfer_id, acquire_token)
./.venv-build/lib/python3.11/site-packages/jmespath/exceptions.py:12:    def __init__(self, lex_position, token_value, token_type, msg=_ERROR_MESSAGE):
./.venv-build/lib/python3.11/site-packages/jmespath/exceptions.py:13:        super(ParseError, self).__init__(lex_position, token_value, token_type)
./.venv-build/lib/python3.11/site-packages/jmespath/exceptions.py:15:        self.token_value = token_value
./.venv-build/lib/python3.11/site-packages/jmespath/exceptions.py:16:        self.token_type = token_type.upper()
./.venv-build/lib/python3.11/site-packages/jmespath/exceptions.py:24:        return "%s: Parse error at column %s, " 'token "%s" (%s), for expression:\n"%s"\n%s' % (
./.venv-build/lib/python3.11/site-packages/jmespath/exceptions.py:27:            self.token_value,
./.venv-build/lib/python3.11/site-packages/jmespath/exceptions.py:28:            self.token_type,
./.venv-build/lib/python3.11/site-packages/jmespath/exceptions.py:39:        self.token_type = None
./.venv-build/lib/python3.11/site-packages/jmespath/exceptions.py:40:        self.token_value = None
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:15:* All the nud/led tokens are on the Parser class itself, and are dispatched
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:18:* We use two passes through the data.  One to create a list of token,
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:19:  then one pass through the tokens to create the AST.  While the lexer actually
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:20:  yields tokens, we convert it to a list so we can easily implement two tokens
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:23:  does not have a large amount of token so this is not an issue.  And
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:24:  interestingly enough, creating a token list first is actually faster than
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:25:  consuming from the token iterator one token at a time.
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:71:    # The maximum binding power for a token that can stop
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:80:        self.tokenizer = None
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:81:        self._tokens = [None] * lookahead
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:109:        self.tokenizer = lexer.Lexer().tokenize(expression)
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:110:        self._tokens = list(self.tokenizer)
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:113:        if not self._current_token() == "eof":
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:114:            t = self._lookahead_token(0)
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:116:                t["start"], t["value"], t["type"], "Unexpected token: %s" % t["value"]
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:121:        left_token = self._lookahead_token(0)
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:123:        nud_function = getattr(self, "_token_nud_%s" % left_token["type"], self._error_nud_token)
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:124:        left = nud_function(left_token)
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:125:        current_token = self._current_token()
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:126:        while binding_power < self.BINDING_POWER[current_token]:
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:127:            led = getattr(self, "_token_led_%s" % current_token, None)
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:129:                error_token = self._lookahead_token(0)
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:130:                self._error_led_token(error_token)
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:134:                current_token = self._current_token()
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:137:    def _token_nud_literal(self, token):
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:138:        return ast.literal(token["value"])
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:140:    def _token_nud_unquoted_identifier(self, token):
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:141:        return ast.field(token["value"])
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:143:    def _token_nud_quoted_identifier(self, token):
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:144:        field = ast.field(token["value"])
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:147:        if self._current_token() == "lparen":
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:148:            t = self._lookahead_token(0)
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:154:    def _token_nud_star(self, token):
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:156:        if self._current_token() == "rbracket":
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:162:    def _token_nud_filter(self, token):
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:163:        return self._token_led_filter(ast.identity())
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:165:    def _token_nud_lbrace(self, token):
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:168:    def _token_nud_lparen(self, token):
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:173:    def _token_nud_flatten(self, token):
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:178:    def _token_nud_not(self, token):
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:182:    def _token_nud_lbracket(self, token):
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:183:        if self._current_token() in ["number", "colon"]:
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:190:        elif self._current_token() == "star" and self._lookahead(1) == "rbracket":
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:202:        #  | current token
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:207:            node = ast.index(self._lookahead_token(0)["value"])
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:218:        current_token = self._current_token()
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:219:        while not current_token == "rbracket" and index < 3:
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:220:            if current_token == "colon":
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:223:                    self._raise_parse_error_for_token(self._lookahead_token(0), "syntax error")
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:225:            elif current_token == "number":
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:226:                parts[index] = self._lookahead_token(0)["value"]
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:229:                self._raise_parse_error_for_token(self._lookahead_token(0), "syntax error")
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:230:            current_token = self._current_token()
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:234:    def _token_nud_current(self, token):
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:237:    def _token_nud_expref(self, token):
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:241:    def _token_led_dot(self, left):
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:242:        if not self._current_token() == "star":
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:255:    def _token_led_pipe(self, left):
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:259:    def _token_led_or(self, left):
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:263:    def _token_led_and(self, left):
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:267:    def _token_led_lparen(self, left):
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:270:            # -1 - '(' token
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:272:            prev_t = self._lookahead_token(-2)
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:281:        while not self._current_token() == "rparen":
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:283:            if self._current_token() == "comma":
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:290:    def _token_led_filter(self, left):
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:294:        if self._current_token() == "flatten":
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:300:    def _token_led_eq(self, left):
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:303:    def _token_led_ne(self, left):
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:306:    def _token_led_gt(self, left):
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:309:    def _token_led_gte(self, left):
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:312:    def _token_led_lt(self, left):
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:315:    def _token_led_lte(self, left):
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:318:    def _token_led_flatten(self, left):
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:323:    def _token_led_lbracket(self, left):
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:324:        token = self._lookahead_token(0)
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:325:        if token["type"] in ["number", "colon"]:
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:360:            if self._current_token() == "rbracket":
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:370:            key_token = self._lookahead_token(0)
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:371:            # Before getting the token value, verify it's
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:373:            self._match_multiple_tokens(token_types=["quoted_identifier", "unquoted_identifier"])
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:374:            key_name = key_token["value"]
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:379:            if self._current_token() == "comma":
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:381:            elif self._current_token() == "rbrace":
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:388:        if self.BINDING_POWER[self._current_token()] < self._PROJECTION_STOP:
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:389:            # BP of 10 are all the tokens that stop a projection.
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:391:        elif self._current_token() == "lbracket":
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:393:        elif self._current_token() == "filter":
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:395:        elif self._current_token() == "dot":
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:399:            self._raise_parse_error_for_token(self._lookahead_token(0), "syntax error")
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:409:        # In terms of tokens that means that after a '.',
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:411:        lookahead = self._current_token()
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:422:            t = self._lookahead_token(0)
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:425:            self._raise_parse_error_for_token(t, msg)
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:427:    def _error_nud_token(self, token):
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:428:        if token["type"] == "eof":
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:430:                token["start"], token["value"], token["type"]
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:432:        self._raise_parse_error_for_token(token, "invalid token")
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:434:    def _error_led_token(self, token):
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:435:        self._raise_parse_error_for_token(token, "invalid token")
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:437:    def _match(self, token_type=None):
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:438:        # inline'd self._current_token()
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:439:        if self._current_token() == token_type:
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:443:            self._raise_parse_error_maybe_eof(token_type, self._lookahead_token(0))
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:445:    def _match_multiple_tokens(self, token_types):
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:446:        if self._current_token() not in token_types:
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:447:            self._raise_parse_error_maybe_eof(token_types, self._lookahead_token(0))
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:453:    def _current_token(self):
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:454:        return self._tokens[self._index]["type"]
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:457:        return self._tokens[self._index + number]["type"]
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:459:    def _lookahead_token(self, number):
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:460:        return self._tokens[self._index + number]
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:462:    def _raise_parse_error_for_token(self, token, reason):
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:463:        lex_position = token["start"]
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:464:        actual_value = token["value"]
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:465:        actual_type = token["type"]
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:468:    def _raise_parse_error_maybe_eof(self, expected_type, token):
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:469:        lex_position = token["start"]
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:470:        actual_value = token["value"]
./.venv-build/lib/python3.11/site-packages/jmespath/parser.py:471:        actual_type = token["type"]
./.venv-build/lib/python3.11/site-packages/jmespath/lexer.py:13:    SIMPLE_TOKENS = {
./.venv-build/lib/python3.11/site-packages/jmespath/lexer.py:26:    def tokenize(self, expression):
./.venv-build/lib/python3.11/site-packages/jmespath/lexer.py:29:            if self._current in self.SIMPLE_TOKENS:
./.venv-build/lib/python3.11/site-packages/jmespath/lexer.py:31:                    "type": self.SIMPLE_TOKENS[self._current],
./.venv-build/lib/python3.11/site-packages/jmespath/lexer.py:91:                        lexer_position=start, lexer_value=buff, message="Unknown token '%s'" % buff
./.venv-build/lib/python3.11/site-packages/jmespath/lexer.py:119:                        lexer_position=position, lexer_value="=", message="Unknown token '='"
./.venv-build/lib/python3.11/site-packages/jmespath/lexer.py:125:                    message="Unknown token %s" % self._current,
./.venv-build/lib/python3.11/site-packages/jmespath/lexer.py:192:                    message="Bad token %s" % lexeme,
./.venv-build/lib/python3.11/site-packages/jmespath/lexer.py:194:        token_len = self._position - start
./.venv-build/lib/python3.11/site-packages/jmespath/lexer.py:195:        return {"type": "literal", "value": parsed_json, "start": start, "end": token_len}
./.venv-build/lib/python3.11/site-packages/jmespath/lexer.py:201:            token_len = self._position - start
./.venv-build/lib/python3.11/site-packages/jmespath/lexer.py:206:                "end": token_len,
./.venv-build/lib/python3.11/site-packages/jmespath/lexer.py:215:        token_len = self._position - start
./.venv-build/lib/python3.11/site-packages/jmespath/lexer.py:216:        return {"type": "literal", "value": lexeme, "start": start, "end": token_len}
./.venv-build/lib/python3.11/site-packages/click/core.py:222:    :param token_normalize_func: an optional function that is used to
./.venv-build/lib/python3.11/site-packages/click/core.py:223:                                 normalize tokens (options, choices,
./.venv-build/lib/python3.11/site-packages/click/core.py:238:        Click 9.0. ``args`` will contain remaining unparsed tokens.
./.venv-build/lib/python3.11/site-packages/click/core.py:261:        ``token_normalize_func`` parameters.
./.venv-build/lib/python3.11/site-packages/click/core.py:284:        token_normalize_func: t.Callable[[str], str] | None = None,
./.venv-build/lib/python3.11/site-packages/click/core.py:390:        if token_normalize_func is None and parent is not None:
./.venv-build/lib/python3.11/site-packages/click/core.py:391:            token_normalize_func = parent.token_normalize_func
./.venv-build/lib/python3.11/site-packages/click/core.py:393:        #: An optional normalization function for tokens.  This is
./.venv-build/lib/python3.11/site-packages/click/core.py:395:        self.token_normalize_func: t.Callable[[str], str] | None = token_normalize_func
./.venv-build/lib/python3.11/site-packages/click/core.py:443:            " 'args' will contain remaining unparsed tokens.",
./.venv-build/lib/python3.11/site-packages/click/core.py:1875:        if cmd is None and ctx.token_normalize_func is not None:
./.venv-build/lib/python3.11/site-packages/click/core.py:1876:            cmd_name = ctx.token_normalize_func(cmd_name)
./.venv-build/lib/python3.11/site-packages/click/core.py:2575:        will be hidden from the user. This is useful for password input.
./.venv-build/lib/python3.11/site-packages/click/parser.py:121:    if ctx is None or ctx.token_normalize_func is None:
./.venv-build/lib/python3.11/site-packages/click/parser.py:124:    return f"{prefix}{ctx.token_normalize_func(opt)}"
./.venv-build/lib/python3.11/site-packages/click/decorators.py:404:def password_option(*param_decls: str, **kwargs: t.Any) -> t.Callable[[FC], FC]:
./.venv-build/lib/python3.11/site-packages/click/decorators.py:405:    """Add a ``--password`` option which prompts for a password, hiding
./.venv-build/lib/python3.11/site-packages/click/decorators.py:409:        value ``"--password"``.
./.venv-build/lib/python3.11/site-packages/click/decorators.py:413:        param_decls = ("--password",)
./.venv-build/lib/python3.11/site-packages/click/types.py:281:        By default uses :meth:`Context.token_normalize_func` and if not case
./.venv-build/lib/python3.11/site-packages/click/types.py:288:        if ctx is not None and ctx.token_normalize_func is not None:
./.venv-build/lib/python3.11/site-packages/click/types.py:289:            normed_value = ctx.token_normalize_func(normed_value)
./.venv-build/lib/python3.11/site-packages/click/shell_completion.py:464:    incomplete escape sequence and uses the partial token as-is.
./.venv-build/lib/python3.11/site-packages/click/shell_completion.py:487:        for token in lex:
./.venv-build/lib/python3.11/site-packages/click/shell_completion.py:488:            out.append(token)
./.venv-build/lib/python3.11/site-packages/click/shell_completion.py:491:        # the partial token as-is. The quote or escape character is in
./.venv-build/lib/python3.11/site-packages/click/shell_completion.py:492:        # lex.state, not lex.token.
./.venv-build/lib/python3.11/site-packages/click/shell_completion.py:493:        out.append(lex.token)
./.venv-build/lib/python3.11/site-packages/click/__init__.py:26:from .decorators import password_option as password_option
./.venv-build/lib/python3.11/site-packages/pytokens/__main__.py:1:"""Support executing the CLI by doing `python -m pytokens`."""
./.venv-build/lib/python3.11/site-packages/pytokens/__main__.py:5:from pytokens.cli import cli
./.venv-build/lib/python3.11/site-packages/pytokens/cli.py:1:"""CLI interface for pytokens."""
./.venv-build/lib/python3.11/site-packages/pytokens/cli.py:8:import tokenize
./.venv-build/lib/python3.11/site-packages/pytokens/cli.py:12:import pytokens
./.venv-build/lib/python3.11/site-packages/pytokens/cli.py:43:                encoding, read_bytes = tokenize.detect_encoding(file.readline)
./.venv-build/lib/python3.11/site-packages/pytokens/cli.py:46:                    # Broken `# coding` comment, tokenizer bails, skip file
./.venv-build/lib/python3.11/site-packages/pytokens/cli.py:65:            for token in pytokens.tokenize(
./.venv-build/lib/python3.11/site-packages/pytokens/cli.py:69:                token_source = source_str[token.start_index : token.end_index]
./.venv-build/lib/python3.11/site-packages/pytokens/cli.py:70:                print(repr(token_source), token)
./.venv-build/lib/python3.11/site-packages/pytokens/cli.py:75:class TokenTuple(NamedTuple):
./.venv-build/lib/python3.11/site-packages/pytokens/cli.py:98:    # For that last newline token that exists on an imaginary line sometimes
./.venv-build/lib/python3.11/site-packages/pytokens/cli.py:102:    builtin_tokens = tokenize.tokenize(source_file.readline)
./.venv-build/lib/python3.11/site-packages/pytokens/cli.py:103:    # drop the encoding token
./.venv-build/lib/python3.11/site-packages/pytokens/cli.py:104:    next(builtin_tokens)
./.venv-build/lib/python3.11/site-packages/pytokens/cli.py:107:        expected_tokens_unprocessed = [
./.venv-build/lib/python3.11/site-packages/pytokens/cli.py:108:            TokenTuple(tokenize.tok_name[token.type], token.start, token.end)
./.venv-build/lib/python3.11/site-packages/pytokens/cli.py:109:            for token in builtin_tokens
./.venv-build/lib/python3.11/site-packages/pytokens/cli.py:111:    except tokenize.TokenError:
./.venv-build/lib/python3.11/site-packages/pytokens/cli.py:115:    expected_tokens = [expected_tokens_unprocessed[0]]
./.venv-build/lib/python3.11/site-packages/pytokens/cli.py:116:    for index, token in enumerate(expected_tokens_unprocessed[1:], start=1):
./.venv-build/lib/python3.11/site-packages/pytokens/cli.py:117:        last_token = expected_tokens[-1]
./.venv-build/lib/python3.11/site-packages/pytokens/cli.py:119:        current_token = token
./.venv-build/lib/python3.11/site-packages/pytokens/cli.py:120:        # Merge consecutive FSTRING_MIDDLE tokens. it's weird cpython has it like that.
./.venv-build/lib/python3.11/site-packages/pytokens/cli.py:121:        if current_token.type == last_token.type == "FSTRING_MIDDLE":
./.venv-build/lib/python3.11/site-packages/pytokens/cli.py:122:            expected_tokens.pop()
./.venv-build/lib/python3.11/site-packages/pytokens/cli.py:123:            current_token = TokenTuple(
./.venv-build/lib/python3.11/site-packages/pytokens/cli.py:124:                current_token.type,
./.venv-build/lib/python3.11/site-packages/pytokens/cli.py:125:                last_token.start,
./.venv-build/lib/python3.11/site-packages/pytokens/cli.py:126:                current_token.end,
./.venv-build/lib/python3.11/site-packages/pytokens/cli.py:129:        if index + 1 < len(expected_tokens_unprocessed):
./.venv-build/lib/python3.11/site-packages/pytokens/cli.py:131:            # the last { char as well as its end index, so we get a `x{` token
./.venv-build/lib/python3.11/site-packages/pytokens/cli.py:132:            # instead of the expected `x{{` token. This fixes that case. Pretty
./.venv-build/lib/python3.11/site-packages/pytokens/cli.py:136:            next_token = expected_tokens_unprocessed[index + 1]
./.venv-build/lib/python3.11/site-packages/pytokens/cli.py:138:                (current_token.type == "FSTRING_MIDDLE" and next_token.type == "OP")
./.venv-build/lib/python3.11/site-packages/pytokens/cli.py:139:                or (current_token.type == "FSTRING_MIDDLE" and next_token.type == "FSTRING_END")
./.venv-build/lib/python3.11/site-packages/pytokens/cli.py:140:                and next_token.start[0] == current_token.end[0]
./.venv-build/lib/python3.11/site-packages/pytokens/cli.py:141:                and next_token.start[1] > current_token.end[1]
./.venv-build/lib/python3.11/site-packages/pytokens/cli.py:143:                expected_tokens.append(
./.venv-build/lib/python3.11/site-packages/pytokens/cli.py:144:                    TokenTuple(
./.venv-build/lib/python3.11/site-packages/pytokens/cli.py:145:                        current_token.type,
./.venv-build/lib/python3.11/site-packages/pytokens/cli.py:146:                        current_token.start,
./.venv-build/lib/python3.11/site-packages/pytokens/cli.py:147:                        next_token.start,
./.venv-build/lib/python3.11/site-packages/pytokens/cli.py:152:        expected_tokens.append(current_token)
./.venv-build/lib/python3.11/site-packages/pytokens/cli.py:155:    our_tokens = (
./.venv-build/lib/python3.11/site-packages/pytokens/cli.py:156:        TokenTuple(
./.venv-build/lib/python3.11/site-packages/pytokens/cli.py:157:            token.type.to_python_token(),
./.venv-build/lib/python3.11/site-packages/pytokens/cli.py:158:            (token.start_line, token.start_col),
./.venv-build/lib/python3.11/site-packages/pytokens/cli.py:159:            (token.end_line, token.end_col),
./.venv-build/lib/python3.11/site-packages/pytokens/cli.py:161:        for token in pytokens.tokenize(source_string, issue_128233_handling=issue_128233_handling)
./.venv-build/lib/python3.11/site-packages/pytokens/cli.py:162:        if token.type != pytokens.TokenType.whitespace
./.venv-build/lib/python3.11/site-packages/pytokens/cli.py:165:    for builtin_token, our_token in zip(expected_tokens, our_tokens, strict=True):
./.venv-build/lib/python3.11/site-packages/pytokens/cli.py:166:        mismatch = builtin_token != our_token
./.venv-build/lib/python3.11/site-packages/pytokens/cli.py:168:            print("EXPECTED", builtin_token)
./.venv-build/lib/python3.11/site-packages/pytokens/cli.py:169:            print("---- GOT", our_token)
./.venv-build/lib/python3.11/site-packages/pytokens/cli.py:174:            # raise AssertionError("Tokens do not match")
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:1:"""pytokens - A Fast, spec compliant Python 3.12+ tokenizer that runs on older Pythons."""
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:11:class TokenizeError(Exception): ...
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:14:class IndentationError(TokenizeError): ...
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:23:class UnterminatedString(TokenizeError): ...
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:26:class UnexpectedEOF(TokenizeError): ...
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:29:class UnexpectedCharacterAfterBackslash(TokenizeError): ...
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:38:class TokenType(enum.IntEnum):
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:46:    _op_start = 7  # marker used to check if a token is an operator
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:56:    _op_end = 17  # marker used to check if a token is an operator
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:71:    errortoken = 28
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:74:        return f"TokenType.{self.name}"
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:76:    def to_python_token(self) -> str:
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:86:        return TokenType._op_start < self < TokenType._op_end
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:90:class Token:
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:91:    type: TokenType
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:104:            (self.type == TokenType.newline or self.type == TokenType.nl)
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:112:            self.type == TokenType.dedent
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:119:        if self.type == TokenType.endmarker:
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:176:class TokenIterator:
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:190:    prev_token: Token | None = None
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:202:    # present, the next token becomes an OP. regardless of what it is.
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:253:    def make_token(self, tok_type: TokenType) -> Token:
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:255:            if tok_type == TokenType.fstring_start:
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:256:                tok_type = TokenType.tstring_start
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:257:            elif tok_type == TokenType.fstring_middle:
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:258:                tok_type = TokenType.tstring_middle
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:259:            elif tok_type == TokenType.fstring_end:
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:260:                tok_type = TokenType.tstring_end
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:262:        token_type = (
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:263:            TokenType.op
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:266:            and tok_type not in (TokenType.number, TokenType.string)
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:272:            # as the next character, i.e. when the token is '\r ',
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:276:            # ' ' token in it anyway so it doesn't classify it as
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:279:            token_str = self.source[self.prev_index : self.current_index]
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:280:            if token_str == "\r ":
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:284:        token = Token(
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:285:            type=token_type,
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:293:        if tok_type == TokenType.newline or tok_type == TokenType.nl:
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:295:        elif tok_type == TokenType.whitespace or tok_type == TokenType.comment:
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:300:        self.prev_token = token
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:306:        return token
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:327:    def newline(self) -> Token:
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:331:        token_type = (
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:332:            TokenType.nl
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:339:            else TokenType.newline
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:341:        token = self.make_token(token_type)
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:343:        return token
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:345:    def endmarker(self) -> Token:
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:351:            return self.make_token(TokenType.dedent)
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:353:        return self.make_token(TokenType.endmarker)
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:355:    def decimal(self) -> Token:
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:361:        # TODO: this is too lax; 1__2 tokenizes successfully
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:398:        # TODO: this is too lax; 1__2 tokenizes successfully
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:424:            return self.make_token(TokenType.op)
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:426:        return self.make_token(TokenType.number)
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:428:    def binary(self) -> Token:
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:451:        return self.make_token(TokenType.number)
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:453:    def octal(self) -> Token:
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:476:        return self.make_token(TokenType.number)
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:478:    def hexadecimal(self) -> Token:
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:499:        return self.make_token(TokenType.number)
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:502:        # Quotes should always be within 3 chars of the beginning of the string token
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:527:    def fstring(self) -> Token:
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:540:            return self.make_token(TokenType.fstring_start)
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:581:                        # If fstring-middle is empty, skip it by returning the next step token
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:585:                        return self.make_token(TokenType.fstring_middle)
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:590:                    # If fstring-middle is empty, skip it by returning the next step token
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:594:                    return self.make_token(TokenType.fstring_middle)
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:605:            return self.make_token(TokenType.lbrace)
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:611:            token = self.make_token(TokenType.fstring_end)
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:614:            return token
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:628:                    # If fstring-middle is empty, skip it by returning the next step token
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:632:                    return self.make_token(TokenType.fstring_middle)
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:635:                    return self.make_token(TokenType.fstring_middle)
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:643:    def string(self) -> Token:
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:647:            return self.make_token(tok_type=TokenType.op)
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:676:                return self.make_token(TokenType.string)
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:682:    def indent(self) -> Token:
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:701:            return self.make_token(TokenType.whitespace)
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:707:            return self.make_token(TokenType.whitespace)
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:713:            return self.make_token(TokenType.whitespace)
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:724:            return self.make_token(TokenType.whitespace)
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:729:            return self.make_token(TokenType.indent)
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:743:            return self.make_token(TokenType.whitespace)
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:764:    def name(self) -> Token:
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:767:            return self.make_token(TokenType.identifier)
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:769:        # According to PEP 3131, any non-ascii character is valid in a NAME token.
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:780:        return self.make_token(TokenType.identifier)
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:782:    def __iter__(self) -> TokenIterator:
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:785:    def __next__(self) -> Token:
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:786:        if self.prev_token is not None and self.prev_token.type == TokenType.endmarker:
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:791:            if self.prev_token is None:
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:794:            if self.prev_token.type in {
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:795:                TokenType.newline,
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:796:                TokenType.nl,
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:797:                TokenType.dedent,
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:818:        # then we produce identical tokens to CPython.
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:827:                if self.prev_token is not None and self.prev_token.type == TokenType.comment:
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:834:                return self.make_token(TokenType.comment)
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:840:            return self.make_token(TokenType.comment)
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:845:            return self.make_token(TokenType.dedent)
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:871:                    # Move to next line without creating a newline token. But,
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:888:            return self.make_token(TokenType.whitespace)
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:898:                indent_token = self.indent()
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:900:                indent_token = None
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:902:            if indent_token is not None:
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:903:                return indent_token
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:908:            return self.make_token(TokenType.whitespace)
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:914:            return self.make_token(TokenType.op)
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:921:                return self.make_token(TokenType.op)
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:927:            return self.make_token(TokenType.op)
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:935:            return self.make_token(TokenType.op)
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:943:            return self.make_token(TokenType.op)
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:951:            return self.make_token(TokenType.op)
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:958:                return self.make_token(TokenType.op)
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:963:            return self.make_token(TokenType.op)
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:967:            return self.make_token(TokenType.op)
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:973:            return self.make_token(TokenType.op)
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:978:            return self.make_token(TokenType.lparen)
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:985:            return self.make_token(TokenType.rparen)
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:990:            return self.make_token(TokenType.lbracket)
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:997:            return self.make_token(TokenType.rbracket)
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:1002:            return self.make_token(TokenType.lbrace)
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:1014:            return self.make_token(TokenType.rbrace)
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:1020:                return self.make_token(TokenType.op)
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:1024:                return self.make_token(TokenType.op)
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:1084:def tokenize(
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:1087:    fstring_tokens: bool = True,
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:1089:) -> Iterator[Token]:
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:1090:    token_iterator = TokenIterator(source, issue_128233_handling=issue_128233_handling)
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:1091:    if fstring_tokens:
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:1092:        return iter(token_iterator)
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:1094:    return merge_fstring_tokens(token_iterator)
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:1097:def merge_fstring_tokens(token_iterator: TokenIterator) -> Iterator[Token]:
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:1098:    """Turn post-Python-3.12 FSTRING-* tokens back to a single STRING token."""
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:1099:    for token in token_iterator:
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:1100:        if token.type not in (TokenType.fstring_start, TokenType.tstring_start):
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:1101:            yield token
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:1104:        start_token = token
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:1105:        end_token = token
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:1109:        for token in token_iterator:
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:1110:            if token.type in (TokenType.fstring_start, TokenType.tstring_start):
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:1112:            if token.type in (TokenType.fstring_end, TokenType.tstring_end):
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:1116:                end_token = token
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:1119:        yield Token(
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:1120:            type=TokenType.string,
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:1121:            start_index=start_token.start_index,
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:1122:            start_line=start_token.start_line,
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:1123:            start_col=start_token.start_col,
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:1124:            end_index=end_token.end_index,
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:1125:            end_line=end_token.end_line,
./.venv-build/lib/python3.11/site-packages/pytokens/__init__.py:1126:            end_col=end_token.end_col,
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:76:        self.tokenstack = []
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:79:    def get_token(self):
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:81:        This function breaks the time string into lexical units (tokens), which
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:89:        any dot-separated strings before breaking it into tokens; as such, this
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:90:        function maintains a "token stack", for when the ambiguous context
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:91:        demands that multiple tokens be parsed at once.
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:93:        if self.tokenstack:
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:94:            return self.tokenstack.pop(0)
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:97:        token = None
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:101:            # We only realize that we've reached the end of a token when we
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:102:            # find a character that's not part of the current token - since
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:103:            # that character may be part of the next token, it's stored in the
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:116:                # First character of the token - determines if we're starting
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:118:                token = nextchar
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:124:                    token = " "
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:125:                    break  # emit token
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:127:                    break  # emit token
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:133:                    token += nextchar
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:135:                    token += nextchar
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:139:                    break  # emit token
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:144:                    token += nextchar
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:145:                elif nextchar == "." or (nextchar == "," and len(token) >= 2):
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:146:                    token += nextchar
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:150:                    break  # emit token
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:153:                # parsing, and the tokens will be broken up later.
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:156:                    token += nextchar
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:157:                elif self.isnum(nextchar) and token[-1] == ".":
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:158:                    token += nextchar
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:162:                    break  # emit token
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:165:                # break up the tokens later.
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:167:                    token += nextchar
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:168:                elif self.isword(nextchar) and token[-1] == ".":
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:169:                    token += nextchar
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:173:                    break  # emit token
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:175:        if state in ("a.", "0.") and (seenletters or token.count(".") > 1 or token[-1] in ".,"):
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:176:            l = self._split_decimal.split(token)
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:177:            token = l[0]
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:180:                    self.tokenstack.append(tok)
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:182:        if state == "0." and token.count(".") == 0:
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:183:            token = token.replace(",", ".")
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:185:        return token
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:191:        token = self.get_token()
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:192:        if token is None:
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:195:        return token
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:634:            ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:636:            a tuple containing the fuzzy tokens.
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:654:        res, skipped_tokens = self._parse(timestr, **kwargs)
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:670:        if kwargs.get("fuzzy_with_tokens", False):
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:671:            return ret, skipped_tokens
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:688:            "any_unused_tokens",
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:691:    def _parse(self, timestr, dayfirst=None, yearfirst=None, fuzzy=False, fuzzy_with_tokens=False):
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:718:        :param fuzzy_with_tokens:
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:727:                >>> parse("Today is January 1, 2047 at 8:21:00AM", fuzzy_with_tokens=True)
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:731:        if fuzzy_with_tokens:
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:743:        l = _timelex.split(timestr)  # Splits the timestr into tokens
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:763:                    # Numeric token
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:764:                    i = self._parse_numeric_token(l, i, info, ymd, res, fuzzy)
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:896:        if fuzzy_with_tokens:
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:897:            skipped_tokens = self._recombine_skipped(l, skipped_idxs)
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:898:            return res, tuple(skipped_tokens)
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:902:    def _parse_numeric_token(self, tokens, idx, info, ymd, res, fuzzy):
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:903:        # Token is a number
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:904:        value_repr = tokens[idx]
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:908:            six.raise_from(ValueError("Unknown numeric token"), e)
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:912:        len_l = len(tokens)
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:918:            and (idx + 1 >= len_l or (tokens[idx + 1] != ":" and info.hms(tokens[idx + 1]) is None))
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:921:            s = tokens[idx]
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:927:        elif len_li == 6 or (len_li > 6 and tokens[idx].find(".") == 6):
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:929:            s = tokens[idx]
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:931:            if not ymd and "." not in tokens[idx]:
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:945:            s = tokens[idx]
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:957:        elif self._find_hms_idx(idx, tokens, info, allow_jump=True) is not None:
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:959:            hms_idx = self._find_hms_idx(idx, tokens, info, allow_jump=True)
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:960:            (idx, hms) = self._parse_hms(idx, tokens, info, hms_idx)
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:966:        elif idx + 2 < len_l and tokens[idx + 1] == ":":
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:969:            value = self._to_decimal(tokens[idx + 2])  # TODO: try/except for this?
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:972:            if idx + 4 < len_l and tokens[idx + 3] == ":":
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:973:                res.second, res.microsecond = self._parsems(tokens[idx + 4])
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:979:        elif idx + 1 < len_l and tokens[idx + 1] in ("-", "/", "."):
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:980:            sep = tokens[idx + 1]
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:983:            if idx + 2 < len_l and not info.jump(tokens[idx + 2]):
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:984:                if tokens[idx + 2].isdigit():
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:986:                    ymd.append(tokens[idx + 2])
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:989:                    value = info.month(tokens[idx + 2])
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:996:                if idx + 3 < len_l and tokens[idx + 3] == sep:
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:998:                    value = info.month(tokens[idx + 4])
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:1003:                        ymd.append(tokens[idx + 4])
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:1009:        elif idx + 1 >= len_l or info.jump(tokens[idx + 1]):
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:1010:            if idx + 2 < len_l and info.ampm(tokens[idx + 2]) is not None:
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:1013:                res.hour = self._adjust_ampm(hour, info.ampm(tokens[idx + 2]))
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:1020:        elif info.ampm(tokens[idx + 1]) is not None and (0 <= value < 24):
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:1023:            res.hour = self._adjust_ampm(hour, info.ampm(tokens[idx + 1]))
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:1034:    def _find_hms_idx(self, idx, tokens, info, allow_jump):
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:1035:        len_l = len(tokens)
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:1037:        if idx + 1 < len_l and info.hms(tokens[idx + 1]) is not None:
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:1038:            # There is an "h", "m", or "s" label following this token.  We take
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:1039:            # assign the upcoming label to the current token.
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:1046:            and tokens[idx + 1] == " "
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:1047:            and info.hms(tokens[idx + 2]) is not None
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:1053:        elif idx > 0 and info.hms(tokens[idx - 1]) is not None:
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:1054:            # There is a "h", "m", or "s" preceding this token.  Since neither
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:1056:            # token, so we use the previous label.
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:1062:            and tokens[idx - 1] == " "
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:1063:            and info.hms(tokens[idx - 2]) is not None
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:1065:            # If we are looking at the final token, we allow for a
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:1091:    def _could_be_tzname(self, hour, tzname, tzoffset, token):
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:1096:            and len(token) <= 5
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:1097:            and (all(x in string.ascii_uppercase for x in token) or token in self.info.UTCZONE)
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:1147:    def _parse_hms(self, idx, tokens, info, hms_idx):
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:1156:            hms = info.hms(tokens[hms_idx])
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:1160:            hms = info.hms(tokens[hms_idx]) + 1
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:1166:    # Handling for individual tokens.  These are kept as methods instead
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:1285:    def _recombine_skipped(self, tokens, skipped_idxs):
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:1287:        >>> tokens = ["foo", " ", "bar", " ", "19June2000", "baz"]
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:1289:        >>> _recombine_skipped(tokens, skipped_idxs)
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:1292:        skipped_tokens = []
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:1295:                skipped_tokens[-1] = skipped_tokens[-1] + tokens[idx]
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:1297:                skipped_tokens.append(tokens[idx])
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:1299:        return skipped_tokens
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:1373:    :param fuzzy_with_tokens:
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:1382:            >>> parse("Today is January 1, 2047 at 8:21:00AM", fuzzy_with_tokens=True)
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:1387:        ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:1389:        a tuple containing the fuzzy tokens.
./.venv-build/lib/python3.11/site-packages/dateutil/parser/_parser.py:1615:        res.any_unused_tokens = not {l[n] for n in unused_idxs}.issubset({",", ":"})
./.venv-build/lib/python3.11/site-packages/dateutil/tz/tz.py:1096:        if res is None or res.any_unused_tokens:
./FIRSTTRY_ENTERPRISE_AUDIT_REPORT_FULL.md:114:- Defaults appear to prefer opt-in remote S3 usage. Licensing reads environment keys; ensure secrets are not hardcoded in repo and CI supplies them via secure variables.
./PHASE2_ENTERPRISE_TIER.md:238:- Detect secrets in codebase
./PHASE2_ENTERPRISE_TIER.md:240:- Fail on secret detection
./CI_CD_HARDENING_COMPLETE.md:51:- âœ… OIDC Auth: `id-token: write` permission for token request
./CI_CD_HARDENING_COMPLETE.md:52:- âœ… Role Assumption: `role-to-assume: ${{ secrets.AWS_ROLE_ARN }}`
./CI_CD_HARDENING_COMPLETE.md:54:- âœ… No Static Keys: Removed AWS_ACCESS_KEY_ID/SECRET_ACCESS_KEY
./CI_CD_HARDENING_COMPLETE.md:149:- Secrets: No static keys (OIDC auth) âœ…
./CI_CD_HARDENING_COMPLETE.md:163:4. âœ… OIDC Configuration: Token permission + role assumption
./CI_CD_HARDENING_COMPLETE.md:196:- Per-job: `id-token:write` only where needed (OIDC)
./CI_CD_HARDENING_COMPLETE.md:197:- Impact: Limits token abuse to minimum scope
./CI_CD_HARDENING_COMPLETE.md:210:- Before: `AWS_ACCESS_KEY_ID` + `AWS_SECRET_ACCESS_KEY` (static secrets)
./CI_CD_HARDENING_COMPLETE.md:211:- After: `role-to-assume: ${{ secrets.AWS_ROLE_ARN }}` (temporary token)
./CI_CD_HARDENING_COMPLETE.md:297:- [x] No hardcoded secrets detected
./CI_CD_HARDENING_COMPLETE.md:313:- [ ] **Step 3:** GitHub Secrets Setup
./CI_CD_HARDENING_COMPLETE.md:314:  - Add organization secret: `AWS_ROLE_ARN`
./CI_CD_HARDENING_COMPLETE.md:380:  Provider: "token.actions.githubusercontent.com"
./CI_CD_HARDENING_COMPLETE.md:388:### GitHub Organization Secret
./CI_CD_HARDENING_COMPLETE.md:449:**Compliance:** âœ… No hardcoded secrets, âœ… OIDC-only auth, âœ… Audit trail enabled
./performance_benchmark.py:103:                "echo 'secrets-scan: manual simulation'",
./tests/test_pro_features_extra.py:20:    # legacy list input: also exercises dangerous token blocking path
./tests/conftest.py:34:        "AWS_SECRET_ACCESS_KEY",
./tests/conftest.py:59:# environment defaults so modules that require secrets at import do not raise
./tests/conftest.py:63:# NOTE: we intentionally do NOT set FIRSTTRY_SHARED_SECRET here; tests that need a
./tests/conftest.py:64:# secret should set it explicitly with monkeypatch so subprocess-based tests can
./tests/conftest.py:65:# accurately simulate missing-secret production behavior. Setting a global
./tests/conftest.py:66:# FIRSTTRY_SHARED_SECRET at session start leaks into subprocess envs and breaks
./tests/conftest.py:67:# assertions that validate behavior when the secret is absent.
./tests/conftest.py:96:def ensure_shared_secret_for_tests(monkeypatch):
./tests/conftest.py:97:    """Ensure a deterministic dev-only FIRSTTRY_SHARED_SECRET is present during tests.
./tests/conftest.py:106:    if not os.environ.get("FIRSTTRY_SHARED_SECRET"):
./tests/conftest.py:108:        monkeypatch.setenv("FIRSTTRY_SHARED_SECRET", "dev-test-secret-0000000000000000")
./tests/conftest.py:145:            # Normalize the first token to basename (handles /usr/bin/python etc.)
./tests/conftest.py:198:            if exe in _blacklist or any(token in _blacklist for token in map(str, cmd_list)):
./tests/test_pro_feature_gate_allow.py:2:from firsttry.license import DEFAULT_SHARED_SECRET, require_license
./tests/test_pro_feature_gate_allow.py:10:        secret=DEFAULT_SHARED_SECRET,
./tests/phase4/test_ci_workflow_validation.py:142:        assert "gitleaks" in content, "Gitleaks secret scanner not found"
./tests/critical/test_scanner_critical_edges.py:51:    f = tmp_path / "pkg" / "secret.py"
./tests/test_cli_routing.py:25:    # Accept any of these tokens to avoid coupling to exact formatting
./tests/ci_parity/test_util_tokenize.py:1:from firsttry.ci_parity.util import normalize_cmd, tokenize_shell_line
./tests/ci_parity/test_util_tokenize.py:4:def test_tokenize_quotes_and_no_backslash():
./tests/ci_parity/test_util_tokenize.py:6:    toks = tokenize_shell_line(line)
./tests/security/test_secret_env_required.py:1:"""Test that FIRSTTRY_SHARED_SECRET is required in production mode."""
./tests/security/test_secret_env_required.py:9:def test_secret_env_required_in_production():
./tests/security/test_secret_env_required.py:10:    """Verify that missing FIRSTTRY_SHARED_SECRET raises in production."""
./tests/security/test_secret_env_required.py:16:# Simulate production: CI=true, no FIRSTTRY_SHARED_SECRET
./tests/security/test_secret_env_required.py:18:os.environ.pop('FIRSTTRY_SHARED_SECRET', None)
./tests/security/test_secret_env_required.py:25:    if 'FIRSTTRY_SHARED_SECRET' in str(e) and 'required in production' in str(e):
./tests/security/test_secret_env_required.py:41:def test_secret_dev_fallback_warns(monkeypatch):
./tests/security/test_secret_env_required.py:43:    # Clear secret env var
./tests/security/test_secret_env_required.py:44:    monkeypatch.delenv("FIRSTTRY_SHARED_SECRET", raising=False)
./tests/security/test_secret_env_required.py:45:    monkeypatch.delenv("FT_SHARED_SECRET", raising=False)
./tests/security/test_secret_env_required.py:55:    with pytest.warns(UserWarning, match="FIRSTTRY_SHARED_SECRET not set.*insecure dev fallback"):
./tests/security/test_secret_env_required.py:58:        assert firsttry.license.DEFAULT_SHARED_SECRET == "dev-only-insecure-fallback"
./tests/security/test_secret_env_required.py:61:def test_secret_from_env(monkeypatch):
./tests/security/test_secret_env_required.py:62:    """Verify that FIRSTTRY_SHARED_SECRET from env is used."""
./tests/security/test_secret_env_required.py:63:    # Set a custom secret
./tests/security/test_secret_env_required.py:64:    test_secret = "test-secret-for-unit-tests-min-32-chars-long"
./tests/security/test_secret_env_required.py:65:    monkeypatch.setenv("FIRSTTRY_SHARED_SECRET", test_secret)
./tests/security/test_secret_env_required.py:74:    assert firsttry.license.DEFAULT_SHARED_SECRET == test_secret
./tests/phase3/test_audit_schema.py:184:            "secrets_found": 0,
./tests/phase3/test_audit_schema.py:194:        assert report["security"]["secrets_found"] == 0
./tests/test_license_hmac.py:3:from firsttry.license import DEFAULT_SHARED_SECRET, build_license_payload, verify_sig
./tests/test_license_hmac.py:11:        secret=DEFAULT_SHARED_SECRET,
./tests/test_license_hmac.py:20:    assert verify_sig(payload, secret=DEFAULT_SHARED_SECRET) is True
./tests/test_license_hmac.py:25:    assert verify_sig(hacked, secret=DEFAULT_SHARED_SECRET) is False
./tests/cli/test_dev_cli_surface_contract.py:29:    # Disallowed tokens that must not appear in Dev help by default
./tests/cli/test_dev_cli_surface_contract.py:35:            raise AssertionError(f"forbidden token '{tok}' appeared in top-level help")
./tests/enterprise/test_release_sbom.py:135:    def __init__(self, private_key: str | None = None):
./tests/enterprise/test_release_sbom.py:136:        self.private_key = private_key or "test-private-key"
./tests/enterprise/test_release_sbom.py:144:        signature = hashlib.sha256(f"{content}{self.private_key}".encode()).hexdigest()
./tests/enterprise/test_commit_validation.py:325:This commit adds JWT token validation to all protected endpoints.
./tests/enterprise/test_commit_validation.py:333:    assert "JWT token validation" in message
./tests/enterprise/test_secrets_scanning.py:1:"""Secrets scanning integration using gitleaks.
./tests/enterprise/test_secrets_scanning.py:4:1. Secret detection and prevention
./tests/enterprise/test_secrets_scanning.py:6:3. CI/CD blocking on secrets found
./tests/enterprise/test_secrets_scanning.py:7:4. Secret pattern customization
./tests/enterprise/test_secrets_scanning.py:35:                "pattern": "-----BEGIN RSA PRIVATE KEY-----",
./tests/enterprise/test_secrets_scanning.py:38:                "id": "github-token",
./tests/enterprise/test_secrets_scanning.py:39:                "description": "GitHub Token",
./tests/enterprise/test_secrets_scanning.py:78:def test_secret_detection_aws_key(tmp_path: Path):
./tests/enterprise/test_secrets_scanning.py:86:SECRET = "wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY"
./tests/enterprise/test_secrets_scanning.py:101:def test_secret_detection_github_token(tmp_path: Path):
./tests/enterprise/test_secrets_scanning.py:102:    """Test detection of GitHub tokens."""
./tests/enterprise/test_secrets_scanning.py:103:    test_file = tmp_path / "token.py"
./tests/enterprise/test_secrets_scanning.py:106:GITHUB_TOKEN = "ghp_abcdefghijklmnopqrstuvwxyz12345678"
./tests/enterprise/test_secrets_scanning.py:110:    # Pattern should match GitHub token format
./tests/enterprise/test_secrets_scanning.py:120:def test_secret_detection_private_key(tmp_path: Path):
./tests/enterprise/test_secrets_scanning.py:124:        """-----BEGIN RSA PRIVATE KEY-----
./tests/enterprise/test_secrets_scanning.py:131:    assert "-----BEGIN RSA PRIVATE KEY-----" in content
./tests/enterprise/test_secrets_scanning.py:154:    """Test that secrets found will block CI/CD pipeline."""
./tests/enterprise/test_secrets_scanning.py:157:    # If secrets are found, CI should:
./tests/enterprise/test_secrets_scanning.py:163:        "fail_on_secrets_found": True,
./tests/enterprise/test_secrets_scanning.py:168:    assert enforcement_rules["fail_on_secrets_found"]
./tests/enterprise/test_secrets_scanning.py:172:def test_secret_pattern_customization(gitleaks_config: Dict[str, Any]):
./tests/enterprise/test_secrets_scanning.py:173:    """Test that secret patterns can be customized."""
./tests/enterprise/test_secrets_scanning.py:178:        "pattern": "API_KEY=['\"][^'\"]+['\"]",
./tests/enterprise/test_secrets_scanning.py:228:    """Test that secrets scanning creates audit trail."""
./tests/enterprise/test_secrets_scanning.py:231:        "action": "secrets_scan",
./tests/enterprise/test_secrets_scanning.py:234:        "secrets_found": 0,
./tests/enterprise/test_secrets_scanning.py:238:    audit_file = tmp_path / "secrets_audit.json"
./tests/enterprise/test_secrets_scanning.py:243:    assert loaded["action"] == "secrets_scan"
./tests/enterprise/test_secrets_scanning.py:247:def test_policy_enforcement_with_secrets(gitleaks_config: Dict[str, Any]):
./tests/enterprise/test_secrets_scanning.py:248:    """Test policy enforcement when secrets are detected."""
./tests/enterprise/test_secrets_scanning.py:249:    # Policy should enforce: no secrets allowed
./tests/enterprise/test_secrets_scanning.py:251:        "secrets_scanning": {
./tests/enterprise/test_secrets_scanning.py:258:    assert policy_enforcement["secrets_scanning"]["fail_on_findings"]
./tests/enterprise/test_secrets_scanning.py:262:    """Test workflow for fixing detected secrets."""
./tests/enterprise/test_secrets_scanning.py:264:        "1. Identify secret in repository history",
./tests/enterprise/test_secrets_scanning.py:266:        "3. Remove secret from commit history (git filter-branch or BFG)",
./tests/enterprise/test_secrets_scanning.py:276:def test_enterprise_secret_vault_integration():
./tests/enterprise/test_secrets_scanning.py:277:    """Test integration with enterprise secret vaults."""
./tests/enterprise/test_secrets_scanning.py:282:        "secret_path": "/secret/firsttry",
./tests/enterprise/test_secrets_scanning.py:290:def test_ci_pipeline_secret_scanning():
./tests/enterprise/test_secrets_scanning.py:291:    """Test secrets scanning in CI pipeline."""
./tests/enterprise/test_secrets_scanning.py:293:        "name": "Secrets Scan",
./tests/enterprise/test_remote_cache_e2e.py:40:        "secret_key": os.getenv("AWS_SECRET_ACCESS_KEY", "test"),
./tests/enterprise/test_remote_cache_e2e.py:125:    env["AWS_SECRET_ACCESS_KEY"] = config["secret_key"]
./tests/enterprise/test_remote_cache_e2e.py:180:    env["AWS_SECRET_ACCESS_KEY"] = config["secret_key"]
./tests/enterprise/test_remote_cache_e2e.py:235:    env["AWS_SECRET_ACCESS_KEY"] = config["secret_key"]
./tests/enterprise/test_remote_cache_e2e.py:325:    env["AWS_SECRET_ACCESS_KEY"] = config["secret_key"]
./.venv_tmp/lib/python3.12/site-packages/black/lines.py:29:from blib2to3.pgen2 import token
./.venv_tmp/lib/python3.12/site-packages/black/lines.py:66:            or leaf.type == token.FSTRING_MIDDLE
./.venv_tmp/lib/python3.12/site-packages/black/lines.py:72:        if leaf.type == token.COLON and self.is_class_paren_empty:
./.venv_tmp/lib/python3.12/site-packages/black/lines.py:115:        return bool(self) and self.leaves[0].type == token.AT
./.venv_tmp/lib/python3.12/site-packages/black/lines.py:130:        return bool(self) and self.leaves[0].type == token.NAME and self.leaves[0].value == "class"
./.venv_tmp/lib/python3.12/site-packages/black/lines.py:135:        return self.is_class and self.leaves[-3:] == [Leaf(token.DOT, ".") for _ in range(3)]
./.venv_tmp/lib/python3.12/site-packages/black/lines.py:149:        return (first_leaf.type == token.NAME and first_leaf.value == "def") or (
./.venv_tmp/lib/python3.12/site-packages/black/lines.py:150:            first_leaf.type == token.ASYNC
./.venv_tmp/lib/python3.12/site-packages/black/lines.py:152:            and second_leaf.type == token.NAME
./.venv_tmp/lib/python3.12/site-packages/black/lines.py:159:        return self.is_def and self.leaves[-4:] == [Leaf(token.COLON, ":")] + [
./.venv_tmp/lib/python3.12/site-packages/black/lines.py:160:            Leaf(token.DOT, ".") for _ in range(3)
./.venv_tmp/lib/python3.12/site-packages/black/lines.py:173:            and self.leaves[2].type == token.LPAR
./.venv_tmp/lib/python3.12/site-packages/black/lines.py:175:            and self.leaves[3].type == token.RPAR
./.venv_tmp/lib/python3.12/site-packages/black/lines.py:182:        if not self or self.leaves[0].type != token.STRING:
./.venv_tmp/lib/python3.12/site-packages/black/lines.py:199:        return [leaf.type for leaf in self.leaves].count(token.EQUAL) > 1
./.venv_tmp/lib/python3.12/site-packages/black/lines.py:206:        return self.leaves[-1].type == token.COLON
./.venv_tmp/lib/python3.12/site-packages/black/lines.py:236:            if leaf_type != token.STRING:
./.venv_tmp/lib/python3.12/site-packages/black/lines.py:251:            if last_leaf.type == token.COMMA or (
./.venv_tmp/lib/python3.12/site-packages/black/lines.py:252:                last_leaf.type == token.RPAR and not last_leaf.value
./.venv_tmp/lib/python3.12/site-packages/black/lines.py:322:            closing.type in CLOSING_BRACKETS and self.leaves and self.leaves[-1].type == token.COMMA
./.venv_tmp/lib/python3.12/site-packages/black/lines.py:326:        if closing.type == token.RBRACE:
./.venv_tmp/lib/python3.12/site-packages/black/lines.py:329:        if closing.type == token.RSQB:
./.venv_tmp/lib/python3.12/site-packages/black/lines.py:338:                    brackets=(token.LSQB, token.RSQB),
./.venv_tmp/lib/python3.12/site-packages/black/lines.py:363:        if comment.type != token.COMMENT:
./.venv_tmp/lib/python3.12/site-packages/black/lines.py:373:            last_leaf.type == token.RPAR
./.venv_tmp/lib/python3.12/site-packages/black/lines.py:620:                    and current_line.leaves[-1].type == token.COLON
./.venv_tmp/lib/python3.12/site-packages/black/lines.py:828:        if leaf.bracket_depth <= max_level_to_update and leaf.type == token.COMMA:
./.venv_tmp/lib/python3.12/site-packages/black/lines.py:878:    if leaves[0].type == token.STRING and leaves[1].type == token.DOT:
./.venv_tmp/lib/python3.12/site-packages/black/lines.py:888:            elif leaf.type == token.DOT:
./.venv_tmp/lib/python3.12/site-packages/black/lines.py:890:            elif leaf.type == token.NAME:
./.venv_tmp/lib/python3.12/site-packages/black/lines.py:891:                if not (next.type == token.DOT or next.type in OPENING_BRACKETS):
./.venv_tmp/lib/python3.12/site-packages/black/lines.py:975:        last.type == token.RPAR
./.venv_tmp/lib/python3.12/site-packages/black/lines.py:976:        or last.type == token.RBRACE
./.venv_tmp/lib/python3.12/site-packages/black/lines.py:980:            last.type == token.RSQB
./.venv_tmp/lib/python3.12/site-packages/black/ranges.py:19:from blib2to3.pgen2.token import ASYNC, NEWLINE
./.venv_tmp/lib/python3.12/site-packages/black/ranges.py:156:    the unformatted code as its value. `STANDALONE_COMMENT` is a "fake" token
./.venv_tmp/lib/python3.12/site-packages/black/ranges.py:165:       line. "unwrapped lines" are divided by the `NEWLINE` token. e.g. a
./.venv_tmp/lib/python3.12/site-packages/black/ranges.py:168:       tokenizer only sees the last '\n' as the `NEWLINE` token.
./.venv_tmp/lib/python3.12/site-packages/black/ranges.py:230:        # `async_funcdef`, the ASYNC token is defined on a separate level by the
./.venv_tmp/lib/python3.12/site-packages/black/ranges.py:250:            # token.
./.venv_tmp/lib/python3.12/site-packages/black/ranges.py:275:            # token is on the grandparent node.
./.venv_tmp/lib/python3.12/site-packages/black/brackets.py:19:from blib2to3.pgen2 import token
./.venv_tmp/lib/python3.12/site-packages/black/brackets.py:37:    token.VBAR: 9,
./.venv_tmp/lib/python3.12/site-packages/black/brackets.py:38:    token.CIRCUMFLEX: 8,
./.venv_tmp/lib/python3.12/site-packages/black/brackets.py:39:    token.AMPER: 7,
./.venv_tmp/lib/python3.12/site-packages/black/brackets.py:40:    token.LEFTSHIFT: 6,
./.venv_tmp/lib/python3.12/site-packages/black/brackets.py:41:    token.RIGHTSHIFT: 6,
./.venv_tmp/lib/python3.12/site-packages/black/brackets.py:42:    token.PLUS: 5,
./.venv_tmp/lib/python3.12/site-packages/black/brackets.py:43:    token.MINUS: 5,
./.venv_tmp/lib/python3.12/site-packages/black/brackets.py:44:    token.STAR: 4,
./.venv_tmp/lib/python3.12/site-packages/black/brackets.py:45:    token.SLASH: 4,
./.venv_tmp/lib/python3.12/site-packages/black/brackets.py:46:    token.DOUBLESLASH: 4,
./.venv_tmp/lib/python3.12/site-packages/black/brackets.py:47:    token.PERCENT: 4,
./.venv_tmp/lib/python3.12/site-packages/black/brackets.py:48:    token.AT: 4,
./.venv_tmp/lib/python3.12/site-packages/black/brackets.py:49:    token.TILDE: 3,
./.venv_tmp/lib/python3.12/site-packages/black/brackets.py:50:    token.DOUBLESTAR: 2,
./.venv_tmp/lib/python3.12/site-packages/black/brackets.py:85:        If a leaf is a delimiter (a token on which Black can split the line if
./.venv_tmp/lib/python3.12/site-packages/black/brackets.py:89:        if leaf.type == token.COMMENT:
./.venv_tmp/lib/python3.12/site-packages/black/brackets.py:164:        tokens between `for` and `in`.
./.venv_tmp/lib/python3.12/site-packages/black/brackets.py:166:        if leaf.type == token.NAME and leaf.value == "for":
./.venv_tmp/lib/python3.12/site-packages/black/brackets.py:178:            and leaf.type == token.NAME
./.venv_tmp/lib/python3.12/site-packages/black/brackets.py:191:        tokens between `lambda` and `:`.
./.venv_tmp/lib/python3.12/site-packages/black/brackets.py:193:        if leaf.type == token.NAME and leaf.value == "lambda":
./.venv_tmp/lib/python3.12/site-packages/black/brackets.py:205:            and leaf.type == token.COLON
./.venv_tmp/lib/python3.12/site-packages/black/brackets.py:215:        return self.bracket_match.get((self.depth - 1, token.RSQB))
./.venv_tmp/lib/python3.12/site-packages/black/brackets.py:226:    if leaf.type == token.COMMA:
./.venv_tmp/lib/python3.12/site-packages/black/brackets.py:246:        leaf.type == token.DOT
./.venv_tmp/lib/python3.12/site-packages/black/brackets.py:263:    if leaf.type == token.STRING and previous is not None and previous.type == token.STRING:
./.venv_tmp/lib/python3.12/site-packages/black/brackets.py:266:    if leaf.type not in {token.NAME, token.ASYNC}:
./.venv_tmp/lib/python3.12/site-packages/black/brackets.py:273:        or leaf.type == token.ASYNC
./.venv_tmp/lib/python3.12/site-packages/black/brackets.py:291:        and not (previous is not None and previous.type == token.NAME and previous.value == "not")
./.venv_tmp/lib/python3.12/site-packages/black/brackets.py:299:        and not (previous is not None and previous.type == token.NAME and previous.value == "is")
./.venv_tmp/lib/python3.12/site-packages/black/brackets.py:320:    if not (first.type == token.LPAR and last.type == token.RPAR):
./.venv_tmp/lib/python3.12/site-packages/black/comments.py:18:from blib2to3.pgen2 import token
./.venv_tmp/lib/python3.12/site-packages/black/comments.py:45:    type: int  # token.COMMENT or STANDALONE_COMMENT
./.venv_tmp/lib/python3.12/site-packages/black/comments.py:57:    in `pgen2/driver.py:Driver.parse_tokens()`.  This was a brilliant implementation
./.venv_tmp/lib/python3.12/site-packages/black/comments.py:69:    Inline comments are emitted as regular token.COMMENT leaves.  Standalone
./.venv_tmp/lib/python3.12/site-packages/black/comments.py:70:    are emitted with a fake STANDALONE_COMMENT token identifier.
./.venv_tmp/lib/python3.12/site-packages/black/comments.py:73:    for pc in list_comments(leaf.prefix, is_endmarker=leaf.type == token.ENDMARKER):
./.venv_tmp/lib/python3.12/site-packages/black/comments.py:109:            comment_type = token.COMMENT  # simple trailing comment
./.venv_tmp/lib/python3.12/site-packages/black/comments.py:263:    while container is not None and container.type != token.ENDMARKER:
./.venv_tmp/lib/python3.12/site-packages/black/comments.py:279:                    child.type == token.INDENT
./.venv_tmp/lib/python3.12/site-packages/black/comments.py:284:                    # level, and we shouldn't swallow the previous INDENT token.
./.venv_tmp/lib/python3.12/site-packages/black/comments.py:290:            if container.type == token.DEDENT and container.next_sibling is None:
./.venv_tmp/lib/python3.12/site-packages/black/comments.py:348:            if current_node.type in (token.NEWLINE, token.INDENT):
./.venv_tmp/lib/python3.12/site-packages/black/comments.py:357:    elif parent is not None and parent.type == syms.suite and leaf.type == token.NEWLINE:
./.venv_tmp/lib/python3.12/site-packages/black/comments.py:366:        # Special case for `async_stmt` where the ASYNC token is on the
./.venv_tmp/lib/python3.12/site-packages/black/comments.py:372:            and grandparent.prev_sibling.type == token.ASYNC
./.venv_tmp/lib/python3.12/site-packages/black/linegen.py:50:    is_lpar_token,
./.venv_tmp/lib/python3.12/site-packages/black/linegen.py:52:    is_name_token,
./.venv_tmp/lib/python3.12/site-packages/black/linegen.py:57:    is_rpar_token,
./.venv_tmp/lib/python3.12/site-packages/black/linegen.py:87:from blib2to3.pgen2 import token
./.venv_tmp/lib/python3.12/site-packages/black/linegen.py:147:                elif comment.type == token.COMMENT:
./.venv_tmp/lib/python3.12/site-packages/black/linegen.py:168:        already_parenthesized = node.prev_sibling and node.prev_sibling.type == token.LPAR
./.venv_tmp/lib/python3.12/site-packages/black/linegen.py:172:            lpar = Leaf(token.LPAR, "")
./.venv_tmp/lib/python3.12/site-packages/black/linegen.py:173:            rpar = Leaf(token.RPAR, "")
./.venv_tmp/lib/python3.12/site-packages/black/linegen.py:218:            if is_name_token(child) and child.value in keywords:
./.venv_tmp/lib/python3.12/site-packages/black/linegen.py:240:                if node.children[i - 1].type == token.COLON:
./.venv_tmp/lib/python3.12/site-packages/black/linegen.py:264:            if child.type == token.RARROW:
./.venv_tmp/lib/python3.12/site-packages/black/linegen.py:267:                if child.type == syms.atom and child.children[0].type == token.LPAR:
./.venv_tmp/lib/python3.12/site-packages/black/linegen.py:302:            if (prev_type is None or prev_type == token.SEMI) and is_arith_like(child):
./.venv_tmp/lib/python3.12/site-packages/black/linegen.py:330:            if child.type == token.ASYNC or child.type == STANDALONE_COMMENT:
./.venv_tmp/lib/python3.12/site-packages/black/linegen.py:353:                leaf.type == token.NUMBER
./.venv_tmp/lib/python3.12/site-packages/black/linegen.py:356:                and next_leaf.children[0].type == token.DOT
./.venv_tmp/lib/python3.12/site-packages/black/linegen.py:391:            and operand.children[1].type == token.DOUBLESTAR
./.venv_tmp/lib/python3.12/site-packages/black/linegen.py:393:            lpar = Leaf(token.LPAR, "(")
./.venv_tmp/lib/python3.12/site-packages/black/linegen.py:394:            rpar = Leaf(token.RPAR, ")")
./.venv_tmp/lib/python3.12/site-packages/black/linegen.py:501:        if self.mode.string_normalization and leaf.type == token.STRING:
./.venv_tmp/lib/python3.12/site-packages/black/linegen.py:515:            if (first.type == token.LSQB and last.type == token.RSQB) or (
./.venv_tmp/lib/python3.12/site-packages/black/linegen.py:516:                first.type == token.LBRACE and last.type == token.RBRACE
./.venv_tmp/lib/python3.12/site-packages/black/linegen.py:565:        #     if leaf.type == token.FSTRING_MIDDLE
./.venv_tmp/lib/python3.12/site-packages/black/linegen.py:753:        if leaf.type == token.COLON:
./.venv_tmp/lib/python3.12/site-packages/black/linegen.py:757:        if leaf.type == token.RARROW:
./.venv_tmp/lib/python3.12/site-packages/black/linegen.py:789:    for leaf_type in [token.LPAR, token.LSQB]:
./.venv_tmp/lib/python3.12/site-packages/black/linegen.py:797:            if index == 2 and leaf.type == token.LSQB:
./.venv_tmp/lib/python3.12/site-packages/black/linegen.py:802:                if leaf.type == token.LSQB:
./.venv_tmp/lib/python3.12/site-packages/black/linegen.py:804:                elif leaf.type == token.RSQB:
./.venv_tmp/lib/python3.12/site-packages/black/linegen.py:814:                and not (leaf_type == token.LPAR and depth > 0)
./.venv_tmp/lib/python3.12/site-packages/black/linegen.py:909:        is_unpacking = body_leaves[0].type in [token.STAR, token.DOUBLESTAR]
./.venv_tmp/lib/python3.12/site-packages/black/linegen.py:929:            if line.mode.magic_trailing_comma and inner_body_leaves[-1].type == token.COMMA:
./.venv_tmp/lib/python3.12/site-packages/black/linegen.py:972:        and rhs.opening_bracket.type == token.LPAR
./.venv_tmp/lib/python3.12/site-packages/black/linegen.py:975:        and rhs.closing_bracket.type == token.RPAR
./.venv_tmp/lib/python3.12/site-packages/black/linegen.py:1049:        return any(leaf.type == token.COLON for leaf in rhs_oop.tail.leaves)
./.venv_tmp/lib/python3.12/site-packages/black/linegen.py:1052:    if not (len(rhs.head.leaves) >= 2 and rhs.head.leaves[-2].type == token.EQUAL):
./.venv_tmp/lib/python3.12/site-packages/black/linegen.py:1070:    rhs_head_equal_count = [leaf.type for leaf in rhs.head.leaves].count(token.EQUAL)
./.venv_tmp/lib/python3.12/site-packages/black/linegen.py:1071:    rhs_oop_head_equal_count = [leaf.type for leaf in rhs_oop.head.leaves].count(token.EQUAL)
./.venv_tmp/lib/python3.12/site-packages/black/linegen.py:1077:        if leaf.type == token.EQUAL:
./.venv_tmp/lib/python3.12/site-packages/black/linegen.py:1089:            any(leaf.type == token.EQUAL for leaf in rhs_oop.head.leaves)
./.venv_tmp/lib/python3.12/site-packages/black/linegen.py:1134:    if any(leaf.type == token.COMMA and not is_part_of_annotation(leaf) for leaf in leaves):
./.venv_tmp/lib/python3.12/site-packages/black/linegen.py:1148:        and leaf_with_parent.parent.next_sibling.type == token.VBAR
./.venv_tmp/lib/python3.12/site-packages/black/linegen.py:1179:                if leaves[i].type != token.COMMA:
./.venv_tmp/lib/python3.12/site-packages/black/linegen.py:1180:                    new_comma = Leaf(token.COMMA, ",")
./.venv_tmp/lib/python3.12/site-packages/black/linegen.py:1235:        and line.leaves[-1].type != token.COMMA
./.venv_tmp/lib/python3.12/site-packages/black/linegen.py:1238:        new_comma = Leaf(token.COMMA, ",")
./.venv_tmp/lib/python3.12/site-packages/black/linegen.py:1410:                and child.prev_sibling.type == token.NAME
./.venv_tmp/lib/python3.12/site-packages/black/linegen.py:1426:                and is_lpar_token(child.children[0])
./.venv_tmp/lib/python3.12/site-packages/black/linegen.py:1427:                and is_rpar_token(child.children[-1])
./.venv_tmp/lib/python3.12/site-packages/black/linegen.py:1439:            elif index == 1 and child.type == token.STAR and node.type == syms.except_clause:
./.venv_tmp/lib/python3.12/site-packages/black/linegen.py:1448:                and child.next_sibling.type == token.COLON
./.venv_tmp/lib/python3.12/site-packages/black/linegen.py:1458:        comma_check = child.type == token.COMMA
./.venv_tmp/lib/python3.12/site-packages/black/linegen.py:1466:    if is_lpar_token(child):
./.venv_tmp/lib/python3.12/site-packages/black/linegen.py:1467:        assert is_rpar_token(parent.children[-1])
./.venv_tmp/lib/python3.12/site-packages/black/linegen.py:1471:    elif child.type != token.STAR:
./.venv_tmp/lib/python3.12/site-packages/black/linegen.py:1473:        parent.insert_child(index, Leaf(token.LPAR, ""))
./.venv_tmp/lib/python3.12/site-packages/black/linegen.py:1474:        parent.append_child(Leaf(token.RPAR, ""))
./.venv_tmp/lib/python3.12/site-packages/black/linegen.py:1478:    if node.children[0].type == token.AWAIT and len(node.children) > 1:
./.venv_tmp/lib/python3.12/site-packages/black/linegen.py:1479:        if node.children[1].type == syms.atom and node.children[1].children[0].type == token.LPAR:
./.venv_tmp/lib/python3.12/site-packages/black/linegen.py:1501:                or bracket_contents.children[0].type == token.AWAIT
./.venv_tmp/lib/python3.12/site-packages/black/linegen.py:1503:                    isinstance(child, Leaf) and child.type == token.DOUBLESTAR
./.venv_tmp/lib/python3.12/site-packages/black/linegen.py:1525:        if node.children[i].type == token.COLON:
./.venv_tmp/lib/python3.12/site-packages/black/linegen.py:1529:        lpar = Leaf(token.LPAR, "")
./.venv_tmp/lib/python3.12/site-packages/black/linegen.py:1530:        rpar = Leaf(token.RPAR, "")
./.venv_tmp/lib/python3.12/site-packages/black/linegen.py:1583:        leaf.type == token.COLONEQUAL for leaf in node.leaves()
./.venv_tmp/lib/python3.12/site-packages/black/linegen.py:1616:            and has_sibling_with_type(node, token.COMMA)
./.venv_tmp/lib/python3.12/site-packages/black/linegen.py:1640:                    and is_name_token(node.next_sibling)
./.venv_tmp/lib/python3.12/site-packages/black/linegen.py:1671:    if is_lpar_token(first) and is_rpar_token(last):
./.venv_tmp/lib/python3.12/site-packages/black/linegen.py:1720:        if last_leaf.type == token.COMMA:
./.venv_tmp/lib/python3.12/site-packages/black/linegen.py:1769:                    and prev.type == token.COMMA
./.venv_tmp/lib/python3.12/site-packages/black/linegen.py:1795:                and prev.type == token.COMMA
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:36:from blib2to3.pgen2 import token
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:73:        if leaf.type == token.DOUBLESTAR:
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:76:        raise CannotTransform("No doublestar token was found in the line.")
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:83:            return handle_is_simple_look_up_prev(line, index, {token.RPAR, token.RSQB})
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:85:            return handle_is_simple_lookup_forward(line, index, {token.LPAR, token.LSQB})
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:91:        if start.type in {token.NAME, token.NUMBER}:
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:94:        if start.type in {token.PLUS, token.MINUS, token.TILDE}:
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:95:            if line.leaves[index + 1].type in {token.NAME, token.NUMBER}:
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:97:                # for simplicity starting from the next token (so it'll hit the check
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:113:            and leaf.type == token.DOUBLESTAR
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:134:    token. This is required because of the need to isolate the chained expression
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:155:    Handling decision is_simple_lookup for the lines behind the doublestar token.
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:163:        if current.type not in {token.NAME, token.DOT} or (
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:164:            current.type == token.NAME and current.value == "for"
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:166:            # If the current token isn't disallowed, we'll assume this is simple as
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:167:            # only the disallowed tokens are semantically attached to this lookup
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:188:    if past_leaf.type == token.NAME:
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:189:        return current_leaf.type in {token.DOT}
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:190:    elif past_leaf.type in {token.RPAR, token.RSQB}:
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:191:        return current_leaf.type in {token.RSQB, token.RPAR}
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:192:    elif past_leaf.type in {token.LPAR, token.LSQB}:
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:193:        return current_leaf.type in {token.NAME, token.LPAR, token.LSQB}
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:276:        if not any(leaf.type == token.STRING for leaf in line.leaves):
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:418:                leaf.type == token.STRING
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:420:                and LL[idx + 1].type == token.STRING
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:427:                    if LL[i].type != token.STRING:
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:439:                while is_valid_index(idx) and LL[idx].type == token.STRING:
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:442:            elif leaf.type == token.STRING and "\\\n" in leaf.value:
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:446:                while is_valid_index(idx) and LL[idx].type == token.STRING:
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:503:                string_leaf.type == token.STRING
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:650:        while not prefix and is_valid_index(next_str_idx) and LL[next_str_idx].type == token.STRING:
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:667:        while is_valid_index(next_str_idx) and LL[next_str_idx].type == token.STRING:
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:692:        S_leaf = Leaf(token.STRING, S)
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:706:        string_leaf = Leaf(token.STRING, S_leaf.value.replace(BREAK_MARK, ""))
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:758:                token.STRING,
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:777:            if leaf.type != token.STRING:
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:781:                if leaf.type == token.COMMA and id(leaf) in line.comments:
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:861:            if leaf.type != token.STRING:
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:871:                or LL[idx - 1].type != token.LPAR
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:881:                LL[idx - 2].type == token.COLON
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:882:                or LL[idx - 2].type == token.NAME
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:899:                if token.PERCENT in {leaf.type for leaf in LL[idx - 1 : next_idx]} and (
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:903:                            token.STAR,
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:904:                            token.AT,
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:905:                            token.SLASH,
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:906:                            token.DOUBLESLASH,
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:907:                            token.PERCENT,
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:908:                            token.TILDE,
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:909:                            token.DOUBLESTAR,
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:910:                            token.AWAIT,
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:911:                            token.LSQB,
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:912:                            token.LPAR,
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:919:                        and (before_lpar.type in {token.PLUS, token.MINUS})
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:927:                and LL[next_idx].type == token.RPAR
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:933:                    token.DOUBLESTAR,
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:934:                    token.LSQB,
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:935:                    token.LPAR,
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:936:                    token.DOT,
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:942:                while idx < len(LL) - 1 and LL[idx + 1].type == token.STRING:
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:985:            lpar_or_rpar_idx = idx - 1 if leaf.type == token.STRING else idx
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:987:            if leaf.type == token.STRING:
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:988:                string_leaf = Leaf(token.STRING, LL[idx].value)
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:1032:        token.EQEQUAL,
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:1033:        token.GREATER,
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:1034:        token.GREATEREQUAL,
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:1035:        token.LESS,
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:1036:        token.LESSEQUAL,
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:1037:        token.NOTEQUAL,
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:1038:        token.PERCENT,
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:1039:        token.PLUS,
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:1040:        token.STAR,
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:1091:            token.STRING,
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:1092:            token.NEWLINE,
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:1147:                LL[string_idx - 1].type == token.LPAR
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:1159:            if P.type == token.COMMA:
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:1163:            if P.type in [token.COLON, token.EQUAL, token.PLUSEQUAL, token.NAME]:
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:1180:            if N.type == token.RPAR and N.value == "" and len(LL) > string_idx + 2:
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:1184:            if N.type == token.COMMA:
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:1191:                if N.type == token.DOT and NN.type == token.NAME:
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:1198:                    if is_valid_index(string_idx + 3) and LL[string_idx + 3].type == token.LPAR:
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:1230:        if LL[0].type != token.STRING:
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:1249:            if (not prev_sibling or prev_sibling.type == token.COMMA) and (
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:1250:                not next_sibling or next_sibling.type == token.COMMA
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:1386:            and [LL[idx].type, LL[idx + 1].type] == [token.NAME, token.NAME]
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:1393:            or LL[idx].type == token.NAME
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:1403:        if not is_valid_index(idx) or LL[idx].type != token.STRING:
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:1417:        if is_valid_index(idx) and LL[idx].type == token.COMMA:
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:1469:        ends_with_comma = is_valid_index(string_idx + 1) and LL[string_idx + 1].type == token.COMMA
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:1587:            next_leaf = Leaf(token.STRING, next_value)
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:1605:        rest_leaf = Leaf(token.STRING, rest_value)
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:1624:                if leaf.type == token.LPAR:
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:1630:                or LL[string_idx + 1].type == token.COMMA
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:1823:        while LL[i].type in self.STRING_OPERATORS + [token.NAME]:
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:1943:            if is_valid_index(idx) and LL[idx].type == token.STRING:
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:1961:        if parent_type(LL[0]) == syms.test and LL[0].type == token.NAME and LL[0].value == "else":
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:1966:            if is_valid_index(idx) and LL[idx].type == token.STRING:
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:1989:                if leaf.type == token.COMMA:
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:1993:                    if is_valid_index(idx) and LL[idx].type == token.STRING:
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:2021:            and LL[0].type == token.NAME
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:2027:                if leaf.type in [token.EQUAL, token.PLUSEQUAL]:
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:2031:                    if is_valid_index(idx) and LL[idx].type == token.STRING:
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:2043:                            and LL[idx].type == token.COMMA
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:2071:                if leaf.type == token.COLON and i < len(LL) - 1:
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:2075:                    if is_valid_index(idx) and LL[idx].type == token.STRING:
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:2083:                        if is_valid_index(idx) and LL[idx].type == token.COMMA:
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:2105:        if LL[comma_idx].type == token.COMMA:
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:2120:        if left_leaves and left_leaves[-1].type == token.LPAR:
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:2127:        lpar_leaf = Leaf(token.LPAR, "(")
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:2154:        string_leaf = Leaf(token.STRING, string_value)
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:2165:                assert right_leaves and right_leaves[-1].type == token.RPAR, (
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:2170:            elif right_leaves and right_leaves[-1].type == token.RPAR:
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:2184:                        and left_leaves[index - 1].type == token.COLON
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:2197:        new_rpar_leaf = Leaf(token.RPAR, ")")
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:2207:            comma_leaf = Leaf(token.COMMA, ",")
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:2240:        assert line.leaves[idx].type == token.PLUS
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:2244:    DEFAULT_TOKEN: Final = 20210605
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:2259:        (START, token.DOT): DOT,
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:2260:        (START, token.PERCENT): PERCENT,
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:2261:        (START, DEFAULT_TOKEN): DONE,
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:2263:        (DOT, token.NAME): NAME,
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:2266:        (NAME, token.LPAR): LPAR,
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:2267:        (NAME, DEFAULT_TOKEN): DONE,
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:2270:        (PERCENT, token.LPAR): LPAR,
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:2271:        (PERCENT, DEFAULT_TOKEN): SINGLE_FMT_ARG,
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:2274:        (SINGLE_FMT_ARG, DEFAULT_TOKEN): DONE,
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:2279:        (RPAR, DEFAULT_TOKEN): DONE,
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:2289:            * @leaves[@string_idx].type == token.STRING
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:2297:        assert leaves[string_idx].type == token.STRING
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:2321:        next_token = leaf.type
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:2322:        if next_token == token.LPAR:
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:2328:        # find the matching RPAR token.
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:2330:            if next_token == token.RPAR:
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:2337:            # token, we use the lookup table.
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:2338:            if (current_state, next_token) in self._goto:
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:2339:                self._state = self._goto[current_state, next_token]
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:2343:                if (current_state, self.DEFAULT_TOKEN) in self._goto:
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:2344:                    self._state = self._goto[current_state, self.DEFAULT_TOKEN]
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:2363:        Let `string_leaf = Leaf(token.STRING, '"foo"')` and `N =
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:2379:        lpar = Leaf(token.LPAR, '(')
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:2382:        bar = Leaf(token.STRING, '"bar"')
./.venv_tmp/lib/python3.12/site-packages/black/trans.py:2385:        rpar = Leaf(token.RPAR, ')')
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:18:from blib2to3.pgen2 import token
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:33:WHITESPACE: Final = {token.DEDENT, token.INDENT, token.NEWLINE}
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:47:token.tok_name[STANDALONE_COMMENT] = "STANDALONE_COMMENT"
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:50:    token.LESS,
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:51:    token.GREATER,
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:52:    token.EQEQUAL,
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:53:    token.NOTEQUAL,
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:54:    token.LESSEQUAL,
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:55:    token.GREATEREQUAL,
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:58:    token.VBAR,
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:59:    token.CIRCUMFLEX,
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:60:    token.AMPER,
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:61:    token.LEFTSHIFT,
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:62:    token.RIGHTSHIFT,
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:63:    token.PLUS,
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:64:    token.MINUS,
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:65:    token.STAR,
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:66:    token.SLASH,
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:67:    token.DOUBLESLASH,
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:68:    token.PERCENT,
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:69:    token.AT,
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:70:    token.TILDE,
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:71:    token.DOUBLESTAR,
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:73:STARS: Final = {token.STAR, token.DOUBLESTAR}
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:74:VARARGS_SPECIALS: Final = STARS | {token.SLASH}
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:130:    token.LPAR: token.RPAR,
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:131:    token.LSQB: token.RSQB,
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:132:    token.LBRACE: token.RBRACE,
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:138:    token.COMMA,
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:140:    token.FSTRING_MIDDLE,
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:141:    token.FSTRING_END,
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:142:    token.BANG,
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:163:            name = token.tok_name[node.type]
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:198:    if t == token.COMMENT:
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:202:    if t == token.COLON and p.type not in {
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:209:    if t == token.LBRACE and p.type == syms.fstring_replacement_field:
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:218:        if t == token.COLON:
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:219:            if prevp.type == token.COLON:
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:222:            elif prevp.type != token.COMMA and not complex_subscript:
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:227:        if prevp.type == token.EQUAL:
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:244:            prevp.type == token.STAR
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:255:        elif prevp.type == token.COLON:
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:262:        elif prevp.type == token.AT and p.parent and p.parent.type == syms.decorator:
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:269:    elif prev.type == token.BANG:
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:274:        if not prev or prev.type != token.COMMA:
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:279:        if prev and prev.type != token.COMMA:
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:287:        if t == token.EQUAL:
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:291:        elif prev.type == token.EQUAL:
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:296:        elif prev.type != token.COMMA:
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:303:            if not prevp or prevp.type != token.COMMA:
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:308:        if t == token.LPAR or t == token.RPAR:
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:312:            if t == token.DOT or t == token.LSQB:
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:315:        elif prev.type != token.COMMA:
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:320:        if t == token.EQUAL:
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:325:            if not prevp or prevp.type == token.LPAR:
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:328:        elif prev.type in {token.EQUAL} | VARARGS_SPECIALS:
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:340:        if not prevp or prevp.type == token.AT or prevp.type == token.DOT:
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:344:        if t == token.LPAR:
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:347:        if prev and prev.type == token.LPAR:
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:359:        elif t == token.COLONEQUAL or prev.type == token.COLONEQUAL:
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:366:        if prev and t == token.DOT:
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:372:        if prev and prev.type == token.DOUBLESTAR:
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:384:            if prevp.type == token.COLON and prevp_parent.type in {
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:390:            elif prevp.type == token.EQUAL and prevp_parent.type == syms.argument:
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:394:        elif t in {token.NAME, token.NUMBER, token.STRING}:
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:398:        if t == token.DOT:
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:399:            if prev and prev.type == token.DOT:
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:402:        elif t == token.NAME:
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:406:            if prev and prev.type == token.DOT:
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:413:        if t == token.STAR:
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:444:def prev_siblings_are(node: Optional[LN], tokens: list[Optional[NodeType]]) -> bool:
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:446:    list of tokens; the provided `node`has its type matched against the last element in
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:449:    if not tokens:
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:451:    if tokens[-1] is None:
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:455:    if node.type != tokens[-1]:
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:457:    return prev_siblings_are(node.prev_sibling, tokens[:-1])
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:545:        if node.type != token.STRING:
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:561:    if prev_siblings_are(node.parent, [None, token.NEWLINE, token.INDENT, syms.simple_stmt]):
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:565:    if prev_siblings_are(node.parent, [syms.parameters, token.COLON, syms.simple_stmt]):
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:578:        and node.children[0].type == token.LPAR
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:579:        and node.children[1].type == token.RPAR
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:590:        return len(gexp.children) == 2 and gexp.children[1].type == token.COMMA
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:595:        and node.children[1].type == token.COMMA
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:647:    brackets: tuple[int, int] = (token.LPAR, token.RPAR),
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:668:        if bracket_depth == depth and leaf.type == token.COMMA:
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:691:            and node.children[0].type == token.DOT
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:692:            and node.children[1].type == token.NAME
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:698:            and node.children[0].type == token.LPAR
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:699:            and node.children[1].type == token.RPAR
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:705:            and node.children[0].type == token.LPAR
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:707:            and node.children[2].type == token.RPAR
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:721:    if node.type == token.NAME:
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:726:                node.children[0].type == token.NAME
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:741:    if is_name_token(node) and node.value == "yield":
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:751:    if lpar.type == token.LPAR and rpar.type == token.RPAR:
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:788:    string_leaf = Leaf(token.STRING, string_without_prefix, prefix=node.prefix)
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:827:        or node.children[0].type != token.NEWLINE
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:828:        or node.children[1].type != token.INDENT
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:829:        or node.children[3].type != token.DEDENT
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:852:        and all(leaf == Leaf(token.DOT, ".") for leaf in child.children)
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:866:        and first.type == token.LPAR
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:869:        and last.type == token.RPAR
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:879:    return leaf.type == token.LPAR and leaf.value == ""
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:883:    return leaf.type == token.RPAR and leaf.value == ""
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:892:        t == token.NAME
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:903:        leaf.type == token.NAME
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:908:        leaf.type == token.ASYNC and leaf.next_sibling and leaf.next_sibling.type == syms.with_stmt
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:919:        leaf.type == token.ASYNC
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:932:    return t in {token.COMMENT, STANDALONE_COMMENT} and v.startswith("# type:")
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:939:    return t in {token.COMMENT, STANDALONE_COMMENT} and is_type_ignore_comment_string(v)
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:956:    lpar = Leaf(token.LPAR, "(" if visible else "")
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:957:    rpar = Leaf(token.RPAR, ")" if visible else "")
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:974:    if not (lpar.type == token.LPAR and rpar.type == token.RPAR):
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:986:    if leaf.type == token.LPAR:
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:988:    elif leaf.type == token.RPAR:
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:992:def is_name_token(nl: NL) -> TypeGuard[Leaf]:
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:993:    return nl.type == token.NAME
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:996:def is_lpar_token(nl: NL) -> TypeGuard[Leaf]:
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:997:    return nl.type == token.LPAR
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:1000:def is_rpar_token(nl: NL) -> TypeGuard[Leaf]:
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:1001:    return nl.type == token.RPAR
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:1004:def is_number_token(nl: NL) -> TypeGuard[Leaf]:
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:1005:    return nl.type == token.NUMBER
./.venv_tmp/lib/python3.12/site-packages/black/nodes.py:1012:        if ancestor.prev_sibling and ancestor.prev_sibling.type == token.RARROW:
./.venv_tmp/lib/python3.12/site-packages/black/debug.py:8:from blib2to3.pgen2 import token
./.venv_tmp/lib/python3.12/site-packages/black/debug.py:38:            _type = token.tok_name.get(node.type, str(node.type))
./.venv_tmp/lib/python3.12/site-packages/black/strings.py:110:    token.STRING`. A more precise description of the pre-conditions that are
./.venv_tmp/lib/python3.12/site-packages/black/__init__.py:6:import tokenize
./.venv_tmp/lib/python3.12/site-packages/black/__init__.py:67:from black.nodes import STARS, is_number_token, is_simple_decorator_expression, syms
./.venv_tmp/lib/python3.12/site-packages/black/__init__.py:83:from blib2to3.pgen2 import token
./.venv_tmp/lib/python3.12/site-packages/black/__init__.py:1265:    encoding, lines = tokenize.detect_encoding(srcbuf.readline)
./.venv_tmp/lib/python3.12/site-packages/black/__init__.py:1322:        if n.type == token.FSTRING_START:
./.venv_tmp/lib/python3.12/site-packages/black/__init__.py:1325:            n.type == token.RBRACE
./.venv_tmp/lib/python3.12/site-packages/black/__init__.py:1327:            and any(child.type == token.EQUAL for child in n.parent.children)
./.venv_tmp/lib/python3.12/site-packages/black/__init__.py:1331:        elif is_number_token(n):
./.venv_tmp/lib/python3.12/site-packages/black/__init__.py:1335:        elif n.type == token.SLASH:
./.venv_tmp/lib/python3.12/site-packages/black/__init__.py:1343:        elif n.type == token.COLONEQUAL:
./.venv_tmp/lib/python3.12/site-packages/black/__init__.py:1353:            and n.children[-1].type == token.COMMA
./.venv_tmp/lib/python3.12/site-packages/black/__init__.py:1388:                and atom_children[0].type == token.LPAR
./.venv_tmp/lib/python3.12/site-packages/black/__init__.py:1390:                and atom_children[2].type == token.RPAR
./.venv_tmp/lib/python3.12/site-packages/black/__init__.py:1414:            and n.children[-2].type == token.EQUAL
./.venv_tmp/lib/python3.12/site-packages/black/__init__.py:1421:            and (n.children[1].type == token.STAR or n.children[1].type == syms.testlist)
./.venv_tmp/lib/python3.12/site-packages/black/__init__.py:1423:            is_star_except = n.children[1].type == token.STAR
./.venv_tmp/lib/python3.12/site-packages/black/__init__.py:1431:                and n.children[is_star_except + 2].type == token.NAME
./.venv_tmp/lib/python3.12/site-packages/black/__init__.py:1452:            and node.children[0].type == token.LPAR
./.venv_tmp/lib/python3.12/site-packages/black/__init__.py:1453:            and node.children[2].type == token.RPAR
./.venv_tmp/lib/python3.12/site-packages/black/__init__.py:1476:                if child.type == token.NAME:
./.venv_tmp/lib/python3.12/site-packages/black/__init__.py:1482:                assert orig_name.type == token.NAME, "Invalid syntax parsing imports"
./.venv_tmp/lib/python3.12/site-packages/black/__init__.py:1500:                and first_child.type == token.STRING
./.venv_tmp/lib/python3.12/site-packages/black/__init__.py:1501:                and child.children[1].type == token.NEWLINE
./.venv_tmp/lib/python3.12/site-packages/black/parsing.py:16:from blib2to3.pgen2.tokenize import TokenError
./.venv_tmp/lib/python3.12/site-packages/black/parsing.py:85:        except TokenError as te:
./.venv_tmp/lib/python3.12/site-packages/black/parsing.py:107:    except (ParseError, TokenError, IndentationError):
./.venv_tmp/lib/python3.12/site-packages/black/handle_ipynb_magics.py:7:import secrets
./.venv_tmp/lib/python3.12/site-packages/black/handle_ipynb_magics.py:30:TOKENS_TO_IGNORE = frozenset(
./.venv_tmp/lib/python3.12/site-packages/black/handle_ipynb_magics.py:62:    installed = find_spec("tokenize_rt") is not None and find_spec("IPython") is not None
./.venv_tmp/lib/python3.12/site-packages/black/handle_ipynb_magics.py:74:    or non-Python cell magics, which might cause tokenizer_rt to break because of
./.venv_tmp/lib/python3.12/site-packages/black/handle_ipynb_magics.py:113:    ``tokenize_rt`` so that round-tripping works fine.
./.venv_tmp/lib/python3.12/site-packages/black/handle_ipynb_magics.py:115:    from tokenize_rt import reversed_enumerate, src_to_tokens, tokens_to_src
./.venv_tmp/lib/python3.12/site-packages/black/handle_ipynb_magics.py:117:    tokens = src_to_tokens(src)
./.venv_tmp/lib/python3.12/site-packages/black/handle_ipynb_magics.py:119:    for idx, token in reversed_enumerate(tokens):
./.venv_tmp/lib/python3.12/site-packages/black/handle_ipynb_magics.py:120:        if token.name in TOKENS_TO_IGNORE:
./.venv_tmp/lib/python3.12/site-packages/black/handle_ipynb_magics.py:122:        if token.name == "OP" and token.src == ";":
./.venv_tmp/lib/python3.12/site-packages/black/handle_ipynb_magics.py:123:            del tokens[idx]
./.venv_tmp/lib/python3.12/site-packages/black/handle_ipynb_magics.py:128:    return tokens_to_src(tokens), True
./.venv_tmp/lib/python3.12/site-packages/black/handle_ipynb_magics.py:135:    ``tokenize_rt`` so that round-tripping works fine.
./.venv_tmp/lib/python3.12/site-packages/black/handle_ipynb_magics.py:139:    from tokenize_rt import reversed_enumerate, src_to_tokens, tokens_to_src
./.venv_tmp/lib/python3.12/site-packages/black/handle_ipynb_magics.py:141:    tokens = src_to_tokens(src)
./.venv_tmp/lib/python3.12/site-packages/black/handle_ipynb_magics.py:142:    for idx, token in reversed_enumerate(tokens):
./.venv_tmp/lib/python3.12/site-packages/black/handle_ipynb_magics.py:143:        if token.name in TOKENS_TO_IGNORE:
./.venv_tmp/lib/python3.12/site-packages/black/handle_ipynb_magics.py:145:        tokens[idx] = token._replace(src=token.src + ";")
./.venv_tmp/lib/python3.12/site-packages/black/handle_ipynb_magics.py:152:    return str(tokens_to_src(tokens))
./.venv_tmp/lib/python3.12/site-packages/black/handle_ipynb_magics.py:197:def create_token(n_chars: int) -> str:
./.venv_tmp/lib/python3.12/site-packages/black/handle_ipynb_magics.py:198:    """Create a randomly generated token that is n_chars characters long."""
./.venv_tmp/lib/python3.12/site-packages/black/handle_ipynb_magics.py:201:    token = secrets.token_hex(n_bytes)
./.venv_tmp/lib/python3.12/site-packages/black/handle_ipynb_magics.py:202:    if len(token) + 3 > n_chars:
./.venv_tmp/lib/python3.12/site-packages/black/handle_ipynb_magics.py:203:        token = token[:-1]
./.venv_tmp/lib/python3.12/site-packages/black/handle_ipynb_magics.py:206:    return f'b"{token}"'
./.venv_tmp/lib/python3.12/site-packages/black/handle_ipynb_magics.py:209:def get_token(src: str, magic: str) -> str:
./.venv_tmp/lib/python3.12/site-packages/black/handle_ipynb_magics.py:210:    """Return randomly generated token to mask IPython magic with.
./.venv_tmp/lib/python3.12/site-packages/black/handle_ipynb_magics.py:213:    token to mask it with would be `"43fdd17f7e5ddc83"`. The token
./.venv_tmp/lib/python3.12/site-packages/black/handle_ipynb_magics.py:219:    token = create_token(n_chars)
./.venv_tmp/lib/python3.12/site-packages/black/handle_ipynb_magics.py:221:    while token in src:
./.venv_tmp/lib/python3.12/site-packages/black/handle_ipynb_magics.py:222:        token = create_token(n_chars)
./.venv_tmp/lib/python3.12/site-packages/black/handle_ipynb_magics.py:230:    return token
./.venv_tmp/lib/python3.12/site-packages/black/handle_ipynb_magics.py:234:    r"""Replace cell magic with token.
./.venv_tmp/lib/python3.12/site-packages/black/handle_ipynb_magics.py:259:    mask = get_token(src, header)
./.venv_tmp/lib/python3.12/site-packages/black/handle_ipynb_magics.py:298:            mask = get_token(src, magic)
./.venv_tmp/lib/python3.12/site-packages/rich/logging.py:275:    log.warning("password was rejected for admin site.")
./.venv_tmp/lib/python3.12/site-packages/rich/containers.py:146:                tokens: List[Text] = []
./.venv_tmp/lib/python3.12/site-packages/rich/containers.py:148:                    tokens.append(word)
./.venv_tmp/lib/python3.12/site-packages/rich/containers.py:153:                        tokens.append(Text(" " * spaces[index], style=space_style))
./.venv_tmp/lib/python3.12/site-packages/rich/containers.py:154:                self[line_index] = Text("").join(tokens)
./.venv_tmp/lib/python3.12/site-packages/rich/console.py:2102:        password: bool = False,
./.venv_tmp/lib/python3.12/site-packages/rich/console.py:2113:            password: (bool, optional): Hide typed text. Defaults to False.
./.venv_tmp/lib/python3.12/site-packages/rich/console.py:2121:        if password:
./.venv_tmp/lib/python3.12/site-packages/rich/syntax.py:27:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/rich/syntax.py:36:    Token,
./.venv_tmp/lib/python3.12/site-packages/rich/syntax.py:53:TokenType = Tuple[str, ...]
./.venv_tmp/lib/python3.12/site-packages/rich/syntax.py:61:ANSI_LIGHT: Dict[TokenType, Style] = {
./.venv_tmp/lib/python3.12/site-packages/rich/syntax.py:62:    Token: Style(),
./.venv_tmp/lib/python3.12/site-packages/rich/syntax.py:90:ANSI_DARK: Dict[TokenType, Style] = {
./.venv_tmp/lib/python3.12/site-packages/rich/syntax.py:91:    Token: Style(),
./.venv_tmp/lib/python3.12/site-packages/rich/syntax.py:127:    def get_style_for_token(self, token_type: TokenType) -> Style:
./.venv_tmp/lib/python3.12/site-packages/rich/syntax.py:128:        """Get a style for a given Pygments token."""
./.venv_tmp/lib/python3.12/site-packages/rich/syntax.py:141:        self._style_cache: Dict[TokenType, Style] = {}
./.venv_tmp/lib/python3.12/site-packages/rich/syntax.py:153:    def get_style_for_token(self, token_type: TokenType) -> Style:
./.venv_tmp/lib/python3.12/site-packages/rich/syntax.py:156:            return self._style_cache[token_type]
./.venv_tmp/lib/python3.12/site-packages/rich/syntax.py:159:                pygments_style = self._pygments_style_class.style_for_token(token_type)
./.venv_tmp/lib/python3.12/site-packages/rich/syntax.py:172:            self._style_cache[token_type] = style
./.venv_tmp/lib/python3.12/site-packages/rich/syntax.py:182:    def __init__(self, style_map: Dict[TokenType, Style]) -> None:
./.venv_tmp/lib/python3.12/site-packages/rich/syntax.py:186:        self._style_cache: Dict[TokenType, Style] = {}
./.venv_tmp/lib/python3.12/site-packages/rich/syntax.py:188:    def get_style_for_token(self, token_type: TokenType) -> Style:
./.venv_tmp/lib/python3.12/site-packages/rich/syntax.py:191:            return self._style_cache[token_type]
./.venv_tmp/lib/python3.12/site-packages/rich/syntax.py:197:            token = tuple(token_type)
./.venv_tmp/lib/python3.12/site-packages/rich/syntax.py:199:            while token:
./.venv_tmp/lib/python3.12/site-packages/rich/syntax.py:200:                _style = get_style(token)
./.venv_tmp/lib/python3.12/site-packages/rich/syntax.py:204:                token = token[:-1]
./.venv_tmp/lib/python3.12/site-packages/rich/syntax.py:205:            self._style_cache[token_type] = style
./.venv_tmp/lib/python3.12/site-packages/rich/syntax.py:423:    def _get_token_color(self, token_type: TokenType) -> Optional[Color]:
./.venv_tmp/lib/python3.12/site-packages/rich/syntax.py:424:        """Get a color (if any) for the given token.
./.venv_tmp/lib/python3.12/site-packages/rich/syntax.py:427:            token_type (TokenType): A token type tuple from Pygments.
./.venv_tmp/lib/python3.12/site-packages/rich/syntax.py:432:        style = self._theme.get_style_for_token(token_type)
./.venv_tmp/lib/python3.12/site-packages/rich/syntax.py:488:        _get_theme_style = self._theme.get_style_for_token
./.venv_tmp/lib/python3.12/site-packages/rich/syntax.py:500:                def line_tokenize() -> Iterable[Tuple[Any, str]]:
./.venv_tmp/lib/python3.12/site-packages/rich/syntax.py:501:                    """Split tokens to one per line."""
./.venv_tmp/lib/python3.12/site-packages/rich/syntax.py:504:                    for token_type, token in lexer.get_tokens(code):
./.venv_tmp/lib/python3.12/site-packages/rich/syntax.py:505:                        while token:
./.venv_tmp/lib/python3.12/site-packages/rich/syntax.py:506:                            line_token, new_line, token = token.partition("\n")
./.venv_tmp/lib/python3.12/site-packages/rich/syntax.py:507:                            yield token_type, line_token + new_line
./.venv_tmp/lib/python3.12/site-packages/rich/syntax.py:509:                def tokens_to_spans() -> Iterable[Tuple[str, Optional[Style]]]:
./.venv_tmp/lib/python3.12/site-packages/rich/syntax.py:510:                    """Convert tokens to spans."""
./.venv_tmp/lib/python3.12/site-packages/rich/syntax.py:511:                    tokens = iter(line_tokenize())
./.venv_tmp/lib/python3.12/site-packages/rich/syntax.py:515:                    # Skip over tokens until line start
./.venv_tmp/lib/python3.12/site-packages/rich/syntax.py:518:                            _token_type, token = next(tokens)
./.venv_tmp/lib/python3.12/site-packages/rich/syntax.py:521:                        yield (token, None)
./.venv_tmp/lib/python3.12/site-packages/rich/syntax.py:522:                        if token.endswith("\n"):
./.venv_tmp/lib/python3.12/site-packages/rich/syntax.py:525:                    for token_type, token in tokens:
./.venv_tmp/lib/python3.12/site-packages/rich/syntax.py:526:                        yield (token, _get_theme_style(token_type))
./.venv_tmp/lib/python3.12/site-packages/rich/syntax.py:527:                        if token.endswith("\n"):
./.venv_tmp/lib/python3.12/site-packages/rich/syntax.py:532:                text.append_tokens(tokens_to_spans())
./.venv_tmp/lib/python3.12/site-packages/rich/syntax.py:535:                text.append_tokens(
./.venv_tmp/lib/python3.12/site-packages/rich/syntax.py:536:                    (token, _get_theme_style(token_type))
./.venv_tmp/lib/python3.12/site-packages/rich/syntax.py:537:                    for token_type, token in lexer.get_tokens(code)
./.venv_tmp/lib/python3.12/site-packages/rich/syntax.py:571:        foreground_color = self._get_token_color(Token.Text)
./.venv_tmp/lib/python3.12/site-packages/rich/syntax.py:599:                self._theme.get_style_for_token(Token.Text),
./.venv_tmp/lib/python3.12/site-packages/rich/syntax.py:605:                self._theme.get_style_for_token(Token.Text),
./.venv_tmp/lib/python3.12/site-packages/rich/syntax.py:669:                + self._theme.get_style_for_token(Comment)
./.venv_tmp/lib/python3.12/site-packages/rich/syntax.py:703:                + self._theme.get_style_for_token(Comment)
./.venv_tmp/lib/python3.12/site-packages/rich/prompt.py:37:        password (bool, optional): Enable password input. Defaults to False.
./.venv_tmp/lib/python3.12/site-packages/rich/prompt.py:57:        password: bool = False,
./.venv_tmp/lib/python3.12/site-packages/rich/prompt.py:67:        self.password = password
./.venv_tmp/lib/python3.12/site-packages/rich/prompt.py:81:        password: bool = False,
./.venv_tmp/lib/python3.12/site-packages/rich/prompt.py:97:        password: bool = False,
./.venv_tmp/lib/python3.12/site-packages/rich/prompt.py:111:        password: bool = False,
./.venv_tmp/lib/python3.12/site-packages/rich/prompt.py:127:            password (bool, optional): Enable password input. Defaults to False.
./.venv_tmp/lib/python3.12/site-packages/rich/prompt.py:137:            password=password,
./.venv_tmp/lib/python3.12/site-packages/rich/prompt.py:188:        password: bool,
./.venv_tmp/lib/python3.12/site-packages/rich/prompt.py:196:            password (bool): Enable password entry.
./.venv_tmp/lib/python3.12/site-packages/rich/prompt.py:201:        return console.input(prompt, password=password, stream=stream)
./.venv_tmp/lib/python3.12/site-packages/rich/prompt.py:278:            value = self.get_input(self.console, prompt, self.password, stream=stream)
./.venv_tmp/lib/python3.12/site-packages/rich/prompt.py:366:            password = Prompt.ask(
./.venv_tmp/lib/python3.12/site-packages/rich/prompt.py:367:                "Please enter a password [cyan](must be at least 5 characters)",
./.venv_tmp/lib/python3.12/site-packages/rich/prompt.py:368:                password=True,
./.venv_tmp/lib/python3.12/site-packages/rich/prompt.py:370:            if len(password) >= 5:
./.venv_tmp/lib/python3.12/site-packages/rich/prompt.py:372:            print("[prompt.invalid]password too short")
./.venv_tmp/lib/python3.12/site-packages/rich/prompt.py:373:        print(f"password={password!r}")
./.venv_tmp/lib/python3.12/site-packages/rich/markdown.py:7:from markdown_it.token import Token
./.venv_tmp/lib/python3.12/site-packages/rich/markdown.py:28:    def create(cls, markdown: Markdown, token: Token) -> MarkdownElement:
./.venv_tmp/lib/python3.12/site-packages/rich/markdown.py:33:            token (Token): A node from markdown-it.
./.venv_tmp/lib/python3.12/site-packages/rich/markdown.py:111:    def create(cls, markdown: Markdown, token: Token) -> Paragraph:
./.venv_tmp/lib/python3.12/site-packages/rich/markdown.py:126:    def create(cls, markdown: Markdown, token: Token) -> Heading:
./.venv_tmp/lib/python3.12/site-packages/rich/markdown.py:127:        return cls(token.tag)
./.venv_tmp/lib/python3.12/site-packages/rich/markdown.py:161:    def create(cls, markdown: Markdown, token: Token) -> CodeBlock:
./.venv_tmp/lib/python3.12/site-packages/rich/markdown.py:162:        node_info = token.info or ""
./.venv_tmp/lib/python3.12/site-packages/rich/markdown.py:282:    def create(cls, markdown: Markdown, token: Token) -> MarkdownElement:
./.venv_tmp/lib/python3.12/site-packages/rich/markdown.py:283:        style = str(token.attrs.get("style")) or ""
./.venv_tmp/lib/python3.12/site-packages/rich/markdown.py:312:    def create(cls, markdown: Markdown, token: Token) -> ListElement:
./.venv_tmp/lib/python3.12/site-packages/rich/markdown.py:313:        return cls(token.type, int(token.attrs.get("start", 1)))
./.venv_tmp/lib/python3.12/site-packages/rich/markdown.py:380:    def create(cls, markdown: Markdown, token: Token) -> MarkdownElement:
./.venv_tmp/lib/python3.12/site-packages/rich/markdown.py:381:        url = token.attrs.get("href", "#")
./.venv_tmp/lib/python3.12/site-packages/rich/markdown.py:382:        return cls(token.content, str(url))
./.venv_tmp/lib/python3.12/site-packages/rich/markdown.py:395:    def create(cls, markdown: Markdown, token: Token) -> MarkdownElement:
./.venv_tmp/lib/python3.12/site-packages/rich/markdown.py:400:            token (Any): A token from markdown-it.
./.venv_tmp/lib/python3.12/site-packages/rich/markdown.py:405:        return cls(str(token.attrs.get("src", "")), markdown.hyperlinks)
./.venv_tmp/lib/python3.12/site-packages/rich/markdown.py:531:    def _flatten_tokens(self, tokens: Iterable[Token]) -> Iterable[Token]:
./.venv_tmp/lib/python3.12/site-packages/rich/markdown.py:532:        """Flattens the token stream."""
./.venv_tmp/lib/python3.12/site-packages/rich/markdown.py:533:        for token in tokens:
./.venv_tmp/lib/python3.12/site-packages/rich/markdown.py:534:            is_fence = token.type == "fence"
./.venv_tmp/lib/python3.12/site-packages/rich/markdown.py:535:            is_image = token.tag == "img"
./.venv_tmp/lib/python3.12/site-packages/rich/markdown.py:536:            if token.children and not (is_image or is_fence):
./.venv_tmp/lib/python3.12/site-packages/rich/markdown.py:537:                yield from self._flatten_tokens(token.children)
./.venv_tmp/lib/python3.12/site-packages/rich/markdown.py:539:                yield token
./.venv_tmp/lib/python3.12/site-packages/rich/markdown.py:552:        tokens = self.parsed
./.venv_tmp/lib/python3.12/site-packages/rich/markdown.py:557:        for token in self._flatten_tokens(tokens):
./.venv_tmp/lib/python3.12/site-packages/rich/markdown.py:558:            node_type = token.type
./.venv_tmp/lib/python3.12/site-packages/rich/markdown.py:559:            tag = token.tag
./.venv_tmp/lib/python3.12/site-packages/rich/markdown.py:561:            entering = token.nesting == 1
./.venv_tmp/lib/python3.12/site-packages/rich/markdown.py:562:            exiting = token.nesting == -1
./.venv_tmp/lib/python3.12/site-packages/rich/markdown.py:563:            self_closing = token.nesting == 0
./.venv_tmp/lib/python3.12/site-packages/rich/markdown.py:566:                context.on_text(token.content, node_type)
./.venv_tmp/lib/python3.12/site-packages/rich/markdown.py:572:                href = str(token.attrs.get("href", ""))
./.venv_tmp/lib/python3.12/site-packages/rich/markdown.py:578:                    context.stack.push(Link.create(self, token))
./.venv_tmp/lib/python3.12/site-packages/rich/markdown.py:597:                    # If it's an opening inline token e.g. strong, em, etc.
./.venv_tmp/lib/python3.12/site-packages/rich/markdown.py:607:                    if token.content:
./.venv_tmp/lib/python3.12/site-packages/rich/markdown.py:608:                        context.on_text(token.content, node_type)
./.venv_tmp/lib/python3.12/site-packages/rich/markdown.py:612:                element_class = self.elements.get(token.type) or UnknownElement
./.venv_tmp/lib/python3.12/site-packages/rich/markdown.py:613:                element = element_class.create(self, token)
./.venv_tmp/lib/python3.12/site-packages/rich/markdown.py:633:                    text = token.content
./.venv_tmp/lib/python3.12/site-packages/rich/_emoji_codes.py:156:    "japanese_secret_button": "ãŠ™",
./.venv_tmp/lib/python3.12/site-packages/rich/_emoji_codes.py:2882:    "secret": "ãŠ™",
./.venv_tmp/lib/python3.12/site-packages/rich/text.py:1013:    def append_tokens(self, tokens: Iterable[Tuple[str, Optional[StyleType]]]) -> "Text":
./.venv_tmp/lib/python3.12/site-packages/rich/text.py:1017:            tokens (Iterable[Tuple[str, Optional[StyleType]]]): An iterable of tuples containing str content and style.
./.venv_tmp/lib/python3.12/site-packages/rich/text.py:1026:        for content, style in tokens:
./.venv_tmp/lib/python3.12/site-packages/rich/ansi.py:20:class _AnsiToken(NamedTuple):
./.venv_tmp/lib/python3.12/site-packages/rich/ansi.py:21:    """Result of ansi tokenized string."""
./.venv_tmp/lib/python3.12/site-packages/rich/ansi.py:28:def _ansi_tokenize(ansi_text: str) -> Iterable[_AnsiToken]:
./.venv_tmp/lib/python3.12/site-packages/rich/ansi.py:29:    """Tokenize a string in to plain text and ANSI codes.
./.venv_tmp/lib/python3.12/site-packages/rich/ansi.py:35:        AnsiToken: A named tuple of (plain, sgr, osc)
./.venv_tmp/lib/python3.12/site-packages/rich/ansi.py:45:            yield _AnsiToken(ansi_text[position:start])
./.venv_tmp/lib/python3.12/site-packages/rich/ansi.py:51:                yield _AnsiToken("", sgr[1:-1], osc)
./.venv_tmp/lib/python3.12/site-packages/rich/ansi.py:53:            yield _AnsiToken("", sgr, osc)
./.venv_tmp/lib/python3.12/site-packages/rich/ansi.py:56:        yield _AnsiToken(ansi_text[position:])
./.venv_tmp/lib/python3.12/site-packages/rich/ansi.py:153:        for plain_text, sgr, osc in _ansi_tokenize(line):
./.venv_tmp/lib/python3.12/site-packages/rich/pretty.py:414:    def iter_tokens(self) -> Iterable[str]:
./.venv_tmp/lib/python3.12/site-packages/rich/pretty.py:415:        """Generate tokens for this node."""
./.venv_tmp/lib/python3.12/site-packages/rich/pretty.py:425:                    yield from self.children[0].iter_tokens()
./.venv_tmp/lib/python3.12/site-packages/rich/pretty.py:429:                        yield from child.iter_tokens()
./.venv_tmp/lib/python3.12/site-packages/rich/pretty.py:447:        for token in self.iter_tokens():
./.venv_tmp/lib/python3.12/site-packages/rich/pretty.py:448:            total_length += cell_len(token)
./.venv_tmp/lib/python3.12/site-packages/rich/pretty.py:454:        repr_text = "".join(self.iter_tokens())
./.venv_tmp/lib/python3.12/site-packages/rich/traceback.py:24:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/rich/traceback.py:31:    Text as TextToken,
./.venv_tmp/lib/python3.12/site-packages/rich/traceback.py:32:    Token,
./.venv_tmp/lib/python3.12/site-packages/rich/traceback.py:605:        token_style = theme.get_style_for_token
./.venv_tmp/lib/python3.12/site-packages/rich/traceback.py:609:                "pretty": token_style(TextToken),
./.venv_tmp/lib/python3.12/site-packages/rich/traceback.py:610:                "pygments.text": token_style(Token),
./.venv_tmp/lib/python3.12/site-packages/rich/traceback.py:611:                "pygments.string": token_style(String),
./.venv_tmp/lib/python3.12/site-packages/rich/traceback.py:612:                "pygments.function": token_style(Name.Function),
./.venv_tmp/lib/python3.12/site-packages/rich/traceback.py:613:                "pygments.number": token_style(Number),
./.venv_tmp/lib/python3.12/site-packages/rich/traceback.py:614:                "repr.indent": token_style(Comment) + Style(dim=True),
./.venv_tmp/lib/python3.12/site-packages/rich/traceback.py:615:                "repr.str": token_style(String),
./.venv_tmp/lib/python3.12/site-packages/rich/traceback.py:616:                "repr.brace": token_style(TextToken) + Style(bold=True),
./.venv_tmp/lib/python3.12/site-packages/rich/traceback.py:617:                "repr.number": token_style(Number),
./.venv_tmp/lib/python3.12/site-packages/rich/traceback.py:618:                "repr.bool_true": token_style(Keyword.Constant),
./.venv_tmp/lib/python3.12/site-packages/rich/traceback.py:619:                "repr.bool_false": token_style(Keyword.Constant),
./.venv_tmp/lib/python3.12/site-packages/rich/traceback.py:620:                "repr.none": token_style(Keyword.Constant),
./.venv_tmp/lib/python3.12/site-packages/rich/traceback.py:621:                "scope.border": token_style(String.Delimiter),
./.venv_tmp/lib/python3.12/site-packages/rich/traceback.py:622:                "scope.equals": token_style(Operator),
./.venv_tmp/lib/python3.12/site-packages/rich/traceback.py:623:                "scope.key": token_style(Name),
./.venv_tmp/lib/python3.12/site-packages/rich/traceback.py:624:                "scope.key.special": token_style(Name.Constant) + Style(dim=True),
./.venv_tmp/lib/python3.12/site-packages/urllib3/connectionpool.py:964:    ``ca_cert_dir``, ``ssl_version``, ``key_password`` are only used if :mod:`ssl`
./.venv_tmp/lib/python3.12/site-packages/urllib3/connectionpool.py:986:        key_password: str | None = None,
./.venv_tmp/lib/python3.12/site-packages/urllib3/connectionpool.py:1012:        self.key_password = key_password
./.venv_tmp/lib/python3.12/site-packages/urllib3/connectionpool.py:1063:            key_password=self.key_password,
./.venv_tmp/lib/python3.12/site-packages/urllib3/poolmanager.py:48:    "key_password",
./.venv_tmp/lib/python3.12/site-packages/urllib3/poolmanager.py:72:    key_key_password: str | None
./.venv_tmp/lib/python3.12/site-packages/urllib3/connection.py:393:                f"Method cannot contain non-token characters {method!r} (found at least {match.group()!r})"
./.venv_tmp/lib/python3.12/site-packages/urllib3/connection.py:633:        key_password: str | None = None,
./.venv_tmp/lib/python3.12/site-packages/urllib3/connection.py:648:        self.key_password = key_password
./.venv_tmp/lib/python3.12/site-packages/urllib3/connection.py:674:        key_password: str | None = None,
./.venv_tmp/lib/python3.12/site-packages/urllib3/connection.py:703:        self.key_password = key_password
./.venv_tmp/lib/python3.12/site-packages/urllib3/connection.py:793:                key_password=self.key_password,
./.venv_tmp/lib/python3.12/site-packages/urllib3/connection.py:870:            key_password=None,
./.venv_tmp/lib/python3.12/site-packages/urllib3/connection.py:896:    key_password: str | None,
./.venv_tmp/lib/python3.12/site-packages/urllib3/connection.py:965:        key_password=key_password,
./.venv_tmp/lib/python3.12/site-packages/urllib3/util/timeout.py:17:    token = -1
./.venv_tmp/lib/python3.12/site-packages/urllib3/util/timeout.py:20:_DEFAULT_TIMEOUT: Final[_TYPE_DEFAULT] = _TYPE_DEFAULT.token
./.venv_tmp/lib/python3.12/site-packages/urllib3/util/request.py:48:    token = 0
./.venv_tmp/lib/python3.12/site-packages/urllib3/util/request.py:51:_FAILEDTELL: Final[_TYPE_FAILEDTELL] = _TYPE_FAILEDTELL.token
./.venv_tmp/lib/python3.12/site-packages/urllib3/util/request.py:91:        Colon-separated username:password string for 'authorization: basic ...'
./.venv_tmp/lib/python3.12/site-packages/urllib3/util/request.py:95:        Colon-separated username:password string for 'proxy-authorization: basic ...'
./.venv_tmp/lib/python3.12/site-packages/urllib3/util/ssl_.py:386:    key_password: str | None = ...,
./.venv_tmp/lib/python3.12/site-packages/urllib3/util/ssl_.py:404:    key_password: str | None = ...,
./.venv_tmp/lib/python3.12/site-packages/urllib3/util/ssl_.py:421:    key_password: str | None = None,
./.venv_tmp/lib/python3.12/site-packages/urllib3/util/ssl_.py:442:    :param key_password:
./.venv_tmp/lib/python3.12/site-packages/urllib3/util/ssl_.py:443:        Optional password if the keyfile is encrypted.
./.venv_tmp/lib/python3.12/site-packages/urllib3/util/ssl_.py:469:    if keyfile and key_password is None and _is_key_file_encrypted(keyfile):
./.venv_tmp/lib/python3.12/site-packages/urllib3/util/ssl_.py:470:        raise SSLError("Client private key is encrypted, password is required")
./.venv_tmp/lib/python3.12/site-packages/urllib3/util/ssl_.py:473:        if key_password is None:
./.venv_tmp/lib/python3.12/site-packages/urllib3/util/ssl_.py:476:            context.load_cert_chain(certfile, keyfile, key_password)
./.venv_tmp/lib/python3.12/site-packages/urllib3/util/url.py:180:            print( urllib3.util.Url("https", "username:password",
./.venv_tmp/lib/python3.12/site-packages/urllib3/util/url.py:184:            # "https://username:password@host.com:80/path?query#fragment"
./.venv_tmp/lib/python3.12/site-packages/urllib3/_base_connection.py:136:        key_password: str | None
./.venv_tmp/lib/python3.12/site-packages/urllib3/_base_connection.py:162:            key_password: str | None = None,
./.venv_tmp/lib/python3.12/site-packages/urllib3/contrib/pyopenssl.py:471:        password: str | None = None,
./.venv_tmp/lib/python3.12/site-packages/urllib3/contrib/pyopenssl.py:475:            if password is not None:
./.venv_tmp/lib/python3.12/site-packages/urllib3/contrib/pyopenssl.py:476:                if not isinstance(password, bytes):
./.venv_tmp/lib/python3.12/site-packages/urllib3/contrib/pyopenssl.py:477:                    password = password.encode("utf-8")  # type: ignore[assignment]
./.venv_tmp/lib/python3.12/site-packages/urllib3/contrib/pyopenssl.py:478:                self._ctx.set_passwd_cb(lambda *_: password)
./.venv_tmp/lib/python3.12/site-packages/urllib3/contrib/emscripten/connection.py:168:    key_password: str | None
./.venv_tmp/lib/python3.12/site-packages/urllib3/contrib/emscripten/connection.py:200:        key_password: str | None = None,
./.venv_tmp/lib/python3.12/site-packages/urllib3/contrib/emscripten/connection.py:216:        self.key_password = key_password
./.venv_tmp/lib/python3.12/site-packages/urllib3/contrib/emscripten/connection.py:239:        key_password: str | None = None,
./.venv_tmp/lib/python3.12/site-packages/urllib3/contrib/socks.py:14:- Usernames and passwords for the SOCKS proxy
./.venv_tmp/lib/python3.12/site-packages/urllib3/contrib/socks.py:31:When connecting to a SOCKS5 proxy the ``username`` and ``password`` portion
./.venv_tmp/lib/python3.12/site-packages/urllib3/contrib/socks.py:32:of the ``proxy_url`` will be sent as the username/password to authenticate
./.venv_tmp/lib/python3.12/site-packages/urllib3/contrib/socks.py:37:    proxy_url="socks5h://<username>:<password>@proxy-host"
./.venv_tmp/lib/python3.12/site-packages/urllib3/contrib/socks.py:80:    password: str | None
./.venv_tmp/lib/python3.12/site-packages/urllib3/contrib/socks.py:116:                proxy_password=self._socks_options["password"],
./.venv_tmp/lib/python3.12/site-packages/urllib3/contrib/socks.py:182:        password: str | None = None,
./.venv_tmp/lib/python3.12/site-packages/urllib3/contrib/socks.py:189:        if username is None and password is None and parsed.auth is not None:
./.venv_tmp/lib/python3.12/site-packages/urllib3/contrib/socks.py:192:                username, password = split
./.venv_tmp/lib/python3.12/site-packages/urllib3/contrib/socks.py:215:            "password": password,
./.venv_tmp/lib/python3.12/site-packages/packaging/markers.py:21:from ._tokenizer import ParserSyntaxError
./.venv_tmp/lib/python3.12/site-packages/packaging/_tokenizer.py:12:class Token:
./.venv_tmp/lib/python3.12/site-packages/packaging/_tokenizer.py:91:class Tokenizer:
./.venv_tmp/lib/python3.12/site-packages/packaging/_tokenizer.py:92:    """Context-sensitive token parsing.
./.venv_tmp/lib/python3.12/site-packages/packaging/_tokenizer.py:94:    Provides methods to examine the input stream to check whether the next token
./.venv_tmp/lib/python3.12/site-packages/packaging/_tokenizer.py:108:        self.next_token: Token | None = None
./.venv_tmp/lib/python3.12/site-packages/packaging/_tokenizer.py:112:        """Move beyond provided token name, if at current position."""
./.venv_tmp/lib/python3.12/site-packages/packaging/_tokenizer.py:117:        """Check whether the next token has the provided name.
./.venv_tmp/lib/python3.12/site-packages/packaging/_tokenizer.py:119:        By default, if the check succeeds, the token *must* be read before
./.venv_tmp/lib/python3.12/site-packages/packaging/_tokenizer.py:120:        another check. If `peek` is set to `True`, the token is not loaded and
./.venv_tmp/lib/python3.12/site-packages/packaging/_tokenizer.py:124:            self.next_token is None
./.venv_tmp/lib/python3.12/site-packages/packaging/_tokenizer.py:125:        ), f"Cannot check for {name!r}, already have {self.next_token!r}"
./.venv_tmp/lib/python3.12/site-packages/packaging/_tokenizer.py:126:        assert name in self.rules, f"Unknown token name: {name!r}"
./.venv_tmp/lib/python3.12/site-packages/packaging/_tokenizer.py:134:            self.next_token = Token(name, match[0], self.position)
./.venv_tmp/lib/python3.12/site-packages/packaging/_tokenizer.py:137:    def expect(self, name: str, *, expected: str) -> Token:
./.venv_tmp/lib/python3.12/site-packages/packaging/_tokenizer.py:138:        """Expect a certain token name next, failing with a syntax error otherwise.
./.venv_tmp/lib/python3.12/site-packages/packaging/_tokenizer.py:140:        The token is *not* read.
./.venv_tmp/lib/python3.12/site-packages/packaging/_tokenizer.py:146:    def read(self) -> Token:
./.venv_tmp/lib/python3.12/site-packages/packaging/_tokenizer.py:147:        """Consume the next token and return it."""
./.venv_tmp/lib/python3.12/site-packages/packaging/_tokenizer.py:148:        token = self.next_token
./.venv_tmp/lib/python3.12/site-packages/packaging/_tokenizer.py:149:        assert token is not None
./.venv_tmp/lib/python3.12/site-packages/packaging/_tokenizer.py:151:        self.position += len(token.text)
./.venv_tmp/lib/python3.12/site-packages/packaging/_tokenizer.py:152:        self.next_token = None
./.venv_tmp/lib/python3.12/site-packages/packaging/_tokenizer.py:154:        return token
./.venv_tmp/lib/python3.12/site-packages/packaging/_tokenizer.py:175:    def enclosing_tokens(self, open_token: str, close_token: str, *, around: str) -> Iterator[None]:
./.venv_tmp/lib/python3.12/site-packages/packaging/_tokenizer.py:176:        if self.check(open_token):
./.venv_tmp/lib/python3.12/site-packages/packaging/_tokenizer.py:187:        if not self.check(close_token):
./.venv_tmp/lib/python3.12/site-packages/packaging/_tokenizer.py:189:                f"Expected matching {close_token} for {open_token}, after {around}",
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:12:from ._tokenizer import DEFAULT_RULES, Tokenizer
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:62:    return _parse_requirement(Tokenizer(source, rules=DEFAULT_RULES))
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:65:def _parse_requirement(tokenizer: Tokenizer) -> ParsedRequirement:
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:69:    tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:71:    name_token = tokenizer.expect(
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:74:    name = name_token.text
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:75:    tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:77:    extras = _parse_extras(tokenizer)
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:78:    tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:80:    url, specifier, marker = _parse_requirement_details(tokenizer)
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:81:    tokenizer.expect("END", expected="end of dependency specifier")
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:87:    tokenizer: Tokenizer,
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:98:    if tokenizer.check("AT"):
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:99:        tokenizer.read()
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:100:        tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:102:        url_start = tokenizer.position
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:103:        url = tokenizer.expect("URL", expected="URL after @").text
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:104:        if tokenizer.check("END", peek=True):
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:107:        tokenizer.expect("WS", expected="whitespace after URL")
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:110:        if tokenizer.check("END", peek=True):
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:114:            tokenizer, span_start=url_start, after="URL and whitespace"
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:117:        specifier_start = tokenizer.position
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:118:        specifier = _parse_specifier(tokenizer)
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:119:        tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:121:        if tokenizer.check("END", peek=True):
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:125:            tokenizer,
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:133:def _parse_requirement_marker(tokenizer: Tokenizer, *, span_start: int, after: str) -> MarkerList:
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:138:    if not tokenizer.check("SEMICOLON"):
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:139:        tokenizer.raise_syntax_error(
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:143:    tokenizer.read()
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:145:    marker = _parse_marker(tokenizer)
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:146:    tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:151:def _parse_extras(tokenizer: Tokenizer) -> list[str]:
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:155:    if not tokenizer.check("LEFT_BRACKET", peek=True):
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:158:    with tokenizer.enclosing_tokens(
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:163:        tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:164:        extras = _parse_extras_list(tokenizer)
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:165:        tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:170:def _parse_extras_list(tokenizer: Tokenizer) -> list[str]:
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:176:    if not tokenizer.check("IDENTIFIER"):
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:179:    extras.append(tokenizer.read().text)
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:182:        tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:183:        if tokenizer.check("IDENTIFIER", peek=True):
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:184:            tokenizer.raise_syntax_error("Expected comma between extra names")
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:185:        elif not tokenizer.check("COMMA"):
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:188:        tokenizer.read()
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:189:        tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:191:        extra_token = tokenizer.expect("IDENTIFIER", expected="extra name after comma")
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:192:        extras.append(extra_token.text)
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:197:def _parse_specifier(tokenizer: Tokenizer) -> str:
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:202:    with tokenizer.enclosing_tokens(
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:207:        tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:208:        parsed_specifiers = _parse_version_many(tokenizer)
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:209:        tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:214:def _parse_version_many(tokenizer: Tokenizer) -> str:
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:219:    while tokenizer.check("SPECIFIER"):
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:220:        span_start = tokenizer.position
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:221:        parsed_specifiers += tokenizer.read().text
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:222:        if tokenizer.check("VERSION_PREFIX_TRAIL", peek=True):
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:223:            tokenizer.raise_syntax_error(
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:226:                span_end=tokenizer.position + 1,
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:228:        if tokenizer.check("VERSION_LOCAL_LABEL_TRAIL", peek=True):
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:229:            tokenizer.raise_syntax_error(
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:232:                span_end=tokenizer.position,
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:234:        tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:235:        if not tokenizer.check("COMMA"):
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:237:        parsed_specifiers += tokenizer.read().text
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:238:        tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:247:    return _parse_full_marker(Tokenizer(source, rules=DEFAULT_RULES))
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:250:def _parse_full_marker(tokenizer: Tokenizer) -> MarkerList:
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:251:    retval = _parse_marker(tokenizer)
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:252:    tokenizer.expect("END", expected="end of marker expression")
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:256:def _parse_marker(tokenizer: Tokenizer) -> MarkerList:
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:260:    expression = [_parse_marker_atom(tokenizer)]
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:261:    while tokenizer.check("BOOLOP"):
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:262:        token = tokenizer.read()
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:263:        expr_right = _parse_marker_atom(tokenizer)
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:264:        expression.extend((token.text, expr_right))
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:268:def _parse_marker_atom(tokenizer: Tokenizer) -> MarkerAtom:
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:274:    tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:275:    if tokenizer.check("LEFT_PARENTHESIS", peek=True):
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:276:        with tokenizer.enclosing_tokens(
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:281:            tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:282:            marker: MarkerAtom = _parse_marker(tokenizer)
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:283:            tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:285:        marker = _parse_marker_item(tokenizer)
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:286:    tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:290:def _parse_marker_item(tokenizer: Tokenizer) -> MarkerItem:
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:294:    tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:295:    marker_var_left = _parse_marker_var(tokenizer)
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:296:    tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:297:    marker_op = _parse_marker_op(tokenizer)
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:298:    tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:299:    marker_var_right = _parse_marker_var(tokenizer)
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:300:    tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:304:def _parse_marker_var(tokenizer: Tokenizer) -> MarkerVar:
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:308:    if tokenizer.check("VARIABLE"):
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:309:        return process_env_var(tokenizer.read().text.replace(".", "_"))
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:310:    elif tokenizer.check("QUOTED_STRING"):
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:311:        return process_python_str(tokenizer.read().text)
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:313:        tokenizer.raise_syntax_error(message="Expected a marker variable or quoted string")
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:328:def _parse_marker_op(tokenizer: Tokenizer) -> Op:
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:332:    if tokenizer.check("IN"):
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:333:        tokenizer.read()
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:335:    elif tokenizer.check("NOT"):
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:336:        tokenizer.read()
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:337:        tokenizer.expect("WS", expected="whitespace after 'not'")
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:338:        tokenizer.expect("IN", expected="'in' after 'not'")
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:340:    elif tokenizer.check("OP"):
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:341:        return Op(tokenizer.read().text)
./.venv_tmp/lib/python3.12/site-packages/packaging/_parser.py:343:        return tokenizer.raise_syntax_error(
./.venv_tmp/lib/python3.12/site-packages/packaging/licenses/__init__.py:67:    # Pad any parentheses so tokenization can be achieved by merely splitting on
./.venv_tmp/lib/python3.12/site-packages/packaging/licenses/__init__.py:81:    tokens = license_expression.split()
./.venv_tmp/lib/python3.12/site-packages/packaging/licenses/__init__.py:86:    python_tokens = []
./.venv_tmp/lib/python3.12/site-packages/packaging/licenses/__init__.py:87:    for token in tokens:
./.venv_tmp/lib/python3.12/site-packages/packaging/licenses/__init__.py:88:        if token not in {"or", "and", "with", "(", ")"}:
./.venv_tmp/lib/python3.12/site-packages/packaging/licenses/__init__.py:89:            python_tokens.append("False")
./.venv_tmp/lib/python3.12/site-packages/packaging/licenses/__init__.py:90:        elif token == "with":
./.venv_tmp/lib/python3.12/site-packages/packaging/licenses/__init__.py:91:            python_tokens.append("or")
./.venv_tmp/lib/python3.12/site-packages/packaging/licenses/__init__.py:92:        elif token == "(" and python_tokens and python_tokens[-1] not in {"or", "and"}:
./.venv_tmp/lib/python3.12/site-packages/packaging/licenses/__init__.py:96:            python_tokens.append(token)
./.venv_tmp/lib/python3.12/site-packages/packaging/licenses/__init__.py:98:    python_expression = " ".join(python_tokens)
./.venv_tmp/lib/python3.12/site-packages/packaging/licenses/__init__.py:109:    normalized_tokens = []
./.venv_tmp/lib/python3.12/site-packages/packaging/licenses/__init__.py:110:    for token in tokens:
./.venv_tmp/lib/python3.12/site-packages/packaging/licenses/__init__.py:111:        if token in {"or", "and", "with", "(", ")"}:
./.venv_tmp/lib/python3.12/site-packages/packaging/licenses/__init__.py:112:            normalized_tokens.append(token.upper())
./.venv_tmp/lib/python3.12/site-packages/packaging/licenses/__init__.py:115:        if normalized_tokens and normalized_tokens[-1] == "WITH":
./.venv_tmp/lib/python3.12/site-packages/packaging/licenses/__init__.py:116:            if token not in EXCEPTIONS:
./.venv_tmp/lib/python3.12/site-packages/packaging/licenses/__init__.py:117:                message = f"Unknown license exception: {token!r}"
./.venv_tmp/lib/python3.12/site-packages/packaging/licenses/__init__.py:120:            normalized_tokens.append(EXCEPTIONS[token]["id"])
./.venv_tmp/lib/python3.12/site-packages/packaging/licenses/__init__.py:122:            if token.endswith("+"):
./.venv_tmp/lib/python3.12/site-packages/packaging/licenses/__init__.py:123:                final_token = token[:-1]
./.venv_tmp/lib/python3.12/site-packages/packaging/licenses/__init__.py:126:                final_token = token
./.venv_tmp/lib/python3.12/site-packages/packaging/licenses/__init__.py:129:            if final_token.startswith("licenseref-"):
./.venv_tmp/lib/python3.12/site-packages/packaging/licenses/__init__.py:130:                if not license_ref_allowed.match(final_token):
./.venv_tmp/lib/python3.12/site-packages/packaging/licenses/__init__.py:131:                    message = f"Invalid licenseref: {final_token!r}"
./.venv_tmp/lib/python3.12/site-packages/packaging/licenses/__init__.py:133:                normalized_tokens.append(license_refs[final_token] + suffix)
./.venv_tmp/lib/python3.12/site-packages/packaging/licenses/__init__.py:135:                if final_token not in LICENSES:
./.venv_tmp/lib/python3.12/site-packages/packaging/licenses/__init__.py:136:                    message = f"Unknown license: {final_token!r}"
./.venv_tmp/lib/python3.12/site-packages/packaging/licenses/__init__.py:138:                normalized_tokens.append(LICENSES[final_token]["id"] + suffix)
./.venv_tmp/lib/python3.12/site-packages/packaging/licenses/__init__.py:140:    normalized_expression = " ".join(normalized_tokens)
./.venv_tmp/lib/python3.12/site-packages/packaging/requirements.py:9:from ._tokenizer import ParserSyntaxError
./.venv_tmp/lib/python3.12/site-packages/iniconfig/_parse.py:37:    tokens = parse_lines(
./.venv_tmp/lib/python3.12/site-packages/iniconfig/_parse.py:47:    for lineno, section, name, value in tokens:
./.venv_tmp/lib/python3.12/site-packages/mypyc/test/testutil.py:245:            # get 'WORDSIZE*n' token
./.venv_tmp/lib/python3.12/site-packages/mypyc/test/testutil.py:246:            word_size_token = line[index:].split()[0]
./.venv_tmp/lib/python3.12/site-packages/mypyc/test/testutil.py:247:            n = int(word_size_token[10:])
./.venv_tmp/lib/python3.12/site-packages/mypyc/test/testutil.py:249:            result.append(line.replace(word_size_token, replace_str))
./.venv_tmp/lib/python3.12/site-packages/mypyc/irbuild/specialize.py:82:from mypyc.irbuild.format_str_tokenizer import (
./.venv_tmp/lib/python3.12/site-packages/mypyc/irbuild/specialize.py:86:    tokenizer_format_call,
./.venv_tmp/lib/python3.12/site-packages/mypyc/irbuild/specialize.py:664:        tokens = tokenizer_format_call(format_str)
./.venv_tmp/lib/python3.12/site-packages/mypyc/irbuild/specialize.py:665:        if tokens is None:
./.venv_tmp/lib/python3.12/site-packages/mypyc/irbuild/specialize.py:667:        literals, format_ops = tokens
./.venv_tmp/lib/python3.12/site-packages/mypyc/irbuild/format_str_tokenizer.py:1:"""Tokenizers for three string formatting methods"""
./.venv_tmp/lib/python3.12/site-packages/mypyc/irbuild/format_str_tokenizer.py:69:def tokenizer_printf_style(format_str: str) -> tuple[list[str], list[FormatOp]] | None:
./.venv_tmp/lib/python3.12/site-packages/mypyc/irbuild/format_str_tokenizer.py:70:    """Tokenize a printf-style format string using regex.
./.venv_tmp/lib/python3.12/site-packages/mypyc/irbuild/format_str_tokenizer.py:96:def tokenizer_format_call(format_str: str) -> tuple[list[str], list[FormatOp]] | None:
./.venv_tmp/lib/python3.12/site-packages/mypyc/irbuild/format_str_tokenizer.py:97:    """Tokenize a str.format() format string.
./.venv_tmp/lib/python3.12/site-packages/mypyc/irbuild/expression.py:94:from mypyc.irbuild.format_str_tokenizer import (
./.venv_tmp/lib/python3.12/site-packages/mypyc/irbuild/expression.py:99:    tokenizer_printf_style,
./.venv_tmp/lib/python3.12/site-packages/mypyc/irbuild/expression.py:930:    tokens = tokenizer_printf_style(format_expr.value)
./.venv_tmp/lib/python3.12/site-packages/mypyc/irbuild/expression.py:931:    if tokens is not None:
./.venv_tmp/lib/python3.12/site-packages/mypyc/irbuild/expression.py:932:        literals, format_ops = tokens
./.venv_tmp/lib/python3.12/site-packages/requests/auth.py:25:def _basic_auth_str(username, password):
./.venv_tmp/lib/python3.12/site-packages/requests/auth.py:45:    if not isinstance(password, basestring):
./.venv_tmp/lib/python3.12/site-packages/requests/auth.py:47:            "Non-string passwords will no longer be supported in Requests "
./.venv_tmp/lib/python3.12/site-packages/requests/auth.py:50:            "problems.".format(type(password)),
./.venv_tmp/lib/python3.12/site-packages/requests/auth.py:53:        password = str(password)
./.venv_tmp/lib/python3.12/site-packages/requests/auth.py:59:    if isinstance(password, str):
./.venv_tmp/lib/python3.12/site-packages/requests/auth.py:60:        password = password.encode("latin1")
./.venv_tmp/lib/python3.12/site-packages/requests/auth.py:62:    authstr = "Basic " + to_native_string(b64encode(b":".join((username, password))).strip())
./.venv_tmp/lib/python3.12/site-packages/requests/auth.py:77:    def __init__(self, username, password):
./.venv_tmp/lib/python3.12/site-packages/requests/auth.py:79:        self.password = password
./.venv_tmp/lib/python3.12/site-packages/requests/auth.py:85:                self.password == getattr(other, "password", None),
./.venv_tmp/lib/python3.12/site-packages/requests/auth.py:93:        r.headers["Authorization"] = _basic_auth_str(self.username, self.password)
./.venv_tmp/lib/python3.12/site-packages/requests/auth.py:101:        r.headers["Proxy-Authorization"] = _basic_auth_str(self.username, self.password)
./.venv_tmp/lib/python3.12/site-packages/requests/auth.py:108:    def __init__(self, username, password):
./.venv_tmp/lib/python3.12/site-packages/requests/auth.py:110:        self.password = password
./.venv_tmp/lib/python3.12/site-packages/requests/auth.py:187:        A1 = f"{self.username}:{realm}:{self.password}"
./.venv_tmp/lib/python3.12/site-packages/requests/auth.py:305:                self.password == getattr(other, "password", None),
./.venv_tmp/lib/python3.12/site-packages/requests/utils.py:235:                # Return with login / password
./.venv_tmp/lib/python3.12/site-packages/requests/utils.py:376:    >>> parse_list_header('token, "quoted value"')
./.venv_tmp/lib/python3.12/site-packages/requests/utils.py:377:    ['token', 'quoted value']
./.venv_tmp/lib/python3.12/site-packages/requests/utils.py:506:    tokens = header.split(";")
./.venv_tmp/lib/python3.12/site-packages/requests/utils.py:507:    content_type, params = tokens[0].strip(), tokens[1:]
./.venv_tmp/lib/python3.12/site-packages/requests/utils.py:1004:    username,password.
./.venv_tmp/lib/python3.12/site-packages/requests/utils.py:1011:        auth = (unquote(parsed.username), unquote(parsed.password))
./.venv_tmp/lib/python3.12/site-packages/requests/sessions.py:317:            username, password = get_auth_from_url(new_proxies[scheme])
./.venv_tmp/lib/python3.12/site-packages/requests/sessions.py:319:            username, password = None, None
./.venv_tmp/lib/python3.12/site-packages/requests/sessions.py:323:        if not scheme.startswith("https") and username and password:
./.venv_tmp/lib/python3.12/site-packages/requests/sessions.py:324:            headers["Proxy-Authorization"] = _basic_auth_str(username, password)
./.venv_tmp/lib/python3.12/site-packages/requests/adapters.py:252:            username, password = get_auth_from_url(proxy)
./.venv_tmp/lib/python3.12/site-packages/requests/adapters.py:256:                password=password,
./.venv_tmp/lib/python3.12/site-packages/requests/adapters.py:569:        username, password = get_auth_from_url(proxy)
./.venv_tmp/lib/python3.12/site-packages/requests/adapters.py:572:            headers["Proxy-Authorization"] = _basic_auth_str(username, password)
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:66:from .tokens import *
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:131:        token = self.get_token()
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:132:        event = StreamStartEvent(token.start_mark, token.end_mark, encoding=token.encoding)
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:142:        if not self.check_token(DirectiveToken, DocumentStartToken, StreamEndToken):
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:144:            token = self.peek_token()
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:145:            start_mark = end_mark = token.start_mark
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:160:        while self.check_token(DocumentEndToken):
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:161:            self.get_token()
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:164:        if not self.check_token(StreamEndToken):
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:165:            token = self.peek_token()
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:166:            start_mark = token.start_mark
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:168:            if not self.check_token(DocumentStartToken):
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:172:                    "expected '<document start>', but found %r" % self.peek_token().id,
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:173:                    self.peek_token().start_mark,
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:175:            token = self.get_token()
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:176:            end_mark = token.end_mark
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:184:            token = self.get_token()
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:185:            event = StreamEndEvent(token.start_mark, token.end_mark)
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:194:        token = self.peek_token()
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:195:        start_mark = end_mark = token.start_mark
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:197:        if self.check_token(DocumentEndToken):
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:198:            token = self.get_token()
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:199:            end_mark = token.end_mark
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:209:        if self.check_token(DirectiveToken, DocumentStartToken, DocumentEndToken, StreamEndToken):
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:210:            event = self.process_empty_scalar(self.peek_token().start_mark)
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:219:        while self.check_token(DirectiveToken):
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:220:            token = self.get_token()
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:221:            if token.name == "YAML":
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:224:                        None, None, "found duplicate YAML directive", token.start_mark
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:226:                major, minor = token.value
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:232:                        token.start_mark,
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:234:                self.yaml_version = token.value
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:235:            elif token.name == "TAG":
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:236:                handle, prefix = token.value
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:239:                        None, None, "duplicate tag handle %r" % handle, token.start_mark
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:277:        if self.check_token(AliasToken):
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:278:            token = self.get_token()
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:279:            event = AliasEvent(token.value, token.start_mark, token.end_mark)
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:285:            if self.check_token(AnchorToken):
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:286:                token = self.get_token()
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:287:                start_mark = token.start_mark
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:288:                end_mark = token.end_mark
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:289:                anchor = token.value
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:290:                if self.check_token(TagToken):
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:291:                    token = self.get_token()
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:292:                    tag_mark = token.start_mark
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:293:                    end_mark = token.end_mark
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:294:                    tag = token.value
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:295:            elif self.check_token(TagToken):
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:296:                token = self.get_token()
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:297:                start_mark = tag_mark = token.start_mark
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:298:                end_mark = token.end_mark
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:299:                tag = token.value
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:300:                if self.check_token(AnchorToken):
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:301:                    token = self.get_token()
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:302:                    end_mark = token.end_mark
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:303:                    anchor = token.value
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:322:                start_mark = end_mark = self.peek_token().start_mark
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:325:            if indentless_sequence and self.check_token(BlockEntryToken):
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:326:                end_mark = self.peek_token().end_mark
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:330:                if self.check_token(ScalarToken):
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:331:                    token = self.get_token()
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:332:                    end_mark = token.end_mark
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:333:                    if (token.plain and tag is None) or tag == "!":
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:340:                        anchor, tag, implicit, token.value, start_mark, end_mark, style=token.style
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:343:                elif self.check_token(FlowSequenceStartToken):
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:344:                    end_mark = self.peek_token().end_mark
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:349:                elif self.check_token(FlowMappingStartToken):
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:350:                    end_mark = self.peek_token().end_mark
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:355:                elif block and self.check_token(BlockSequenceStartToken):
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:356:                    end_mark = self.peek_token().start_mark
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:361:                elif block and self.check_token(BlockMappingStartToken):
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:362:                    end_mark = self.peek_token().start_mark
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:377:                    token = self.peek_token()
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:381:                        "expected the node content, but found %r" % token.id,
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:382:                        token.start_mark,
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:389:        token = self.get_token()
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:390:        self.marks.append(token.start_mark)
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:394:        if self.check_token(BlockEntryToken):
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:395:            token = self.get_token()
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:396:            if not self.check_token(BlockEntryToken, BlockEndToken):
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:401:                return self.process_empty_scalar(token.end_mark)
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:402:        if not self.check_token(BlockEndToken):
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:403:            token = self.peek_token()
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:407:                "expected <block end>, but found %r" % token.id,
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:408:                token.start_mark,
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:410:        token = self.get_token()
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:411:        event = SequenceEndEvent(token.start_mark, token.end_mark)
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:419:        if self.check_token(BlockEntryToken):
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:420:            token = self.get_token()
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:421:            if not self.check_token(BlockEntryToken, KeyToken, ValueToken, BlockEndToken):
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:426:                return self.process_empty_scalar(token.end_mark)
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:427:        token = self.peek_token()
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:428:        event = SequenceEndEvent(token.start_mark, token.start_mark)
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:438:        token = self.get_token()
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:439:        self.marks.append(token.start_mark)
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:443:        if self.check_token(KeyToken):
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:444:            token = self.get_token()
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:445:            if not self.check_token(KeyToken, ValueToken, BlockEndToken):
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:450:                return self.process_empty_scalar(token.end_mark)
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:451:        if not self.check_token(BlockEndToken):
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:452:            token = self.peek_token()
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:456:                "expected <block end>, but found %r" % token.id,
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:457:                token.start_mark,
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:459:        token = self.get_token()
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:460:        event = MappingEndEvent(token.start_mark, token.end_mark)
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:466:        if self.check_token(ValueToken):
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:467:            token = self.get_token()
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:468:            if not self.check_token(KeyToken, ValueToken, BlockEndToken):
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:473:                return self.process_empty_scalar(token.end_mark)
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:476:            token = self.peek_token()
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:477:            return self.process_empty_scalar(token.start_mark)
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:491:        token = self.get_token()
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:492:        self.marks.append(token.start_mark)
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:496:        if not self.check_token(FlowSequenceEndToken):
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:498:                if self.check_token(FlowEntryToken):
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:499:                    self.get_token()
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:501:                    token = self.peek_token()
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:505:                        "expected ',' or ']', but got %r" % token.id,
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:506:                        token.start_mark,
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:509:            if self.check_token(KeyToken):
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:510:                token = self.peek_token()
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:512:                    None, None, True, token.start_mark, token.end_mark, flow_style=True
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:516:            elif not self.check_token(FlowSequenceEndToken):
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:519:        token = self.get_token()
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:520:        event = SequenceEndEvent(token.start_mark, token.end_mark)
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:526:        token = self.get_token()
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:527:        if not self.check_token(ValueToken, FlowEntryToken, FlowSequenceEndToken):
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:532:            return self.process_empty_scalar(token.end_mark)
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:535:        if self.check_token(ValueToken):
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:536:            token = self.get_token()
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:537:            if not self.check_token(FlowEntryToken, FlowSequenceEndToken):
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:542:                return self.process_empty_scalar(token.end_mark)
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:545:            token = self.peek_token()
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:546:            return self.process_empty_scalar(token.start_mark)
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:550:        token = self.peek_token()
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:551:        return MappingEndEvent(token.start_mark, token.start_mark)
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:560:        token = self.get_token()
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:561:        self.marks.append(token.start_mark)
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:565:        if not self.check_token(FlowMappingEndToken):
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:567:                if self.check_token(FlowEntryToken):
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:568:                    self.get_token()
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:570:                    token = self.peek_token()
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:574:                        "expected ',' or '}', but got %r" % token.id,
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:575:                        token.start_mark,
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:577:            if self.check_token(KeyToken):
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:578:                token = self.get_token()
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:579:                if not self.check_token(ValueToken, FlowEntryToken, FlowMappingEndToken):
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:584:                    return self.process_empty_scalar(token.end_mark)
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:585:            elif not self.check_token(FlowMappingEndToken):
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:588:        token = self.get_token()
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:589:        event = MappingEndEvent(token.start_mark, token.end_mark)
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:595:        if self.check_token(ValueToken):
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:596:            token = self.get_token()
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:597:            if not self.check_token(FlowEntryToken, FlowMappingEndToken):
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:602:                return self.process_empty_scalar(token.end_mark)
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:605:            token = self.peek_token()
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:606:            return self.process_empty_scalar(token.start_mark)
./.venv_tmp/lib/python3.12/site-packages/yaml/parser.py:610:        return self.process_empty_scalar(self.peek_token().start_mark)
./.venv_tmp/lib/python3.12/site-packages/yaml/tokens.py:1:class Token(object):
./.venv_tmp/lib/python3.12/site-packages/yaml/tokens.py:13:# class BOMToken(Token):
./.venv_tmp/lib/python3.12/site-packages/yaml/tokens.py:17:class DirectiveToken(Token):
./.venv_tmp/lib/python3.12/site-packages/yaml/tokens.py:27:class DocumentStartToken(Token):
./.venv_tmp/lib/python3.12/site-packages/yaml/tokens.py:31:class DocumentEndToken(Token):
./.venv_tmp/lib/python3.12/site-packages/yaml/tokens.py:35:class StreamStartToken(Token):
./.venv_tmp/lib/python3.12/site-packages/yaml/tokens.py:44:class StreamEndToken(Token):
./.venv_tmp/lib/python3.12/site-packages/yaml/tokens.py:48:class BlockSequenceStartToken(Token):
./.venv_tmp/lib/python3.12/site-packages/yaml/tokens.py:52:class BlockMappingStartToken(Token):
./.venv_tmp/lib/python3.12/site-packages/yaml/tokens.py:56:class BlockEndToken(Token):
./.venv_tmp/lib/python3.12/site-packages/yaml/tokens.py:60:class FlowSequenceStartToken(Token):
./.venv_tmp/lib/python3.12/site-packages/yaml/tokens.py:64:class FlowMappingStartToken(Token):
./.venv_tmp/lib/python3.12/site-packages/yaml/tokens.py:68:class FlowSequenceEndToken(Token):
./.venv_tmp/lib/python3.12/site-packages/yaml/tokens.py:72:class FlowMappingEndToken(Token):
./.venv_tmp/lib/python3.12/site-packages/yaml/tokens.py:76:class KeyToken(Token):
./.venv_tmp/lib/python3.12/site-packages/yaml/tokens.py:80:class ValueToken(Token):
./.venv_tmp/lib/python3.12/site-packages/yaml/tokens.py:84:class BlockEntryToken(Token):
./.venv_tmp/lib/python3.12/site-packages/yaml/tokens.py:88:class FlowEntryToken(Token):
./.venv_tmp/lib/python3.12/site-packages/yaml/tokens.py:92:class AliasToken(Token):
./.venv_tmp/lib/python3.12/site-packages/yaml/tokens.py:101:class AnchorToken(Token):
./.venv_tmp/lib/python3.12/site-packages/yaml/tokens.py:110:class TagToken(Token):
./.venv_tmp/lib/python3.12/site-packages/yaml/tokens.py:119:class ScalarToken(Token):
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:1:# Scanner produces tokens of the following types:
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:29:from .tokens import *
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:39:    def __init__(self, token_number, required, index, line, column, mark):
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:40:        self.token_number = token_number
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:68:        # List of processed tokens that are not yet emitted.
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:69:        self.tokens = []
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:71:        # Add the STREAM-START token.
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:74:        # Number of tokens that were emitted through the `get_token` method.
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:75:        self.tokens_taken = 0
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:91:        # We emit the KEY token before all keys, so when we find a potential
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:108:        #   (token_number, required, index, line, column, mark)
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:110:        # '[', or '{' tokens.
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:115:    def check_token(self, *choices):
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:116:        # Check if the next token is one of the given types.
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:117:        while self.need_more_tokens():
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:118:            self.fetch_more_tokens()
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:119:        if self.tokens:
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:123:                if isinstance(self.tokens[0], choice):
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:127:    def peek_token(self):
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:128:        # Return the next token, but do not delete if from the queue.
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:129:        # Return None if no more tokens.
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:130:        while self.need_more_tokens():
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:131:            self.fetch_more_tokens()
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:132:        if self.tokens:
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:133:            return self.tokens[0]
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:137:    def get_token(self):
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:138:        # Return the next token.
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:139:        while self.need_more_tokens():
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:140:            self.fetch_more_tokens()
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:141:        if self.tokens:
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:142:            self.tokens_taken += 1
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:143:            return self.tokens.pop(0)
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:147:    def need_more_tokens(self):
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:150:        if not self.tokens:
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:152:        # The current token may be a potential simple key, so we
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:155:        if self.next_possible_simple_key() == self.tokens_taken:
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:158:    def fetch_more_tokens(self):
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:160:        # Eat whitespaces and comments until we reach the next token.
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:161:        self.scan_to_next_token()
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:166:        # Compare the current indentation and column. It may add some tokens
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:191:        #    return self.fetch_bom()    <-- issue BOMToken
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:261:            "while scanning for the next token",
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:263:            "found character %r that cannot start any token" % ch,
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:276:        #           min(self.possible_simple_keys.keys())].token_number
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:277:        min_token_number = None
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:280:            if min_token_number is None or key.token_number < min_token_number:
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:281:                min_token_number = key.token_number
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:282:        return min_token_number
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:304:        # The next token may start a simple key. We check if it's possible
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:311:        # The next token might be a simple key. Let's save it's number and
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:315:            token_number = self.tokens_taken + len(self.tokens)
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:317:                token_number, required, self.index, self.line, self.column, self.get_mark()
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:340:        ## In flow context, tokens should respect indentation.
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:356:        # In block context, we may need to issue the BLOCK-END tokens.
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:360:            self.tokens.append(BlockEndToken(mark, mark))
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:373:        # We always add STREAM-START as the first token and STREAM-END as the
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:374:        # last token.
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:376:        # Read the token.
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:380:        self.tokens.append(StreamStartToken(mark, mark, encoding=self.encoding))
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:392:        # Read the token.
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:396:        self.tokens.append(StreamEndToken(mark, mark))
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:411:        self.tokens.append(self.scan_directive())
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:414:        self.fetch_document_indicator(DocumentStartToken)
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:417:        self.fetch_document_indicator(DocumentEndToken)
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:419:    def fetch_document_indicator(self, TokenClass):
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:433:        self.tokens.append(TokenClass(start_mark, end_mark))
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:436:        self.fetch_flow_collection_start(FlowSequenceStartToken)
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:439:        self.fetch_flow_collection_start(FlowMappingStartToken)
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:441:    def fetch_flow_collection_start(self, TokenClass):
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:456:        self.tokens.append(TokenClass(start_mark, end_mark))
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:459:        self.fetch_flow_collection_end(FlowSequenceEndToken)
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:462:        self.fetch_flow_collection_end(FlowMappingEndToken)
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:464:    def fetch_flow_collection_end(self, TokenClass):
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:479:        self.tokens.append(TokenClass(start_mark, end_mark))
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:493:        self.tokens.append(FlowEntryToken(start_mark, end_mark))
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:509:                self.tokens.append(BlockSequenceStartToken(mark, mark))
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:526:        self.tokens.append(BlockEntryToken(start_mark, end_mark))
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:540:                self.tokens.append(BlockMappingStartToken(mark, mark))
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:552:        self.tokens.append(KeyToken(start_mark, end_mark))
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:562:            self.tokens.insert(key.token_number - self.tokens_taken, KeyToken(key.mark, key.mark))
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:568:                    self.tokens.insert(
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:569:                        key.token_number - self.tokens_taken,
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:570:                        BlockMappingStartToken(key.mark, key.mark),
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:597:                    self.tokens.append(BlockMappingStartToken(mark, mark))
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:609:        self.tokens.append(ValueToken(start_mark, end_mark))
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:620:        self.tokens.append(self.scan_anchor(AliasToken))
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:631:        self.tokens.append(self.scan_anchor(AnchorToken))
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:642:        self.tokens.append(self.scan_tag())
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:659:        self.tokens.append(self.scan_block_scalar(style))
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:676:        self.tokens.append(self.scan_flow_scalar(style))
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:689:        self.tokens.append(self.scan_plain())
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:761:    def scan_to_next_token(self):
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:771:        #   Tabs cannot precede tokens
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:813:        return DirectiveToken(name, value, start_mark, end_mark)
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:934:    def scan_anchor(self, TokenClass):
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:973:        return TokenClass(value, start_mark, end_mark)
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:1021:        return TagToken(value, start_mark, end_mark)
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:1095:        return ScalarToken("".join(chunks), False, start_mark, end_mark, style)
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:1212:        return ScalarToken("".join(chunks), False, start_mark, end_mark, style)
./.venv_tmp/lib/python3.12/site-packages/yaml/scanner.py:1383:        return ScalarToken("".join(chunks), True, start_mark, end_mark)
./.venv_tmp/lib/python3.12/site-packages/yaml/__init__.py:6:from .tokens import *
./.venv_tmp/lib/python3.12/site-packages/yaml/__init__.py:31:    Scan a YAML stream and produce scanning tokens.
./.venv_tmp/lib/python3.12/site-packages/yaml/__init__.py:35:        while loader.check_token():
./.venv_tmp/lib/python3.12/site-packages/yaml/__init__.py:36:            yield loader.get_token()
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:13:your own tokenizer.  You can also customize how expressions behave and
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:32:# Token types for standard operators and parens
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:33:TOKEN_AND = 1
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:34:TOKEN_OR = 2
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:35:TOKEN_NOT = 3
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:36:TOKEN_LPAR = 4
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:37:TOKEN_RPAR = 5
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:38:TOKEN_TRUE = 6
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:39:TOKEN_FALSE = 7
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:40:TOKEN_SYMBOL = 8
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:42:TOKEN_TYPES = {
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:43:    TOKEN_AND: "AND",
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:44:    TOKEN_OR: "OR",
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:45:    TOKEN_NOT: "NOT",
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:46:    TOKEN_LPAR: "(",
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:47:    TOKEN_RPAR: ")",
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:48:    TOKEN_TRUE: "TRUE",
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:49:    TOKEN_FALSE: "FALSE",
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:50:    TOKEN_SYMBOL: "SYMBOL",
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:54:PARSE_UNKNOWN_TOKEN = 1
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:62:    PARSE_UNKNOWN_TOKEN: "Unknown token",
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:73:    Raised when the parser or tokenizer encounters a syntax error. Instances of
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:74:    this class have attributes token_type, token_string, position, error_code to
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:79:    def __init__(self, token_type=None, token_string="", position=-1, error_code=0):
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:80:        self.token_type = token_type
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:81:        self.token_string = token_string
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:89:        if self.token_string:
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:90:            tstr = f' for token: "{self.token_string}"'
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:104:    - the tokenizer used when parsing expressions from strings.
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:118:        allowed_in_token=(".", ":", "_"),
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:163:        # Set the set of characters allowed in tokens
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:164:        self.allowed_in_token = allowed_in_token
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:182:        or tokens iterable.
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:188:        If `expr` is a string, the standard `tokenizer` is used for tokenization
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:190:        instances from Symbol tokens.
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:192:        If `expr` is an iterable, it should contain 3-tuples of: (token_type,
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:193:        token_string, token_position). In this case, the `token_type` can be
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:194:        a Symbol instance or one of the TOKEN_* constant types.
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:195:        See the `tokenize()` method for detailed specification.
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:198:        precedence = {self.NOT: 5, self.AND: 10, self.OR: 15, TOKEN_LPAR: 20}
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:201:            tokenized = self.tokenize(expr)
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:203:            tokenized = iter(expr)
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:206:            tokenized = list(tokenized)
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:207:            print("tokens:")
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:208:            for t in tokenized:
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:210:            tokenized = iter(tokenized)
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:213:        # process tokens
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:219:            return isinstance(_t, Symbol) or _t in (TOKEN_TRUE, TOKEN_FALSE, TOKEN_SYMBOL)
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:222:            return _t in (TOKEN_AND, TOKEN_OR)
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:224:        prev_token = None
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:225:        for token_type, token_string, token_position in tokenized:
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:228:                    "\nprocessing token_type:",
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:229:                    repr(token_type),
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:230:                    "token_string:",
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:231:                    repr(token_string),
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:232:                    "token_position:",
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:233:                    repr(token_position),
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:236:            if prev_token:
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:237:                prev_token_type, _prev_token_string, _prev_token_position = prev_token
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:239:                    print("  prev_token:", repr(prev_token))
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:241:                if is_sym(prev_token_type) and (
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:242:                    is_sym(token_type)
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:243:                ):  # or token_type == TOKEN_LPAR) :
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:245:                        token_type, token_string, token_position, PARSE_INVALID_SYMBOL_SEQUENCE
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:248:                if is_operator(prev_token_type) and (
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:249:                    is_operator(token_type) or token_type == TOKEN_RPAR
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:252:                        token_type, token_string, token_position, PARSE_INVALID_OPERATOR_SEQUENCE
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:256:                if is_operator(token_type):
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:258:                        token_type, token_string, token_position, PARSE_INVALID_OPERATOR_SEQUENCE
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:261:            if token_type == TOKEN_SYMBOL:
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:262:                ast.append(self.Symbol(token_string))
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:264:                    print(" ast: token_type is TOKEN_SYMBOL: append new symbol", repr(ast))
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:266:            elif isinstance(token_type, Symbol):
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:267:                ast.append(token_type)
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:269:                    print(" ast: token_type is Symbol): append existing symbol", repr(ast))
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:271:            elif token_type == TOKEN_TRUE:
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:274:                    print(" ast: token_type is TOKEN_TRUE:", repr(ast))
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:276:            elif token_type == TOKEN_FALSE:
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:279:                    print(" ast: token_type is TOKEN_FALSE:", repr(ast))
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:281:            elif token_type == TOKEN_NOT:
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:284:                    print(" ast: token_type is TOKEN_NOT:", repr(ast))
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:286:            elif token_type == TOKEN_AND:
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:289:                    print("  ast:token_type is TOKEN_AND: start_operation", ast)
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:291:            elif token_type == TOKEN_OR:
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:294:                    print("  ast:token_type is TOKEN_OR: start_operation", ast)
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:296:            elif token_type == TOKEN_LPAR:
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:297:                if prev_token:
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:300:                    if prev_token_type not in (TOKEN_NOT, TOKEN_AND, TOKEN_OR, TOKEN_LPAR):
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:302:                            token_type, token_string, token_position, PARSE_INVALID_NESTING
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:304:                ast = [ast, TOKEN_LPAR]
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:306:            elif token_type == TOKEN_RPAR:
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:310:                            token_type,
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:311:                            token_string,
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:312:                            token_position,
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:316:                    if ast[1] is TOKEN_LPAR:
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:327:                            token_type,
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:328:                            token_string,
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:329:                            token_position,
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:337:                            token_type, token_string, token_position, PARSE_INVALID_NESTING
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:348:                raise ParseError(token_type, token_string, token_position, PARSE_UNKNOWN_TOKEN)
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:350:            prev_token = (token_type, token_string, token_position)
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:445:    def tokenize(self, expr):
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:447:        Return an iterable of 3-tuple describing each token given an expression
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:450:        This 3-tuple contains (token, token string, position):
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:452:        - token: either a Symbol instance or one of TOKEN_* token types.
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:453:        - token string: the original token unicode string.
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:455:          original token string in the `expr` string. It can be an int for a
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:458:        The token position is used only for error reporting and can be None or
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:462:        (token_string, position, error message)
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:464:        You can use this tokenizer as a base to create specialized tokenizers
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:466:        tests for other examples of alternative tokenizers.
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:468:        This tokenizer has these characteristics:
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:472:        - The returned position is the starting character offset of a token.
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:473:        - A TOKEN_SYMBOL is returned for valid identifiers which is a string
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:480:                - dotted names : foo.bar consist of one token.
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:481:                - names with colons: foo:bar consist of one token.
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:501:        # mapping of lowercase token strings to a token type id for the standard
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:503:        # default tokenizer implementation.
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:504:        TOKENS = {
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:505:            "*": TOKEN_AND,
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:506:            "&": TOKEN_AND,
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:507:            "and": TOKEN_AND,
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:508:            "+": TOKEN_OR,
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:509:            "|": TOKEN_OR,
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:510:            "or": TOKEN_OR,
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:511:            "~": TOKEN_NOT,
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:512:            "!": TOKEN_NOT,
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:513:            "not": TOKEN_NOT,
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:514:            "(": TOKEN_LPAR,
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:515:            ")": TOKEN_RPAR,
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:516:            "[": TOKEN_LPAR,
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:517:            "]": TOKEN_RPAR,
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:518:            "true": TOKEN_TRUE,
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:519:            "1": TOKEN_TRUE,
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:520:            "false": TOKEN_FALSE,
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:521:            "0": TOKEN_FALSE,
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:522:            "none": TOKEN_FALSE,
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:536:                    if char.isalnum() or char in self.allowed_in_token:
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:544:                yield TOKENS[tok.lower()], tok, position
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:547:                    yield TOKEN_SYMBOL, tok, position
./.venv_tmp/lib/python3.12/site-packages/boolean/boolean.py:550:                        token_string=tok, position=position, error_code=PARSE_UNKNOWN_TOKEN
./.venv_tmp/lib/python3.12/site-packages/boolean/test_boolean.py:14:    TOKEN_AND,
./.venv_tmp/lib/python3.12/site-packages/boolean/test_boolean.py:15:    TOKEN_FALSE,
./.venv_tmp/lib/python3.12/site-packages/boolean/test_boolean.py:16:    TOKEN_LPAR,
./.venv_tmp/lib/python3.12/site-packages/boolean/test_boolean.py:17:    TOKEN_NOT,
./.venv_tmp/lib/python3.12/site-packages/boolean/test_boolean.py:18:    TOKEN_OR,
./.venv_tmp/lib/python3.12/site-packages/boolean/test_boolean.py:19:    TOKEN_RPAR,
./.venv_tmp/lib/python3.12/site-packages/boolean/test_boolean.py:20:    TOKEN_SYMBOL,
./.venv_tmp/lib/python3.12/site-packages/boolean/test_boolean.py:21:    TOKEN_TRUE,
./.venv_tmp/lib/python3.12/site-packages/boolean/test_boolean.py:31:    PARSE_UNKNOWN_TOKEN,
./.venv_tmp/lib/python3.12/site-packages/boolean/test_boolean.py:77:    def test_parse_recognizes_trueish_and_falsish_symbol_tokens(self):
./.venv_tmp/lib/python3.12/site-packages/boolean/test_boolean.py:93:    def test_parse_can_use_iterable_from_alternative_tokenizer(self):
./.venv_tmp/lib/python3.12/site-packages/boolean/test_boolean.py:101:            def tokenize(self, s):
./.venv_tmp/lib/python3.12/site-packages/boolean/test_boolean.py:102:                "Sample tokenizer using custom operators and symbols"
./.venv_tmp/lib/python3.12/site-packages/boolean/test_boolean.py:104:                    "WHY_NOT": TOKEN_OR,
./.venv_tmp/lib/python3.12/site-packages/boolean/test_boolean.py:105:                    "ALSO": TOKEN_AND,
./.venv_tmp/lib/python3.12/site-packages/boolean/test_boolean.py:106:                    "NEITHER": TOKEN_NOT,
./.venv_tmp/lib/python3.12/site-packages/boolean/test_boolean.py:107:                    "(": TOKEN_LPAR,
./.venv_tmp/lib/python3.12/site-packages/boolean/test_boolean.py:108:                    ")": TOKEN_RPAR,
./.venv_tmp/lib/python3.12/site-packages/boolean/test_boolean.py:118:                            yield TOKEN_SYMBOL, tok, (row, col)
./.venv_tmp/lib/python3.12/site-packages/boolean/test_boolean.py:140:    def test_parse_with_advanced_tokenizer_example(self):
./.venv_tmp/lib/python3.12/site-packages/boolean/test_boolean.py:141:        import tokenize
./.venv_tmp/lib/python3.12/site-packages/boolean/test_boolean.py:151:            def tokenize(self, expr):
./.venv_tmp/lib/python3.12/site-packages/boolean/test_boolean.py:153:                Example custom tokenizer derived from the standard Python tokenizer
./.venv_tmp/lib/python3.12/site-packages/boolean/test_boolean.py:156:                symbols. In contrast with the standard tokenizer, only these
./.venv_tmp/lib/python3.12/site-packages/boolean/test_boolean.py:159:                For more advanced tokenization you could also consider forking the
./.venv_tmp/lib/python3.12/site-packages/boolean/test_boolean.py:160:                `tokenize` standard library module.
./.venv_tmp/lib/python3.12/site-packages/boolean/test_boolean.py:166:                # mapping of lowercase token strings to a token object instance for
./.venv_tmp/lib/python3.12/site-packages/boolean/test_boolean.py:168:                TOKENS = {
./.venv_tmp/lib/python3.12/site-packages/boolean/test_boolean.py:169:                    "&": TOKEN_AND,
./.venv_tmp/lib/python3.12/site-packages/boolean/test_boolean.py:170:                    "and": TOKEN_AND,
./.venv_tmp/lib/python3.12/site-packages/boolean/test_boolean.py:171:                    "|": TOKEN_OR,
./.venv_tmp/lib/python3.12/site-packages/boolean/test_boolean.py:172:                    "or": TOKEN_OR,
./.venv_tmp/lib/python3.12/site-packages/boolean/test_boolean.py:173:                    "!": TOKEN_NOT,
./.venv_tmp/lib/python3.12/site-packages/boolean/test_boolean.py:174:                    "not": TOKEN_NOT,
./.venv_tmp/lib/python3.12/site-packages/boolean/test_boolean.py:175:                    "(": TOKEN_LPAR,
./.venv_tmp/lib/python3.12/site-packages/boolean/test_boolean.py:176:                    ")": TOKEN_RPAR,
./.venv_tmp/lib/python3.12/site-packages/boolean/test_boolean.py:177:                    "true": TOKEN_TRUE,
./.venv_tmp/lib/python3.12/site-packages/boolean/test_boolean.py:178:                    "1": TOKEN_TRUE,
./.venv_tmp/lib/python3.12/site-packages/boolean/test_boolean.py:179:                    "false": TOKEN_FALSE,
./.venv_tmp/lib/python3.12/site-packages/boolean/test_boolean.py:180:                    "0": TOKEN_FALSE,
./.venv_tmp/lib/python3.12/site-packages/boolean/test_boolean.py:181:                    "none": TOKEN_FALSE,
./.venv_tmp/lib/python3.12/site-packages/boolean/test_boolean.py:184:                ignored_token_types = (
./.venv_tmp/lib/python3.12/site-packages/boolean/test_boolean.py:185:                    tokenize.NL,
./.venv_tmp/lib/python3.12/site-packages/boolean/test_boolean.py:186:                    tokenize.NEWLINE,
./.venv_tmp/lib/python3.12/site-packages/boolean/test_boolean.py:187:                    tokenize.COMMENT,
./.venv_tmp/lib/python3.12/site-packages/boolean/test_boolean.py:188:                    tokenize.INDENT,
./.venv_tmp/lib/python3.12/site-packages/boolean/test_boolean.py:189:                    tokenize.DEDENT,
./.venv_tmp/lib/python3.12/site-packages/boolean/test_boolean.py:190:                    tokenize.ENDMARKER,
./.venv_tmp/lib/python3.12/site-packages/boolean/test_boolean.py:193:                # note: an unbalanced expression may raise a TokenError here.
./.venv_tmp/lib/python3.12/site-packages/boolean/test_boolean.py:194:                tokens = (
./.venv_tmp/lib/python3.12/site-packages/boolean/test_boolean.py:204:                    ), _, _ in tokenize.generate_tokens(StringIO(expr).readline)
./.venv_tmp/lib/python3.12/site-packages/boolean/test_boolean.py:221:                # accumulator for dotted symbols that span several `tokenize` tokens
./.venv_tmp/lib/python3.12/site-packages/boolean/test_boolean.py:224:                for toktype, tok, row, col in tokens:
./.venv_tmp/lib/python3.12/site-packages/boolean/test_boolean.py:225:                    if toktype in ignored_token_types:
./.venv_tmp/lib/python3.12/site-packages/boolean/test_boolean.py:234:                    std_token = TOKENS.get(tok.lower())
./.venv_tmp/lib/python3.12/site-packages/boolean/test_boolean.py:235:                    if std_token is not None:
./.venv_tmp/lib/python3.12/site-packages/boolean/test_boolean.py:242:                        yield std_token, tok, (row, col)
./.venv_tmp/lib/python3.12/site-packages/boolean/test_boolean.py:246:                    if toktype == tokenize.NAME or (toktype == tokenize.OP and tok in COLON_DOT):
./.venv_tmp/lib/python3.12/site-packages/boolean/test_boolean.py:254:                            "Unknown token: %(tok)r at line: %(row)r, column: %(col)r" % locals()
./.venv_tmp/lib/python3.12/site-packages/boolean/test_boolean.py:274:    def test_allowing_additional_characters_in_tokens(self):
./.venv_tmp/lib/python3.12/site-packages/boolean/test_boolean.py:275:        algebra = BooleanAlgebra(allowed_in_token=(".", "_", "-", "+"))
./.venv_tmp/lib/python3.12/site-packages/boolean/test_boolean.py:290:            assert pe.error_code == PARSE_UNKNOWN_TOKEN
./.venv_tmp/lib/python3.12/site-packages/boolean/test_boolean.py:299:            assert pe.error_code == PARSE_UNKNOWN_TOKEN
./.venv_tmp/lib/python3.12/site-packages/boolean/test_boolean.py:308:            assert pe.error_code == PARSE_UNKNOWN_TOKEN
./.venv_tmp/lib/python3.12/site-packages/boolean/test_boolean.py:317:            assert pe.error_code == PARSE_UNKNOWN_TOKEN
./.venv_tmp/lib/python3.12/site-packages/boolean/test_boolean.py:326:            assert pe.error_code == PARSE_UNKNOWN_TOKEN
./.venv_tmp/lib/python3.12/site-packages/boolean/test_boolean.py:335:            assert pe.error_code == PARSE_UNKNOWN_TOKEN
./.venv_tmp/lib/python3.12/site-packages/boolean/test_boolean.py:344:            assert pe.error_code == PARSE_UNKNOWN_TOKEN
./.venv_tmp/lib/python3.12/site-packages/boolean/__init__.py:19:    TOKEN_AND,
./.venv_tmp/lib/python3.12/site-packages/boolean/__init__.py:20:    TOKEN_FALSE,
./.venv_tmp/lib/python3.12/site-packages/boolean/__init__.py:21:    TOKEN_LPAR,
./.venv_tmp/lib/python3.12/site-packages/boolean/__init__.py:22:    TOKEN_NOT,
./.venv_tmp/lib/python3.12/site-packages/boolean/__init__.py:23:    TOKEN_OR,
./.venv_tmp/lib/python3.12/site-packages/boolean/__init__.py:24:    TOKEN_RPAR,
./.venv_tmp/lib/python3.12/site-packages/boolean/__init__.py:25:    TOKEN_SYMBOL,
./.venv_tmp/lib/python3.12/site-packages/boolean/__init__.py:26:    TOKEN_TRUE,
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:78:    - ``collect_all_And_tokens`` - flag to enable fix for Issue #63 that fixes erroneous grouping
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:86:    collect_all_And_tokens = True
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:90:        collect_all_And_tokens
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:97:    warn_multiple_tokens_in_named_alternation = False
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:98:    warn_ungrouped_named_tokens_in_collection = False
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:120:    - ``warn_multiple_tokens_in_named_alternation`` - flag to enable warnings when a results
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:122:    - ``warn_ungrouped_named_tokens_in_collection`` - flag to enable warnings when a results
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:140:    warn_multiple_tokens_in_named_alternation = 0
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:141:    warn_ungrouped_named_tokens_in_collection = 1
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:591:        Define name for referencing matching tokens as a nested attribute
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:659:        update external data structures, or enhance or replace the parsed tokens.
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:665:        - ``toks`` = a list of the matched tokens, packaged as a :class:`ParseResults` object
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:667:        The parsed tokens are passed to the parse action as ParseResults. They can be
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:670:        to add, update, or remove any named results. If the tokens are modified in place,
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:673:        Parse actions can also completely replace the given tokens, with another ``ParseResults``
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:872:    def postParse(self, instring, loc, tokenlist):
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:873:        return tokenlist
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:889:                tokens_start = pre_loc
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:891:                    self.debugActions.debug_try(instring, tokens_start, self, False)
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:894:                        loc, tokens = self.parseImpl(instring, pre_loc, do_actions)
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:898:                    loc, tokens = self.parseImpl(instring, pre_loc, do_actions)
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:902:                    self.debugActions.debug_fail(instring, tokens_start, self, err, False)
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:904:                    self.failAction(instring, tokens_start, self, err)
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:911:            tokens_start = pre_loc
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:914:                    loc, tokens = self.parseImpl(instring, pre_loc, do_actions)
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:918:                loc, tokens = self.parseImpl(instring, pre_loc, do_actions)
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:920:        tokens = self.postParse(instring, loc, tokens)
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:922:        ret_tokens = ParseResults(
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:923:            tokens, self.resultsName, asList=self.saveAsList, modal=self.modalResults
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:930:                            tokens = fn(instring, tokens_start, ret_tokens)  # type: ignore [call-arg, arg-type]
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:935:                        if tokens is not None and tokens is not ret_tokens:
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:936:                            ret_tokens = ParseResults(
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:937:                                tokens,
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:939:                                asList=self.saveAsList and isinstance(tokens, (ParseResults, list)),
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:945:                        self.debugActions.debug_fail(instring, tokens_start, self, err, False)
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:950:                        tokens = fn(instring, tokens_start, ret_tokens)  # type: ignore [call-arg, arg-type]
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:955:                    if tokens is not None and tokens is not ret_tokens:
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:956:                        ret_tokens = ParseResults(
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:957:                            tokens,
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:959:                            asList=self.saveAsList and isinstance(tokens, (ParseResults, list)),
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:963:            # print("Matched", self, "->", ret_tokens.as_list())
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:965:                self.debugActions.debug_match(instring, tokens_start, loc, self, ret_tokens, False)
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:967:        return loc, ret_tokens
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:1277:            loc, tokens = self._parse(instring, 0)
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:1291:            return tokens
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:1305:        matching tokens, start location, and end location.  May be called with optional
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:1319:            for tokens, start, end in Word(alphas).scan_string(source):
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:1321:                print(' '*start + tokens[0])
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:1362:                    tokens: ParseResults
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:1363:                    nextLoc, tokens = parseFn(instring, preloc, callPreParse=False)
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:1372:                                    "tokens": tokens.asList(),
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:1377:                        yield tokens, preloc, nextLoc
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:1397:        Extension to :class:`scan_string`, to modify matching text with modified tokens that may
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:1399:        attach a parse action to it that modifies the returned token list.
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:1463:        Another extension to :class:`scan_string`, simplifying the access to the tokens found
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:1970:                                      parsed_tokens: ParseResults,
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:2586:class Token(ParserElement):
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:2598:class NoMatch(Token):
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:2600:    A token that will never match.
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:2607:        self.errmsg = "Unmatchable token"
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:2613:class Literal(Token):
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:2615:    Token to exactly match a specified string.
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:2672:    An empty token, will always match.
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:2697:class Keyword(Token):
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:2699:    Token to exactly match a specified string as a keyword, that is,
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:2831:    Token to match a specified string, ignoring case of letters.
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:2885:class CloseMatch(Token):
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:2983:class Word(Token):
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:2984:    """Token for matching words composed of allowed character sets.
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:3275:class Regex(Token):
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:3276:    r"""Token for matching strings that match a given regular
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:3476:            def pa(tokens):
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:3477:                return tokens[0].expand(repl)
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:3481:            def pa(tokens):
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:3482:                return self.re.sub(repl, tokens[0])
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:3487:class QuotedString(Token):
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:3489:    Token for matching strings that are delimited by quoting characters.
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:3719:class CharsNotIn(Token):
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:3720:    """Token for matching words composed of characters *not* in a given
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:3807:class White(Token):
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:3884:class PositionToken(Token):
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:3891:class GoToColumn(PositionToken):
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:3892:    """Token to advance to a specific column of input text; useful for
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:3921:class LineStart(PositionToken):
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:3975:class LineEnd(PositionToken):
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:3998:class StringStart(PositionToken):
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:4015:class StringEnd(PositionToken):
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:4035:class WordStart(PositionToken):
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:4058:class WordEnd(PositionToken):
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:4082:class Tag(Token):
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:4085:    tokens that may be checked later in a parse action or while
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:4118:    def _add_tag(self, tokens: ParseResults):
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:4119:        tokens[self.tag_name] = self.tag_value
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:4127:    post-processing parsed tokens.
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:4263:            __diag__.warn_ungrouped_named_tokens_in_collection
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:4264:            and Diagnostics.warn_ungrouped_named_tokens_in_collection not in self.suppress_warnings_
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:4273:                    Diagnostics.warn_ungrouped_named_tokens_in_collection
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:4278:                    "warn_ungrouped_named_tokens_in_collection:"
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:4421:                    loc, exprtokens = e._parse(instring, loc, do_actions)
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:4430:                loc, exprtokens = e._parse(instring, loc, do_actions)
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:4431:            resultlist += exprtokens
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:4592:            __diag__.warn_multiple_tokens_in_named_alternation
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:4593:            and Diagnostics.warn_multiple_tokens_in_named_alternation not in self.suppress_warnings_
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:4597:                and Diagnostics.warn_multiple_tokens_in_named_alternation
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:4602:                    "warn_multiple_tokens_in_named_alternation:"
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:4604:                    " will return a list of all parsed tokens in an And alternative,"
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:4605:                    " in prior versions only the first token was returned; enclose"
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:4699:            __diag__.warn_multiple_tokens_in_named_alternation
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:4700:            and Diagnostics.warn_multiple_tokens_in_named_alternation not in self.suppress_warnings_
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:4704:                and Diagnostics.warn_multiple_tokens_in_named_alternation
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:4709:                    "warn_multiple_tokens_in_named_alternation:"
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:4711:                    " will return a list of all parsed tokens in an And alternative,"
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:4712:                    " in prior versions only the first token was returned; enclose"
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:4903:    post-processing parsed tokens.
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:4910:            if issubclass(self._literalStringClass, Token):
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:5135:    always returns a null token list. If any results names are defined
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:5179:    returns a null token list, but if a results name is defined on the
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:5221:        elif isinstance(expr, PositionToken):
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:5258:    Decorates a returned token with its starting and ending
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:5289:        loc, tokens = self.expr._parse(instring, start, do_actions, callPreParse=False)
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:5290:        ret_tokens = ParseResults([start, tokens, loc])
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:5291:        ret_tokens["locn_start"] = start
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:5292:        ret_tokens["value"] = tokens
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:5293:        ret_tokens["locn_end"] = loc
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:5296:            return loc, [ret_tokens]
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:5298:            return loc, ret_tokens
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:5308:    a null token list.  May be constructed using the ``'~'`` operator.
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:5335:        self.errmsg = f"Found unwanted token, {self.expr}"
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:5380:        loc, tokens = self_expr_parse(instring, loc, do_actions)
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:5390:                loc, tmptokens = self_expr_parse(instring, preloc, do_actions)
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:5391:                tokens += tmptokens
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:5395:        return loc, tokens
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:5399:            __diag__.warn_ungrouped_named_tokens_in_collection
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:5400:            and Diagnostics.warn_ungrouped_named_tokens_in_collection not in self.suppress_warnings_
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:5407:                        Diagnostics.warn_ungrouped_named_tokens_in_collection
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:5412:                        "warn_ungrouped_named_tokens_in_collection:"
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:5506:    ``combine`` is set to ``True``, the matching tokens are
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:5507:    returned as a single token string, with the delimiters included;
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:5508:    otherwise, the matching tokens are returned as a list of tokens,
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:5574:class _NullToken:
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:5629:    __optionalNotMatched = _NullToken()
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:5642:            loc, tokens = self_expr._parse(instring, loc, do_actions, callPreParse=False)
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:5647:                    tokens = ParseResults([default_value])
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:5648:                    tokens[self_expr.resultsName] = default_value
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:5650:                    tokens = [default_value]  # type: ignore[assignment]
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:5652:                tokens = []  # type: ignore[assignment]
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:5653:        return loc, tokens
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:5668:    Token for skipping over all undefined text until the matched
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:5702:        # - parse action will call token.strip() for each matched token, i.e., the description body
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:5704:        string_data.set_parse_action(token_map(str.strip))
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:6088:class TokenConverter(ParseElementEnhance):
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:6098:class Combine(TokenConverter):
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:6099:    """Converter to concatenate all matching tokens to a single string.
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:6151:    def postParse(self, instring, loc, tokenlist):
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:6152:        retToks = tokenlist.copy()
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:6155:            ["".join(tokenlist._asStringList(self.joinString))], modal=self.modalResults
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:6164:class Group(TokenConverter):
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:6165:    """Converter to return the matched tokens as a list - useful for
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:6166:    returning tokens of :class:`ZeroOrMore` and :class:`OneOrMore` expressions.
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:6169:    parsed tokens as a Python list instead of a pyparsing ParseResults.
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:6192:    def postParse(self, instring, loc, tokenlist):
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:6195:                tokenlist.asList() if isinstance(tokenlist, ParseResults) else list(tokenlist)
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:6198:        return [tokenlist]
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:6201:class Dict(TokenConverter):
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:6204:    token in the expression as its key. Useful for tabular report
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:6208:    parsed tokens as a Python dict instead of a pyparsing ParseResults.
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:6261:    def postParse(self, instring, loc, tokenlist):
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:6262:        for i, tok in enumerate(tokenlist):
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:6271:                tokenlist[ikey] = _ParseResultsWithOffset("", i)
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:6274:                tokenlist[ikey] = _ParseResultsWithOffset(tok[1], i)
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:6291:                    tokenlist[ikey] = _ParseResultsWithOffset(dictvalue, i)
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:6293:                    tokenlist[ikey] = _ParseResultsWithOffset(dictvalue[0], i)
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:6296:            return [tokenlist.as_dict()] if self.resultsName else tokenlist.as_dict()
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:6298:        return [tokenlist] if self.resultsName else tokenlist
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:6301:class Suppress(TokenConverter):
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:6348:    def postParse(self, instring, loc, tokenlist):
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:6360:    ``">> entering method-name(line:<current_source_line>, <parse_location>, <matched_tokens>)"``.
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:6380:        def remove_duplicate_chars(tokens):
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:6381:            return ''.join(sorted(set(''.join(tokens))))
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:6480:def token_map(func, *args) -> ParseAction:
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:6484:    after the token, as in
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:6485:    ``hex_integer = Word(hexnums).set_parse_action(token_map(int, 16))``,
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:6490:        hex_ints = Word(hexnums)[1, ...].set_parse_action(token_map(int, 16))
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:6495:        upperword = Word(alphas).set_parse_action(token_map(str.upper))
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:6500:        wd = Word(alphas).set_parse_action(token_map(str.title))
./.venv_tmp/lib/python3.12/site-packages/pyparsing/core.py:6603:tokenMap = replaced_by_pep8("tokenMap", token_map)
./.venv_tmp/lib/python3.12/site-packages/pyparsing/actions.py:53:            raise ParseException(strg, locn, f"matched token not at column {n}")
./.venv_tmp/lib/python3.12/site-packages/pyparsing/actions.py:180:    def pa(s: str, l: int, tokens: ParseResults) -> None:
./.venv_tmp/lib/python3.12/site-packages/pyparsing/actions.py:182:            if attrName not in tokens:
./.venv_tmp/lib/python3.12/site-packages/pyparsing/actions.py:184:            if attrValue != with_attribute.ANY_VALUE and tokens[attrName] != attrValue:  # type: ignore [attr-defined]
./.venv_tmp/lib/python3.12/site-packages/pyparsing/actions.py:188:                    f"attribute {attrName!r} has value {tokens[attrName]!r}, must be {attrValue!r}",
./.venv_tmp/lib/python3.12/site-packages/pyparsing/results.py:272:        # fixup indices in token dictionary
./.venv_tmp/lib/python3.12/site-packages/pyparsing/results.py:313:        semantics and pop tokens from the list of parsed tokens. If passed
./.venv_tmp/lib/python3.12/site-packages/pyparsing/results.py:327:           >>> def remove_first(tokens):
./.venv_tmp/lib/python3.12/site-packages/pyparsing/results.py:328:           ...     tokens.pop(0)
./.venv_tmp/lib/python3.12/site-packages/pyparsing/results.py:344:           >>> def remove_LABEL(tokens):
./.venv_tmp/lib/python3.12/site-packages/pyparsing/results.py:345:           ...     tokens.pop("LABEL")
./.venv_tmp/lib/python3.12/site-packages/pyparsing/results.py:346:           ...     return tokens
./.venv_tmp/lib/python3.12/site-packages/pyparsing/results.py:400:        Inserts new element at location index in the list of parsed tokens.
./.venv_tmp/lib/python3.12/site-packages/pyparsing/results.py:414:           >>> def insert_locn(locn, tokens):
./.venv_tmp/lib/python3.12/site-packages/pyparsing/results.py:415:           ...     tokens.insert(0, locn)
./.venv_tmp/lib/python3.12/site-packages/pyparsing/results.py:424:        # fixup indices in token dictionary
./.venv_tmp/lib/python3.12/site-packages/pyparsing/results.py:443:           >>> def append_sum(tokens):
./.venv_tmp/lib/python3.12/site-packages/pyparsing/results.py:444:           ...     tokens.append(sum(map(int, tokens)))
./.venv_tmp/lib/python3.12/site-packages/pyparsing/results.py:465:           def make_palindrome(tokens):
./.venv_tmp/lib/python3.12/site-packages/pyparsing/results.py:466:               tokens.extend(reversed([t[::-1] for t in tokens]))
./.venv_tmp/lib/python3.12/site-packages/pyparsing/results.py:467:               return ''.join(tokens)
./.venv_tmp/lib/python3.12/site-packages/pyparsing/results.py:556:        Returns the parse results as a nested list of matching tokens, all converted to strings.
./.venv_tmp/lib/python3.12/site-packages/pyparsing/results.py:681:        Returns the results name for this token expression.
./.venv_tmp/lib/python3.12/site-packages/pyparsing/tools/cvt_pyparsing_pep8_names.py:17:dblSlashComment defaultName dictOf disableMemoization downcaseTokens enableLeftRecursion enablePackrat getName 
./.venv_tmp/lib/python3.12/site-packages/pyparsing/tools/cvt_pyparsing_pep8_names.py:22:setFailAction setName setParseAction setResultsName setWhitespaceChars sglQuotedString stringEnd stringStart tokenMap 
./.venv_tmp/lib/python3.12/site-packages/pyparsing/tools/cvt_pyparsing_pep8_names.py:23:traceParseAction transformString tryParse unicodeString upcaseTokens withAttribute withClass
./.venv_tmp/lib/python3.12/site-packages/pyparsing/common.py:31:    - :class:`upcase_tokens`
./.venv_tmp/lib/python3.12/site-packages/pyparsing/common.py:32:    - :class:`downcase_tokens`
./.venv_tmp/lib/python3.12/site-packages/pyparsing/common.py:173:        pyparsing_common.uuid.set_parse_action(token_map(uuid.UUID))
./.venv_tmp/lib/python3.12/site-packages/pyparsing/common.py:188:    convert_to_integer = token_map(int)
./.venv_tmp/lib/python3.12/site-packages/pyparsing/common.py:193:    convert_to_float = token_map(float)
./.venv_tmp/lib/python3.12/site-packages/pyparsing/common.py:201:    hex_integer = Word(hexnums).set_name("hex integer").set_parse_action(token_map(int, 16))
./.venv_tmp/lib/python3.12/site-packages/pyparsing/common.py:361:    def strip_html_tags(s: str, l: int, tokens: ParseResults):
./.venv_tmp/lib/python3.12/site-packages/pyparsing/common.py:381:        return pyparsing_common._html_stripper.transform_string(tokens[0])
./.venv_tmp/lib/python3.12/site-packages/pyparsing/common.py:400:    upcase_tokens = staticmethod(token_map(lambda t: t.upper()))
./.venv_tmp/lib/python3.12/site-packages/pyparsing/common.py:401:    """Parse action to convert tokens to upper case."""
./.venv_tmp/lib/python3.12/site-packages/pyparsing/common.py:403:    downcase_tokens = staticmethod(token_map(lambda t: t.lower()))
./.venv_tmp/lib/python3.12/site-packages/pyparsing/common.py:404:    """Parse action to convert tokens to lower case."""
./.venv_tmp/lib/python3.12/site-packages/pyparsing/common.py:469:    upcaseTokens = staticmethod(replaced_by_pep8("upcaseTokens", upcase_tokens))
./.venv_tmp/lib/python3.12/site-packages/pyparsing/common.py:470:    downcaseTokens = staticmethod(replaced_by_pep8("downcaseTokens", downcase_tokens))
./.venv_tmp/lib/python3.12/site-packages/pyparsing/diagram/__init__.py:408:        Used when we encounter the same token twice in the same tree. When this
./.venv_tmp/lib/python3.12/site-packages/pyparsing/diagram/__init__.py:409:        happens, we replace all instances of that token with a terminal, and
./.venv_tmp/lib/python3.12/site-packages/pyparsing/diagram/__init__.py:410:        create a new subdiagram for the token
./.venv_tmp/lib/python3.12/site-packages/pyparsing/diagram/__init__.py:498:        pyparsing.PositionToken,
./.venv_tmp/lib/python3.12/site-packages/pyparsing/diagram/__init__.py:543:                # pyparsing.TokenConverter,
./.venv_tmp/lib/python3.12/site-packages/pyparsing/diagram/__init__.py:629:    elif isinstance(element, pyparsing.TokenConverter):
./.venv_tmp/lib/python3.12/site-packages/pyparsing/diagram/__init__.py:631:        if label == "tokenconverter":
./.venv_tmp/lib/python3.12/site-packages/pyparsing/testing.py:42:                        # are not included in the parsed tokens
./.venv_tmp/lib/python3.12/site-packages/pyparsing/testing.py:77:                "collect_all_And_tokens": __compat__.collect_all_And_tokens
./.venv_tmp/lib/python3.12/site-packages/pyparsing/testing.py:102:            __compat__.collect_all_And_tokens = self._save_context["__compat__"]
./.venv_tmp/lib/python3.12/site-packages/pyparsing/__init__.py:202:    "PositionToken",
./.venv_tmp/lib/python3.12/site-packages/pyparsing/__init__.py:211:    "Token",
./.venv_tmp/lib/python3.12/site-packages/pyparsing/__init__.py:212:    "TokenConverter",
./.venv_tmp/lib/python3.12/site-packages/pyparsing/__init__.py:271:    "token_map",
./.venv_tmp/lib/python3.12/site-packages/pyparsing/__init__.py:317:    "tokenMap",
./.venv_tmp/lib/python3.12/site-packages/pyparsing/helpers.py:35:    The matched tokens returns the array of expr tokens as a list - the
./.venv_tmp/lib/python3.12/site-packages/pyparsing/helpers.py:36:    leading count token is suppressed.
./.venv_tmp/lib/python3.12/site-packages/pyparsing/helpers.py:95:    the tokens matched in a previous expression, that is, it looks for
./.venv_tmp/lib/python3.12/site-packages/pyparsing/helpers.py:112:    def copy_token_to_repeater(s, l, t):
./.venv_tmp/lib/python3.12/site-packages/pyparsing/helpers.py:121:        # flatten t tokens
./.venv_tmp/lib/python3.12/site-packages/pyparsing/helpers.py:125:    expr.add_parse_action(copy_token_to_repeater, callDuringTry=True)
./.venv_tmp/lib/python3.12/site-packages/pyparsing/helpers.py:132:    the tokens matched in a previous expression, that is, it looks for
./.venv_tmp/lib/python3.12/site-packages/pyparsing/helpers.py:151:    def copy_token_to_repeater(s, l, t):
./.venv_tmp/lib/python3.12/site-packages/pyparsing/helpers.py:152:        matchTokens = _flatten(t.as_list())
./.venv_tmp/lib/python3.12/site-packages/pyparsing/helpers.py:154:        def must_match_these_tokens(s, l, t):
./.venv_tmp/lib/python3.12/site-packages/pyparsing/helpers.py:155:            theseTokens = _flatten(t.as_list())
./.venv_tmp/lib/python3.12/site-packages/pyparsing/helpers.py:156:            if theseTokens != matchTokens:
./.venv_tmp/lib/python3.12/site-packages/pyparsing/helpers.py:157:                raise ParseException(s, l, f"Expected {matchTokens}, found{theseTokens}")
./.venv_tmp/lib/python3.12/site-packages/pyparsing/helpers.py:159:        rep.set_parse_action(must_match_these_tokens, callDuringTry=True)
./.venv_tmp/lib/python3.12/site-packages/pyparsing/helpers.py:161:    expr.add_parse_action(copy_token_to_repeater, callDuringTry=True)
./.venv_tmp/lib/python3.12/site-packages/pyparsing/helpers.py:283:    # last resort, just use MatchFirst of Token class corresponding to caseless
./.venv_tmp/lib/python3.12/site-packages/pyparsing/helpers.py:299:    :class:`Group` tokens in the proper order.  The key pattern
./.venv_tmp/lib/python3.12/site-packages/pyparsing/helpers.py:303:    can include named token fields.
./.venv_tmp/lib/python3.12/site-packages/pyparsing/helpers.py:355:    """Helper to return the original, untokenized text for a given
./.venv_tmp/lib/python3.12/site-packages/pyparsing/helpers.py:357:    tag into the raw tag text itself, or to revert separate tokens with
./.venv_tmp/lib/python3.12/site-packages/pyparsing/helpers.py:364:    were originally matched, and a single token containing the original
./.venv_tmp/lib/python3.12/site-packages/pyparsing/helpers.py:405:    matchExpr.suppress_warning(Diagnostics.warn_ungrouped_named_tokens_in_collection)
./.venv_tmp/lib/python3.12/site-packages/pyparsing/helpers.py:413:    return TokenConverter(expr).add_parse_action(lambda t: t[0])
./.venv_tmp/lib/python3.12/site-packages/pyparsing/helpers.py:421:    Helper to decorate a returned token with its starting and ending
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/logging.py:275:    log.warning("password was rejected for admin site.")
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/containers.py:146:                tokens: List[Text] = []
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/containers.py:148:                    tokens.append(word)
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/containers.py:153:                        tokens.append(Text(" " * spaces[index], style=space_style))
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/containers.py:154:                self[line_index] = Text("").join(tokens)
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/console.py:2106:        password: bool = False,
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/console.py:2117:            password: (bool, optional): Hide typed text. Defaults to False.
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/console.py:2125:        if password:
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:27:from pip._vendor.pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:36:    Token,
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:53:TokenType = Tuple[str, ...]
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:61:ANSI_LIGHT: Dict[TokenType, Style] = {
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:62:    Token: Style(),
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:90:ANSI_DARK: Dict[TokenType, Style] = {
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:91:    Token: Style(),
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:127:    def get_style_for_token(self, token_type: TokenType) -> Style:
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:128:        """Get a style for a given Pygments token."""
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:141:        self._style_cache: Dict[TokenType, Style] = {}
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:153:    def get_style_for_token(self, token_type: TokenType) -> Style:
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:156:            return self._style_cache[token_type]
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:159:                pygments_style = self._pygments_style_class.style_for_token(token_type)
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:172:            self._style_cache[token_type] = style
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:182:    def __init__(self, style_map: Dict[TokenType, Style]) -> None:
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:186:        self._style_cache: Dict[TokenType, Style] = {}
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:188:    def get_style_for_token(self, token_type: TokenType) -> Style:
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:191:            return self._style_cache[token_type]
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:197:            token = tuple(token_type)
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:199:            while token:
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:200:                _style = get_style(token)
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:204:                token = token[:-1]
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:205:            self._style_cache[token_type] = style
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:423:    def _get_token_color(self, token_type: TokenType) -> Optional[Color]:
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:424:        """Get a color (if any) for the given token.
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:427:            token_type (TokenType): A token type tuple from Pygments.
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:432:        style = self._theme.get_style_for_token(token_type)
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:488:        _get_theme_style = self._theme.get_style_for_token
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:500:                def line_tokenize() -> Iterable[Tuple[Any, str]]:
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:501:                    """Split tokens to one per line."""
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:504:                    for token_type, token in lexer.get_tokens(code):
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:505:                        while token:
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:506:                            line_token, new_line, token = token.partition("\n")
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:507:                            yield token_type, line_token + new_line
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:509:                def tokens_to_spans() -> Iterable[Tuple[str, Optional[Style]]]:
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:510:                    """Convert tokens to spans."""
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:511:                    tokens = iter(line_tokenize())
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:515:                    # Skip over tokens until line start
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:518:                            _token_type, token = next(tokens)
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:521:                        yield (token, None)
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:522:                        if token.endswith("\n"):
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:525:                    for token_type, token in tokens:
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:526:                        yield (token, _get_theme_style(token_type))
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:527:                        if token.endswith("\n"):
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:532:                text.append_tokens(tokens_to_spans())
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:535:                text.append_tokens(
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:536:                    (token, _get_theme_style(token_type))
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:537:                    for token_type, token in lexer.get_tokens(code)
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:571:        foreground_color = self._get_token_color(Token.Text)
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:599:                self._theme.get_style_for_token(Token.Text),
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:605:                self._theme.get_style_for_token(Token.Text),
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:669:                + self._theme.get_style_for_token(Comment)
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py:703:                + self._theme.get_style_for_token(Comment)
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/prompt.py:37:        password (bool, optional): Enable password input. Defaults to False.
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/prompt.py:57:        password: bool = False,
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/prompt.py:67:        self.password = password
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/prompt.py:81:        password: bool = False,
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/prompt.py:97:        password: bool = False,
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/prompt.py:111:        password: bool = False,
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/prompt.py:127:            password (bool, optional): Enable password input. Defaults to False.
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/prompt.py:137:            password=password,
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/prompt.py:188:        password: bool,
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/prompt.py:196:            password (bool): Enable password entry.
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/prompt.py:201:        return console.input(prompt, password=password, stream=stream)
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/prompt.py:278:            value = self.get_input(self.console, prompt, self.password, stream=stream)
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/prompt.py:366:            password = Prompt.ask(
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/prompt.py:367:                "Please enter a password [cyan](must be at least 5 characters)",
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/prompt.py:368:                password=True,
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/prompt.py:370:            if len(password) >= 5:
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/prompt.py:372:            print("[prompt.invalid]password too short")
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/prompt.py:373:        print(f"password={password!r}")
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/_emoji_codes.py:156:    "japanese_secret_button": "ãŠ™",
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/_emoji_codes.py:2882:    "secret": "ãŠ™",
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/text.py:1013:    def append_tokens(self, tokens: Iterable[Tuple[str, Optional[StyleType]]]) -> "Text":
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/text.py:1017:            tokens (Iterable[Tuple[str, Optional[StyleType]]]): An iterable of tuples containing str content and style.
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/text.py:1026:        for content, style in tokens:
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/ansi.py:20:class _AnsiToken(NamedTuple):
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/ansi.py:21:    """Result of ansi tokenized string."""
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/ansi.py:28:def _ansi_tokenize(ansi_text: str) -> Iterable[_AnsiToken]:
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/ansi.py:29:    """Tokenize a string in to plain text and ANSI codes.
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/ansi.py:35:        AnsiToken: A named tuple of (plain, sgr, osc)
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/ansi.py:45:            yield _AnsiToken(ansi_text[position:start])
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/ansi.py:51:                yield _AnsiToken("", sgr[1:-1], osc)
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/ansi.py:53:            yield _AnsiToken("", sgr, osc)
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/ansi.py:56:        yield _AnsiToken(ansi_text[position:])
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/ansi.py:153:        for plain_text, sgr, osc in _ansi_tokenize(line):
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/pretty.py:414:    def iter_tokens(self) -> Iterable[str]:
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/pretty.py:415:        """Generate tokens for this node."""
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/pretty.py:425:                    yield from self.children[0].iter_tokens()
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/pretty.py:429:                        yield from child.iter_tokens()
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/pretty.py:447:        for token in self.iter_tokens():
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/pretty.py:448:            total_length += cell_len(token)
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/pretty.py:454:        repr_text = "".join(self.iter_tokens())
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/traceback.py:24:from pip._vendor.pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/traceback.py:31:    Text as TextToken,
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/traceback.py:32:    Token,
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/traceback.py:605:        token_style = theme.get_style_for_token
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/traceback.py:609:                "pretty": token_style(TextToken),
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/traceback.py:610:                "pygments.text": token_style(Token),
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/traceback.py:611:                "pygments.string": token_style(String),
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/traceback.py:612:                "pygments.function": token_style(Name.Function),
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/traceback.py:613:                "pygments.number": token_style(Number),
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/traceback.py:614:                "repr.indent": token_style(Comment) + Style(dim=True),
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/traceback.py:615:                "repr.str": token_style(String),
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/traceback.py:616:                "repr.brace": token_style(TextToken) + Style(bold=True),
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/traceback.py:617:                "repr.number": token_style(Number),
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/traceback.py:618:                "repr.bool_true": token_style(Keyword.Constant),
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/traceback.py:619:                "repr.bool_false": token_style(Keyword.Constant),
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/traceback.py:620:                "repr.none": token_style(Keyword.Constant),
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/traceback.py:621:                "scope.border": token_style(String.Delimiter),
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/traceback.py:622:                "scope.equals": token_style(Operator),
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/traceback.py:623:                "scope.key": token_style(Name),
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/rich/traceback.py:624:                "scope.key.special": token_style(Name.Constant) + Style(dim=True),
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/urllib3/packages/six.py:426:    MovedAttribute("HTTPPasswordMgr", "urllib2", "urllib.request"),
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/urllib3/packages/six.py:427:    MovedAttribute("HTTPPasswordMgrWithDefaultRealm", "urllib2", "urllib.request"),
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/urllib3/connectionpool.py:916:    ``ca_cert_dir``, ``ssl_version``, ``key_password`` are only used if :mod:`ssl`
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/urllib3/connectionpool.py:939:        key_password=None,
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/urllib3/connectionpool.py:966:        self.key_password = key_password
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/urllib3/connectionpool.py:982:                key_password=self.key_password,
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/urllib3/connectionpool.py:1036:            key_password=self.key_password,
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/urllib3/poolmanager.py:36:    "key_password",
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/urllib3/poolmanager.py:52:    "key_key_password",  # str
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/urllib3/connection.py:211:                "Method cannot contain non-token characters %r (found at least %r)"
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/urllib3/connection.py:302:        key_password=None,
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/urllib3/connection.py:314:        self.key_password = key_password
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/urllib3/connection.py:327:        key_password=None,
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/urllib3/connection.py:348:        self.key_password = key_password
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/urllib3/connection.py:417:            key_password=self.key_password,
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/urllib3/util/request.py:45:        Colon-separated username:password string for 'authorization: basic ...'
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/urllib3/util/request.py:49:        Colon-separated username:password string for 'proxy-authorization: basic ...'
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/urllib3/util/ssl_.py:371:    key_password=None,
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/urllib3/util/ssl_.py:390:    :param key_password:
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/urllib3/util/ssl_.py:391:        Optional password if the keyfile is encrypted.
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/urllib3/util/ssl_.py:418:    if keyfile and key_password is None and _is_key_file_encrypted(keyfile):
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/urllib3/util/ssl_.py:419:        raise SSLError("Client private key is encrypted, password is required")
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/urllib3/util/ssl_.py:422:        if key_password is None:
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/urllib3/util/ssl_.py:425:            context.load_cert_chain(certfile, keyfile, key_password)
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/urllib3/util/url.py:142:            >>> Url('http', 'username:password', 'host.com', 80,
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/urllib3/util/url.py:144:            'http://username:password@host.com:80/path?query#fragment'
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/urllib3/contrib/securetransport.py:863:    def load_cert_chain(self, certfile, keyfile=None, password=None):
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/urllib3/contrib/securetransport.py:866:        self._client_cert_passphrase = password
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/urllib3/contrib/_securetransport/low_level.py:212:    credentials. This keychain uses a one-time password and a temporary file to
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/urllib3/contrib/_securetransport/low_level.py:224:    # some random bytes to password-protect the keychain we're creating, so we
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/urllib3/contrib/_securetransport/low_level.py:228:    password = base64.b16encode(random_bytes[8:])  # Must be valid UTF-8
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/urllib3/contrib/_securetransport/low_level.py:236:        keychain_path, len(password), password, False, None, ctypes.byref(keychain)
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/urllib3/contrib/pyopenssl.py:467:    def load_cert_chain(self, certfile, keyfile=None, password=None):
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/urllib3/contrib/pyopenssl.py:469:        if password is not None:
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/urllib3/contrib/pyopenssl.py:470:            if not isinstance(password, six.binary_type):
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/urllib3/contrib/pyopenssl.py:471:                password = password.encode("utf-8")
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/urllib3/contrib/pyopenssl.py:472:            self._ctx.set_passwd_cb(lambda *_: password)
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/urllib3/contrib/ntlmpool.py:39:        pw is the password for the user.
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/urllib3/contrib/ntlmpool.py:105:                raise Exception("Server rejected request: wrong username or password")
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/urllib3/contrib/socks.py:15:- Usernames and passwords for the SOCKS proxy
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/urllib3/contrib/socks.py:32:When connecting to a SOCKS5 proxy the ``username`` and ``password`` portion
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/urllib3/contrib/socks.py:33:of the ``proxy_url`` will be sent as the username/password to authenticate
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/urllib3/contrib/socks.py:38:    proxy_url="socks5h://<username>:<password>@proxy-host"
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/urllib3/contrib/socks.py:101:                proxy_password=self._socks_options["password"],
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/urllib3/contrib/socks.py:168:        password=None,
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/urllib3/contrib/socks.py:175:        if username is None and password is None and parsed.auth is not None:
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/urllib3/contrib/socks.py:178:                username, password = split
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/urllib3/contrib/socks.py:201:            "password": password,
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/markers.py:21:from ._tokenizer import ParserSyntaxError
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_tokenizer.py:12:class Token:
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_tokenizer.py:91:class Tokenizer:
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_tokenizer.py:92:    """Context-sensitive token parsing.
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_tokenizer.py:94:    Provides methods to examine the input stream to check whether the next token
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_tokenizer.py:108:        self.next_token: Token | None = None
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_tokenizer.py:112:        """Move beyond provided token name, if at current position."""
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_tokenizer.py:117:        """Check whether the next token has the provided name.
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_tokenizer.py:119:        By default, if the check succeeds, the token *must* be read before
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_tokenizer.py:120:        another check. If `peek` is set to `True`, the token is not loaded and
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_tokenizer.py:124:            self.next_token is None
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_tokenizer.py:125:        ), f"Cannot check for {name!r}, already have {self.next_token!r}"
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_tokenizer.py:126:        assert name in self.rules, f"Unknown token name: {name!r}"
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_tokenizer.py:134:            self.next_token = Token(name, match[0], self.position)
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_tokenizer.py:137:    def expect(self, name: str, *, expected: str) -> Token:
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_tokenizer.py:138:        """Expect a certain token name next, failing with a syntax error otherwise.
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_tokenizer.py:140:        The token is *not* read.
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_tokenizer.py:146:    def read(self) -> Token:
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_tokenizer.py:147:        """Consume the next token and return it."""
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_tokenizer.py:148:        token = self.next_token
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_tokenizer.py:149:        assert token is not None
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_tokenizer.py:151:        self.position += len(token.text)
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_tokenizer.py:152:        self.next_token = None
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_tokenizer.py:154:        return token
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_tokenizer.py:175:    def enclosing_tokens(self, open_token: str, close_token: str, *, around: str) -> Iterator[None]:
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_tokenizer.py:176:        if self.check(open_token):
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_tokenizer.py:187:        if not self.check(close_token):
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_tokenizer.py:189:                f"Expected matching {close_token} for {open_token}, after {around}",
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:12:from ._tokenizer import DEFAULT_RULES, Tokenizer
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:62:    return _parse_requirement(Tokenizer(source, rules=DEFAULT_RULES))
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:65:def _parse_requirement(tokenizer: Tokenizer) -> ParsedRequirement:
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:69:    tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:71:    name_token = tokenizer.expect(
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:74:    name = name_token.text
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:75:    tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:77:    extras = _parse_extras(tokenizer)
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:78:    tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:80:    url, specifier, marker = _parse_requirement_details(tokenizer)
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:81:    tokenizer.expect("END", expected="end of dependency specifier")
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:87:    tokenizer: Tokenizer,
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:98:    if tokenizer.check("AT"):
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:99:        tokenizer.read()
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:100:        tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:102:        url_start = tokenizer.position
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:103:        url = tokenizer.expect("URL", expected="URL after @").text
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:104:        if tokenizer.check("END", peek=True):
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:107:        tokenizer.expect("WS", expected="whitespace after URL")
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:110:        if tokenizer.check("END", peek=True):
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:114:            tokenizer, span_start=url_start, after="URL and whitespace"
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:117:        specifier_start = tokenizer.position
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:118:        specifier = _parse_specifier(tokenizer)
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:119:        tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:121:        if tokenizer.check("END", peek=True):
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:125:            tokenizer,
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:133:def _parse_requirement_marker(tokenizer: Tokenizer, *, span_start: int, after: str) -> MarkerList:
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:138:    if not tokenizer.check("SEMICOLON"):
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:139:        tokenizer.raise_syntax_error(
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:143:    tokenizer.read()
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:145:    marker = _parse_marker(tokenizer)
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:146:    tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:151:def _parse_extras(tokenizer: Tokenizer) -> list[str]:
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:155:    if not tokenizer.check("LEFT_BRACKET", peek=True):
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:158:    with tokenizer.enclosing_tokens(
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:163:        tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:164:        extras = _parse_extras_list(tokenizer)
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:165:        tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:170:def _parse_extras_list(tokenizer: Tokenizer) -> list[str]:
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:176:    if not tokenizer.check("IDENTIFIER"):
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:179:    extras.append(tokenizer.read().text)
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:182:        tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:183:        if tokenizer.check("IDENTIFIER", peek=True):
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:184:            tokenizer.raise_syntax_error("Expected comma between extra names")
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:185:        elif not tokenizer.check("COMMA"):
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:188:        tokenizer.read()
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:189:        tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:191:        extra_token = tokenizer.expect("IDENTIFIER", expected="extra name after comma")
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:192:        extras.append(extra_token.text)
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:197:def _parse_specifier(tokenizer: Tokenizer) -> str:
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:202:    with tokenizer.enclosing_tokens(
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:207:        tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:208:        parsed_specifiers = _parse_version_many(tokenizer)
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:209:        tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:214:def _parse_version_many(tokenizer: Tokenizer) -> str:
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:219:    while tokenizer.check("SPECIFIER"):
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:220:        span_start = tokenizer.position
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:221:        parsed_specifiers += tokenizer.read().text
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:222:        if tokenizer.check("VERSION_PREFIX_TRAIL", peek=True):
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:223:            tokenizer.raise_syntax_error(
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:226:                span_end=tokenizer.position + 1,
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:228:        if tokenizer.check("VERSION_LOCAL_LABEL_TRAIL", peek=True):
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:229:            tokenizer.raise_syntax_error(
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:232:                span_end=tokenizer.position,
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:234:        tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:235:        if not tokenizer.check("COMMA"):
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:237:        parsed_specifiers += tokenizer.read().text
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:238:        tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:247:    return _parse_full_marker(Tokenizer(source, rules=DEFAULT_RULES))
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:250:def _parse_full_marker(tokenizer: Tokenizer) -> MarkerList:
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:251:    retval = _parse_marker(tokenizer)
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:252:    tokenizer.expect("END", expected="end of marker expression")
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:256:def _parse_marker(tokenizer: Tokenizer) -> MarkerList:
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:260:    expression = [_parse_marker_atom(tokenizer)]
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:261:    while tokenizer.check("BOOLOP"):
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:262:        token = tokenizer.read()
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:263:        expr_right = _parse_marker_atom(tokenizer)
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:264:        expression.extend((token.text, expr_right))
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:268:def _parse_marker_atom(tokenizer: Tokenizer) -> MarkerAtom:
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:274:    tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:275:    if tokenizer.check("LEFT_PARENTHESIS", peek=True):
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:276:        with tokenizer.enclosing_tokens(
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:281:            tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:282:            marker: MarkerAtom = _parse_marker(tokenizer)
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:283:            tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:285:        marker = _parse_marker_item(tokenizer)
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:286:    tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:290:def _parse_marker_item(tokenizer: Tokenizer) -> MarkerItem:
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:294:    tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:295:    marker_var_left = _parse_marker_var(tokenizer)
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:296:    tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:297:    marker_op = _parse_marker_op(tokenizer)
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:298:    tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:299:    marker_var_right = _parse_marker_var(tokenizer)
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:300:    tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:304:def _parse_marker_var(tokenizer: Tokenizer) -> MarkerVar:
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:308:    if tokenizer.check("VARIABLE"):
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:309:        return process_env_var(tokenizer.read().text.replace(".", "_"))
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:310:    elif tokenizer.check("QUOTED_STRING"):
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:311:        return process_python_str(tokenizer.read().text)
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:313:        tokenizer.raise_syntax_error(message="Expected a marker variable or quoted string")
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:328:def _parse_marker_op(tokenizer: Tokenizer) -> Op:
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:332:    if tokenizer.check("IN"):
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:333:        tokenizer.read()
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:335:    elif tokenizer.check("NOT"):
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:336:        tokenizer.read()
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:337:        tokenizer.expect("WS", expected="whitespace after 'not'")
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:338:        tokenizer.expect("IN", expected="'in' after 'not'")
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:340:    elif tokenizer.check("OP"):
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:341:        return Op(tokenizer.read().text)
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/_parser.py:343:        return tokenizer.raise_syntax_error(
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/licenses/__init__.py:67:    # Pad any parentheses so tokenization can be achieved by merely splitting on
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/licenses/__init__.py:81:    tokens = license_expression.split()
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/licenses/__init__.py:86:    python_tokens = []
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/licenses/__init__.py:87:    for token in tokens:
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/licenses/__init__.py:88:        if token not in {"or", "and", "with", "(", ")"}:
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/licenses/__init__.py:89:            python_tokens.append("False")
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/licenses/__init__.py:90:        elif token == "with":
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/licenses/__init__.py:91:            python_tokens.append("or")
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/licenses/__init__.py:92:        elif token == "(" and python_tokens and python_tokens[-1] not in {"or", "and"}:
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/licenses/__init__.py:96:            python_tokens.append(token)
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/licenses/__init__.py:98:    python_expression = " ".join(python_tokens)
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/licenses/__init__.py:109:    normalized_tokens = []
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/licenses/__init__.py:110:    for token in tokens:
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/licenses/__init__.py:111:        if token in {"or", "and", "with", "(", ")"}:
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/licenses/__init__.py:112:            normalized_tokens.append(token.upper())
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/licenses/__init__.py:115:        if normalized_tokens and normalized_tokens[-1] == "WITH":
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/licenses/__init__.py:116:            if token not in EXCEPTIONS:
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/licenses/__init__.py:117:                message = f"Unknown license exception: {token!r}"
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/licenses/__init__.py:120:            normalized_tokens.append(EXCEPTIONS[token]["id"])
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/licenses/__init__.py:122:            if token.endswith("+"):
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/licenses/__init__.py:123:                final_token = token[:-1]
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/licenses/__init__.py:126:                final_token = token
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/licenses/__init__.py:129:            if final_token.startswith("licenseref-"):
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/licenses/__init__.py:130:                if not license_ref_allowed.match(final_token):
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/licenses/__init__.py:131:                    message = f"Invalid licenseref: {final_token!r}"
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/licenses/__init__.py:133:                normalized_tokens.append(license_refs[final_token] + suffix)
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/licenses/__init__.py:135:                if final_token not in LICENSES:
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/licenses/__init__.py:136:                    message = f"Unknown license: {final_token!r}"
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/licenses/__init__.py:138:                normalized_tokens.append(LICENSES[final_token]["id"] + suffix)
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/licenses/__init__.py:140:    normalized_expression = " ".join(normalized_tokens)
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/packaging/requirements.py:9:from ._tokenizer import ParserSyntaxError
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/requests/auth.py:25:def _basic_auth_str(username, password):
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/requests/auth.py:45:    if not isinstance(password, basestring):
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/requests/auth.py:47:            "Non-string passwords will no longer be supported in Requests "
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/requests/auth.py:50:            "problems.".format(type(password)),
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/requests/auth.py:53:        password = str(password)
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/requests/auth.py:59:    if isinstance(password, str):
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/requests/auth.py:60:        password = password.encode("latin1")
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/requests/auth.py:62:    authstr = "Basic " + to_native_string(b64encode(b":".join((username, password))).strip())
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/requests/auth.py:77:    def __init__(self, username, password):
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/requests/auth.py:79:        self.password = password
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/requests/auth.py:85:                self.password == getattr(other, "password", None),
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/requests/auth.py:93:        r.headers["Authorization"] = _basic_auth_str(self.username, self.password)
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/requests/auth.py:101:        r.headers["Proxy-Authorization"] = _basic_auth_str(self.username, self.password)
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/requests/auth.py:108:    def __init__(self, username, password):
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/requests/auth.py:110:        self.password = password
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/requests/auth.py:187:        A1 = f"{self.username}:{realm}:{self.password}"
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/requests/auth.py:305:                self.password == getattr(other, "password", None),
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/requests/utils.py:235:                # Return with login / password
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/requests/utils.py:376:    >>> parse_list_header('token, "quoted value"')
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/requests/utils.py:377:    ['token', 'quoted value']
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/requests/utils.py:506:    tokens = header.split(";")
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/requests/utils.py:507:    content_type, params = tokens[0].strip(), tokens[1:]
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/requests/utils.py:1004:    username,password.
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/requests/utils.py:1011:        auth = (unquote(parsed.username), unquote(parsed.password))
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/requests/sessions.py:317:            username, password = get_auth_from_url(new_proxies[scheme])
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/requests/sessions.py:319:            username, password = None, None
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/requests/sessions.py:323:        if not scheme.startswith("https") and username and password:
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/requests/sessions.py:324:            headers["Proxy-Authorization"] = _basic_auth_str(username, password)
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/requests/adapters.py:252:            username, password = get_auth_from_url(proxy)
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/requests/adapters.py:256:                password=password,
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/requests/adapters.py:569:        username, password = get_auth_from_url(proxy)
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/requests/adapters.py:572:            headers["Proxy-Authorization"] = _basic_auth_str(username, password)
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/truststore/_api.py:31:_PasswordType: typing.TypeAlias = str | bytes | typing.Callable[[], str | bytes]
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/truststore/_api.py:166:        password: _PasswordType | None = None,
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/truststore/_api.py:168:        return self._ctx.load_cert_chain(certfile=certfile, keyfile=keyfile, password=password)
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/distro/distro.py:1105:        tokens = list(lexer)
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/distro/distro.py:1106:        for token in tokens:
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/distro/distro.py:1110:            # stripped, etc.), so the tokens are now either:
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/distro/distro.py:1113:            # Ignore any tokens that are not variable assignments
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/distro/distro.py:1114:            if "=" in token:
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/distro/distro.py:1115:                k, v = token.split("=", 1)
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/distlib/util.py:848:    username = password = None
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/distlib/util.py:854:            username, password = prefix.split(":", 1)
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/distlib/util.py:857:    if password:
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/distlib/util.py:858:        password = unquote(password)
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/distlib/util.py:859:    return username, password, netloc
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/distlib/util.py:1871:                            ("password", None),
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/distlib/util.py:1894:                    "password": config.get(server, "password"),
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/distlib/util.py:1901:    def update(self, username, password):
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/distlib/util.py:1909:        config.set("pypi", "password", password)
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/distlib/util.py:1922:    PyPIRCFile().update(index.username, index.password)
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/distlib/compat.py:49:        HTTPPasswordMgr,
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/distlib/compat.py:101:        HTTPPasswordMgr,
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/distlib/compat.py:401:    from tokenize import detect_encoding
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/distlib/compat.py:408:        """Imitates get_normal_name in tokenizer.c."""
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/distlib/compat.py:423:        in the same way as the tokenize() generator.
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/formatters/_mapping.py:10:        "Format tokens with BBcodes. These formatting codes are used by many bulletin boards, so you can highlight your sourcecode with pygments before posting it there.",
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/formatters/_mapping.py:31:        "Format tokens with groff escapes to change their color and font style.",
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/formatters/_mapping.py:38:        "Format tokens as HTML 4 ``<span>`` tags. By default, the content is enclosed in a ``<pre>`` tag, itself wrapped in a ``<div>`` tag (but see the `nowrap` option). The ``<div>``'s CSS class can be set by the `cssclass` option.",
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/formatters/_mapping.py:45:        "Format tokens with IRC color sequences",
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/formatters/_mapping.py:66:        "Format tokens as LaTeX code. This needs the `fancyvrb` and `color` standard packages.",
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/formatters/_mapping.py:80:        "Format tokens as Pango Markup code. It can then be rendered to an SVG.",
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/formatters/_mapping.py:82:    "RawTokenFormatter": (
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/formatters/_mapping.py:84:        "Raw tokens",
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/formatters/_mapping.py:85:        ("raw", "tokens"),
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/formatters/_mapping.py:87:        "Format tokens as a raw representation for storing token streams.",
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/formatters/_mapping.py:94:        "Format tokens as RTF markup. This formatter automatically outputs full RTF documents with color information and other useful stuff. Perfect for Copy and Paste into Microsoft(R) Word(R) documents.",
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/formatters/_mapping.py:101:        "Format tokens as an SVG graphics file.  This formatter is still experimental. Each line of code is a ``<text>`` element with explicit ``x`` and ``y`` coordinates containing ``<tspan>`` elements with the individual token styles.",
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/formatters/_mapping.py:108:        "Format tokens with ANSI color sequences, for output in a 256-color terminal or console.  Like in `TerminalFormatter` color sequences are terminated at newlines, so that paging the output works correctly.",
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/formatters/_mapping.py:115:        "Format tokens with ANSI color sequences, for output in a text console. Color sequences are terminated at newlines, so that paging the output works correctly.",
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/formatters/_mapping.py:122:        "Format tokens with ANSI color sequences, for output in a true-color terminal or console.  Like in `TerminalFormatter` color sequences are terminated at newlines, so that paging the output works correctly.",
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/formatters/_mapping.py:129:        "Format tokens as appropriate for a new testcase.",
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/formatter.py:27:    Converts a token stream to text.
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/formatter.py:58:        convert the Unicode token strings to byte strings in the
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/formatter.py:114:    def format(self, tokensource, outfile):
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/formatter.py:116:        This method must format the tokens from the `tokensource` iterable and
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/formatter.py:119:        Formatter options can control how exactly the tokens are converted.
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/formatter.py:124:        return self.format_unencoded(tokensource, outfile)
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/token.py:2:pygments.token
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/token.py:5:Basic token types and the standard tokens.
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/token.py:12:class _TokenType(tuple):
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/token.py:34:        new = _TokenType(self + (val,))
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/token.py:41:        return "Token" + (self and "." or "") + ".".join(self)
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/token.py:52:Token = _TokenType()
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/token.py:54:# Special token types
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/token.py:55:Text = Token.Text
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/token.py:57:Escape = Token.Escape
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/token.py:58:Error = Token.Error
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/token.py:60:Other = Token.Other
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/token.py:62:# Common token types for source code
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/token.py:63:Keyword = Token.Keyword
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/token.py:64:Name = Token.Name
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/token.py:65:Literal = Token.Literal
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/token.py:68:Punctuation = Token.Punctuation
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/token.py:69:Operator = Token.Operator
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/token.py:70:Comment = Token.Comment
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/token.py:73:Generic = Token.Generic
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/token.py:75:# String and some others are not direct children of Token.
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/token.py:77:Token.Token = Token
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/token.py:78:Token.String = String
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/token.py:79:Token.Number = Number
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/token.py:82:def is_token_subtype(ttype, other):
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/token.py:91:def string_to_tokentype(s):
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/token.py:93:    Convert a string into a token type::
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/token.py:95:        >>> string_to_token('String.Double')
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/token.py:96:        Token.Literal.String.Double
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/token.py:97:        >>> string_to_token('Token.Literal.Number')
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/token.py:98:        Token.Literal.Number
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/token.py:99:        >>> string_to_token('')
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/token.py:100:        Token
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/token.py:102:    Tokens that are already tokens are returned unchanged:
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/token.py:104:        >>> string_to_token(String)
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/token.py:105:        Token.Literal.String
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/token.py:107:    if isinstance(s, _TokenType):
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/token.py:110:        return Token
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/token.py:111:    node = Token
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/token.py:117:# Map standard token types to short names, used in CSS class naming.
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/token.py:121:    Token: "",
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/style.py:11:from pip._vendor.pygments.token import STANDARD_TYPES, Token
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/style.py:62:        for token in STANDARD_TYPES:
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/style.py:63:            if token not in obj.styles:
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/style.py:64:                obj.styles[token] = ""
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/style.py:84:            for token in ttype.split():
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/style.py:85:                if token in _styles:
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/style.py:87:                ndef = _styles.get(token.parent, None)
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/style.py:88:                styledefs = obj.styles.get(token, "").split()
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/style.py:89:                if not ndef or token is None:
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/style.py:91:                elif "noinherit" in styledefs and token is not Token:
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/style.py:92:                    ndef = _styles[Token][:]
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/style.py:95:                _styles[token] = ndef
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/style.py:96:                for styledef in obj.styles.get(token, "").split():
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/style.py:126:    def style_for_token(cls, token):
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/style.py:127:        t = cls._styles[token]
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/style.py:159:    def styles_token(cls, ttype):
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/style.py:163:        for token in cls._styles:
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/style.py:164:            yield token, cls.style_for_token(token)
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/style.py:190:    #: Style definitions for individual token types.
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexers/_mapping.py:2760:    "RawTokenLexer": (
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexers/_mapping.py:2762:        "Raw token data",
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexers/_mapping.py:2765:        ("application/x-pygments-tokens",),
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexers/python.py:25:from pip._vendor.pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexers/python.py:130:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexers/python.py:746:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexers/python.py:1200:    Code tokens are output as ``Token.Other.Code``, traceback tokens as
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexers/python.py:1201:    ``Token.Other.Traceback``.
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexers/python.py:1203:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexers/python.py:1270:        # different tokens.  TODO: DelegatingLexer should support this
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexers/python.py:1272:        # distinguishing tokens. Then we wouldn't need this intermediary
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexers/python.py:1297:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexers/python.py:1359:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexers/python.py:1402:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexers/python.py:1727:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexers/python.py:2298:    def get_tokens_unprocessed(self, text):
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexers/python.py:2299:        for index, token, value in PythonLexer.get_tokens_unprocessed(self, text):
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexers/python.py:2300:            if token is Name and value in self.EXTRA_KEYWORDS:
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexers/python.py:2303:                yield index, token, value
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:18:from pip._vendor.pygments.token import Error, Other, Text, Whitespace, _TokenType
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:276:    def get_tokens(self, text, unfiltered=False):
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:280:        iterable of ``(tokentype, value)`` pairs from `text`.
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:284:        (`stripnl`, `stripall` and so on), and then yields all tokens
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:285:        from `get_tokens_unprocessed()`, with the ``index`` dropped.
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:293:            for _, t, v in self.get_tokens_unprocessed(text):
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:301:    def get_tokens_unprocessed(self, text):
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:304:        ``(index, tokentype, value)`` tuples where ``index`` is the starting
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:305:        position of the token within the input text.
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:317:    lexer, afterwards all ``Other`` tokens are lexed using the root
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:329:    def get_tokens_unprocessed(self, text):
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:333:        for i, t, v in self.language_lexer.get_tokens_unprocessed(text):
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:343:        return do_insertions(insertions, self.root_lexer.get_tokens_unprocessed(buffered))
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:420:            elif type(action) is _TokenType:
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:481:            for i, t, v in lx.get_tokens_unprocessed(match.group(), **gt_kwargs):
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:494:            for i, t, v in lx.get_tokens_unprocessed(match.group(), **gt_kwargs):
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:505:    For example default('#pop') is equivalent to ('', Token, '#pop')
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:534:    Metaclass for RegexLexer, creates the self._tokens attribute from
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:535:    self.tokens on the first instantiation.
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:539:        """Preprocess the regular expression component of a token definition."""
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:544:    def _process_token(cls, token):
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:545:        """Preprocess the token component of a token definition."""
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:546:        assert type(token) is _TokenType or callable(
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:547:            token
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:548:        ), f"token type must be simple type or callable, not {token!r}"
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:549:        return token
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:552:        """Preprocess the state transition action of a token definition."""
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:569:            itokens = []
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:572:                itokens.extend(cls._process_state(unprocessed, processed, istate))
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:573:            processed[tmp_state] = itokens
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:591:        tokens = processed[state] = []
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:597:                tokens.extend(cls._process_state(unprocessed, processed, str(tdef)))
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:606:                tokens.append((re.compile("").match, None, new_state))
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:618:            token = cls._process_token(tdef[1])
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:625:            tokens.append((rex, token, new_state))
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:626:        return tokens
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:628:    def process_tokendef(cls, name, tokendefs=None):
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:629:        """Preprocess a dictionary of token definitions."""
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:630:        processed = cls._all_tokens[name] = {}
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:631:        tokendefs = tokendefs or cls.tokens[name]
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:632:        for state in list(tokendefs):
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:633:            cls._process_state(tokendefs, processed, state)
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:636:    def get_tokendefs(cls):
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:638:        Merge tokens from superclasses in MRO order, returning a single tokendef
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:648:        tokens = {}
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:651:            toks = c.__dict__.get("tokens", {})
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:654:                curitems = tokens.get(state)
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:660:                    tokens[state] = items
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:683:        return tokens
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:686:        """Instantiate cls after preprocessing its token definitions."""
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:687:        if "_tokens" not in cls.__dict__:
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:688:            cls._all_tokens = {}
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:690:            if hasattr(cls, "token_variants") and cls.token_variants:
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:694:                cls._tokens = cls.process_tokendef("", cls.get_tokendefs())
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:713:    #: Dict of ``{'state': [(regex, tokentype, new_state), ...], ...}``
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:732:    tokens = {}
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:734:    def get_tokens_unprocessed(self, text, stack=("root",)):
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:736:        Split ``text`` into (tokentype, text) pairs.
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:741:        tokendefs = self._tokens
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:743:        statetokens = tokendefs[statestack[-1]]
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:745:            for rexmatch, action, new_state in statetokens:
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:749:                        if type(action) is _TokenType:
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:777:                        statetokens = tokendefs[statestack[-1]]
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:780:                # We are here only if all state tokens have been considered
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:786:                        statetokens = tokendefs["root"]
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:816:    def get_tokens_unprocessed(self, text=None, context=None):
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:818:        Split ``text`` into (tokentype, text) pairs.
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:821:        tokendefs = self._tokens
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:824:            statetokens = tokendefs["root"]
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:827:            statetokens = tokendefs[ctx.stack[-1]]
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:830:            for rexmatch, action, new_state in statetokens:
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:834:                        if type(action) is _TokenType:
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:841:                                statetokens = tokendefs[ctx.stack[-1]]
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:864:                        statetokens = tokendefs[ctx.stack[-1]]
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:873:                        statetokens = tokendefs["root"]
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:883:def do_insertions(insertions, tokens):
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:888:    ``insertions`` is a list of ``(index, itokens)`` pairs.
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:889:    Each ``itokens`` iterable should be inserted at position
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:890:    ``index`` into the token stream given by the ``tokens``
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:893:    The result is a combined token stream.
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:899:        index, itokens = next(insertions)
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:902:        yield from tokens
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:908:    # iterate over the token stream where we want to insert
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:909:    # the tokens from the insertion list.
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:910:    for i, t, v in tokens:
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:920:            for it_index, it_token, it_value in itokens:
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:921:                yield realpos, it_token, it_value
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:925:                index, itokens = next(insertions)
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:933:    # leftover tokens
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:935:        # no normal tokens, set realpos to zero
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:937:        for p, t, v in itokens:
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:941:            index, itokens = next(insertions)
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:975:    def get_tokens_unprocessed(self, text, stack=("root",)):
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py:978:        yield from RegexLexer.get_tokens_unprocessed(self, text, stack)
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/__init__.py:39:    and return an iterable of tokens. Currently, this only calls
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/__init__.py:40:    `lexer.get_tokens()`.
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/__init__.py:43:        return lexer.get_tokens(code)
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/__init__.py:53:def format(tokens, formatter, outfile=None):  # pylint: disable=redefined-builtin
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/__init__.py:55:    Format ``tokens`` (an iterable of tokens) with the formatter ``formatter``
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/__init__.py:65:            formatter.format(tokens, realoutfile)
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/__init__.py:68:            formatter.format(tokens, outfile)
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/filters/__init__.py:16:from pip._vendor.pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/filters/__init__.py:23:    string_to_tokentype,
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/filters/__init__.py:723:    """Highlight a normal Name (and Name.*) token with a different token type.
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/filters/__init__.py:729:            tokentype=Name.Function,
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/filters/__init__.py:733:    as functions. `Name.Function` is the default token type.
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/filters/__init__.py:738:      A list of names that should be given the different token type.
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/filters/__init__.py:740:    `tokentype` : TokenType or string
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/filters/__init__.py:741:      A token type or a string containing a token type name that is
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/filters/__init__.py:749:        tokentype = options.get("tokentype")
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/filters/__init__.py:750:        if tokentype:
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/filters/__init__.py:751:            self.tokentype = string_to_tokentype(tokentype)
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/filters/__init__.py:753:            self.tokentype = Name.Function
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/filters/__init__.py:758:                yield self.tokentype, value
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/filters/__init__.py:763:class ErrorToken(Exception):
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/filters/__init__.py:767:class RaiseOnErrorTokenFilter(Filter):
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/filters/__init__.py:768:    """Raise an exception when the lexer generates an error token.
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/filters/__init__.py:774:      The default is `pygments.filters.ErrorToken`.
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/filters/__init__.py:781:        self.exception = options.get("excclass", ErrorToken)
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/filters/__init__.py:818:    `wstokentype` : bool
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/filters/__init__.py:819:      If true, give whitespace the special `Whitespace` token type.  This allows
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/filters/__init__.py:839:        self.wstt = get_bool_opt(options, "wstokentype", True)
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/filters/__init__.py:901:            # Remove ``left`` tokens from first line, ``n`` from all others.
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/filters/__init__.py:912:class TokenMergeFilter(Filter):
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/filters/__init__.py:913:    """Merges consecutive tokens with the same token type in the output
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/filters/__init__.py:941:    "raiseonerror": RaiseOnErrorTokenFilter,
./.venv_tmp/lib/python3.12/site-packages/pip/_vendor/pygments/filters/__init__.py:944:    "tokenmerge": TokenMergeFilter,
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/vcs/versioncontrol.py:359:        information can be provided via the --username and --password options
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/vcs/versioncontrol.py:363:        Returns: (netloc, (username, password)).
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/vcs/versioncontrol.py:373:        Returns: (url, rev, (username, password)).
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/vcs/versioncontrol.py:398:    def make_rev_args(username: str | None, password: HiddenText | None) -> CommandArgs:
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/vcs/versioncontrol.py:409:        secret_url, rev, user_pass = self.get_url_rev_and_auth(url.secret)
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/vcs/versioncontrol.py:410:        username, secret_password = user_pass
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/vcs/versioncontrol.py:411:        password: HiddenText | None = None
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/vcs/versioncontrol.py:412:        if secret_password is not None:
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/vcs/versioncontrol.py:413:            password = hide_value(secret_password)
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/vcs/versioncontrol.py:414:        extra_args = self.make_rev_args(username, password)
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/vcs/versioncontrol.py:417:        return hide_url(secret_url), rev_options
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/vcs/versioncontrol.py:507:            if self.compare_urls(existing_url, url.secret):
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/vcs/mercurial.py:77:            config.set("paths", "default", url.secret)
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/vcs/subversion.py:80:        --username and --password options instead of via the URL.
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/vcs/subversion.py:83:            # The --username and --password options can't be used for
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/vcs/subversion.py:98:    def make_rev_args(username: str | None, password: HiddenText | None) -> CommandArgs:
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/vcs/subversion.py:102:        if password:
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/vcs/subversion.py:103:            extra_args += ["--password", password]
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/vcs/subversion.py:160:                # is being used to prompt for passwords, because passwords
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/vcs/subversion.py:269:        # the user can be prompted for a password, if required.
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/models/link.py:418:            # includes a username and password.
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/models/direct_url.py:171:        """url with user:password part removed unless it is formed with
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/req/req_file.py:435:    tokens = line.split(" ")
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/req/req_file.py:437:    options = tokens[:]
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/req/req_file.py:438:    for token in tokens:
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/req/req_file.py:439:        if token.startswith(("-", "--")):
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/req/req_file.py:442:            args.append(token)
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/commands/completion.py:41:                (commandline --current-process --tokenize --cut-at-cursor) \\
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/commands/completion.py:42:                (commandline --current-token --cut-at-cursor)
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/utils/wheel.py:70:        # and RuntimeError for password-protected files
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/utils/subprocess.py:54:    return [arg.secret if isinstance(arg, HiddenText) else arg for arg in args]
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/utils/misc.py:227:def ask_password(message: str) -> str:
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/utils/misc.py:228:    """Ask for a password interactively."""
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/utils/misc.py:416:    Returns: (netloc, (username, password)).
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/utils/misc.py:423:    # the password attribute of urlsplit()'s return value).
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/utils/misc.py:429:        # using the password attribute of the return value)
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/utils/misc.py:447:        - "accesstoken@example.com" returns "****@example.com"
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/utils/misc.py:449:    netloc, (user, password) = split_auth_from_netloc(netloc)
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/utils/misc.py:452:    if password is None:
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/utils/misc.py:454:        password = ""
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/utils/misc.py:457:        password = ":****"
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/utils/misc.py:458:    return f"{user}{password}@{netloc}"
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/utils/misc.py:495:    Returns: (url_without_auth, netloc, (username, password))
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/utils/misc.py:502:    """Return a copy of url with 'username:password@' removed."""
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/utils/misc.py:509:    """Replace the password in a given url with ****."""
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/utils/misc.py:514:    """Replace the password in a given requirement url with ****."""
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/utils/misc.py:522:    secret: str
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/utils/misc.py:538:        return self.secret == other.secret
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/network/auth.py:26:    ask_password,
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/network/auth.py:43:    password: str
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/network/auth.py:55:    def save_auth_info(self, url: str, username: str, password: str) -> None: ...
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/network/auth.py:66:    def save_auth_info(self, url: str, username: str, password: str) -> None:
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/network/auth.py:88:                return cred.username, cred.password
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/network/auth.py:92:            logger.debug("Getting password from keyring for %s", url)
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/network/auth.py:93:            password = self.keyring.get_password(url, username)
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/network/auth.py:94:            if password:
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/network/auth.py:95:                return username, password
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/network/auth.py:98:    def save_auth_info(self, url: str, username: str, password: str) -> None:
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/network/auth.py:99:        self.keyring.set_password(url, username, password)
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/network/auth.py:120:            password = self._get_password(url, username)
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/network/auth.py:121:            if password is not None:
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/network/auth.py:122:                return username, password
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/network/auth.py:125:    def save_auth_info(self, url: str, username: str, password: str) -> None:
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/network/auth.py:126:        return self._set_password(url, username, password)
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/network/auth.py:128:    def _get_password(self, service_name: str, username: str) -> str | None:
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/network/auth.py:129:        """Mirror the implementation of keyring.get_password using cli"""
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/network/auth.py:146:    def _set_password(self, service_name: str, username: str, password: str) -> None:
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/network/auth.py:147:        """Mirror the implementation of keyring.set_password using cli"""
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/network/auth.py:154:            input=f"{password}{os.linesep}".encode(),
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/network/auth.py:233:        self.passwords: dict[str, AuthInfo] = {}
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/network/auth.py:292:        The provided url should have had its username and password
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/network/auth.py:343:        url, netloc, url_user_password = split_auth_netloc_from_url(
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/network/auth.py:348:        username, password = url_user_password
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/network/auth.py:349:        if username is not None and password is not None:
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/network/auth.py:351:            return url_user_password
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/network/auth.py:359:                index_url, _, index_url_user_password = index_info
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/network/auth.py:363:        if index_url and index_url_user_password[0] is not None:
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/network/auth.py:364:            username, password = index_url_user_password
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/network/auth.py:365:            if username is not None and password is not None:
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/network/auth.py:367:                return index_url_user_password
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/network/auth.py:376:        # If we don't have a password and keyring is available, use it.
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/network/auth.py:389:        return username, password
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/network/auth.py:397:        Returns (url_without_credentials, username, password). Note
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/network/auth.py:399:        function may return a different username and password.
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/network/auth.py:404:        username, password = self._get_new_credentials(original_url)
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/network/auth.py:407:        # Do this if either the username or the password is missing.
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/network/auth.py:409:        # the username in the index url, but the password comes from keyring.
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/network/auth.py:410:        if (username is None or password is None) and netloc in self.passwords:
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/network/auth.py:411:            un, pw = self.passwords[netloc]
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/network/auth.py:415:                username, password = un, pw
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/network/auth.py:417:        if username is not None or password is not None:
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/network/auth.py:418:            # Convert the username and password if they're None, so that
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/network/auth.py:423:            password = password or ""
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/network/auth.py:426:            self.passwords[netloc] = (username, password)
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/network/auth.py:430:            (username is not None and password is not None)
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/network/auth.py:432:            or (username is None and password is None)
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/network/auth.py:435:        return url, username, password
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/network/auth.py:439:        url, username, password = self._get_url_and_credentials(req.url)
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/network/auth.py:444:        if username is not None and password is not None:
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/network/auth.py:446:            req = HTTPBasicAuth(username, password)(req)
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/network/auth.py:454:    def _prompt_for_password(self, netloc: str) -> tuple[str | None, str | None, bool]:
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/network/auth.py:462:        password = ask_password("Password: ")
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/network/auth.py:463:        return username, password, True
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/network/auth.py:466:    def _should_save_password_to_keyring(self) -> bool:
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/network/auth.py:477:        username, password = None, None
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/network/auth.py:481:            username, password = self._get_new_credentials(
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/network/auth.py:488:        if not self.prompting and not username and not password:
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/network/auth.py:493:        # Prompt the user for a new username and password
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/network/auth.py:495:        if not username and not password:
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/network/auth.py:496:            username, password, save = self._prompt_for_password(parsed.netloc)
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/network/auth.py:498:        # Store the new username and password to use for future requests
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/network/auth.py:500:        if username is not None and password is not None:
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/network/auth.py:501:            self.passwords[parsed.netloc] = (username, password)
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/network/auth.py:503:            # Prompt to save the password to keyring
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/network/auth.py:504:            if save and self._should_save_password_to_keyring():
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/network/auth.py:508:                    password=password,
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/network/auth.py:518:        # Add our new username and password to the request
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/network/auth.py:519:        req = HTTPBasicAuth(username or "", password or "")(resp.request)
./.venv_tmp/lib/python3.12/site-packages/pip/_internal/network/auth.py:551:                self.keyring_provider.save_auth_info(creds.url, creds.username, creds.password)
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/driver.py:31:from blib2to3.pgen2.tokenize import TokenInfo
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/driver.py:35:from . import grammar, parse, pgen, token, tokenize
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/driver.py:44:    tokens: list[Any] = field(default_factory=list)
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/driver.py:47:        total_eaten = len(self.tokens)
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/driver.py:51:class TokenProxy:
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/driver.py:53:        self._tokens = generator
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/driver.py:58:    def release(self) -> Iterator["TokenProxy"]:
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/driver.py:69:        eaten_tokens = self._release_ranges[-1].tokens
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/driver.py:70:        if point < len(eaten_tokens):
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/driver.py:71:            return eaten_tokens[point]
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/driver.py:73:            while point >= len(eaten_tokens):
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/driver.py:74:                token = next(self._tokens)
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/driver.py:75:                eaten_tokens.append(token)
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/driver.py:76:            return token
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/driver.py:78:    def __iter__(self) -> "TokenProxy":
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/driver.py:83:        # return the eaten token, if not just go further on the given
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/driver.py:84:        # token producer.
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/driver.py:90:                token = release_range.tokens[self._counter - start]
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/driver.py:93:            token = next(self._tokens)
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/driver.py:95:        return token
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/driver.py:115:    def parse_tokens(self, tokens: Iterable[TokenInfo], debug: bool = False) -> NL:
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/driver.py:116:        """Parse a series of tokens and return the syntax tree."""
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/driver.py:117:        # XXX Move the prefix computation into a wrapper around tokenize.
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/driver.py:118:        proxy = TokenProxy(tokens)
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/driver.py:141:            if type in (tokenize.COMMENT, tokenize.NL):
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/driver.py:148:            if type == token.OP:
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/driver.py:152:                self.logger.debug("%s %r (prefix=%r)", token.tok_name[type], value, prefix)
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/driver.py:153:            if type == token.INDENT:
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/driver.py:158:            elif type == token.DEDENT:
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/driver.py:161:            if p.addtoken(cast(int, type), value, (prefix, start)):
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/driver.py:166:            if type in {token.INDENT, token.DEDENT}:
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/driver.py:169:            # FSTRING_MIDDLE is the only token that can end with a newline, and
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/driver.py:171:            if value.endswith("\n") and type != token.FSTRING_MIDDLE:
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/driver.py:189:        tokens = tokenize.tokenize(text, grammar=self.grammar)
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/driver.py:190:        return self.parse_tokens(tokens, debug)
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:6:"""Tokenization help for Python programs.
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:8:generate_tokens(readline) is a generator that breaks a stream of
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:9:text into Python tokens.  It accepts a readline-like method which is called
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:13:    the token type (see token.py)
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:14:    the token (a string)
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:15:    the starting (row, column) indices of the token (a 2-tuple of ints)
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:16:    the ending (row, column) indices of the token (a 2-tuple of ints)
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:19:It is designed to match the working of the Python tokenizer exactly, except
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:20:that it produces COMMENT tokens for comments and gives type OP for all
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:24:    tokenize_loop(readline, tokeneater)
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:25:    tokenize(readline, tokeneater=printtoken)
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:26:are the same, except instead of generating tokens, tokeneater is a callback
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:28:each time a new token is found."""
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:35:from blib2to3.pgen2.token import (
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:41:    ERRORTOKEN,
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:58:import pytokens
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:59:from pytokens import TokenType
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:61:from . import token as _token
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:63:__all__ = [x for x in dir(_token) if x[0] != "_"] + [
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:64:    "tokenize",
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:65:    "generate_tokens",
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:66:    "untokenize",
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:68:del _token
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:71:TokenInfo = tuple[int, str, Coord, Coord, str]
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:73:TOKEN_TYPE_MAP = {
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:74:    TokenType.indent: INDENT,
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:75:    TokenType.dedent: DEDENT,
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:76:    TokenType.newline: NEWLINE,
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:77:    TokenType.nl: NL,
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:78:    TokenType.comment: COMMENT,
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:79:    TokenType.semicolon: OP,
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:80:    TokenType.lparen: OP,
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:81:    TokenType.rparen: OP,
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:82:    TokenType.lbracket: OP,
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:83:    TokenType.rbracket: OP,
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:84:    TokenType.lbrace: OP,
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:85:    TokenType.rbrace: OP,
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:86:    TokenType.colon: OP,
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:87:    TokenType.op: OP,
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:88:    TokenType.identifier: NAME,
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:89:    TokenType.number: NUMBER,
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:90:    TokenType.string: STRING,
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:91:    TokenType.fstring_start: FSTRING_START,
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:92:    TokenType.fstring_middle: FSTRING_MIDDLE,
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:93:    TokenType.fstring_end: FSTRING_END,
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:94:    TokenType.endmarker: ENDMARKER,
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:98:class TokenError(Exception): ...
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:102:    token: pytokens.Token, source: str, prev_token: Optional[pytokens.Token]
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:103:) -> pytokens.Token:
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:105:    Black treats `\\\n` at the end of a line as a 'NL' token, while it
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:111:        token.type == TokenType.whitespace
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:112:        and prev_token is not None
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:113:        and prev_token.type not in (TokenType.nl, TokenType.newline)
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:115:        token_str = source[token.start_index : token.end_index]
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:116:        if token_str.startswith("\\\r\n"):
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:117:            return pytokens.Token(
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:118:                TokenType.nl,
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:119:                token.start_index,
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:120:                token.start_index + 3,
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:121:                token.start_line,
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:122:                token.start_col,
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:123:                token.start_line,
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:124:                token.start_col + 3,
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:126:        elif token_str.startswith("\\\n") or token_str.startswith("\\\r"):
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:127:            return pytokens.Token(
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:128:                TokenType.nl,
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:129:                token.start_index,
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:130:                token.start_index + 2,
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:131:                token.start_line,
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:132:                token.start_col,
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:133:                token.start_line,
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:134:                token.start_col + 2,
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:137:    return token
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:140:def tokenize(source: str, grammar: Optional[Grammar] = None) -> Iterator[TokenInfo]:
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:142:    lines += [""]  # For newline tokens in files that don't end in a newline
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:145:    prev_token: Optional[pytokens.Token] = None
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:147:        for token in pytokens.tokenize(source):
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:148:            token = transform_whitespace(token, source, prev_token)
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:150:            line, column = token.start_line, token.start_col
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:151:            if token.type == TokenType.whitespace:
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:154:            token_str = source[token.start_index : token.end_index]
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:156:            if token.type == TokenType.newline and token_str == "":
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:157:                # Black doesn't yield empty newline tokens at the end of a file
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:159:                prev_token = token
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:162:            source_line = lines[token.start_line - 1]
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:164:            if token.type == TokenType.identifier and token_str in ("async", "await"):
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:165:                # Black uses `async` and `await` token types just for those two keywords
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:167:                    ASYNC if token_str == "async" else AWAIT,
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:168:                    token_str,
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:169:                    (token.start_line, token.start_col),
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:170:                    (token.end_line, token.end_col),
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:173:            elif token.type == TokenType.op and token_str == "...":
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:174:                # Black doesn't have an ellipsis token yet, yield 3 DOTs instead
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:175:                assert token.start_line == token.end_line
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:176:                assert token.end_col == token.start_col + 3
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:178:                token_str = "."
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:179:                for start_col in range(token.start_col, token.start_col + 3):
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:182:                        TOKEN_TYPE_MAP[token.type],
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:183:                        token_str,
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:184:                        (token.start_line, start_col),
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:185:                        (token.end_line, end_col),
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:190:                    TOKEN_TYPE_MAP[token.type],
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:191:                    token_str,
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:192:                    (token.start_line, token.start_col),
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:193:                    (token.end_line, token.end_col),
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:196:            prev_token = token
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:198:    except pytokens.UnexpectedEOF:
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:199:        raise TokenError("Unexpected EOF in multi-line statement", (line, column))
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:200:    except pytokens.TokenizeError as exc:
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:201:        raise TokenError(f"Failed to parse: {type(exc).__name__}", (line, column))
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:204:def printtoken(
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:205:    type: int, token: str, srow_col: Coord, erow_col: Coord, line: str
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:209:    print(f"{srow},{scol}-{erow},{ecol}:\t{tok_name[type]}\t{token!r}")
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:214:        token_iterator = tokenize(open(sys.argv[1]).read())
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:216:        token_iterator = tokenize(sys.stdin.read())
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:218:    for tok in token_iterator:
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/tokenize.py:219:        printtoken(*tok)
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/grammar.py:10:token module; the Python tokenize module reports all operators as the
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/grammar.py:11:fallback token code OP, but the parser needs the actual token code.
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/grammar.py:22:from . import token
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/grammar.py:50:                     them from token numbers, which are between 0 and
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/grammar.py:66:                     above, and first is a set of tokens that can
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/grammar.py:70:    labels        -- a list of (x, y) pairs where x is either a token
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/grammar.py:81:    tokens        -- a dict mapping token numbers to arc labels.
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/grammar.py:93:        self.tokens: dict[int, int] = {}
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/grammar.py:140:            "tokens",
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/grammar.py:168:# Map from operator to number (since tokenize doesn't do this)
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/grammar.py:226:        opmap[op] = getattr(token, name)
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/token.py:1:"""Token constants (from "token.h")."""
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/token.py:5:#  Taken from Python (r53757) and modified to include some tokens
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/token.py:6:#   originally monkeypatched in by pgen2.tokenize
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/token.py:67:ERRORTOKEN: Final = 58
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/token.py:73:N_TOKENS: Final = 64
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/conv.py:14:Note that the token numbers are constants determined by the standard
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/conv.py:15:Python tokenizer.  The standard token module defines these numbers and
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/conv.py:16:their names (the names are not used much).  The token numbers are
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/conv.py:17:hardcoded into the Python tokenizer and into pgen.  A Python
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/conv.py:18:implementation of the Python tokenizer is also available, in the
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/conv.py:19:standard tokenize module.
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/conv.py:35:from pgen2 import grammar, token
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/conv.py:251:        self.tokens = {}  # map from numeric token values to arc labels
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/conv.py:253:            if type == token.NAME and value is not None:
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/conv.py:256:                self.tokens[type] = ilabel
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/pgen.py:8:from blib2to3.pgen2 import grammar, token, tokenize
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/pgen.py:9:from blib2to3.pgen2.tokenize import TokenInfo
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/pgen.py:21:    generator: Iterator[TokenInfo]
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/pgen.py:30:        self.generator = tokenize.tokenize(stream.read())
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/pgen.py:31:        self.gettoken()  # Initialize lookahead
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/pgen.py:35:        self.first = {}  # map from symbol name to set of tokens
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/pgen.py:77:            # Either a symbol name or a named token
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/pgen.py:87:                # A named token (NAME, NUMBER, STRING)
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/pgen.py:88:                itoken = getattr(token, label, None)
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/pgen.py:89:                assert isinstance(itoken, int), label
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/pgen.py:90:                assert itoken in token.tok_name, label
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/pgen.py:91:                if itoken in c.tokens:
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/pgen.py:92:                    return c.tokens[itoken]
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/pgen.py:94:                    c.labels.append((itoken, None))
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/pgen.py:95:                    c.tokens[itoken] = ilabel
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/pgen.py:111:                    c.labels.append((token.NAME, value))
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/pgen.py:115:                # An operator (any non-numeric token)
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/pgen.py:116:                itoken = grammar.opmap[value]  # Fails if unknown token
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/pgen.py:117:                if itoken in c.tokens:
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/pgen.py:118:                    return c.tokens[itoken]
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/pgen.py:120:                    c.labels.append((itoken, None))
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/pgen.py:121:                    c.tokens[itoken] = ilabel
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/pgen.py:168:        while self.type != token.ENDMARKER:
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/pgen.py:169:            while self.type == token.NEWLINE:
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/pgen.py:170:                self.gettoken()
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/pgen.py:172:            name = self.expect(token.NAME)
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/pgen.py:173:            self.expect(token.OP, ":")
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/pgen.py:175:            self.expect(token.NEWLINE)
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/pgen.py:283:                self.gettoken()
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/pgen.py:292:        while self.value in ("(", "[") or self.type in (token.NAME, token.STRING):
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/pgen.py:301:            self.gettoken()
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/pgen.py:303:            self.expect(token.OP, "]")
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/pgen.py:311:            self.gettoken()
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/pgen.py:321:            self.gettoken()
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/pgen.py:323:            self.expect(token.OP, ")")
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/pgen.py:325:        elif self.type in (token.NAME, token.STRING):
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/pgen.py:329:            self.gettoken()
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/pgen.py:338:        self.gettoken()
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/pgen.py:341:    def gettoken(self) -> None:
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/pgen.py:343:        while tup[0] in (tokenize.COMMENT, tokenize.NL):
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/pgen.py:346:        # print token.tok_name[self.type], repr(self.value)
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/parse.py:20:from . import grammar, token, tokenize
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/parse.py:23:    from blib2to3.pgen2.driver import TokenProxy
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/parse.py:91:    def add_token(self, tok_type: int, tok_val: str, raw: bool = False) -> None:
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/parse.py:95:                    self.parser._addtoken(ilabel, tok_type, tok_val, self.context)
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/parse.py:97:                    self.parser.addtoken(tok_type, tok_val, self.context)
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/parse.py:132:    <for each input token>:
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/parse.py:133:        if p.addtoken(...):           # parse a token; may raise ParseError
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/parse.py:139:    A Parser instance contains state pertaining to the current token
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/parse.py:141:    to parse separate token sequences.
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/parse.py:143:    See driver.py for how to get input tokens by tokenizing a file or
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/parse.py:146:    Parsing is complete when addtoken() returns True; the root of the
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/parse.py:148:    instance variable.  When a syntax error occurs, addtoken() raises
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/parse.py:179:        tuple, where type is the node type (a token or symbol number),
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/parse.py:180:        value is None for symbols and a string for tokens, context is
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/parse.py:183:        symbols, and None for tokens.
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/parse.py:193:        self.last_token: Optional[int] = None
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/parse.py:195:    def setup(self, proxy: "TokenProxy", start: Optional[int] = None) -> None:
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/parse.py:219:        self.last_token = None
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/parse.py:221:    def addtoken(self, type: int, value: str, context: Context) -> bool:
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/parse.py:222:        """Add a token; return True iff this is the end of the program."""
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/parse.py:223:        # Map from token to label
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/parse.py:231:            return self._addtoken(ilabel, type, value, context)
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/parse.py:245:            recorder.add_token(type, value, raw=True)
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/parse.py:247:            next_token_value = value
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/parse.py:248:            while recorder.determine_route(next_token_value) is None:
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/parse.py:253:                next_token_type, next_token_value, *_ = proxy.eat(counter)
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/parse.py:254:                if next_token_type in (tokenize.COMMENT, tokenize.NL):
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/parse.py:258:                if next_token_type == tokenize.OP:
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/parse.py:259:                    next_token_type = grammar.opmap[next_token_value]
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/parse.py:261:                recorder.add_token(next_token_type, next_token_value)
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/parse.py:264:            ilabel = cast(int, recorder.determine_route(next_token_value, force=force))
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/parse.py:267:        return self._addtoken(ilabel, type, value, context)
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/parse.py:269:    def _addtoken(self, ilabel: int, type: int, value: str, context: Context) -> bool:
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/parse.py:270:        # Loop until the token is shifted; may raise exceptions
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/parse.py:289:                    # Shift a token; we're done with it
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/parse.py:300:                    # Done with this token
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/parse.py:301:                    self.last_token = type
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/parse.py:309:                        # Done parsing, but another token is input
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/parse.py:316:        """Turn a token into a label.  (Internal)
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/parse.py:320:        if type == token.NAME:
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/parse.py:327:                assert type in self.grammar.tokens
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/parse.py:333:                if self.last_token not in (
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/parse.py:335:                    token.INDENT,
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/parse.py:336:                    token.DEDENT,
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/parse.py:337:                    token.NEWLINE,
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/parse.py:338:                    token.SEMI,
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/parse.py:339:                    token.COLON,
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/parse.py:341:                    return [self.grammar.tokens[type]]
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/parse.py:343:                    self.grammar.tokens[type],
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/parse.py:347:        ilabel = self.grammar.tokens.get(type)
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/parse.py:349:            raise ParseError("bad token", type, value, context)
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pgen2/parse.py:353:        """Shift a token.  (Internal)"""
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pytree.py:7:This is a very concrete parse tree; we need to keep every token and
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pytree.py:8:even the comments and whitespace between tokens.
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pytree.py:38:        # printing tokens is possible but not as useful
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pytree.py:39:        # from .pgen2 import token // token.__dict__.items():
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pytree.py:65:    type: int  # int: token number (< 256) or symbol number (>= 256)
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pytree.py:378:    _prefix = ""  # Whitespace and comments preceding this token in the input
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pytree.py:379:    lineno: int = 0  # Line where this token starts in the input
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pytree.py:380:    column: int = 0  # Column where this token starts in the input
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pytree.py:399:        Takes a type constant (a token number < 256), a string value, and an
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pytree.py:417:        from .pgen2.token import tok_name
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pytree.py:460:        The whitespace and comments preceding this token in the input.
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pytree.py:497:    It looks for a specific node type (token or symbol), and
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pytree.py:510:    type = None  # Node type (token if < 256, symbol if >= 256)
./.venv_tmp/lib/python3.12/site-packages/blib2to3/pytree.py:594:        The type, if given must be a token type (< 256).  If not given,
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/markers.py:19:from ._tokenizer import ParserSyntaxError
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_tokenizer.py:10:class Token:
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_tokenizer.py:88:class Tokenizer:
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_tokenizer.py:89:    """Context-sensitive token parsing.
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_tokenizer.py:91:    Provides methods to examine the input stream to check whether the next token
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_tokenizer.py:105:        self.next_token: Optional[Token] = None
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_tokenizer.py:109:        """Move beyond provided token name, if at current position."""
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_tokenizer.py:114:        """Check whether the next token has the provided name.
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_tokenizer.py:116:        By default, if the check succeeds, the token *must* be read before
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_tokenizer.py:117:        another check. If `peek` is set to `True`, the token is not loaded and
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_tokenizer.py:121:            self.next_token is None
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_tokenizer.py:122:        ), f"Cannot check for {name!r}, already have {self.next_token!r}"
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_tokenizer.py:123:        assert name in self.rules, f"Unknown token name: {name!r}"
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_tokenizer.py:131:            self.next_token = Token(name, match[0], self.position)
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_tokenizer.py:134:    def expect(self, name: str, *, expected: str) -> Token:
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_tokenizer.py:135:        """Expect a certain token name next, failing with a syntax error otherwise.
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_tokenizer.py:137:        The token is *not* read.
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_tokenizer.py:143:    def read(self) -> Token:
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_tokenizer.py:144:        """Consume the next token and return it."""
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_tokenizer.py:145:        token = self.next_token
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_tokenizer.py:146:        assert token is not None
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_tokenizer.py:148:        self.position += len(token.text)
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_tokenizer.py:149:        self.next_token = None
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_tokenizer.py:151:        return token
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_tokenizer.py:172:    def enclosing_tokens(self, open_token: str, close_token: str, *, around: str) -> Iterator[None]:
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_tokenizer.py:173:        if self.check(open_token):
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_tokenizer.py:184:        if not self.check(close_token):
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_tokenizer.py:186:                f"Expected matching {close_token} for {open_token}, after {around}",
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:10:from ._tokenizer import DEFAULT_RULES, Tokenizer
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:64:    return _parse_requirement(Tokenizer(source, rules=DEFAULT_RULES))
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:67:def _parse_requirement(tokenizer: Tokenizer) -> ParsedRequirement:
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:71:    tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:73:    name_token = tokenizer.expect(
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:76:    name = name_token.text
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:77:    tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:79:    extras = _parse_extras(tokenizer)
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:80:    tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:82:    url, specifier, marker = _parse_requirement_details(tokenizer)
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:83:    tokenizer.expect("END", expected="end of dependency specifier")
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:89:    tokenizer: Tokenizer,
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:100:    if tokenizer.check("AT"):
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:101:        tokenizer.read()
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:102:        tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:104:        url_start = tokenizer.position
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:105:        url = tokenizer.expect("URL", expected="URL after @").text
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:106:        if tokenizer.check("END", peek=True):
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:109:        tokenizer.expect("WS", expected="whitespace after URL")
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:112:        if tokenizer.check("END", peek=True):
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:116:            tokenizer, span_start=url_start, after="URL and whitespace"
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:119:        specifier_start = tokenizer.position
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:120:        specifier = _parse_specifier(tokenizer)
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:121:        tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:123:        if tokenizer.check("END", peek=True):
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:127:            tokenizer,
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:135:def _parse_requirement_marker(tokenizer: Tokenizer, *, span_start: int, after: str) -> MarkerList:
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:140:    if not tokenizer.check("SEMICOLON"):
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:141:        tokenizer.raise_syntax_error(
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:145:    tokenizer.read()
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:147:    marker = _parse_marker(tokenizer)
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:148:    tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:153:def _parse_extras(tokenizer: Tokenizer) -> List[str]:
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:157:    if not tokenizer.check("LEFT_BRACKET", peek=True):
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:160:    with tokenizer.enclosing_tokens(
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:165:        tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:166:        extras = _parse_extras_list(tokenizer)
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:167:        tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:172:def _parse_extras_list(tokenizer: Tokenizer) -> List[str]:
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:178:    if not tokenizer.check("IDENTIFIER"):
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:181:    extras.append(tokenizer.read().text)
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:184:        tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:185:        if tokenizer.check("IDENTIFIER", peek=True):
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:186:            tokenizer.raise_syntax_error("Expected comma between extra names")
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:187:        elif not tokenizer.check("COMMA"):
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:190:        tokenizer.read()
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:191:        tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:193:        extra_token = tokenizer.expect("IDENTIFIER", expected="extra name after comma")
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:194:        extras.append(extra_token.text)
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:199:def _parse_specifier(tokenizer: Tokenizer) -> str:
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:204:    with tokenizer.enclosing_tokens(
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:209:        tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:210:        parsed_specifiers = _parse_version_many(tokenizer)
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:211:        tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:216:def _parse_version_many(tokenizer: Tokenizer) -> str:
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:221:    while tokenizer.check("SPECIFIER"):
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:222:        span_start = tokenizer.position
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:223:        parsed_specifiers += tokenizer.read().text
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:224:        if tokenizer.check("VERSION_PREFIX_TRAIL", peek=True):
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:225:            tokenizer.raise_syntax_error(
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:228:                span_end=tokenizer.position + 1,
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:230:        if tokenizer.check("VERSION_LOCAL_LABEL_TRAIL", peek=True):
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:231:            tokenizer.raise_syntax_error(
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:234:                span_end=tokenizer.position,
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:236:        tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:237:        if not tokenizer.check("COMMA"):
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:239:        parsed_specifiers += tokenizer.read().text
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:240:        tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:249:    return _parse_full_marker(Tokenizer(source, rules=DEFAULT_RULES))
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:252:def _parse_full_marker(tokenizer: Tokenizer) -> MarkerList:
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:253:    retval = _parse_marker(tokenizer)
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:254:    tokenizer.expect("END", expected="end of marker expression")
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:258:def _parse_marker(tokenizer: Tokenizer) -> MarkerList:
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:262:    expression = [_parse_marker_atom(tokenizer)]
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:263:    while tokenizer.check("BOOLOP"):
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:264:        token = tokenizer.read()
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:265:        expr_right = _parse_marker_atom(tokenizer)
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:266:        expression.extend((token.text, expr_right))
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:270:def _parse_marker_atom(tokenizer: Tokenizer) -> MarkerAtom:
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:276:    tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:277:    if tokenizer.check("LEFT_PARENTHESIS", peek=True):
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:278:        with tokenizer.enclosing_tokens(
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:283:            tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:284:            marker: MarkerAtom = _parse_marker(tokenizer)
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:285:            tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:287:        marker = _parse_marker_item(tokenizer)
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:288:    tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:292:def _parse_marker_item(tokenizer: Tokenizer) -> MarkerItem:
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:296:    tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:297:    marker_var_left = _parse_marker_var(tokenizer)
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:298:    tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:299:    marker_op = _parse_marker_op(tokenizer)
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:300:    tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:301:    marker_var_right = _parse_marker_var(tokenizer)
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:302:    tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:306:def _parse_marker_var(tokenizer: Tokenizer) -> MarkerVar:
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:310:    if tokenizer.check("VARIABLE"):
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:311:        return process_env_var(tokenizer.read().text.replace(".", "_"))
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:312:    elif tokenizer.check("QUOTED_STRING"):
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:313:        return process_python_str(tokenizer.read().text)
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:315:        tokenizer.raise_syntax_error(message="Expected a marker variable or quoted string")
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:330:def _parse_marker_op(tokenizer: Tokenizer) -> Op:
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:334:    if tokenizer.check("IN"):
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:335:        tokenizer.read()
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:337:    elif tokenizer.check("NOT"):
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:338:        tokenizer.read()
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:339:        tokenizer.expect("WS", expected="whitespace after 'not'")
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:340:        tokenizer.expect("IN", expected="'in' after 'not'")
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:342:    elif tokenizer.check("OP"):
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:343:        return Op(tokenizer.read().text)
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/_parser.py:345:        return tokenizer.raise_syntax_error(
./.venv_tmp/lib/python3.12/site-packages/wheel/vendored/packaging/requirements.py:8:from ._tokenizer import ParserSyntaxError
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:160:     - collect_all_And_tokens - flag to enable fix for Issue #63 that fixes erroneous grouping
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:165:__compat__.collect_all_And_tokens = True
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:170:     - warn_multiple_tokens_in_named_alternation - flag to enable warnings when a results
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:172:       (only warns if __compat__.collect_all_And_tokens is False)
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:173:     - warn_ungrouped_named_tokens_in_collection - flag to enable warnings when a results
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:183:__diag__.warn_multiple_tokens_in_named_alternation = False
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:184:__diag__.warn_ungrouped_named_tokens_in_collection = False
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:194:    __diag__.warn_multiple_tokens_in_named_alternation = True
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:195:    __diag__.warn_ungrouped_named_tokens_in_collection = True
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:248:    "Token",
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:249:    "TokenConverter",
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:271:    "downcaseTokens",
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:305:    "upcaseTokens",
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:314:    "tokenMap",
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:755:            # fixup indices in token dictionary
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:834:        semantics and pop tokens from the list of parsed tokens. If passed
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:842:            def remove_first(tokens):
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:843:                tokens.pop(0)
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:853:            def remove_LABEL(tokens):
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:854:                tokens.pop("LABEL")
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:855:                return tokens
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:907:        Inserts new element at location index in the list of parsed tokens.
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:916:            def insert_locn(locn, tokens):
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:917:                tokens.insert(0, locn)
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:921:        # fixup indices in token dictionary
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:935:            def append_sum(tokens):
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:936:                tokens.append(sum(map(int, tokens)))
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:950:            def make_palindrome(tokens):
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:951:                tokens.extend(reversed([t[::-1] for t in tokens]))
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:952:                return ''.join(tokens)
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:1030:        Returns the parse results as a nested list of matching tokens, all converted to strings.
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:1094:        (Deprecated) Returns the parse results as XML. Tags are created for tokens and lists that have defined results names.
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:1140:                # individual token, see if there is a name for it
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:1164:        Returns the results name for this token expression. Useful when several
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:1655:        Define name for referencing matching tokens as a nested attribute
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:1715:        - toks = a list of the matched tokens, packaged as a :class:`ParseResults` object
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:1717:        If the functions in fns modify the tokens, they can return them as the return
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:1718:        value from fn, and the modified list of tokens will replace the original.
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:1837:    def postParse(self, instring, loc, tokenlist):
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:1838:        return tokenlist
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:1854:                tokensStart = preloc
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:1857:                        loc, tokens = self.parseImpl(instring, preloc, doActions)
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:1861:                    loc, tokens = self.parseImpl(instring, preloc, doActions)
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:1865:                    self.debugActions[FAIL](instring, tokensStart, self, err)
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:1867:                    self.failAction(instring, tokensStart, self, err)
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:1874:            tokensStart = preloc
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:1877:                    loc, tokens = self.parseImpl(instring, preloc, doActions)
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:1881:                loc, tokens = self.parseImpl(instring, preloc, doActions)
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:1883:        tokens = self.postParse(instring, loc, tokens)
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:1885:        retTokens = ParseResults(
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:1886:            tokens, self.resultsName, asList=self.saveAsList, modal=self.modalResults
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:1893:                            tokens = fn(instring, tokensStart, retTokens)
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:1899:                        if tokens is not None and tokens is not retTokens:
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:1900:                            retTokens = ParseResults(
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:1901:                                tokens,
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:1903:                                asList=self.saveAsList and isinstance(tokens, (ParseResults, list)),
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:1909:                        self.debugActions[FAIL](instring, tokensStart, self, err)
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:1914:                        tokens = fn(instring, tokensStart, retTokens)
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:1920:                    if tokens is not None and tokens is not retTokens:
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:1921:                        retTokens = ParseResults(
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:1922:                            tokens,
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:1924:                            asList=self.saveAsList and isinstance(tokens, (ParseResults, list)),
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:1928:            # ~ print ("Matched", self, "->", retTokens.asList())
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:1930:                self.debugActions[MATCH](instring, tokensStart, loc, self, retTokens)
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:1932:        return loc, retTokens
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:2148:            loc, tokens = self._parse(instring, 0)
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:2162:            return tokens
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:2167:        matching tokens, start location, and end location.  May be called with optional
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:2179:            for tokens, start, end in Word(alphas).scanString(source):
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:2181:                print(' '*start + tokens[0])
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:2212:                    nextLoc, tokens = parseFn(instring, preloc, callPreParse=False)
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:2218:                        yield tokens, preloc, nextLoc
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:2240:        Extension to :class:`scanString`, to modify matching text with modified tokens that may
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:2242:        attach a parse action to it that modifies the returned token list.
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:2288:        Another extension to :class:`scanString`, simplifying the access to the tokens found
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:3086:class Token(ParserElement):
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:3092:        super(Token, self).__init__(savelist=False)
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:3095:class Empty(Token):
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:3096:    """An empty token, will always match."""
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:3105:class NoMatch(Token):
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:3106:    """A token that will never match."""
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:3113:        self.errmsg = "Unmatchable token"
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:3119:class Literal(Token):
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:3120:    """Token to exactly match a specified string.
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:3172:class Keyword(Token):
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:3173:    """Token to exactly match a specified string as a keyword, that is,
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:3260:    """Token to match a specified string, ignoring case of letters.
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:3299:class CloseMatch(Token):
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:3372:class Word(Token):
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:3373:    """Token for matching words composed of allowed character sets.
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:3575:class Regex(Token):
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:3576:    r"""Token for matching strings that match a given regular
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:3716:            def pa(tokens):
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:3717:                return tokens[0].expand(repl)
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:3721:            def pa(tokens):
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:3722:                return self.re.sub(repl, tokens[0])
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:3727:class QuotedString(Token):
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:3729:    Token for matching strings that are delimited by quoting characters.
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:3905:class CharsNotIn(Token):
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:3906:    """Token for matching words composed of characters *not* in a given
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:3984:class White(Token):
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:4056:class _PositionToken(Token):
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:4058:        super(_PositionToken, self).__init__()
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:4064:class GoToColumn(_PositionToken):
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:4065:    """Token to advance to a specific column of input text; useful for
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:4091:class LineStart(_PositionToken):
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:4124:class LineEnd(_PositionToken):
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:4146:class StringStart(_PositionToken):
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:4163:class StringEnd(_PositionToken):
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:4181:class WordStart(_PositionToken):
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:4203:class WordEnd(_PositionToken):
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:4228:    post-processing parsed tokens.
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:4341:        if __diag__.warn_ungrouped_named_tokens_in_collection:
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:4347:                            "warn_ungrouped_named_tokens_in_collection",
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:4436:                    loc, exprtokens = e._parse(instring, loc, doActions)
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:4445:                loc, exprtokens = e._parse(instring, loc, doActions)
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:4446:            if exprtokens or exprtokens.haskeys():
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:4447:                resultlist += exprtokens
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:4499:        if __compat__.collect_all_And_tokens:
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:4584:            not __compat__.collect_all_And_tokens
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:4585:            and __diag__.warn_multiple_tokens_in_named_alternation
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:4590:                    "may only return a single token for an And alternative, "
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:4591:                    "in future will return the full list of tokens".format(
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:4592:                        "warn_multiple_tokens_in_named_alternation", name, type(self).__name__
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:4627:        if __compat__.collect_all_And_tokens:
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:4676:            not __compat__.collect_all_And_tokens
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:4677:            and __diag__.warn_multiple_tokens_in_named_alternation
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:4682:                    "may only return a single token for an And alternative, "
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:4683:                    "in future will return the full list of tokens".format(
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:4684:                        "warn_multiple_tokens_in_named_alternation", name, type(self).__name__
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:4834:    post-processing parsed tokens.
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:4840:            if issubclass(self._literalStringClass, Token):
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:4917:    always returns a null token list. If any results names are defined
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:4953:    returns a null token list, but if a results name is defined on the
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:4992:        elif isinstance(expr, _PositionToken):
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:5030:    a null token list.  May be constructed using the '~' operator.
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:5055:        self.errmsg = "Found unwanted token, " + _ustr(self.expr)
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:5098:        loc, tokens = self_expr_parse(instring, loc, doActions, callPreParse=False)
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:5108:                loc, tmptokens = self_expr_parse(instring, preloc, doActions)
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:5109:                if tmptokens or tmptokens.haskeys():
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:5110:                    tokens += tmptokens
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:5114:        return loc, tokens
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:5117:        if __diag__.warn_ungrouped_named_tokens_in_collection:
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:5123:                            "warn_ungrouped_named_tokens_in_collection",
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:5202:class _NullToken(object):
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:5250:    __optionalNotMatched = _NullToken()
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:5260:            loc, tokens = self.expr._parse(instring, loc, doActions, callPreParse=False)
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:5264:                    tokens = ParseResults([self.defaultValue])
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:5265:                    tokens[self.expr.resultsName] = self.defaultValue
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:5267:                    tokens = [self.defaultValue]
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:5269:                tokens = []
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:5270:        return loc, tokens
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:5283:    """Token for skipping over all undefined text until the matched
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:5311:        # - parse action will call token.strip() for each matched token, i.e., the description body
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:5313:        string_data.setParseAction(tokenMap(str.strip))
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:5512:class TokenConverter(ParseElementEnhance):
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:5518:        super(TokenConverter, self).__init__(expr)  # , savelist)
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:5522:class Combine(TokenConverter):
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:5523:    """Converter to concatenate all matching tokens to a single string.
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:5558:    def postParse(self, instring, loc, tokenlist):
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:5559:        retToks = tokenlist.copy()
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:5562:            ["".join(tokenlist._asStringList(self.joinString))], modal=self.modalResults
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:5571:class Group(TokenConverter):
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:5572:    """Converter to return the matched tokens as a list - useful for
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:5573:    returning tokens of :class:`ZeroOrMore` and :class:`OneOrMore` expressions.
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:5591:    def postParse(self, instring, loc, tokenlist):
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:5592:        return [tokenlist]
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:5595:class Dict(TokenConverter):
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:5598:    token in the expression as its key. Useful for tabular report
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:5639:    def postParse(self, instring, loc, tokenlist):
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:5640:        for i, tok in enumerate(tokenlist):
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:5647:                tokenlist[ikey] = _ParseResultsWithOffset("", i)
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:5649:                tokenlist[ikey] = _ParseResultsWithOffset(tok[1], i)
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:5656:                    tokenlist[ikey] = _ParseResultsWithOffset(dictvalue, i)
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:5658:                    tokenlist[ikey] = _ParseResultsWithOffset(dictvalue[0], i)
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:5661:            return [tokenlist]
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:5663:            return tokenlist
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:5666:class Suppress(TokenConverter):
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:5689:    def postParse(self, instring, loc, tokenlist):
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:5718:    ``">> entering method-name(line:<current_source_line>, <parse_location>, <matched_tokens>)"``.
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:5727:        def remove_duplicate_chars(tokens):
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:5728:            return ''.join(sorted(set(''.join(tokens))))
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:5770:    ``combine`` is set to ``True``, the matching tokens are
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:5771:    returned as a single token string, with the delimiters included;
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:5772:    otherwise, the matching tokens are returned as a list of tokens,
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:5795:    The matched tokens returns the array of expr tokens as a list - the
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:5796:    leading count token is suppressed.
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:5838:    the tokens matched in a previous expression, that is, it looks for
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:5853:    def copyTokenToRepeater(s, l, t):
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:5858:                # flatten t tokens
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:5864:    expr.addParseAction(copyTokenToRepeater, callDuringTry=True)
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:5871:    the tokens matched in a previous expression, that is, it looks for
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:5888:    def copyTokenToRepeater(s, l, t):
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:5889:        matchTokens = _flatten(t.asList())
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:5891:        def mustMatchTheseTokens(s, l, t):
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:5892:            theseTokens = _flatten(t.asList())
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:5893:            if theseTokens != matchTokens:
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:5896:        rep.setParseAction(mustMatchTheseTokens, callDuringTry=True)
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:5898:    expr.addParseAction(copyTokenToRepeater, callDuringTry=True)
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:6015:    :class:`Group` tokens in the proper order.  The key pattern
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:6019:    can include named token fields.
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:6052:    """Helper to return the original, untokenized text for a given
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:6054:    tag into the raw tag text itself, or to revert separate tokens with
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:6061:    were originally matched, and a single token containing the original
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:6100:    return TokenConverter(expr).addParseAction(lambda t: t[0])
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:6104:    """Helper to decorate a returned token with its starting and ending
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:6200:            raise ParseException(strg, locn, "matched token not at column %d" % n)
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:6237:def tokenMap(func, *args):
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:6241:    after the token, as in
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:6242:    ``hex_integer = Word(hexnums).setParseAction(tokenMap(int, 16))``,
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:6247:        hex_ints = OneOrMore(Word(hexnums)).setParseAction(tokenMap(int, 16))
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:6252:        upperword = Word(alphas).setParseAction(tokenMap(str.upper))
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:6257:        wd = Word(alphas).setParseAction(tokenMap(str.title))
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:6286:upcaseTokens = tokenMap(lambda t: _ustr(t).upper())
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:6287:"""(Deprecated) Helper parse action to convert tokens to upper case.
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:6288:Deprecated in favor of :class:`pyparsing_common.upcaseTokens`"""
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:6290:downcaseTokens = tokenMap(lambda t: _ustr(t).lower())
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:6291:"""(Deprecated) Helper parse action to convert tokens to lower case.
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:6292:Deprecated in favor of :class:`pyparsing_common.downcaseTokens`"""
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:6323:                        tagAttrName.setParseAction(downcaseTokens)
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:6445:    def pa(s, l, tokens):
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:6447:            if attrName not in tokens:
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:6449:            if attrValue != withAttribute.ANY_VALUE and tokens[attrName] != attrValue:
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:6454:                    % (attrName, tokens[attrName], attrValue),
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:6985:     - :class:`upcaseTokens`
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:6986:     - :class:`downcaseTokens`
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:7031:        pyparsing_common.uuid.setParseAction(tokenMap(uuid.UUID))
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:7109:    convertToInteger = tokenMap(int)
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:7114:    convertToFloat = tokenMap(float)
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:7122:    hex_integer = Word(hexnums).setName("hex integer").setParseAction(tokenMap(int, 16))
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:7261:    def stripHTMLTags(s, l, tokens):
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:7276:        return pyparsing_common._html_stripper.transformString(tokens[0])
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:7295:    upcaseTokens = staticmethod(tokenMap(lambda t: _ustr(t).upper()))
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:7296:    """Parse action to convert tokens to upper case."""
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:7298:    downcaseTokens = staticmethod(tokenMap(lambda t: _ustr(t).lower()))
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:7299:    """Parse action to convert tokens to lower case."""
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:7552:                # assert that the '()' characters are not included in the parsed tokens
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:7571:                "collect_all_And_tokens": __compat__.collect_all_And_tokens
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:7585:            __compat__.collect_all_And_tokens = self._save_context["__compat__"]
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:7695:    selectToken = CaselessLiteral("select")
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:7696:    fromToken = CaselessLiteral("from")
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:7700:    columnName = delimitedList(ident, ".", combine=True).setParseAction(upcaseTokens)
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:7704:    tableName = delimitedList(ident, ".", combine=True).setParseAction(upcaseTokens)
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:7707:    simpleSQL = selectToken("command") + columnSpec("columns") + fromToken + tableNameList("tables")
./.venv_tmp/lib/python3.12/site-packages/pip_api/_vendor/pyparsing.py:7768:    pyparsing_common.uuid.setParseAction(tokenMap(uuid.UUID))
./.venv_tmp/lib/python3.12/site-packages/pip_api/_parse_requirements.py:66:            # includes a username and password.
./.venv_tmp/lib/python3.12/site-packages/pip_api/_parse_requirements.py:124:    # the password attribute of urlsplit()'s return value).
./.venv_tmp/lib/python3.12/site-packages/pip_api/_parse_requirements.py:130:        # using the password attribute of the return value)
./.venv_tmp/lib/python3.12/site-packages/pip_api/_parse_requirements.py:262:    # Fall back on tokenizing setup.py and walk the syntax tree to find the
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/markers.py:21:from ._tokenizer import ParserSyntaxError
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_tokenizer.py:12:class Token:
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_tokenizer.py:90:class Tokenizer:
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_tokenizer.py:91:    """Context-sensitive token parsing.
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_tokenizer.py:93:    Provides methods to examine the input stream to check whether the next token
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_tokenizer.py:107:        self.next_token: Token | None = None
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_tokenizer.py:111:        """Move beyond provided token name, if at current position."""
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_tokenizer.py:116:        """Check whether the next token has the provided name.
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_tokenizer.py:118:        By default, if the check succeeds, the token *must* be read before
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_tokenizer.py:119:        another check. If `peek` is set to `True`, the token is not loaded and
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_tokenizer.py:123:            self.next_token is None
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_tokenizer.py:124:        ), f"Cannot check for {name!r}, already have {self.next_token!r}"
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_tokenizer.py:125:        assert name in self.rules, f"Unknown token name: {name!r}"
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_tokenizer.py:133:            self.next_token = Token(name, match[0], self.position)
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_tokenizer.py:136:    def expect(self, name: str, *, expected: str) -> Token:
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_tokenizer.py:137:        """Expect a certain token name next, failing with a syntax error otherwise.
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_tokenizer.py:139:        The token is *not* read.
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_tokenizer.py:145:    def read(self) -> Token:
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_tokenizer.py:146:        """Consume the next token and return it."""
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_tokenizer.py:147:        token = self.next_token
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_tokenizer.py:148:        assert token is not None
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_tokenizer.py:150:        self.position += len(token.text)
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_tokenizer.py:151:        self.next_token = None
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_tokenizer.py:153:        return token
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_tokenizer.py:174:    def enclosing_tokens(self, open_token: str, close_token: str, *, around: str) -> Iterator[None]:
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_tokenizer.py:175:        if self.check(open_token):
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_tokenizer.py:186:        if not self.check(close_token):
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_tokenizer.py:188:                f"Expected matching {close_token} for {open_token}, after {around}",
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:12:from ._tokenizer import DEFAULT_RULES, Tokenizer
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:62:    return _parse_requirement(Tokenizer(source, rules=DEFAULT_RULES))
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:65:def _parse_requirement(tokenizer: Tokenizer) -> ParsedRequirement:
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:69:    tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:71:    name_token = tokenizer.expect(
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:74:    name = name_token.text
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:75:    tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:77:    extras = _parse_extras(tokenizer)
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:78:    tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:80:    url, specifier, marker = _parse_requirement_details(tokenizer)
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:81:    tokenizer.expect("END", expected="end of dependency specifier")
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:87:    tokenizer: Tokenizer,
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:98:    if tokenizer.check("AT"):
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:99:        tokenizer.read()
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:100:        tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:102:        url_start = tokenizer.position
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:103:        url = tokenizer.expect("URL", expected="URL after @").text
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:104:        if tokenizer.check("END", peek=True):
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:107:        tokenizer.expect("WS", expected="whitespace after URL")
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:110:        if tokenizer.check("END", peek=True):
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:114:            tokenizer, span_start=url_start, after="URL and whitespace"
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:117:        specifier_start = tokenizer.position
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:118:        specifier = _parse_specifier(tokenizer)
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:119:        tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:121:        if tokenizer.check("END", peek=True):
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:125:            tokenizer,
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:133:def _parse_requirement_marker(tokenizer: Tokenizer, *, span_start: int, after: str) -> MarkerList:
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:138:    if not tokenizer.check("SEMICOLON"):
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:139:        tokenizer.raise_syntax_error(
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:143:    tokenizer.read()
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:145:    marker = _parse_marker(tokenizer)
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:146:    tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:151:def _parse_extras(tokenizer: Tokenizer) -> list[str]:
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:155:    if not tokenizer.check("LEFT_BRACKET", peek=True):
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:158:    with tokenizer.enclosing_tokens(
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:163:        tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:164:        extras = _parse_extras_list(tokenizer)
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:165:        tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:170:def _parse_extras_list(tokenizer: Tokenizer) -> list[str]:
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:176:    if not tokenizer.check("IDENTIFIER"):
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:179:    extras.append(tokenizer.read().text)
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:182:        tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:183:        if tokenizer.check("IDENTIFIER", peek=True):
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:184:            tokenizer.raise_syntax_error("Expected comma between extra names")
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:185:        elif not tokenizer.check("COMMA"):
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:188:        tokenizer.read()
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:189:        tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:191:        extra_token = tokenizer.expect("IDENTIFIER", expected="extra name after comma")
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:192:        extras.append(extra_token.text)
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:197:def _parse_specifier(tokenizer: Tokenizer) -> str:
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:202:    with tokenizer.enclosing_tokens(
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:207:        tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:208:        parsed_specifiers = _parse_version_many(tokenizer)
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:209:        tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:214:def _parse_version_many(tokenizer: Tokenizer) -> str:
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:219:    while tokenizer.check("SPECIFIER"):
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:220:        span_start = tokenizer.position
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:221:        parsed_specifiers += tokenizer.read().text
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:222:        if tokenizer.check("VERSION_PREFIX_TRAIL", peek=True):
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:223:            tokenizer.raise_syntax_error(
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:226:                span_end=tokenizer.position + 1,
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:228:        if tokenizer.check("VERSION_LOCAL_LABEL_TRAIL", peek=True):
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:229:            tokenizer.raise_syntax_error(
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:232:                span_end=tokenizer.position,
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:234:        tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:235:        if not tokenizer.check("COMMA"):
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:237:        parsed_specifiers += tokenizer.read().text
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:238:        tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:247:    return _parse_full_marker(Tokenizer(source, rules=DEFAULT_RULES))
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:250:def _parse_full_marker(tokenizer: Tokenizer) -> MarkerList:
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:251:    retval = _parse_marker(tokenizer)
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:252:    tokenizer.expect("END", expected="end of marker expression")
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:256:def _parse_marker(tokenizer: Tokenizer) -> MarkerList:
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:260:    expression = [_parse_marker_atom(tokenizer)]
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:261:    while tokenizer.check("BOOLOP"):
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:262:        token = tokenizer.read()
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:263:        expr_right = _parse_marker_atom(tokenizer)
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:264:        expression.extend((token.text, expr_right))
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:268:def _parse_marker_atom(tokenizer: Tokenizer) -> MarkerAtom:
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:274:    tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:275:    if tokenizer.check("LEFT_PARENTHESIS", peek=True):
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:276:        with tokenizer.enclosing_tokens(
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:281:            tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:282:            marker: MarkerAtom = _parse_marker(tokenizer)
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:283:            tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:285:        marker = _parse_marker_item(tokenizer)
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:286:    tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:290:def _parse_marker_item(tokenizer: Tokenizer) -> MarkerItem:
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:294:    tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:295:    marker_var_left = _parse_marker_var(tokenizer)
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:296:    tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:297:    marker_op = _parse_marker_op(tokenizer)
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:298:    tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:299:    marker_var_right = _parse_marker_var(tokenizer)
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:300:    tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:304:def _parse_marker_var(tokenizer: Tokenizer) -> MarkerVar:
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:308:    if tokenizer.check("VARIABLE"):
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:309:        return process_env_var(tokenizer.read().text.replace(".", "_"))
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:310:    elif tokenizer.check("QUOTED_STRING"):
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:311:        return process_python_str(tokenizer.read().text)
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:313:        tokenizer.raise_syntax_error(message="Expected a marker variable or quoted string")
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:328:def _parse_marker_op(tokenizer: Tokenizer) -> Op:
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:332:    if tokenizer.check("IN"):
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:333:        tokenizer.read()
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:335:    elif tokenizer.check("NOT"):
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:336:        tokenizer.read()
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:337:        tokenizer.expect("WS", expected="whitespace after 'not'")
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:338:        tokenizer.expect("IN", expected="'in' after 'not'")
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:340:    elif tokenizer.check("OP"):
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:341:        return Op(tokenizer.read().text)
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/_parser.py:343:        return tokenizer.raise_syntax_error(
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/licenses/__init__.py:67:    # Pad any parentheses so tokenization can be achieved by merely splitting on
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/licenses/__init__.py:81:    tokens = license_expression.split()
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/licenses/__init__.py:86:    python_tokens = []
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/licenses/__init__.py:87:    for token in tokens:
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/licenses/__init__.py:88:        if token not in {"or", "and", "with", "(", ")"}:
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/licenses/__init__.py:89:            python_tokens.append("False")
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/licenses/__init__.py:90:        elif token == "with":
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/licenses/__init__.py:91:            python_tokens.append("or")
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/licenses/__init__.py:92:        elif token == "(" and python_tokens and python_tokens[-1] not in {"or", "and"}:
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/licenses/__init__.py:96:            python_tokens.append(token)
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/licenses/__init__.py:98:    python_expression = " ".join(python_tokens)
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/licenses/__init__.py:109:    normalized_tokens = []
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/licenses/__init__.py:110:    for token in tokens:
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/licenses/__init__.py:111:        if token in {"or", "and", "with", "(", ")"}:
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/licenses/__init__.py:112:            normalized_tokens.append(token.upper())
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/licenses/__init__.py:115:        if normalized_tokens and normalized_tokens[-1] == "WITH":
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/licenses/__init__.py:116:            if token not in EXCEPTIONS:
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/licenses/__init__.py:117:                message = f"Unknown license exception: {token!r}"
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/licenses/__init__.py:120:            normalized_tokens.append(EXCEPTIONS[token]["id"])
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/licenses/__init__.py:122:            if token.endswith("+"):
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/licenses/__init__.py:123:                final_token = token[:-1]
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/licenses/__init__.py:126:                final_token = token
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/licenses/__init__.py:129:            if final_token.startswith("licenseref-"):
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/licenses/__init__.py:130:                if not license_ref_allowed.match(final_token):
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/licenses/__init__.py:131:                    message = f"Invalid licenseref: {final_token!r}"
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/licenses/__init__.py:133:                normalized_tokens.append(license_refs[final_token] + suffix)
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/licenses/__init__.py:135:                if final_token not in LICENSES:
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/licenses/__init__.py:136:                    message = f"Unknown license: {final_token!r}"
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/licenses/__init__.py:138:                normalized_tokens.append(LICENSES[final_token]["id"] + suffix)
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/licenses/__init__.py:140:    normalized_expression = " ".join(normalized_tokens)
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/packaging/requirements.py:9:from ._tokenizer import ParserSyntaxError
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/markers.py:19:from ._tokenizer import ParserSyntaxError
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_tokenizer.py:10:class Token:
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_tokenizer.py:88:class Tokenizer:
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_tokenizer.py:89:    """Context-sensitive token parsing.
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_tokenizer.py:91:    Provides methods to examine the input stream to check whether the next token
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_tokenizer.py:105:        self.next_token: Optional[Token] = None
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_tokenizer.py:109:        """Move beyond provided token name, if at current position."""
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_tokenizer.py:114:        """Check whether the next token has the provided name.
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_tokenizer.py:116:        By default, if the check succeeds, the token *must* be read before
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_tokenizer.py:117:        another check. If `peek` is set to `True`, the token is not loaded and
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_tokenizer.py:121:            self.next_token is None
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_tokenizer.py:122:        ), f"Cannot check for {name!r}, already have {self.next_token!r}"
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_tokenizer.py:123:        assert name in self.rules, f"Unknown token name: {name!r}"
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_tokenizer.py:131:            self.next_token = Token(name, match[0], self.position)
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_tokenizer.py:134:    def expect(self, name: str, *, expected: str) -> Token:
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_tokenizer.py:135:        """Expect a certain token name next, failing with a syntax error otherwise.
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_tokenizer.py:137:        The token is *not* read.
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_tokenizer.py:143:    def read(self) -> Token:
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_tokenizer.py:144:        """Consume the next token and return it."""
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_tokenizer.py:145:        token = self.next_token
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_tokenizer.py:146:        assert token is not None
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_tokenizer.py:148:        self.position += len(token.text)
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_tokenizer.py:149:        self.next_token = None
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_tokenizer.py:151:        return token
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_tokenizer.py:172:    def enclosing_tokens(self, open_token: str, close_token: str, *, around: str) -> Iterator[None]:
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_tokenizer.py:173:        if self.check(open_token):
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_tokenizer.py:184:        if not self.check(close_token):
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_tokenizer.py:186:                f"Expected matching {close_token} for {open_token}, after {around}",
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:10:from ._tokenizer import DEFAULT_RULES, Tokenizer
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:64:    return _parse_requirement(Tokenizer(source, rules=DEFAULT_RULES))
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:67:def _parse_requirement(tokenizer: Tokenizer) -> ParsedRequirement:
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:71:    tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:73:    name_token = tokenizer.expect(
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:76:    name = name_token.text
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:77:    tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:79:    extras = _parse_extras(tokenizer)
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:80:    tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:82:    url, specifier, marker = _parse_requirement_details(tokenizer)
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:83:    tokenizer.expect("END", expected="end of dependency specifier")
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:89:    tokenizer: Tokenizer,
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:100:    if tokenizer.check("AT"):
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:101:        tokenizer.read()
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:102:        tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:104:        url_start = tokenizer.position
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:105:        url = tokenizer.expect("URL", expected="URL after @").text
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:106:        if tokenizer.check("END", peek=True):
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:109:        tokenizer.expect("WS", expected="whitespace after URL")
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:112:        if tokenizer.check("END", peek=True):
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:116:            tokenizer, span_start=url_start, after="URL and whitespace"
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:119:        specifier_start = tokenizer.position
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:120:        specifier = _parse_specifier(tokenizer)
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:121:        tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:123:        if tokenizer.check("END", peek=True):
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:127:            tokenizer,
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:135:def _parse_requirement_marker(tokenizer: Tokenizer, *, span_start: int, after: str) -> MarkerList:
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:140:    if not tokenizer.check("SEMICOLON"):
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:141:        tokenizer.raise_syntax_error(
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:145:    tokenizer.read()
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:147:    marker = _parse_marker(tokenizer)
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:148:    tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:153:def _parse_extras(tokenizer: Tokenizer) -> List[str]:
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:157:    if not tokenizer.check("LEFT_BRACKET", peek=True):
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:160:    with tokenizer.enclosing_tokens(
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:165:        tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:166:        extras = _parse_extras_list(tokenizer)
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:167:        tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:172:def _parse_extras_list(tokenizer: Tokenizer) -> List[str]:
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:178:    if not tokenizer.check("IDENTIFIER"):
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:181:    extras.append(tokenizer.read().text)
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:184:        tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:185:        if tokenizer.check("IDENTIFIER", peek=True):
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:186:            tokenizer.raise_syntax_error("Expected comma between extra names")
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:187:        elif not tokenizer.check("COMMA"):
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:190:        tokenizer.read()
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:191:        tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:193:        extra_token = tokenizer.expect("IDENTIFIER", expected="extra name after comma")
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:194:        extras.append(extra_token.text)
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:199:def _parse_specifier(tokenizer: Tokenizer) -> str:
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:204:    with tokenizer.enclosing_tokens(
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:209:        tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:210:        parsed_specifiers = _parse_version_many(tokenizer)
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:211:        tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:216:def _parse_version_many(tokenizer: Tokenizer) -> str:
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:221:    while tokenizer.check("SPECIFIER"):
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:222:        span_start = tokenizer.position
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:223:        parsed_specifiers += tokenizer.read().text
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:224:        if tokenizer.check("VERSION_PREFIX_TRAIL", peek=True):
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:225:            tokenizer.raise_syntax_error(
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:228:                span_end=tokenizer.position + 1,
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:230:        if tokenizer.check("VERSION_LOCAL_LABEL_TRAIL", peek=True):
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:231:            tokenizer.raise_syntax_error(
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:234:                span_end=tokenizer.position,
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:236:        tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:237:        if not tokenizer.check("COMMA"):
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:239:        parsed_specifiers += tokenizer.read().text
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:240:        tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:249:    return _parse_full_marker(Tokenizer(source, rules=DEFAULT_RULES))
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:252:def _parse_full_marker(tokenizer: Tokenizer) -> MarkerList:
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:253:    retval = _parse_marker(tokenizer)
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:254:    tokenizer.expect("END", expected="end of marker expression")
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:258:def _parse_marker(tokenizer: Tokenizer) -> MarkerList:
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:262:    expression = [_parse_marker_atom(tokenizer)]
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:263:    while tokenizer.check("BOOLOP"):
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:264:        token = tokenizer.read()
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:265:        expr_right = _parse_marker_atom(tokenizer)
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:266:        expression.extend((token.text, expr_right))
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:270:def _parse_marker_atom(tokenizer: Tokenizer) -> MarkerAtom:
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:276:    tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:277:    if tokenizer.check("LEFT_PARENTHESIS", peek=True):
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:278:        with tokenizer.enclosing_tokens(
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:283:            tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:284:            marker: MarkerAtom = _parse_marker(tokenizer)
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:285:            tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:287:        marker = _parse_marker_item(tokenizer)
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:288:    tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:292:def _parse_marker_item(tokenizer: Tokenizer) -> MarkerItem:
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:296:    tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:297:    marker_var_left = _parse_marker_var(tokenizer)
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:298:    tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:299:    marker_op = _parse_marker_op(tokenizer)
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:300:    tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:301:    marker_var_right = _parse_marker_var(tokenizer)
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:302:    tokenizer.consume("WS")
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:306:def _parse_marker_var(tokenizer: Tokenizer) -> MarkerVar:
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:310:    if tokenizer.check("VARIABLE"):
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:311:        return process_env_var(tokenizer.read().text.replace(".", "_"))
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:312:    elif tokenizer.check("QUOTED_STRING"):
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:313:        return process_python_str(tokenizer.read().text)
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:315:        tokenizer.raise_syntax_error(message="Expected a marker variable or quoted string")
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:330:def _parse_marker_op(tokenizer: Tokenizer) -> Op:
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:334:    if tokenizer.check("IN"):
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:335:        tokenizer.read()
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:337:    elif tokenizer.check("NOT"):
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:338:        tokenizer.read()
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:339:        tokenizer.expect("WS", expected="whitespace after 'not'")
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:340:        tokenizer.expect("IN", expected="'in' after 'not'")
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:342:    elif tokenizer.check("OP"):
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:343:        return Op(tokenizer.read().text)
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/_parser.py:345:        return tokenizer.raise_syntax_error(
./.venv_tmp/lib/python3.12/site-packages/setuptools/_vendor/wheel/vendored/packaging/requirements.py:8:from ._tokenizer import ParserSyntaxError
./.venv_tmp/lib/python3.12/site-packages/setuptools/tests/config/test_expand.py:45:    secrets = Path(str(dir_) + "secrets")
./.venv_tmp/lib/python3.12/site-packages/setuptools/tests/config/test_expand.py:46:    secrets.mkdir(exist_ok=True)
./.venv_tmp/lib/python3.12/site-packages/setuptools/tests/config/test_expand.py:47:    write_files({"secrets.txt": "secret keys"}, secrets)
./.venv_tmp/lib/python3.12/site-packages/setuptools/tests/config/test_expand.py:57:        cannot_access_secrets_msg = r"Cannot access '.*secrets\.txt'"
./.venv_tmp/lib/python3.12/site-packages/setuptools/tests/config/test_expand.py:58:        with pytest.raises(DistutilsOptionError, match=cannot_access_secrets_msg):
./.venv_tmp/lib/python3.12/site-packages/setuptools/tests/config/test_expand.py:59:            expand.read_files(["../dir_secrets/secrets.txt"])
./.venv_tmp/lib/python3.12/site-packages/setuptools/build_meta.py:39:import tokenize
./.venv_tmp/lib/python3.12/site-packages/setuptools/build_meta.py:134:    return tokenize.open(setup_script)
./.venv_tmp/lib/python3.12/site-packages/setuptools/_imp.py:9:import tokenize
./.venv_tmp/lib/python3.12/site-packages/setuptools/_imp.py:62:            file = tokenize.open(path)
./.venv_tmp/lib/python3.12/site-packages/setuptools/launch.py:10:import tokenize
./.venv_tmp/lib/python3.12/site-packages/setuptools/launch.py:27:    open_ = getattr(tokenize, "open", open)
./.venv_tmp/lib/python3.12/site-packages/setuptools/_distutils/core.py:13:import tokenize
./.venv_tmp/lib/python3.12/site-packages/setuptools/_distutils/core.py:265:            # tokenize.open supports automatic encoding detection
./.venv_tmp/lib/python3.12/site-packages/setuptools/_distutils/core.py:266:            with tokenize.open(script_name) as f:
./.venv_tmp/lib/python3.12/site-packages/setuptools/_distutils/util.py:177:            # password database, do nothing
./.venv_tmp/lib/python3.12/site-packages/setuptools/_distutils/dist.py:226:        self.password = ""
./.venv_tmp/lib/python3.12/site-packages/setuptools/_distutils/command/build_scripts.py:7:import tokenize
./.venv_tmp/lib/python3.12/site-packages/setuptools/_distutils/command/build_scripts.py:91:            f = tokenize.open(script)
./.venv_tmp/lib/python3.12/site-packages/toml/decoder.py:860:                                "Found tokens after a closed " + "string. Invalid TOML."
./.venv_tmp/lib/python3.12/site-packages/_pytest/compat.py:40:    token = 0
./.venv_tmp/lib/python3.12/site-packages/_pytest/compat.py:41:NOTSET: Final = NotSetType.token
./.venv_tmp/lib/python3.12/site-packages/_pytest/assertion/rewrite.py:17:import tokenize
./.venv_tmp/lib/python3.12/site-packages/_pytest/assertion/rewrite.py:562:    tokens = tokenize.tokenize(io.BytesIO(src).readline)
./.venv_tmp/lib/python3.12/site-packages/_pytest/assertion/rewrite.py:563:    for tp, source, (lineno, offset), _, line in tokens:
./.venv_tmp/lib/python3.12/site-packages/_pytest/assertion/rewrite.py:564:        if tp == tokenize.NAME and source == "assert":
./.venv_tmp/lib/python3.12/site-packages/_pytest/assertion/rewrite.py:568:            if tp == tokenize.OP and source in "([{":
./.venv_tmp/lib/python3.12/site-packages/_pytest/assertion/rewrite.py:570:            elif tp == tokenize.OP and source in ")]}":
./.venv_tmp/lib/python3.12/site-packages/_pytest/assertion/rewrite.py:577:            elif depth == 0 and tp == tokenize.OP and source == ",":
./.venv_tmp/lib/python3.12/site-packages/_pytest/assertion/rewrite.py:589:            elif tp in {tokenize.NEWLINE, tokenize.ENDMARKER}:
./.venv_tmp/lib/python3.12/site-packages/_pytest/mark/structures.py:40:    token = 0
./.venv_tmp/lib/python3.12/site-packages/_pytest/mark/structures.py:44:HIDDEN_PARAM = _HiddenParam.token
./.venv_tmp/lib/python3.12/site-packages/_pytest/mark/expression.py:40:class TokenType(enum.Enum):
./.venv_tmp/lib/python3.12/site-packages/_pytest/mark/expression.py:54:class Token:
./.venv_tmp/lib/python3.12/site-packages/_pytest/mark/expression.py:56:    type: TokenType
./.venv_tmp/lib/python3.12/site-packages/_pytest/mark/expression.py:77:    __slots__ = ("current", "tokens")
./.venv_tmp/lib/python3.12/site-packages/_pytest/mark/expression.py:80:        self.tokens = self.lex(input)
./.venv_tmp/lib/python3.12/site-packages/_pytest/mark/expression.py:81:        self.current = next(self.tokens)
./.venv_tmp/lib/python3.12/site-packages/_pytest/mark/expression.py:83:    def lex(self, input: str) -> Iterator[Token]:
./.venv_tmp/lib/python3.12/site-packages/_pytest/mark/expression.py:89:                yield Token(TokenType.LPAREN, "(", pos)
./.venv_tmp/lib/python3.12/site-packages/_pytest/mark/expression.py:92:                yield Token(TokenType.RPAREN, ")", pos)
./.venv_tmp/lib/python3.12/site-packages/_pytest/mark/expression.py:95:                yield Token(TokenType.EQUAL, "=", pos)
./.venv_tmp/lib/python3.12/site-packages/_pytest/mark/expression.py:98:                yield Token(TokenType.COMMA, ",", pos)
./.venv_tmp/lib/python3.12/site-packages/_pytest/mark/expression.py:113:                yield Token(TokenType.STRING, value, pos)
./.venv_tmp/lib/python3.12/site-packages/_pytest/mark/expression.py:120:                        yield Token(TokenType.OR, value, pos)
./.venv_tmp/lib/python3.12/site-packages/_pytest/mark/expression.py:122:                        yield Token(TokenType.AND, value, pos)
./.venv_tmp/lib/python3.12/site-packages/_pytest/mark/expression.py:124:                        yield Token(TokenType.NOT, value, pos)
./.venv_tmp/lib/python3.12/site-packages/_pytest/mark/expression.py:126:                        yield Token(TokenType.IDENT, value, pos)
./.venv_tmp/lib/python3.12/site-packages/_pytest/mark/expression.py:133:        yield Token(TokenType.EOF, "", pos)
./.venv_tmp/lib/python3.12/site-packages/_pytest/mark/expression.py:136:    def accept(self, type: TokenType, *, reject: Literal[True]) -> Token: ...
./.venv_tmp/lib/python3.12/site-packages/_pytest/mark/expression.py:139:    def accept(self, type: TokenType, *, reject: Literal[False] = False) -> Token | None: ...
./.venv_tmp/lib/python3.12/site-packages/_pytest/mark/expression.py:141:    def accept(self, type: TokenType, *, reject: bool = False) -> Token | None:
./.venv_tmp/lib/python3.12/site-packages/_pytest/mark/expression.py:143:            token = self.current
./.venv_tmp/lib/python3.12/site-packages/_pytest/mark/expression.py:144:            if token.type is not TokenType.EOF:
./.venv_tmp/lib/python3.12/site-packages/_pytest/mark/expression.py:145:                self.current = next(self.tokens)
./.venv_tmp/lib/python3.12/site-packages/_pytest/mark/expression.py:146:            return token
./.venv_tmp/lib/python3.12/site-packages/_pytest/mark/expression.py:151:    def reject(self, expected: Sequence[TokenType]) -> NoReturn:
./.venv_tmp/lib/python3.12/site-packages/_pytest/mark/expression.py:168:    if s.accept(TokenType.EOF):
./.venv_tmp/lib/python3.12/site-packages/_pytest/mark/expression.py:172:        s.accept(TokenType.EOF, reject=True)
./.venv_tmp/lib/python3.12/site-packages/_pytest/mark/expression.py:178:    while s.accept(TokenType.OR):
./.venv_tmp/lib/python3.12/site-packages/_pytest/mark/expression.py:186:    while s.accept(TokenType.AND):
./.venv_tmp/lib/python3.12/site-packages/_pytest/mark/expression.py:193:    if s.accept(TokenType.NOT):
./.venv_tmp/lib/python3.12/site-packages/_pytest/mark/expression.py:195:    if s.accept(TokenType.LPAREN):
./.venv_tmp/lib/python3.12/site-packages/_pytest/mark/expression.py:197:        s.accept(TokenType.RPAREN, reject=True)
./.venv_tmp/lib/python3.12/site-packages/_pytest/mark/expression.py:199:    ident = s.accept(TokenType.IDENT)
./.venv_tmp/lib/python3.12/site-packages/_pytest/mark/expression.py:202:        if s.accept(TokenType.LPAREN):
./.venv_tmp/lib/python3.12/site-packages/_pytest/mark/expression.py:204:            s.accept(TokenType.RPAREN, reject=True)
./.venv_tmp/lib/python3.12/site-packages/_pytest/mark/expression.py:209:    s.reject((TokenType.NOT, TokenType.LPAREN, TokenType.IDENT))
./.venv_tmp/lib/python3.12/site-packages/_pytest/mark/expression.py:216:    keyword_name = s.accept(TokenType.IDENT, reject=True)
./.venv_tmp/lib/python3.12/site-packages/_pytest/mark/expression.py:227:    s.accept(TokenType.EQUAL, reject=True)
./.venv_tmp/lib/python3.12/site-packages/_pytest/mark/expression.py:229:    if value_token := s.accept(TokenType.STRING):
./.venv_tmp/lib/python3.12/site-packages/_pytest/mark/expression.py:230:        value: str | int | bool | None = value_token.value[1:-1]  # strip quotes
./.venv_tmp/lib/python3.12/site-packages/_pytest/mark/expression.py:232:        value_token = s.accept(TokenType.IDENT, reject=True)
./.venv_tmp/lib/python3.12/site-packages/_pytest/mark/expression.py:233:        if (number := value_token.value).isdigit() or (
./.venv_tmp/lib/python3.12/site-packages/_pytest/mark/expression.py:237:        elif value_token.value in BUILTIN_MATCHERS:
./.venv_tmp/lib/python3.12/site-packages/_pytest/mark/expression.py:238:            value = BUILTIN_MATCHERS[value_token.value]
./.venv_tmp/lib/python3.12/site-packages/_pytest/mark/expression.py:241:                value_token.pos + 1,
./.venv_tmp/lib/python3.12/site-packages/_pytest/mark/expression.py:242:                f'unexpected character/s "{value_token.value}"',
./.venv_tmp/lib/python3.12/site-packages/_pytest/mark/expression.py:251:    while s.accept(TokenType.COMMA):
./.venv_tmp/lib/python3.12/site-packages/_pytest/_code/source.py:7:import tokenize
./.venv_tmp/lib/python3.12/site-packages/_pytest/_code/source.py:208:            for tok in tokenize.generate_tokens(lambda: next(it)):
./.venv_tmp/lib/python3.12/site-packages/_pytest/_code/source.py:209:                block_finder.tokeneater(*tok)
./.venv_tmp/lib/python3.12/site-packages/_pytest/python.py:856:        Format is <prm_1_token>-...-<prm_n_token>[counter], where prm_x_token is
./.venv_tmp/lib/python3.12/site-packages/mypy/modulefinder.py:964:    {..., 'secrets': ((3, 6), None), 'symbol': ((2, 7), (3, 9)), ...}
./.venv_tmp/lib/python3.12/site-packages/mypy/stubdoc.py:13:import tokenize
./.venv_tmp/lib/python3.12/site-packages/mypy/stubdoc.py:188:    def add_token(self, token: tokenize.TokenInfo) -> None:
./.venv_tmp/lib/python3.12/site-packages/mypy/stubdoc.py:189:        """Process next token from the token stream."""
./.venv_tmp/lib/python3.12/site-packages/mypy/stubdoc.py:191:            token.type == tokenize.NAME
./.venv_tmp/lib/python3.12/site-packages/mypy/stubdoc.py:192:            and token.string == self.function_name
./.venv_tmp/lib/python3.12/site-packages/mypy/stubdoc.py:198:            token.type == tokenize.OP
./.venv_tmp/lib/python3.12/site-packages/mypy/stubdoc.py:199:            and token.string == "("
./.venv_tmp/lib/python3.12/site-packages/mypy/stubdoc.py:212:            token.type == tokenize.OP
./.venv_tmp/lib/python3.12/site-packages/mypy/stubdoc.py:213:            and token.string in ("[", "(", "{")
./.venv_tmp/lib/python3.12/site-packages/mypy/stubdoc.py:216:            self.accumulator += token.string
./.venv_tmp/lib/python3.12/site-packages/mypy/stubdoc.py:220:            token.type == tokenize.OP
./.venv_tmp/lib/python3.12/site-packages/mypy/stubdoc.py:221:            and token.string in ("]", ")", "}")
./.venv_tmp/lib/python3.12/site-packages/mypy/stubdoc.py:224:            self.accumulator += token.string
./.venv_tmp/lib/python3.12/site-packages/mypy/stubdoc.py:228:            token.type == tokenize.OP
./.venv_tmp/lib/python3.12/site-packages/mypy/stubdoc.py:229:            and token.string == ":"
./.venv_tmp/lib/python3.12/site-packages/mypy/stubdoc.py:237:            token.type == tokenize.OP
./.venv_tmp/lib/python3.12/site-packages/mypy/stubdoc.py:238:            and token.string == "="
./.venv_tmp/lib/python3.12/site-packages/mypy/stubdoc.py:250:            token.type == tokenize.OP
./.venv_tmp/lib/python3.12/site-packages/mypy/stubdoc.py:251:            and token.string in (",", ")")
./.venv_tmp/lib/python3.12/site-packages/mypy/stubdoc.py:273:                        token.string == ")" and self.accumulator.strip() == ""
./.venv_tmp/lib/python3.12/site-packages/mypy/stubdoc.py:279:            if token.string == ")":
./.venv_tmp/lib/python3.12/site-packages/mypy/stubdoc.py:309:            token.type == tokenize.OP
./.venv_tmp/lib/python3.12/site-packages/mypy/stubdoc.py:310:            and token.string == "/"
./.venv_tmp/lib/python3.12/site-packages/mypy/stubdoc.py:313:            if token.string == "/":
./.venv_tmp/lib/python3.12/site-packages/mypy/stubdoc.py:325:        elif token.type == tokenize.OP and token.string == "->" and self.state[-1] == STATE_INIT:
./.venv_tmp/lib/python3.12/site-packages/mypy/stubdoc.py:330:        elif token.type in (tokenize.NEWLINE, tokenize.ENDMARKER) and self.state[-1] in (
./.venv_tmp/lib/python3.12/site-packages/mypy/stubdoc.py:351:            self.accumulator += token.string
./.venv_tmp/lib/python3.12/site-packages/mypy/stubdoc.py:392:    with contextlib.suppress(tokenize.TokenError):
./.venv_tmp/lib/python3.12/site-packages/mypy/stubdoc.py:394:            tokens = tokenize.tokenize(io.BytesIO(docstr.encode("utf-8")).readline)
./.venv_tmp/lib/python3.12/site-packages/mypy/stubdoc.py:395:            for token in tokens:
./.venv_tmp/lib/python3.12/site-packages/mypy/stubdoc.py:396:                state.add_token(token)
./.venv_tmp/lib/python3.12/site-packages/mypy/report.py:12:import tokenize
./.venv_tmp/lib/python3.12/site-packages/mypy/report.py:143:        with tokenize.open(path) as input_file:
./.venv_tmp/lib/python3.12/site-packages/mypy/checker.py:1120:            # TODO not best fix, better have dedicated yield token
./.venv_tmp/lib/python3.12/site-packages/isort/io.py:5:import tokenize
./.venv_tmp/lib/python3.12/site-packages/isort/io.py:26:            return tokenize.detect_encoding(readline)[0]
./.venv_tmp/lib/python3.12/site-packages/isort/stdlibs/py36.py:151:    "secrets",
./.venv_tmp/lib/python3.12/site-packages/isort/stdlibs/py36.py:194:    "token",
./.venv_tmp/lib/python3.12/site-packages/isort/stdlibs/py36.py:195:    "tokenize",
./.venv_tmp/lib/python3.12/site-packages/isort/stdlibs/py37.py:152:    "secrets",
./.venv_tmp/lib/python3.12/site-packages/isort/stdlibs/py37.py:195:    "token",
./.venv_tmp/lib/python3.12/site-packages/isort/stdlibs/py37.py:196:    "tokenize",
./.venv_tmp/lib/python3.12/site-packages/isort/stdlibs/py313.py:136:    "secrets",
./.venv_tmp/lib/python3.12/site-packages/isort/stdlibs/py313.py:173:    "token",
./.venv_tmp/lib/python3.12/site-packages/isort/stdlibs/py313.py:174:    "tokenize",
./.venv_tmp/lib/python3.12/site-packages/isort/stdlibs/py38.py:157:    "secrets",
./.venv_tmp/lib/python3.12/site-packages/isort/stdlibs/py38.py:200:    "token",
./.venv_tmp/lib/python3.12/site-packages/isort/stdlibs/py38.py:201:    "tokenize",
./.venv_tmp/lib/python3.12/site-packages/isort/stdlibs/py39.py:157:    "secrets",
./.venv_tmp/lib/python3.12/site-packages/isort/stdlibs/py39.py:200:    "token",
./.venv_tmp/lib/python3.12/site-packages/isort/stdlibs/py39.py:201:    "tokenize",
./.venv_tmp/lib/python3.12/site-packages/isort/stdlibs/py312.py:150:    "secrets",
./.venv_tmp/lib/python3.12/site-packages/isort/stdlibs/py312.py:191:    "token",
./.venv_tmp/lib/python3.12/site-packages/isort/stdlibs/py312.py:192:    "tokenize",
./.venv_tmp/lib/python3.12/site-packages/isort/stdlibs/py314.py:137:    "secrets",
./.venv_tmp/lib/python3.12/site-packages/isort/stdlibs/py314.py:174:    "token",
./.venv_tmp/lib/python3.12/site-packages/isort/stdlibs/py314.py:175:    "tokenize",
./.venv_tmp/lib/python3.12/site-packages/isort/stdlibs/py27.py:271:    "token",
./.venv_tmp/lib/python3.12/site-packages/isort/stdlibs/py27.py:272:    "tokenize",
./.venv_tmp/lib/python3.12/site-packages/isort/stdlibs/py311.py:154:    "secrets",
./.venv_tmp/lib/python3.12/site-packages/isort/stdlibs/py311.py:196:    "token",
./.venv_tmp/lib/python3.12/site-packages/isort/stdlibs/py311.py:197:    "tokenize",
./.venv_tmp/lib/python3.12/site-packages/isort/stdlibs/py310.py:155:    "secrets",
./.venv_tmp/lib/python3.12/site-packages/isort/stdlibs/py310.py:197:    "token",
./.venv_tmp/lib/python3.12/site-packages/isort/stdlibs/py310.py:198:    "tokenize",
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/rtf.py:22:    Format tokens as RTF markup. This formatter automatically outputs full RTF
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/rtf.py:193:    def _split_tokens_on_newlines(self, tokensource):
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/rtf.py:195:        Split tokens containing newline characters into multiple token
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/rtf.py:199:        for ttype, value in tokensource:
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/rtf.py:280:    def format_unencoded(self, tokensource, outfile):
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/rtf.py:284:        tokensource = self._split_tokens_on_newlines(tokensource)
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/rtf.py:286:        # first pass of tokens to count lines, needed for line numbering
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/rtf.py:289:            tokens = []  # for copying the token source generator
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/rtf.py:290:            for ttype, value in tokensource:
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/rtf.py:291:                tokens.append((ttype, value))
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/rtf.py:298:            tokensource = tokens
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/rtf.py:303:        for ttype, value in tokensource:
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/rtf.py:315:            while not self.style.styles_token(ttype) and ttype.parent:
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/rtf.py:317:            style = self.style.style_for_token(ttype)
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/img.py:493:        Get the correct color for the token from the style.
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/img.py:503:        Get the correct background color for the token from the style.
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/img.py:544:    def _create_drawables(self, tokensource):
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/img.py:546:        Create drawables for the token content.
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/img.py:550:        for ttype, value in tokensource:
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/img.py:611:    def format(self, tokensource, outfile):
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/img.py:613:        Format ``tokensource``, an iterable of ``(tokentype, tokenstring)``
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/img.py:616:        This implementation calculates where it should draw each token on the
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/img.py:619:        self._create_drawables(tokensource)
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/groff.py:21:    Format tokens with groff escapes to change their color and font style.
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/groff.py:132:    def format_unencoded(self, tokensource, outfile):
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/groff.py:140:        for ttype, value in tokensource:
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/_mapping.py:10:        "Format tokens with BBcodes. These formatting codes are used by many bulletin boards, so you can highlight your sourcecode with pygments before posting it there.",
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/_mapping.py:31:        "Format tokens with groff escapes to change their color and font style.",
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/_mapping.py:38:        "Format tokens as HTML 4 ``<span>`` tags. By default, the content is enclosed in a ``<pre>`` tag, itself wrapped in a ``<div>`` tag (but see the `nowrap` option). The ``<div>``'s CSS class can be set by the `cssclass` option.",
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/_mapping.py:45:        "Format tokens with IRC color sequences",
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/_mapping.py:66:        "Format tokens as LaTeX code. This needs the `fancyvrb` and `color` standard packages.",
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/_mapping.py:80:        "Format tokens as Pango Markup code. It can then be rendered to an SVG.",
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/_mapping.py:82:    "RawTokenFormatter": (
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/_mapping.py:84:        "Raw tokens",
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/_mapping.py:85:        ("raw", "tokens"),
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/_mapping.py:87:        "Format tokens as a raw representation for storing token streams.",
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/_mapping.py:94:        "Format tokens as RTF markup. This formatter automatically outputs full RTF documents with color information and other useful stuff. Perfect for Copy and Paste into Microsoft(R) Word(R) documents.",
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/_mapping.py:101:        "Format tokens as an SVG graphics file.  This formatter is still experimental. Each line of code is a ``<text>`` element with explicit ``x`` and ``y`` coordinates containing ``<tspan>`` elements with the individual token styles.",
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/_mapping.py:108:        "Format tokens with ANSI color sequences, for output in a 256-color terminal or console.  Like in `TerminalFormatter` color sequences are terminated at newlines, so that paging the output works correctly.",
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/_mapping.py:115:        "Format tokens with ANSI color sequences, for output in a text console. Color sequences are terminated at newlines, so that paging the output works correctly.",
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/_mapping.py:122:        "Format tokens with ANSI color sequences, for output in a true-color terminal or console.  Like in `TerminalFormatter` color sequences are terminated at newlines, so that paging the output works correctly.",
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/_mapping.py:129:        "Format tokens as appropriate for a new testcase.",
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/pangomarkup.py:29:    Format tokens as Pango Markup code. It can then be rendered to an SVG.
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/pangomarkup.py:43:        for token, style in self.style:
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/pangomarkup.py:58:            self.styles[token] = (start, end)
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/pangomarkup.py:60:    def format_unencoded(self, tokensource, outfile):
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/pangomarkup.py:66:        for ttype, value in tokensource:
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/latex.py:15:from pygments.token import STANDARD_TYPES, Token
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/latex.py:64:# each token type defined in the current style.  That obviously is
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/latex.py:70:# specific token type, thus falling back to the parent token type if one
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/latex.py:72:# forms given in token.STANDARD_TYPES.
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/latex.py:150:    Format tokens as LaTeX code. This needs the `fancyvrb` and `color`
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/latex.py:178:        If set to ``True``, don't wrap the tokens at all, not even inside a
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/latex.py:225:        in comment tokens is not escaped so that LaTeX can render it (default:
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/latex.py:282:        t2n = self.ttype2name = {Token: ""}
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/latex.py:342:    def format_unencoded(self, tokensource, outfile):
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/latex.py:366:        for ttype, value in tokensource:
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/latex.py:367:            if ttype in Token.Comment:
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/latex.py:405:            elif ttype not in Token.Escape:
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/latex.py:408:            while ttype is not Token:
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/latex.py:457:    strings and comments. All other consecutive tokens are merged and
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/latex.py:459:    the Token.Escape type. Finally text that is not escaped is scanned
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/latex.py:469:    def get_tokens_unprocessed(self, text):
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/latex.py:470:        # find and remove all the escape tokens (replace with an empty string)
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/latex.py:471:        # this is very similar to DelegatingLexer.get_tokens_unprocessed.
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/latex.py:475:        for i, t, v in self._find_safe_escape_tokens(text):
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/latex.py:485:        return do_insertions(insertions, self.lang.get_tokens_unprocessed(buffered))
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/latex.py:487:    def _find_safe_escape_tokens(self, text):
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/latex.py:488:        """find escape tokens that are not in strings or comments"""
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/latex.py:490:            self.lang.get_tokens_unprocessed(text),
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/latex.py:491:            lambda t: t in Token.Comment or t in Token.String,
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/latex.py:494:                for i2, t2, v2 in self._find_escape_tokens(v):
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/latex.py:500:        """Keep only the tokens that match `pred`, merge the others together"""
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/latex.py:516:    def _find_escape_tokens(self, text):
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/latex.py:517:        """Find escape tokens within text, give token=None otherwise"""
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/latex.py:527:                    yield index + len(sep1), Token.Escape, b
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/latex.py:530:                    yield index, Token.Error, sep1
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/svg.py:12:from pygments.token import Comment
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/svg.py:34:    Format tokens as an SVG graphics file.  This formatter is still experimental.
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/svg.py:36:    coordinates containing ``<tspan>`` elements with the individual token styles.
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/svg.py:119:    def format_unencoded(self, tokensource, outfile):
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/svg.py:121:        Format ``tokensource``, an iterable of ``(tokentype, tokenstring)``
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/svg.py:155:        for ttype, value in tokensource:
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/svg.py:180:    def _get_style(self, tokentype):
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/svg.py:181:        if tokentype in self._stylecache:
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/svg.py:182:            return self._stylecache[tokentype]
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/svg.py:183:        otokentype = tokentype
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/svg.py:184:        while not self.style.styles_token(tokentype):
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/svg.py:185:            tokentype = tokentype.parent
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/svg.py:186:        value = self.style.style_for_token(tokentype)
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/svg.py:194:        self._stylecache[otokentype] = result
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/other.py:5:Other formatters: NullFormatter, RawTokenFormatter.
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/other.py:13:from pygments.token import Token
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/other.py:16:__all__ = ["NullFormatter", "RawTokenFormatter", "TestcaseFormatter"]
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/other.py:28:    def format(self, tokensource, outfile):
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/other.py:30:        for ttype, value in tokensource:
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/other.py:37:class RawTokenFormatter(Formatter):
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/other.py:39:    Format tokens as a raw representation for storing token streams.
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/other.py:41:    The format is ``tokentype<TAB>repr(tokenstring)\n``. The output can later
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/other.py:42:    be converted to a token stream with the `RawTokenLexer`, described in the
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/other.py:51:        If set to a color name, highlight error tokens using that color.  If
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/other.py:58:    name = "Raw tokens"
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/other.py:59:    aliases = ["raw", "tokens"]
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/other.py:68:        # The RawTokenFormatter outputs only ASCII. Override here.
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/other.py:80:    def format(self, tokensource, outfile):
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/other.py:84:            raise TypeError("The raw tokens formatter needs a binary " "output file")
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/other.py:109:            for ttype, value in tokensource:
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/other.py:111:                if ttype is Token.Error:
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/other.py:116:            for ttype, value in tokensource:
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/other.py:124:        tokens = [
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/other.py:128:        assert list(lexer.get_tokens(fragment)) == tokens
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/other.py:134:    Format tokens as appropriate for a new testcase.
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/other.py:147:    def format(self, tokensource, outfile):
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/other.py:151:        for ttype, value in tokensource:
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/terminal256.py:99:    Format tokens with ANSI color sequences, for output in a 256-color
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/terminal256.py:248:    def format(self, tokensource, outfile):
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/terminal256.py:249:        return Formatter.format(self, tokensource, outfile)
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/terminal256.py:251:    def format_unencoded(self, tokensource, outfile):
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/terminal256.py:255:        for ttype, value in tokensource:
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/terminal256.py:293:    Format tokens with ANSI color sequences, for output in a true-color
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/irc.py:12:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/irc.py:21:    Token,
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/irc.py:29:#: Map token types to a tuple of color values for light and dark
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/irc.py:32:    Token: ("", ""),
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/irc.py:108:    Format tokens with IRC color sequences
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/irc.py:120:        A dictionary mapping token types to (lightbg, darkbg) color names or
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/irc.py:144:    def format_unencoded(self, tokensource, outfile):
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/irc.py:147:        for ttype, value in tokensource:
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/html.py:18:from pygments.token import STANDARD_TYPES, Text, Token
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/html.py:126:    Format tokens as HTML 4 ``<span>`` tags. By default, the content is enclosed
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/html.py:180:    `get_style_defs()` method to request multiple prefixes for the tokens:
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/html.py:200:        around the tokens. This disables most other options (default: ``False``).
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/html.py:217:        If set to true, token ``<span>`` tags (as well as line number elements)
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/html.py:223:        Since the token types use relatively short class names, they may clash
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/html.py:226:        CSS class names for token types.
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/html.py:362:    `debug_token_types`
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/html.py:363:        Add ``title`` attributes to all token ``<span>`` tags that show the
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/html.py:364:        name of the token.
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/html.py:438:        self.debug_token_types = get_bool_opt(options, "debug_token_types", False)
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/html.py:474:        """Return the css class of this token type prefixed with
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/html.py:482:        """Return the CSS classes of this token type prefixed with the classprefix option."""
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/html.py:490:        """Return the inline CSS styles for this token type."""
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/html.py:498:        t2c = self.ttype2class = {Token: ""}
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/html.py:525:        insert before the token type classes.
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/html.py:531:        style_lines.extend(self.get_token_style_defs(arg))
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/html.py:535:    def get_token_style_defs(self, arg=None):
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/html.py:844:    def _format_lines(self, tokensource):
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/html.py:846:        Just format the tokens, without any wrapping tags.
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/html.py:855:        for ttype, value in tokensource:
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/html.py:859:                title = ' title="{}"'.format(".".join(ttype)) if self.debug_token_types else ""
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/html.py:877:            if tagsfile and ttype in Token.Name:
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/html.py:926:    def _lookup_ctag(self, token):
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/html.py:928:        if self._ctags.find(entry, token.encode(), 0):
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/html.py:933:    def _highlight_lines(self, tokensource):
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/html.py:936:        post-processing the token stream coming from `_format_lines`.
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/html.py:940:        for i, (t, value) in enumerate(tokensource):
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/html.py:969:    def format_unencoded(self, tokensource, outfile):
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/html.py:978:        is part of the original tokensource being highlighted, if it's
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/html.py:983:        source = self._format_lines(tokensource)
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/bbcode.py:19:    Format tokens with BBcodes. These formatting codes are used by many
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/bbcode.py:78:    def format_unencoded(self, tokensource, outfile):
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/bbcode.py:87:        for ttype, value in tokensource:
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/terminal.py:13:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/terminal.py:22:    Token,
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/terminal.py:30:#: Map token types to a tuple of color values for light and dark
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/terminal.py:33:    Token: ("", ""),
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/terminal.py:64:    Format tokens with ANSI color sequences, for output in a text console.
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/terminal.py:78:        A dictionary mapping token types to (lightbg, darkbg) color names or
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/terminal.py:97:    def format(self, tokensource, outfile):
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/terminal.py:98:        return Formatter.format(self, tokensource, outfile)
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/terminal.py:106:        # have to walk the tree of dots.  The base Token type must be a key,
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/terminal.py:114:    def format_unencoded(self, tokensource, outfile):
./.venv_tmp/lib/python3.12/site-packages/pygments/formatters/terminal.py:118:        for ttype, value in tokensource:
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/sas.py:14:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/lovelace.py:16:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/default.py:12:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/nord.py:13:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/nord.py:24:    Token,
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/nord.py:47:        Token: "#d8dee9",
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/nord.py:109:        Token: "#d8dee9",
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/native.py:12:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/native.py:21:    Token,
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/native.py:40:        Token: "#d0d0d0",
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/manni.py:15:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/dracula.py:15:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/stata_dark.py:14:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/stata_dark.py:23:    Token,
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/stata_dark.py:37:        Token: "#cccccc",
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/monokai.py:14:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/monokai.py:26:    Token,
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/monokai.py:45:        Token: "#f8f8f2",  # class:  ''
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/perldoc.py:14:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/gruvbox.py:13:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/gruvbox.py:22:    Token,
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/gruvbox.py:39:        Token: "#dddddd",
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/lilypond.py:12:from pygments.token import Token
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/lilypond.py:31:        Token.Text: "",
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/lilypond.py:32:        Token.Keyword: "bold",
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/lilypond.py:33:        Token.Comment: "italic #A3AAB2",
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/lilypond.py:34:        Token.String: "#AB0909",
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/lilypond.py:35:        Token.String.Escape: "#C46C6C",
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/lilypond.py:36:        Token.String.Symbol: "noinherit",
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/lilypond.py:37:        Token.Pitch: "",  # "#911520",
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/lilypond.py:38:        Token.Number: "#976806",  # includes durations
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/lilypond.py:41:        Token.ChordModifier: "#976806",
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/lilypond.py:42:        Token.Name.Lvalue: "#08547A",
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/lilypond.py:43:        Token.Name.BackslashReference: "#08547A",
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/lilypond.py:44:        Token.Name.Builtin.MusicCommand: "bold #08547A",
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/lilypond.py:45:        Token.Name.Builtin.PaperVariable: "bold #6C5A05",
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/lilypond.py:46:        Token.Name.Builtin.HeaderVariable: "bold #6C5A05",
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/lilypond.py:47:        Token.Name.Builtin.MusicFunction: "bold #08547A",
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/lilypond.py:48:        Token.Name.Builtin.Clef: "bold #08547A",
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/lilypond.py:49:        Token.Name.Builtin.Scale: "bold #08547A",
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/lilypond.py:50:        Token.Name.Builtin.RepeatType: "#08547A",
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/lilypond.py:51:        Token.Name.Builtin.Dynamic: "#68175A",
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/lilypond.py:52:        Token.Name.Builtin.Articulation: "#68175A",
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/lilypond.py:53:        Token.Name.Builtin.SchemeFunction: "bold #A83401",
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/lilypond.py:54:        Token.Name.Builtin.SchemeBuiltin: "bold",
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/lilypond.py:55:        Token.Name.Builtin.MarkupCommand: "bold #831E71",
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/lilypond.py:56:        Token.Name.Builtin.Context: "bold #038B8B",
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/lilypond.py:57:        Token.Name.Builtin.ContextProperty: "#038B8B",
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/lilypond.py:58:        Token.Name.Builtin.Grob: "bold #0C7441",
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/lilypond.py:59:        Token.Name.Builtin.GrobProperty: "#0C7441",
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/lilypond.py:60:        Token.Name.Builtin.Translator: "bold #6200A4",
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/onedark.py:15:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/onedark.py:23:    Token,
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/onedark.py:41:        Token: "#ABB2BF",
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/colorful.py:12:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/rainbow_dash.py:14:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/friendly.py:12:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/vs.py:12:from pygments.token import Comment, Error, Generic, Keyword, Name, Operator, String
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/paraiso_dark.py:16:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/rrt.py:12:from pygments.token import Comment, Keyword, Name, Number, Operator, String, Token
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/rrt.py:28:        Token: "#dddddd",
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/algol.py:33:from pygments.token import Comment, Error, Keyword, Name, Operator, String
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/igor.py:12:from pygments.token import Comment, Keyword, Name, String
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/borland.py:12:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/staroffice.py:12:from pygments.token import Comment, Error, Literal, Name, Token
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/staroffice.py:25:        Token: "#000080",  # Blue
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/abap.py:12:from pygments.token import Comment, Error, Keyword, Name, Number, Operator, String
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/algol_nu.py:33:from pygments.token import Comment, Error, Keyword, Name, Operator, String
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/friendly_grayscale.py:15:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/xcode.py:12:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/xcode.py:45:        # In Obj-C code this token is used to colour Cocoa types
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/gh_dark.py:13:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/gh_dark.py:24:    Token,
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/gh_dark.py:71:        Token: FG_DEFAULT,
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/arduino.py:12:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/tango.py:24:Token types, unlike most (if not all) of the styles included in the
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/tango.py:40:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/lightbulb.py:12:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/lightbulb.py:23:    Token,
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/lightbulb.py:108:        Token: COLORS["white"],
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/coffee.py:12:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/coffee.py:23:    Token,
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/coffee.py:90:        Token: "#ddd0c0",
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/material.py:14:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/bw.py:12:from pygments.token import Comment, Error, Generic, Keyword, Name, Operator, String
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/stata_light.py:13:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/trac.py:12:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/vim.py:12:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/vim.py:21:    Token,
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/vim.py:39:        Token: "#cccccc",
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/fruity.py:12:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/fruity.py:19:    Token,
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/fruity.py:38:        Token: "#ffffff",
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/murphy.py:12:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/autumn.py:12:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/inkpot.py:12:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/pastie.py:14:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/solarized.py:15:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/solarized.py:24:    Token,
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/solarized.py:32:        Token: colors["base0"],
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/zenburn.py:15:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/zenburn.py:26:    Token,
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/zenburn.py:47:        Token: "#dcdccc",
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/emacs.py:12:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/styles/paraiso_light.py:16:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/formatter.py:27:    Converts a token stream to text.
./.venv_tmp/lib/python3.12/site-packages/pygments/formatter.py:58:        convert the Unicode token strings to byte strings in the
./.venv_tmp/lib/python3.12/site-packages/pygments/formatter.py:114:    def format(self, tokensource, outfile):
./.venv_tmp/lib/python3.12/site-packages/pygments/formatter.py:116:        This method must format the tokens from the `tokensource` iterable and
./.venv_tmp/lib/python3.12/site-packages/pygments/formatter.py:119:        Formatter options can control how exactly the tokens are converted.
./.venv_tmp/lib/python3.12/site-packages/pygments/formatter.py:124:        return self.format_unencoded(tokensource, outfile)
./.venv_tmp/lib/python3.12/site-packages/pygments/token.py:2:pygments.token
./.venv_tmp/lib/python3.12/site-packages/pygments/token.py:5:Basic token types and the standard tokens.
./.venv_tmp/lib/python3.12/site-packages/pygments/token.py:12:class _TokenType(tuple):
./.venv_tmp/lib/python3.12/site-packages/pygments/token.py:34:        new = _TokenType(self + (val,))
./.venv_tmp/lib/python3.12/site-packages/pygments/token.py:41:        return "Token" + (self and "." or "") + ".".join(self)
./.venv_tmp/lib/python3.12/site-packages/pygments/token.py:52:Token = _TokenType()
./.venv_tmp/lib/python3.12/site-packages/pygments/token.py:54:# Special token types
./.venv_tmp/lib/python3.12/site-packages/pygments/token.py:55:Text = Token.Text
./.venv_tmp/lib/python3.12/site-packages/pygments/token.py:57:Escape = Token.Escape
./.venv_tmp/lib/python3.12/site-packages/pygments/token.py:58:Error = Token.Error
./.venv_tmp/lib/python3.12/site-packages/pygments/token.py:60:Other = Token.Other
./.venv_tmp/lib/python3.12/site-packages/pygments/token.py:62:# Common token types for source code
./.venv_tmp/lib/python3.12/site-packages/pygments/token.py:63:Keyword = Token.Keyword
./.venv_tmp/lib/python3.12/site-packages/pygments/token.py:64:Name = Token.Name
./.venv_tmp/lib/python3.12/site-packages/pygments/token.py:65:Literal = Token.Literal
./.venv_tmp/lib/python3.12/site-packages/pygments/token.py:68:Punctuation = Token.Punctuation
./.venv_tmp/lib/python3.12/site-packages/pygments/token.py:69:Operator = Token.Operator
./.venv_tmp/lib/python3.12/site-packages/pygments/token.py:70:Comment = Token.Comment
./.venv_tmp/lib/python3.12/site-packages/pygments/token.py:73:Generic = Token.Generic
./.venv_tmp/lib/python3.12/site-packages/pygments/token.py:75:# String and some others are not direct children of Token.
./.venv_tmp/lib/python3.12/site-packages/pygments/token.py:77:Token.Token = Token
./.venv_tmp/lib/python3.12/site-packages/pygments/token.py:78:Token.String = String
./.venv_tmp/lib/python3.12/site-packages/pygments/token.py:79:Token.Number = Number
./.venv_tmp/lib/python3.12/site-packages/pygments/token.py:82:def is_token_subtype(ttype, other):
./.venv_tmp/lib/python3.12/site-packages/pygments/token.py:91:def string_to_tokentype(s):
./.venv_tmp/lib/python3.12/site-packages/pygments/token.py:93:    Convert a string into a token type::
./.venv_tmp/lib/python3.12/site-packages/pygments/token.py:95:        >>> string_to_token('String.Double')
./.venv_tmp/lib/python3.12/site-packages/pygments/token.py:96:        Token.Literal.String.Double
./.venv_tmp/lib/python3.12/site-packages/pygments/token.py:97:        >>> string_to_token('Token.Literal.Number')
./.venv_tmp/lib/python3.12/site-packages/pygments/token.py:98:        Token.Literal.Number
./.venv_tmp/lib/python3.12/site-packages/pygments/token.py:99:        >>> string_to_token('')
./.venv_tmp/lib/python3.12/site-packages/pygments/token.py:100:        Token
./.venv_tmp/lib/python3.12/site-packages/pygments/token.py:102:    Tokens that are already tokens are returned unchanged:
./.venv_tmp/lib/python3.12/site-packages/pygments/token.py:104:        >>> string_to_token(String)
./.venv_tmp/lib/python3.12/site-packages/pygments/token.py:105:        Token.Literal.String
./.venv_tmp/lib/python3.12/site-packages/pygments/token.py:107:    if isinstance(s, _TokenType):
./.venv_tmp/lib/python3.12/site-packages/pygments/token.py:110:        return Token
./.venv_tmp/lib/python3.12/site-packages/pygments/token.py:111:    node = Token
./.venv_tmp/lib/python3.12/site-packages/pygments/token.py:117:# Map standard token types to short names, used in CSS class naming.
./.venv_tmp/lib/python3.12/site-packages/pygments/token.py:121:    Token: "",
./.venv_tmp/lib/python3.12/site-packages/pygments/style.py:11:from pygments.token import STANDARD_TYPES, Token
./.venv_tmp/lib/python3.12/site-packages/pygments/style.py:62:        for token in STANDARD_TYPES:
./.venv_tmp/lib/python3.12/site-packages/pygments/style.py:63:            if token not in obj.styles:
./.venv_tmp/lib/python3.12/site-packages/pygments/style.py:64:                obj.styles[token] = ""
./.venv_tmp/lib/python3.12/site-packages/pygments/style.py:84:            for token in ttype.split():
./.venv_tmp/lib/python3.12/site-packages/pygments/style.py:85:                if token in _styles:
./.venv_tmp/lib/python3.12/site-packages/pygments/style.py:87:                ndef = _styles.get(token.parent, None)
./.venv_tmp/lib/python3.12/site-packages/pygments/style.py:88:                styledefs = obj.styles.get(token, "").split()
./.venv_tmp/lib/python3.12/site-packages/pygments/style.py:89:                if not ndef or token is None:
./.venv_tmp/lib/python3.12/site-packages/pygments/style.py:91:                elif "noinherit" in styledefs and token is not Token:
./.venv_tmp/lib/python3.12/site-packages/pygments/style.py:92:                    ndef = _styles[Token][:]
./.venv_tmp/lib/python3.12/site-packages/pygments/style.py:95:                _styles[token] = ndef
./.venv_tmp/lib/python3.12/site-packages/pygments/style.py:96:                for styledef in obj.styles.get(token, "").split():
./.venv_tmp/lib/python3.12/site-packages/pygments/style.py:126:    def style_for_token(cls, token):
./.venv_tmp/lib/python3.12/site-packages/pygments/style.py:127:        t = cls._styles[token]
./.venv_tmp/lib/python3.12/site-packages/pygments/style.py:159:    def styles_token(cls, ttype):
./.venv_tmp/lib/python3.12/site-packages/pygments/style.py:163:        for token in cls._styles:
./.venv_tmp/lib/python3.12/site-packages/pygments/style.py:164:            yield token, cls.style_for_token(token)
./.venv_tmp/lib/python3.12/site-packages/pygments/style.py:190:    #: Style definitions for individual token types.
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/pointless.py:12:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/pointless.py:92:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/verification.py:12:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/verification.py:37:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/verification.py:102:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/hexdump.py:12:from pygments.token import Name, Number, Punctuation, String, Whitespace
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/hexdump.py:45:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/arturo.py:20:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/arturo.py:84:            yield from do_insertions([], lexer.get_tokens_unprocessed(code))
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/arturo.py:88:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/sas.py:14:from pygments.token import Comment, Generic, Keyword, Name, Number, Other, String, Text
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/sas.py:484:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/textfmts.py:15:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/textfmts.py:74:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/textfmts.py:129:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/textfmts.py:161:    def get_tokens_unprocessed(self, text, stack=("root",)):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/textfmts.py:164:        return RegexLexer.get_tokens_unprocessed(self, text, stack)
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/textfmts.py:204:                    for idx, token, value in lexer.get_tokens_unprocessed(content):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/textfmts.py:205:                        yield offset + idx, token, value
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/textfmts.py:209:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/textfmts.py:265:    # Aliases mapping standard token types of Todo.txt format concepts
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/textfmts.py:294:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/textfmts.py:324:            # Tokenize contexts and projects
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/textfmts.py:327:            # Tokenize non-whitespace text
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/textfmts.py:329:            # Tokenize whitespace not containing a newline
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/textfmts.py:336:            # Tokenize contexts and projects
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/textfmts.py:339:            # Tokenize non-whitespace text
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/textfmts.py:341:            # Tokenize whitespace not containing a newline
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/textfmts.py:374:        yield from lexer.get_tokens_unprocessed(code)
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/textfmts.py:376:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/textfmts.py:437:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/boa.py:12:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/boa.py:229:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/maple.py:12:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/maple.py:272:        yield from self.get_tokens_unprocessed(context=ctx)
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/maple.py:278:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/wren.py:14:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/wren.py:41:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/haskell.py:24:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/haskell.py:113:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/haskell.py:252:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/haskell.py:375:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/haskell.py:518:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/haskell.py:561:        "comment": HaskellLexer.tokens["comment"],
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/haskell.py:562:        "character": HaskellLexer.tokens["character"],
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/haskell.py:563:        "string": HaskellLexer.tokens["string"],
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/haskell.py:564:        "escape": HaskellLexer.tokens["escape"],
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/haskell.py:635:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/haskell.py:771:    def get_tokens_unprocessed(self, text):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/haskell.py:773:        for index, token, value in RegexLexer.get_tokens_unprocessed(self, text, stack):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/haskell.py:774:            if token is Name and value in self.EXTRA_KEYWORDS:
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/haskell.py:777:                yield index, token, value
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/haskell.py:799:    def get_tokens_unprocessed(self, text):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/haskell.py:834:                    insertions.append((len(code), list(lxlexer.get_tokens_unprocessed(latex))))
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/haskell.py:838:            insertions.append((len(code), list(lxlexer.get_tokens_unprocessed(latex))))
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/haskell.py:839:        yield from do_insertions(insertions, self.baselexer.get_tokens_unprocessed(code))
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/haskell.py:1031:    # koka token abstractions
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/haskell.py:1032:    tokenType = Name.Attribute
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/haskell.py:1033:    tokenTypeDef = Name.Class
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/haskell.py:1034:    tokenConstructor = Generic.Emph
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/haskell.py:1037:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/haskell.py:1041:            (r"::?" + sboundary, tokenType, "type"),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/haskell.py:1042:            (r"(alias)(\s+)([a-z]\w*)?", bygroups(Keyword, Whitespace, tokenTypeDef), "alias-type"),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/haskell.py:1045:                bygroups(Keyword, Whitespace, tokenTypeDef),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/haskell.py:1050:                bygroups(Keyword, Whitespace, tokenTypeDef),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/haskell.py:1053:            # special sequences of tokens (we use ?: for non-capturing group as
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/haskell.py:1093:            (r"((?:[a-z]\w*/)*)([A-Z]\w*)", bygroups(Name.Namespace, tokenConstructor)),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/haskell.py:1115:        "type": [(r"[(\[<]", tokenType, "type-nested"), include("type-content")],
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/haskell.py:1118:            (r"[)\]>]", tokenType, "#pop"),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/haskell.py:1119:            (r"[(\[<]", tokenType, "type-nested"),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/haskell.py:1120:            (r",", tokenType),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/haskell.py:1121:            (r"([a-z]\w*)(\s*)(:)(?!:)", bygroups(Name, Whitespace, tokenType)),  # parameter name
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/haskell.py:1135:            (r"[EPHVX]" + boundary, tokenType),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/haskell.py:1137:            (r"[a-z][0-9]*(?![\w/])", tokenType),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/haskell.py:1138:            (r"_\w*", tokenType.Variable),  # Generic.Emph
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/haskell.py:1139:            (r"((?:[a-z]\w*/)*)([A-Z]\w*)", bygroups(Name.Namespace, tokenType)),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/haskell.py:1140:            (r"((?:[a-z]\w*/)*)([a-z]\w+)", bygroups(Name.Namespace, tokenType)),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/haskell.py:1142:            (r"::|->|[.:|]", tokenType),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/hare.py:12:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/hare.py:41:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/apl.py:12:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/apl.py:45:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/apl.py:63:            # This token type is used for diamond and parenthesis
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/apl.py:69:            # Since this token type is very important in APL, it is not included in
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/apl.py:70:            # the punctuation token type but rather in the following one
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/apl.py:98:            (r"[\.\\\/âŒ¿â€Â¨â£â¨â â¤âˆ˜âŒ¸&âŒ¶@âŒºâ¥â›â¢]", Name.Attribute),  # closest token type
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/sgf.py:12:from pygments.token import Literal, Name, Punctuation, String, Whitespace
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/sgf.py:31:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/sgf.py:34:            # tokens:
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/elm.py:12:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/elm.py:97:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/tact.py:12:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/tact.py:35:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_mapping.py:2244:    "RawTokenLexer": (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_mapping.py:2246:        "Raw token data",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_mapping.py:2249:        ("application/x-pygments-tokens",),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_usd_builtins.py:99:    "token",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/dylan.py:14:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/dylan.py:300:    def get_tokens_unprocessed(self, text):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/dylan.py:301:        for index, token, value in RegexLexer.get_tokens_unprocessed(self, text):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/dylan.py:302:            if token is Name:
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/dylan.py:316:            yield index, token, value
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/dylan.py:318:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/dylan.py:353:                r"(\?" + valid_name + ")(:)" r"(token|name|variable|expression|body|case-body|\*)",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/dylan.py:357:                r"(\?)(:)(token|name|variable|expression|body|case-body|\*)",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/dylan.py:416:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/dylan.py:448:    def get_tokens_unprocessed(self, text):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/dylan.py:462:                    yield from do_insertions(insertions, dylexer.get_tokens_unprocessed(curcode))
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/dylan.py:467:            yield from do_insertions(insertions, dylexer.get_tokens_unprocessed(curcode))
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/scripting.py:14:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/scripting.py:87:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/scripting.py:198:    def get_tokens_unprocessed(self, text):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/scripting.py:199:        for index, token, value in RegexLexer.get_tokens_unprocessed(self, text):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/scripting.py:200:            if token is Name.Builtin and value not in self._functions:
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/scripting.py:209:            yield index, token, value
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/scripting.py:271:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/scripting.py:475:    def get_tokens_unprocessed(self, text):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/scripting.py:476:        for index, token, value in RegexLexer.get_tokens_unprocessed(self, text):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/scripting.py:477:            if token is Name or token is Name.Other:
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/scripting.py:495:                        if token is Name:
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/scripting.py:506:            yield index, token, value
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/scripting.py:521:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/scripting.py:587:    def get_tokens_unprocessed(self, text):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/scripting.py:589:        for index, token, value in LuaLexer.get_tokens_unprocessed(self, text):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/scripting.py:590:            if token == Punctuation and value == ".":
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/scripting.py:591:                token = Operator
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/scripting.py:592:            yield index, token, value
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/scripting.py:609:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/scripting.py:696:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/scripting.py:1521:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/scripting.py:1585:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/scripting.py:1770:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/scripting.py:1818:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/scripting.py:2309:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/scripting.py:2464:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/scripting.py:2553:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/ada.py:15:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/ada.py:43:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/nimrod.py:14:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/nimrod.py:135:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/objective.py:24:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/objective.py:64:        tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/objective.py:270:        def get_tokens_unprocessed(self, text, stack=("root",)):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/objective.py:277:            for index, token, value in baselexer.get_tokens_unprocessed(self, text, stack):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/objective.py:278:                if token is Name or token is Name.Class:
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/objective.py:284:                        token = Name.Builtin.Pseudo
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/objective.py:286:                yield index, token, value
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/objective.py:330:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/objective.py:395:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/objective.py:902:    def get_tokens_unprocessed(self, text):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/objective.py:909:        for index, token, value in RegexLexer.get_tokens_unprocessed(self, text):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/objective.py:910:            if token is Name or token is Name.Class:
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/objective.py:916:                    token = Name.Builtin.Pseudo
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/objective.py:918:            yield index, token, value
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/numbair.py:12:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/numbair.py:40:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/php.py:25:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/php.py:61:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/php.py:140:    def get_tokens_unprocessed(self, text):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/php.py:154:                    yield from do_insertions(insertions, phplexer.get_tokens_unprocessed(curcode))
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/php.py:159:            yield from do_insertions(insertions, phplexer.get_tokens_unprocessed(curcode))
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/php.py:209:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/php.py:362:    def get_tokens_unprocessed(self, text):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/php.py:366:        for index, token, value in RegexLexer.get_tokens_unprocessed(self, text, stack):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/php.py:367:            if token is Name.Other:
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/php.py:371:            yield index, token, value
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/ruby.py:25:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/ruby.py:113:            yield from self.get_tokens_unprocessed(context=ctx)
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/ruby.py:146:            for i, t, v in self.get_tokens_unprocessed(context=nctx):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/ruby.py:154:            for i, t, v in self.get_tokens_unprocessed(context=nctx):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/ruby.py:242:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/ruby.py:599:    tokens.update(gen_rubystrings_rules())
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/ruby.py:619:    def get_tokens_unprocessed(self, text):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/ruby.py:633:                    yield from do_insertions(insertions, rblexer.get_tokens_unprocessed(curcode))
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/ruby.py:638:            yield from do_insertions(insertions, rblexer.get_tokens_unprocessed(curcode))
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/ruby.py:657:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/wgsl.py:13:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/wgsl.py:36:# https://www.w3.org/TR/WGSL/#syntax-ident_pattern_token
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/wgsl.py:37:ident_pattern_token = f"([{uni.xid_start}][{uni.xid_continue}]+)|[{uni.xid_start}]"
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/wgsl.py:367:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/wgsl.py:382:            (ident_pattern_token, Name.Decorator, "#pop"),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/wgsl.py:435:            (ident_pattern_token, Name),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/wgsl.py:436:            # TODO: templates start and end tokens.
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/roboconf.py:12:from pygments.token import Comment, Keyword, Name, Operator, Text
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/roboconf.py:29:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/roboconf.py:66:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/tcl.py:12:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/tcl.py:166:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/make.py:15:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/make.py:55:    def get_tokens_unprocessed(self, text):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/make.py:69:        yield from do_insertions(ins, lex.get_tokens_unprocessed(done))
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/make.py:89:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/make.py:152:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/verifpal.py:12:from pygments.token import Comment, Keyword, Name, Punctuation, String, Whitespace
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/verifpal.py:29:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/verifpal.py:75:            (words(("password",), suffix=r"\b"), Keyword.Constant),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/automation.py:12:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/automation.py:38:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/automation.py:329:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/json5.py:12:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/json5.py:46:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/fantom.py:14:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/fantom.py:55:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/teal.py:12:from pygments.token import Comment, Keyword, Name, Number, String, Text, Whitespace
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/teal.py:115:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/textedit.py:16:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/textedit.py:43:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/textedit.py:107:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/textedit.py:175:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/textedit.py:225:        we match `\b\w+\b` and then call is_in() on those tokens.  See
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/textedit.py:239:    def get_tokens_unprocessed(self, text):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/textedit.py:240:        # TODO: builtins are only subsequent tokens on lines
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/textedit.py:243:        for index, token, value in RegexLexer.get_tokens_unprocessed(self, text):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/textedit.py:244:            if token is Name.Other:
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/textedit.py:252:                yield index, token, value
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/algebra.py:14:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/algebra.py:41:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/algebra.py:127:    def get_tokens_unprocessed(self, text):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/algebra.py:146:                    yield from do_insertions(insertions, gaplexer.get_tokens_unprocessed(curcode))
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/algebra.py:158:            yield from do_insertions(insertions, gaplexer.get_tokens_unprocessed(curcode))
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/algebra.py:230:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/algebra.py:260:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/algebra.py:347:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/go.py:12:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/go.py:39:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/go.py:180:            # Tokens
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/tnt.py:14:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/tnt.py:70:        """Tokenize whitespace."""
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/tnt.py:84:        """Tokenize a variable."""
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/tnt.py:94:        """Tokenize a term."""
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/tnt.py:120:        """Tokenize a formula."""
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/tnt.py:154:        """Tokenize a rule."""
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/tnt.py:171:        """Tokenize a line referral."""
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/tnt.py:200:    def get_tokens_unprocessed(self, text):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/tnt.py:201:        """Returns a list of TNT tokens."""
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/dsls.py:24:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/dsls.py:63:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/dsls.py:160:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/dsls.py:399:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/dsls.py:612:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/dsls.py:790:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/dsls.py:926:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/dsls.py:983:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/dsls.py:1024:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/dsls.py:1086:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/dsls.py:1262:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/dsls.py:1453:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/dsls.py:1538:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/dsls.py:1628:    def get_tokens_unprocessed(self, text=None, context=None):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/dsls.py:1630:        return ExtendedRegexLexer.get_tokens_unprocessed(self, text, context)
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/tal.py:14:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/tal.py:76:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/srcinfo.py:15:from pygments.token import Comment, Keyword, Name, Operator, Text, Whitespace
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/srcinfo.py:65:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/carbon.py:14:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/carbon.py:43:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/carbon.py:114:            # tokens
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/ampl.py:12:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/ampl.py:38:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/iolang.py:12:from pygments.token import Comment, Keyword, Name, Number, Operator, String, Whitespace
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/iolang.py:28:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/special.py:14:from pygments.token import Error, Generic, Text, Token
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/special.py:17:__all__ = ["TextLexer", "OutputLexer", "RawTokenLexer"]
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/special.py:34:    def get_tokens_unprocessed(self, text):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/special.py:43:    Simple lexer that highlights everything as ``Token.Generic.Output``.
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/special.py:52:    def get_tokens_unprocessed(self, text):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/special.py:59:class RawTokenLexer(Lexer):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/special.py:61:    Recreate a token stream formatted with the `RawTokenFormatter`.
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/special.py:66:        If set to ``"gz"`` or ``"bz2"``, decompress the token stream with
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/special.py:70:    name = "Raw token data"
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/special.py:73:    mimetypes = ["application/x-pygments-tokens"]
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/special.py:74:    url = "https://pygments.org/docs/formatters/#RawTokenFormatter"
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/special.py:81:    def get_tokens(self, text):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/special.py:99:        # do not call Lexer.get_tokens() because stripping is not optional.
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/special.py:101:        for i, t, v in self.get_tokens_unprocessed(text):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/special.py:104:    def get_tokens_unprocessed(self, text):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/special.py:111:                    ttype = Token
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/special.py:115:                            raise ValueError("malformed token name")
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/modula2.py:14:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/modula2.py:103:    No whitespace is permitted between the tokens of a dialect tag.
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/modula2.py:178:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/modula2.py:419:    # Lexemes to Mark as Error Tokens for PIM Modula-2
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/modula2.py:482:    # Lexemes to Mark as Error Tokens for ISO Modula-2
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/modula2.py:633:    # Lexemes to Mark as Error Tokens for Modula-2 R10
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/modula2.py:782:    # Lexemes to Mark as Error Tokens for Objective Modula-2
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/modula2.py:1813:    # intercept the token stream, modify token attributes and return them
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/modula2.py:1814:    def get_tokens_unprocessed(self, text):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/modula2.py:1815:        for index, token, value in RegexLexer.get_tokens_unprocessed(self, text):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/modula2.py:1818:            if not self.dialect_set_by_tag and token == Comment.Special:
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/modula2.py:1821:                    # token is a dialect indicator
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/modula2.py:1827:            if token is Name:
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/modula2.py:1829:                    token = Keyword.Reserved
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/modula2.py:1834:                    token = Name.Builtin
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/modula2.py:1839:                    token = Name.Builtin.Pseudo
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/modula2.py:1845:                        token = Name.Namespace
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/modula2.py:1847:                        token = Name.Builtin.Pseudo
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/modula2.py:1852:                    token = Name.Namespace
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/modula2.py:1855:                    token = Name.Class
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/modula2.py:1858:                    token = Name.Function
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/modula2.py:1861:                    token = Name.Variable
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/modula2.py:1864:                    token = Name.Constant
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/modula2.py:1866:            elif token in Number:
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/modula2.py:1871:                        token = Error
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/modula2.py:1875:                    if token is Number.Oct:
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/modula2.py:1876:                        token = Error
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/modula2.py:1878:                    elif token is Number.Hex and "H" in value:
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/modula2.py:1879:                        token = Error
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/modula2.py:1881:                    elif token is Number.Float and "E" in value:
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/modula2.py:1882:                        token = Error
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/modula2.py:1884:            elif token in Comment:
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/modula2.py:1887:                if token is Comment.Single:
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/modula2.py:1889:                        token = Error
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/modula2.py:1891:                if token is Comment.Preproc:
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/modula2.py:1894:                        token = Error
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/modula2.py:1901:                        token = Comment.Multiline
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/modula2.py:1903:            else:  # token is neither Name nor Comment
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/modula2.py:1905:                # mark lexemes matching the dialect's error token set as errors
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/modula2.py:1907:                    token = Error
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/modula2.py:1923:            yield index, token, value
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:16:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:70:    token_end = r"""
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:81:    def get_tokens_unprocessed(self, text):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:82:        for index, token, value in super().get_tokens_unprocessed(text):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:83:            if token is Name.Function or token is Name.Variable:
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:89:                    yield index, token, value
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:91:                yield index, token, value
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:184:          # Need to ensure we have a full token. 1+ is not a
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:187:          {token_end}
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:198:            token_type = Number.Float  # includes [+-](inf|nan).0
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:200:            token_type = Number.Integer
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:201:        yield match.start(), token_type, match.group()
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:216:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:283:            (rf"(?x).*?{token_end}", Comment, "#pop"),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:326:    # symbol token, reverse-engineered from hyperspec
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:350:    def get_tokens_unprocessed(self, text):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:352:        for index, token, value in RegexLexer.get_tokens_unprocessed(self, text, stack):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:353:            if token is Name.Variable:
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:375:            yield index, token, value
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:377:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:570:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:615:        "py-keywords": PythonLexer.tokens["keywords"],
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:616:        "py-builtins": PythonLexer.tokens["builtins"],
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:3076:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:3086:            # onto Pygments token types; some judgment calls here.
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:3597:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:3639:    An ELisp lexer, parsing a stream and outputting the tokens
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:3659:    # symbol token, reverse-engineered from hyperspec
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:5196:    def get_tokens_unprocessed(self, text):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:5198:        for index, token, value in RegexLexer.get_tokens_unprocessed(self, text, stack):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:5199:            if token is Name.Variable:
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:5218:            yield index, token, value
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:5220:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:5513:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:5538:    def get_tokens_unprocessed(self, text):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:5539:        tokens = RegexLexer.get_tokens_unprocessed(self, text)
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:5540:        tokens = self._process_symbols(tokens)
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:5541:        tokens = self._process_declarations(tokens)
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:5542:        return tokens
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:5544:    def _relevant(self, token):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:5545:        return token not in (Text, Whitespace, Comment.Single, Comment.Multiline)
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:5547:    def _process_declarations(self, tokens):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:5549:        for index, token, value in tokens:
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:5550:            yield index, token, value
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:5551:            if self._relevant(token):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:5552:                if opening_paren and token == Keyword and value in self.DECLARATIONS:
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:5554:                    yield from self._process_declaration(declaration, tokens)
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:5555:                opening_paren = value == "(" and token == Punctuation
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:5557:    def _process_symbols(self, tokens):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:5559:        for index, token, value in tokens:
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:5560:            if opening_paren and token in (Literal, Name.Variable):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:5561:                token = self.MAPPINGS.get(value, Name.Function)
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:5562:            elif token == Literal and value in self.BUILTINS_ANYWHERE:
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:5563:                token = Name.Builtin
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:5564:            opening_paren = value == "(" and token == Punctuation
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:5565:            yield index, token, value
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:5567:    def _process_declaration(self, declaration, tokens):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:5568:        for index, token, value in tokens:
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:5569:            if self._relevant(token):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:5571:            yield index, token, value
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:5575:            token = Keyword.Type if token == Literal else token
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:5576:            yield index, token, value
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:5577:            for index, token, value in tokens:
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:5578:                if prev_was_colon and token == Literal:
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:5579:                    token = Keyword.Type
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:5580:                yield index, token, value
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:5581:                if self._relevant(token):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:5582:                    prev_was_colon = token == Literal and value == ":"
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:5584:            token = Name.Namespace if token == Literal else token
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:5585:            yield index, token, value
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:5587:            token = Name.Function if token == Literal else token
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:5588:            yield index, token, value
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:5589:            for index, token, value in tokens:
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:5590:                if self._relevant(token):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:5592:                yield index, token, value
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:5593:            if value == "{" and token == Literal:
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:5595:                for index, token, value in self._process_signature(tokens):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:5596:                    yield index, token, value
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:5598:                yield index, token, value
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:5600:            token = Name.Function if token == Literal else token
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:5601:            yield index, token, value
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:5605:    def _process_signature(self, tokens):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:5606:        for index, token, value in tokens:
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:5607:            if token == Literal and value == "}":
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:5610:            elif token in (Literal, Name.Function):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:5611:                token = Name.Variable if value.istitle() else Keyword.Type
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:5612:            yield index, token, value
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:5670:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:6098:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:6308:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:7022:    # _token_end = r'''
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:7032:    _token_end = r"(?=\s|#|[)\]]|$)"
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:7054:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:7097:            (words(constants, suffix=_token_end), Keyword.Constants),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:7101:            (words(builtin_variables, suffix=_token_end), Name.Variable.Global),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:7102:            (words(special_forms, prefix=r"(?<=\()", suffix=_token_end), Keyword.Reserved),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:7103:            (words(builtin_macros, prefix=r"(?<=\()", suffix=_token_end), Name.Builtin),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lisp.py:7104:            (words(builtin_functions, prefix=r"(?<=\()", suffix=_token_end), Name.Function),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/sieve.py:21:from pygments.token import Comment, Keyword, Literal, Name, Punctuation, String, Text
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/sieve.py:37:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/sieve.py:52:            # tokens:
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/factor.py:12:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/factor.py:801:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/meson.py:12:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/meson.py:45:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_cocoa_builtins.py:93:    "ASAccountAuthenticationModificationReplacePasswordWithSignInWithAppleRequest",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_cocoa_builtins.py:95:    "ASAccountAuthenticationModificationUpgradePasswordToStrongPasswordRequest",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_cocoa_builtins.py:104:    "ASAuthorizationPasswordProvider",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_cocoa_builtins.py:105:    "ASAuthorizationPasswordRequest",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_cocoa_builtins.py:117:    "ASPasswordCredential",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_cocoa_builtins.py:118:    "ASPasswordCredentialIdentity",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_cocoa_builtins.py:409:    "CKFetchWebAuthTokenOperation",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_cocoa_builtins.py:434:    "CKServerChangeToken",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_cocoa_builtins.py:951:    "HMAccessoryOwnershipToken",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_cocoa_builtins.py:1840:    "NIDiscoveryToken",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_cocoa_builtins.py:1853:    "NLTokenizer",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_cocoa_builtins.py:2031:    "NSPersistentHistoryToken",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_cocoa_builtins.py:2053:    "NSQueryGenerationToken",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_cocoa_builtins.py:2264:    "PKPaymentToken",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_cocoa_builtins.py:2488:    "TKSmartCardToken",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_cocoa_builtins.py:2489:    "TKSmartCardTokenDriver",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_cocoa_builtins.py:2490:    "TKSmartCardTokenSession",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_cocoa_builtins.py:2496:    "TKToken",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_cocoa_builtins.py:2497:    "TKTokenAuthOperation",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_cocoa_builtins.py:2498:    "TKTokenConfiguration",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_cocoa_builtins.py:2499:    "TKTokenDriver",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_cocoa_builtins.py:2500:    "TKTokenDriverConfiguration",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_cocoa_builtins.py:2501:    "TKTokenKeyAlgorithm",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_cocoa_builtins.py:2502:    "TKTokenKeyExchangeParameters",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_cocoa_builtins.py:2503:    "TKTokenKeychainCertificate",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_cocoa_builtins.py:2504:    "TKTokenKeychainContents",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_cocoa_builtins.py:2505:    "TKTokenKeychainItem",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_cocoa_builtins.py:2506:    "TKTokenKeychainKey",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_cocoa_builtins.py:2507:    "TKTokenPasswordAuthOperation",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_cocoa_builtins.py:2508:    "TKTokenSession",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_cocoa_builtins.py:2509:    "TKTokenSmartCardPINAuthOperation",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_cocoa_builtins.py:2510:    "TKTokenWatcher",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_cocoa_builtins.py:2751:    "UISearchToken",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_cocoa_builtins.py:2796:    "UITextInputPasswordRules",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_cocoa_builtins.py:2797:    "UITextInputStringTokenizer",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_cocoa_builtins.py:3591:    "TKSmartCardTokenDriverDelegate",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_cocoa_builtins.py:3593:    "TKTokenDelegate",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_cocoa_builtins.py:3594:    "TKTokenDriverDelegate",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_cocoa_builtins.py:3595:    "TKTokenSessionDelegate",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_cocoa_builtins.py:3726:    "UITextInputTokenizer",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_cocoa_builtins.py:4124:    "opaqueCMBufferQueueTriggerToken",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/codeql.py:19:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/codeql.py:41:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/macaulay2.py:12:from pygments.token import Comment, Keyword, Name, String, Text
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/macaulay2.py:1786:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/fift.py:12:from pygments.token import Comment, Literal, Name, Number, String, Whitespace
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/fift.py:28:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/pony.py:12:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/pony.py:39:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/parsers.py:22:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/parsers.py:72:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/parsers.py:167:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/parsers.py:367:    _TOKEN_REF = r"[A-Z]\w*"
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/parsers.py:372:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/parsers.py:389:            # tokensSpec
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/parsers.py:390:            (r"tokens\b", Keyword, "tokens"),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/parsers.py:465:            # Tokens start with capital letter.
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/parsers.py:476:        "tokens": [
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/parsers.py:481:                r"(" + _TOKEN_REF + r")(\s*)(=)?(\s*)(" + _STRING_LITERAL + r")?(\s*)(;)",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/parsers.py:746:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/parsers.py:835:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/css.py:24:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/css.py:698:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/css.py:832:common_sass_tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/css.py:1136:def _starts_block(token, state):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/css.py:1138:        yield match.start(), token, match.group(0)
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/css.py:1165:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/css.py:1217:    for group, common in common_sass_tokens.items():
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/css.py:1218:        tokens[group] = copy.copy(common)
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/css.py:1219:    tokens["value"].append((r"\n", Whitespace, "root"))
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/css.py:1220:    tokens["selector"].append((r"\n", Whitespace, "root"))
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/css.py:1236:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/css.py:1267:    for group, common in common_sass_tokens.items():
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/css.py:1268:        tokens[group] = copy.copy(common)
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/css.py:1269:    tokens["value"].extend([(r"\n", Whitespace), (r"[;{}]", Punctuation, "#pop")])
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/css.py:1270:    tokens["selector"].extend([(r"\n", Whitespace), (r"[;{}]", Punctuation, "#pop")])
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/css.py:1285:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_sourcemod_builtins.py:403:    "SetAdminPassword",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_sourcemod_builtins.py:404:    "GetAdminPassword",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/stata.py:15:from pygments.token import Comment, Keyword, Name, Number, Operator, String, Text
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/stata.py:38:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/foxpro.py:14:from pygments.token import Comment, Keyword, Name, Operator, Punctuation, String, Text
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/foxpro.py:35:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/foxpro.py:228:                r"Parent|Partition|PasswordChar|PictureMargin|"
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/crystal.py:21:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/crystal.py:101:            yield from self.get_tokens_unprocessed(context=ctx)
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/crystal.py:217:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/crystal.py:481:    tokens.update(gen_crystalstrings_rules())
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/gdscript.py:17:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/gdscript.py:59:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/soong.py:12:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/soong.py:32:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/asc.py:14:from pygments.token import Comment, Generic, Name, Operator, String, Whitespace
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/asc.py:48:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/rust.py:12:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/rust.py:169:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/markup.py:29:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/markup.py:74:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/markup.py:108:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/markup.py:202:        yield from do_insertions(ins, lexer.get_tokens_unprocessed(code))
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/markup.py:211:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/markup.py:362:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/markup.py:422:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/markup.py:479:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/markup.py:540:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/markup.py:635:            yield from do_insertions([], lexer.get_tokens_unprocessed(code))
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/markup.py:639:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/markup.py:744:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/markup.py:810:            (_inline(r"=", r"="), String),  # TODO token
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/markup.py:877:        yield from do_insertions([], lexer.get_tokens_unprocessed(code))
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/markup.py:904:        yield from do_insertions([], lexer.get_tokens_unprocessed(code))
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/markup.py:908:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/markup.py:1058:    def text_rules(token):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/markup.py:1060:            (r"\w+", token),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/markup.py:1061:            (r"[^\S\n]+", token),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/markup.py:1062:            (r"(?s).", token),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/markup.py:1080:            yield from self.get_tokens_unprocessed(attr_content, stack=["root", "attr"])
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/markup.py:1083:        yield from self.get_tokens_unprocessed(attr, stack=["root", "attr"])
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/markup.py:1101:            yield from lexer.get_tokens_unprocessed(content)
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/markup.py:1116:            yield from self.get_tokens_unprocessed(attr_content, stack=["root", "attr"])
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/markup.py:1120:        yield from self.get_tokens_unprocessed(attr, stack=["root", "attr"])
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/markup.py:1128:            yield from LilyPondLexer().get_tokens_unprocessed(content)
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/markup.py:1529:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/rego.py:12:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/rego.py:61:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/prolog.py:14:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/prolog.py:40:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/prolog.py:119:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/forth.py:14:from pygments.token import Comment, Keyword, Name, Number, String, Text, Whitespace
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/forth.py:33:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/dalvik.py:14:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/dalvik.py:41:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/console.py:12:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/console.py:40:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/console.py:67:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/console.py:84:            (r"(None|descr|ConstClass|ConstPtr|TargetToken)", Name),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/console.py:97:                r"force_token|quasiimmut_field|same_as|virtual_ref_finish|"
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/urbi.py:14:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/urbi.py:43:    # - handle Experimental and deprecated tags with specific tokens
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/urbi.py:44:    # - handle Angles and Durations with specific tokens
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/urbi.py:67:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/urbi.py:155:            # deprecated keywords, use a meaningful token when available
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/urbi.py:157:            # ignored keywords, use a meaningful token when available
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/kusto.py:12:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/kusto.py:146:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/typoscript.py:23:from pygments.token import Comment, Name, Number, Operator, Punctuation, String, Text
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/typoscript.py:38:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/typoscript.py:82:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/typoscript.py:131:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/unicon.py:14:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/unicon.py:42:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/unicon.py:467:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/unicon.py:843:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/yang.py:12:from pygments.token import Comment, Name, Number, String, Text, Token
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/yang.py:136:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/yang.py:145:            (r"[{};]+", Token.Punctuation),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/yang.py:146:            (r"(?<![\-\w])(and|or|not|\+|\.)(?![\-\w])", Token.Operator),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/yang.py:154:                bygroups(Name.Namespace, Token.Punctuation, Name.Variable),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/yang.py:160:            (words(TOP_STMTS_KEYWORDS, suffix=suffix_re_pattern), Token.Keyword),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/yang.py:161:            (words(MODULE_HEADER_STMT_KEYWORDS, suffix=suffix_re_pattern), Token.Keyword),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/yang.py:162:            (words(META_STMT_KEYWORDS, suffix=suffix_re_pattern), Token.Keyword),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/yang.py:163:            (words(LINKAGE_STMTS_KEYWORDS, suffix=suffix_re_pattern), Token.Keyword),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/yang.py:164:            (words(BODY_STMT_KEYWORDS, suffix=suffix_re_pattern), Token.Keyword),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/yang.py:165:            (words(DATA_DEF_STMT_KEYWORDS, suffix=suffix_re_pattern), Token.Keyword),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/yang.py:166:            (words(TYPE_STMT_KEYWORDS, suffix=suffix_re_pattern), Token.Keyword),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/yang.py:167:            (words(LIST_STMT_KEYWORDS, suffix=suffix_re_pattern), Token.Keyword),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lilypond.py:38:from pygments.token import Token
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lilypond.py:42:# In LilyPond, (unquoted) name tokens only contain letters, hyphens,
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lilypond.py:44:# a name token.
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lilypond.py:93:    def get_tokens_unprocessed(self, text):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lilypond.py:95:        for index, token, value in super().get_tokens_unprocessed(text):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lilypond.py:96:            if token is Token.Name.Function or token is Token.Name.Variable:
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lilypond.py:98:                    token = Token.Name.Builtin.SchemeFunction
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lilypond.py:99:            elif token is Token.Name.Builtin:
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lilypond.py:100:                token = Token.Name.Builtin.SchemeBuiltin
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lilypond.py:101:            yield index, token, value
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lilypond.py:103:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lilypond.py:106:            (r"\s+", Token.Text.Whitespace),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lilypond.py:108:            (r"%\{.*?%\}", Token.Comment.Multiline),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lilypond.py:110:            (r"%.*?$", Token.Comment.Single),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lilypond.py:112:            (r"#\}", Token.Punctuation, "#pop"),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lilypond.py:116:            (r"[#$]@?", Token.Punctuation, "value"),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lilypond.py:136:                Token.Punctuation,
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lilypond.py:140:            (words(pitches, suffix=r"=?[',]*!?\??" + NAME_END_RE), Token.Pitch),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lilypond.py:142:            (r'[\-_^]?"', Token.String, "string"),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lilypond.py:144:            (r"-?\d+\.\d+", Token.Number.Float),  # 5. and .5 are not allowed
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lilypond.py:145:            (r"-?\d+/\d+", Token.Number.Fraction),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lilypond.py:160:                Token.Number,
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lilypond.py:163:            (r"\*", Token.Number),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lilypond.py:165:            (r"[~()[\]]", Token.Name.Builtin.Articulation),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lilypond.py:168:            (r"[\-_^][>^_!.\-+]", Token.Name.Builtin.Articulation),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lilypond.py:170:            (r"[\-_^]?\\?\d+", Token.Name.Builtin.Articulation),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lilypond.py:172:            (builtin_words(keywords, "mandatory"), Token.Keyword),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lilypond.py:173:            (builtin_words(pitch_language_names, "disallowed"), Token.Name.PitchLanguage),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lilypond.py:174:            (builtin_words(clefs, "disallowed"), Token.Name.Builtin.Clef),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lilypond.py:175:            (builtin_words(scales, "mandatory"), Token.Name.Builtin.Scale),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lilypond.py:176:            (builtin_words(repeat_types, "disallowed"), Token.Name.Builtin.RepeatType),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lilypond.py:177:            (builtin_words(units, "mandatory"), Token.Number),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lilypond.py:178:            (builtin_words(chord_modifiers, "disallowed"), Token.ChordModifier),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lilypond.py:179:            (builtin_words(music_functions, "mandatory"), Token.Name.Builtin.MusicFunction),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lilypond.py:180:            (builtin_words(dynamics, "mandatory"), Token.Name.Builtin.Dynamic),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lilypond.py:182:            (builtin_words(articulations, "mandatory"), Token.Name.Builtin.Articulation),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lilypond.py:183:            (builtin_words(music_commands, "mandatory"), Token.Name.Builtin.MusicCommand),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lilypond.py:184:            (builtin_words(markup_commands, "mandatory"), Token.Name.Builtin.MarkupCommand),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lilypond.py:185:            (builtin_words(grobs, "disallowed"), Token.Name.Builtin.Grob),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lilypond.py:186:            (builtin_words(translators, "disallowed"), Token.Name.Builtin.Translator),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lilypond.py:188:            (builtin_words(contexts, "optional"), Token.Name.Builtin.Context),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lilypond.py:189:            (builtin_words(context_properties, "disallowed"), Token.Name.Builtin.ContextProperty),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lilypond.py:192:                Token.Name.Builtin.GrobProperty,
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lilypond.py:198:            (builtin_words(paper_variables, "optional"), Token.Name.Builtin.PaperVariable),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lilypond.py:199:            (builtin_words(header_variables, "optional"), Token.Name.Builtin.HeaderVariable),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lilypond.py:202:            (r"[\-_^]?\\.+?" + NAME_END_RE, Token.Name.BackslashReference),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lilypond.py:210:                Token.Name.Lvalue,
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lilypond.py:215:            (r"([^\W\d]|-)+?" + NAME_END_RE, Token.Text),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lilypond.py:216:            (r".", Token.Text),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lilypond.py:219:            (r'"', Token.String, "#pop"),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lilypond.py:220:            (r"\\.", Token.String.Escape),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lilypond.py:221:            (r'[^\\"]+', Token.String),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lilypond.py:226:            (r"#\{", Token.Punctuation, ("#pop", "root")),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lilypond.py:234:            (r"\s+", Token.Text.Whitespace),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lilypond.py:237:                bygroups(Token.Punctuation, Token.Name.Builtin.GrobProperty),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/modeling.py:16:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/modeling.py:47:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/modeling.py:239:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/modeling.py:264:            # SLexer makes these tokens Operators.
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/modeling.py:388:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/modeling.py:458:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/vyper.py:12:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/vyper.py:36:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/clean.py:12:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/clean.py:77:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/gleam.py:12:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/gleam.py:66:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/jsonnet.py:12:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/jsonnet.py:26:jsonnet_token = r"[^\W\d]\w*"
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/jsonnet.py:27:jsonnet_function_token = jsonnet_token + r"(?=\()"
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/jsonnet.py:50:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/jsonnet.py:98:            (r"std\." + jsonnet_function_token, Name.Builtin, "function_args"),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/jsonnet.py:99:            (jsonnet_function_token, Name.Function, "function_args"),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/jsonnet.py:100:            (jsonnet_token, Name.Variable),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/jsonnet.py:111:            (jsonnet_function_token, Name.Function, "function_params"),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/jsonnet.py:112:            (jsonnet_token, Name.Variable),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/jsonnet.py:127:            (jsonnet_token, Name.Variable),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/jsonnet.py:146:            (rf"(?={jsonnet_token})", Text, "field_name"),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/jsonnet.py:153:            (jsonnet_function_token, Name.Function, ("field_separator", "function_params")),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/jsonnet.py:154:            (jsonnet_token, Name.Variable, "field_separator"),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/jsonnet.py:182:            (jsonnet_token, Name.Variable, ("#pop", "object_local_value")),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/hdl.py:14:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/hdl.py:44:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/hdl.py:368:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/hdl.py:954:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/j.py:12:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/j.py:40:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/mips.py:12:from pygments.token import Comment, Keyword, Name, String, Text, Whitespace
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/mips.py:278:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/capnproto.py:12:from pygments.token import Comment, Keyword, Literal, Name, Text, Whitespace
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/capnproto.py:28:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/qvt.py:12:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/qvt.py:35:    Notable tokens assignments:
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/qvt.py:55:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/mime.py:15:from pygments.token import Comment, Name, Operator, Other, String, Text
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/mime.py:64:    def get_header_tokens(self, match):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/mime.py:73:            for i, t, v in self.get_tokens_unprocessed(body, ("root", field.lower())):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/mime.py:79:    def get_body_tokens(self, match):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/mime.py:91:            for i, t, v in self.get_bodypart_tokens(entire_body):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/mime.py:110:        # process tokens of each body part
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/mime.py:116:            for i, t, v in self.get_bodypart_tokens(part):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/mime.py:128:    def get_bodypart_tokens(self, text):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/mime.py:153:        return lexer.get_tokens_unprocessed(text)
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/mime.py:164:    def get_content_type_subtokens(self, match):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/mime.py:183:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/mime.py:185:            (r"^([\w-]+):( *)([\s\S]*?\n)(?![ \t])", get_header_tokens),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/mime.py:186:            (r"^$[\s\S]+", get_body_tokens),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/mime.py:202:                get_content_type_subtokens,
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/asm.py:24:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/asm.py:74:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/asm.py:146:def _objdump_lexer_tokens(asm_lexer):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/asm.py:148:    Common objdump lexer tokens to wrap an ASM lexer.
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/asm.py:240:    tokens = _objdump_lexer_tokens(GasLexer)
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/asm.py:333:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/asm.py:652:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/asm.py:996:                        "token",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/asm.py:1090:                        "token",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/asm.py:1114:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/asm.py:1287:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/asm.py:1295:            # Consume everything else in one token for efficiency
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/asm.py:1413:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/asm.py:1479:    tokens = _objdump_lexer_tokens(NasmLexer)
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/asm.py:1520:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/asm.py:1589:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/asm.py:1686:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/xorg.py:12:from pygments.token import Comment, Name, String, Text
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/xorg.py:27:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/elpi.py:12:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/elpi.py:53:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/q.py:12:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/q.py:38:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/q.py:225:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:28:from pygments.token import Token
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:33:HEADING = Token.Generic.Heading
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:34:SETTING = Token.Keyword.Namespace
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:35:IMPORT = Token.Name.Namespace
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:36:TC_KW_NAME = Token.Generic.Subheading
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:37:KEYWORD = Token.Name.Function
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:38:ARGUMENT = Token.String
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:39:VARIABLE = Token.Name.Variable
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:40:COMMENT = Token.Comment
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:41:SEPARATOR = Token.Punctuation
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:42:SYNTAX = Token.Punctuation
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:43:GHERKIN = Token.Generic.Emph
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:44:ERROR = Token.Error
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:74:    def get_tokens_unprocessed(self, text):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:75:        row_tokenizer = RowTokenizer()
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:76:        var_tokenizer = VariableTokenizer()
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:79:            for value, token in row_tokenizer.tokenize(row):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:80:                for value, token in var_tokenizer.tokenize(value, token):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:82:                        yield index, token, str(value)
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:86:class VariableTokenizer:
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:88:    def tokenize(self, string, token):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:90:        if var.start < 0 or token in (COMMENT, ERROR):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:91:            yield string, token
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:93:        for value, token in self._tokenize(var, string, token):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:95:                yield value, token
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:97:    def _tokenize(self, var, string, orig_token):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:99:        yield before, orig_token
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:101:        yield from self.tokenize(var.base, VARIABLE)
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:105:            yield from self.tokenize(var.index, VARIABLE)
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:107:        yield from self.tokenize(string[var.end :], orig_token)
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:110:class RowTokenizer:
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:135:    def tokenize(self, row):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:146:            yield from self._tokenize(value, index, commented, separator, heading)
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:153:    def _tokenize(self, value, index, commented, separator, heading):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:161:            yield from self._table.tokenize(value, index)
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:187:class Tokenizer:
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:188:    _tokens = None
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:193:    def tokenize(self, value):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:194:        values_and_tokens = self._tokenize(value, self._index)
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:196:        if isinstance(values_and_tokens, type(Token)):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:197:            values_and_tokens = [(value, values_and_tokens)]
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:198:        return values_and_tokens
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:200:    def _tokenize(self, value, index):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:201:        index = min(index, len(self._tokens) - 1)
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:202:        return self._tokens[index]
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:211:class Comment(Tokenizer):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:212:    _tokens = (COMMENT,)
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:215:class Setting(Tokenizer):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:216:    _tokens = (SETTING, ARGUMENT)
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:240:    _custom_tokenizer = None
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:243:        Tokenizer.__init__(self)
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:246:    def _tokenize(self, value, index):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:252:                self._custom_tokenizer = KeywordCall(support_assign=False)
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:254:                self._custom_tokenizer = ImportSetting()
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:257:        elif self._custom_tokenizer:
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:258:            return self._custom_tokenizer.tokenize(value)
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:259:        return Tokenizer._tokenize(self, value, index)
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:262:class ImportSetting(Tokenizer):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:263:    _tokens = (IMPORT, ARGUMENT)
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:271:    def _tokenize(self, value, index):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:273:            type = Setting._tokenize(self, value[1:-1], index)
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:275:        return Setting._tokenize(self, value, index)
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:283:class Variable(Tokenizer):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:284:    _tokens = (SYNTAX, ARGUMENT)
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:286:    def _tokenize(self, value, index):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:289:        return Tokenizer._tokenize(self, value, index)
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:292:class KeywordCall(Tokenizer):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:293:    _tokens = (KEYWORD, ARGUMENT)
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:296:        Tokenizer.__init__(self)
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:300:    def _tokenize(self, value, index):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:303:            return SYNTAX  # VariableTokenizer tokenizes this later.
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:305:            return Tokenizer._tokenize(self, value, index - self._assigns)
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:307:        return GherkinTokenizer().tokenize(value, KEYWORD)
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:310:class GherkinTokenizer:
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:313:    def tokenize(self, value, token):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:316:            return [(value, token)]
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:318:        return [(value[:end], GHERKIN), (value[end:], token)]
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:321:class TemplatedKeywordCall(Tokenizer):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:322:    _tokens = (ARGUMENT,)
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:325:class ForLoop(Tokenizer):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:328:        Tokenizer.__init__(self)
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:331:    def _tokenize(self, value, index):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:332:        token = self._in_arguments and ARGUMENT or SYNTAX
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:335:        return token
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:339:    _tokenizer_class = None
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:341:    def __init__(self, prev_tokenizer=None):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:342:        self._tokenizer = self._tokenizer_class()
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:343:        self._prev_tokenizer = prev_tokenizer
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:346:    def tokenize(self, value, index):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:348:            self._tokenizer = self._prev_tokenizer
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:351:            yield from self._tokenize(value, index)
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:360:    def _tokenize(self, value, index):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:361:        return self._tokenizer.tokenize(value)
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:364:        self.__init__(prev_tokenizer=self._tokenizer)
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:368:    _tokenizer_class = Comment
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:375:    _tokenizer_class = Variable
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:379:    _tokenizer_class = Setting
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:381:    def __init__(self, template_setter, prev_tokenizer=None):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:382:        _Table.__init__(self, prev_tokenizer)
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:385:    def _tokenize(self, value, index):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:387:            self._tokenizer = Setting(self._template_setter)
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:388:        return _Table._tokenize(self, value, index)
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:391:        self.__init__(self._template_setter, prev_tokenizer=self._tokenizer)
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:400:    def _tokenizer_class(self):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:408:    def _tokenize(self, value, index):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:412:            return GherkinTokenizer().tokenize(value, TC_KW_NAME)
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:416:                self._tokenizer = self._setting_class(self.set_test_template)
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:418:                self._tokenizer = self._setting_class()
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:420:            self._tokenizer = ForLoop()
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:423:        return _Table._tokenize(self, value, index)
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/robotframework.py:445:    _tokenizer_class = KeywordCall
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/prql.py:12:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/prql.py:70:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/apdlexer.py:14:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/apdlexer.py:2161:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/ptx.py:12:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/ptx.py:45:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:14:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:53:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:567:                        "tokens",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:660:    def get_tokens_unprocessed(self, text):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:662:        # If the token two tokens after 'in' is ')', 'in' is a keyword:
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:667:        objectloop_token_count = -1
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:668:        previous_token = None
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:669:        for index, token, value in RegexLexer.get_tokens_unprocessed(self, text):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:670:            if previous_token is Name.Variable and value == "in":
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:671:                objectloop_queue = [[index, token, value]]
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:672:                objectloop_token_count = 2
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:673:            elif objectloop_token_count > 0:
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:674:                if token not in Comment and token not in Text:
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:675:                    objectloop_token_count -= 1
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:676:                objectloop_queue.append((index, token, value))
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:678:                if objectloop_token_count == 0:
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:683:                    objectloop_token_count = -1
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:684:                yield index, token, value
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:685:            if token not in Comment and token not in Text:
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:686:                previous_token = token
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:721:    # and use options, tokens in braces are treated as I7. Use options
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:723:    tokens = {}
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:724:    token_variants = ["+i6t-not-inline", "+i6t-inline", "+i6t-use-option"]
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:726:    for level in token_variants:
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:727:        tokens[level] = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:728:            "+i6-root": list(Inform6Lexer.tokens["root"]),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:906:        for token in Inform6Lexer.tokens:
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:907:            if token == "root":
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:909:            tokens[level][token] = list(Inform6Lexer.tokens[token])
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:910:            if not token.startswith("_"):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:911:                tokens[level][token][:0] = [include("+i6t"), include(level)]
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:915:        if level not in self._all_tokens:
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:916:            self._tokens = self.__class__.process_tokendef(level)
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:918:            self._tokens = self._all_tokens[level]
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:932:    def get_tokens_unprocessed(self, text, stack=("+i6t-root",)):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:933:        return Inform7Lexer.get_tokens_unprocessed(self, text, stack)
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:964:        token = String.Double if double else String.Single
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:971:                (rf"{char}{{3,}}", token, "#pop"),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:973:                (char, token),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:976:            state.append((char, token, "#pop"))
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:977:        state += [include("s/verbatim"), (rf"[^\\<&{{}}{char}]+", token)]
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:1017:            (r"[\\&{}<]", token),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:1025:        token = String.Double if double else String.Single
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:1028:            (rf"{char}{quantifier}", token, "#pop:2"),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:1046:        token = (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:1053:        host_token = String.Double if host_double else String.Single
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:1056:            (rf"{host_char}{host_quantifier}", host_token, "#pop:3"),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:1057:            (r"{}{}".format(r"" if token is String.Other else r"\\?", terminator), token, "#pop"),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:1064:            (r'([^\s"\'<%s{}\\&])+' % (r">" if token is String.Other else r""), token),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:1066:            (r'["\'\s&{<}\\]', token),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:1069:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:1185:            # Two-token keywords
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:1339:            (r"token\b", Keyword, ("#pop", "constants")),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:1513:    def get_tokens_unprocessed(self, text, **kwargs):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:1516:        for index, token, value in RegexLexer.get_tokens_unprocessed(self, text, **kwargs):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:1518:                if token is Comment.Preproc and re.match(
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:1523:                if token is Comment.Preproc:
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:1531:                    token = Comment
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/int_fiction.py:1532:            yield index, token, value
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/diff.py:14:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/diff.py:40:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/diff.py:80:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/diff.py:168:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/webmisc.py:25:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/webmisc.py:55:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/webmisc.py:75:    An XQuery lexer, parsing a stream and outputting the tokens needed to
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/webmisc.py:335:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/webmisc.py:884:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/webmisc.py:963:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/webmisc.py:1012:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_php_builtins.py:1743:        "oci_password_change",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_php_builtins.py:1896:        "openssl_x509_check_private_key",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_php_builtins.py:2137:    "Password Hashing": (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_php_builtins.py:2138:        "password_algos",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_php_builtins.py:2139:        "password_get_info",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_php_builtins.py:2140:        "password_hash",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_php_builtins.py:2141:        "password_needs_rehash",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_php_builtins.py:2142:        "password_verify",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_php_builtins.py:2312:        "radius_server_secret",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_php_builtins.py:2409:        "ssh2_auth_password",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_php_builtins.py:2611:        "sodium_crypto_box_keypair_from_secretkey_and_publickey",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_php_builtins.py:2614:        "sodium_crypto_box_publickey_from_secretkey",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_php_builtins.py:2618:        "sodium_crypto_box_secretkey",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_php_builtins.py:2631:        "sodium_crypto_kx_secretkey",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_php_builtins.py:2643:        "sodium_crypto_secretbox_keygen",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_php_builtins.py:2644:        "sodium_crypto_secretbox_open",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_php_builtins.py:2645:        "sodium_crypto_secretbox",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_php_builtins.py:2646:        "sodium_crypto_secretstream_xchacha20poly1305_init_pull",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_php_builtins.py:2647:        "sodium_crypto_secretstream_xchacha20poly1305_init_push",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_php_builtins.py:2648:        "sodium_crypto_secretstream_xchacha20poly1305_keygen",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_php_builtins.py:2649:        "sodium_crypto_secretstream_xchacha20poly1305_pull",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_php_builtins.py:2650:        "sodium_crypto_secretstream_xchacha20poly1305_push",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_php_builtins.py:2651:        "sodium_crypto_secretstream_xchacha20poly1305_rekey",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_php_builtins.py:2657:        "sodium_crypto_sign_keypair_from_secretkey_and_publickey",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_php_builtins.py:2660:        "sodium_crypto_sign_publickey_from_secretkey",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_php_builtins.py:2662:        "sodium_crypto_sign_secretkey",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_php_builtins.py:2869:    "Tokenizer": ("token_get_all", "token_name"),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/maxima.py:16:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/maxima.py:96:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/graphviz.py:12:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/graphviz.py:37:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/tls.py:14:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/tls.py:42:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/tls.py:58:            # tokens
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/graph.py:14:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/graph.py:43:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/haxe.py:14:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/haxe.py:97:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/haxe.py:258:        # same as 'ident' but set token as Name.Decorator instead of Name
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/haxe.py:834:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/gsql.py:14:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/gsql.py:41:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/igor.py:14:from pygments.token import Comment, Keyword, Name, String, Text, Whitespace
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/igor.py:1618:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/floscript.py:12:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/floscript.py:53:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/rebol.py:14:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/rebol.py:136:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/rebol.py:349:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/ncl.py:14:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/ncl.py:42:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/templates.py:33:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/templates.py:44:    Token,
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/templates.py:141:    def get_tokens_unprocessed(self, text):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/templates.py:147:        tokens = self._block_re.split(text)
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/templates.py:148:        tokens.reverse()
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/templates.py:154:                    val = tokens.pop()
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/templates.py:160:                    tag = tokens.pop()
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/templates.py:169:                        val = tokens.pop()
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/templates.py:177:                        data = tokens.pop()
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/templates.py:179:                        for r_idx, r_token, r_value in self.ruby_lexer.get_tokens_unprocessed(data):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/templates.py:180:                            yield r_idx + idx, r_token, r_value
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/templates.py:191:                        for r_idx, r_token, r_value in self.ruby_lexer.get_tokens_unprocessed(
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/templates.py:194:                            yield idx + 1 + r_idx, r_token, r_value
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/templates.py:199:                    tag = tokens.pop()
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/templates.py:231:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/templates.py:292:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/templates.py:412:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/templates.py:508:    markup is yielded as `Token.Other`.
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/templates.py:518:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/templates.py:639:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/templates.py:691:    markup is yielded as `Token.Other`.
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/templates.py:701:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/templates.py:827:    Lexer for handling Cheetah's special $ tokens in Python syntax.
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/templates.py:830:    def get_tokens_unprocessed(self, text):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/templates.py:832:        for pos, type_, value in pylexer.get_tokens_unprocessed(text):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/templates.py:833:            if type_ == Token.Error and value == "$":
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/templates.py:841:    markup is yielded as `Token.Other`.  This also works for
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/templates.py:854:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/templates.py:952:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/templates.py:989:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/templates.py:1511:    Base for the `JspLexer`. Yields `Token.Other` for area outside of
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/templates.py:1517:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/templates.py:1571:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/templates.py:1685:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/templates.py:1738:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/templates.py:1838:    Base for the `TeaTemplateLexer`. Yields `Token.Other` for area outside of
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/templates.py:1844:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/templates.py:2004:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/templates.py:2103:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/templates.py:2304:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/templates.py:2420:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/eiffel.py:12:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/eiffel.py:38:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/ldap.py:14:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/ldap.py:42:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/ldap.py:164:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_mysql_builtins.py:479:    "validate_password_strength",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_mysql_builtins.py:840:    "master_password",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_mysql_builtins.py:937:    "password",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_mysql_builtins.py:938:    "password_lock_time",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_mysql_builtins.py:1082:    "source_password",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/pddl.py:12:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/pddl.py:39:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/typst.py:12:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/typst.py:93:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/typst.py:236:    def get_tokens_unprocessed(self, text):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/typst.py:241:        yield from RegexLexer.get_tokens_unprocessed(self, text, stack)
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/csound.py:22:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/csound.py:43:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/csound.py:145:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/csound.py:204:        type_annotation_token = Keyword.Type
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/csound.py:212:            type_annotation_token = Name
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/csound.py:222:            yield match.start(3), type_annotation_token, match.group(3)
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/csound.py:224:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/csound.py:409:    # These tokens are based on those in XmlLexer in pygments/lexers/html.py. Making
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/csound.py:416:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/dotnet.py:25:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/dotnet.py:107:    tokens = {}
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/dotnet.py:108:    token_variants = True
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/dotnet.py:111:        tokens[levelname] = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/dotnet.py:339:        level = get_choice_opt(options, "unicodelevel", list(self.tokens), "basic")
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/dotnet.py:340:        if level not in self._all_tokens:
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/dotnet.py:342:            self._tokens = self.__class__.process_tokendef(level)
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/dotnet.py:344:            self._tokens = self._all_tokens[level]
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/dotnet.py:402:    tokens = {}
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/dotnet.py:403:    token_variants = True
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/dotnet.py:406:        tokens[levelname] = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/dotnet.py:512:        level = get_choice_opt(options, "unicodelevel", list(self.tokens), "basic")
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/dotnet.py:513:        if level not in self._all_tokens:
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/dotnet.py:515:            self._tokens = self.__class__.process_tokendef(level)
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/dotnet.py:517:            self._tokens = self._all_tokens[level]
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/dotnet.py:544:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/dotnet.py:624:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/dotnet.py:838:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/dotnet.py:1084:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/dotnet.py:1559:    tokens = {}
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/dotnet.py:1561:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/futhark.py:13:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/futhark.py:106:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/comal.py:14:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/comal.py:47:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_openedge_builtins.py:76:    "APPSERVER-PASSWORD",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_openedge_builtins.py:1780:    "PASSWORD-FIELD",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_openedge_builtins.py:1892:    "PROXY-PASSWORD",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_openedge_builtins.py:2434:    "URL-PASSWORD",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/archetype.py:17:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/archetype.py:41:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/archetype.py:171:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/archetype.py:220:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/archetype.py:294:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/ride.py:12:from pygments.token import Comment, Keyword, Name, Number, Punctuation, String, Text
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/ride.py:225:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/cplint.py:13:from pygments.token import Keyword, Name, Operator, Punctuation, String
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/cplint.py:31:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/bare.py:12:from pygments.token import Comment, Keyword, Literal, Name, Text, Whitespace
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/bare.py:51:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/usd.py:19:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/usd.py:54:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/usd.py:59:                    Keyword.Token,
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/usd.py:61:                    Keyword.Token,
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/usd.py:67:                    Name.Keyword.Tokens,
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/usd.py:75:                    Keyword.Token,
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/usd.py:81:                    Name.Keyword.Tokens,
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/usd.py:89:                    Keyword.Token,
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/usd.py:95:                    Name.Keyword.Tokens,
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/usd.py:107:                    Name.Keyword.Tokens,
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/usd.py:113:        + _keywords(KEYWORDS, Keyword.Tokens)
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/rnc.py:12:from pygments.token import Comment, Keyword, Name, Operator, Punctuation, String, Text
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/rnc.py:28:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/vip.py:14:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/vip.py:54:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/vip.py:187:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/vip.py:230:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/berry.py:12:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/berry.py:40:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/snobol.py:12:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/snobol.py:41:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/webassembly.py:16:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/webassembly.py:239:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/inferno.py:14:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/inferno.py:44:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/shell.py:25:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/shell.py:84:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/shell.py:188:    def get_tokens_unprocessed(self, text):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/shell.py:189:        for index, token, value in BashLexer.get_tokens_unprocessed(self, text):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/shell.py:190:            if token is Text and value in self.EXTRA_KEYWORDS:
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/shell.py:192:            elif token is Comment.Single and "SBATCH" in value:
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/shell.py:195:                yield index, token, value
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/shell.py:208:    def get_tokens_unprocessed(self, text):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/shell.py:251:                    toks = innerlexer.get_tokens_unprocessed(curcode)
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/shell.py:258:            for i, t, v in do_insertions(insertions, innerlexer.get_tokens_unprocessed(curcode)):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/shell.py:304:    _token_terminator = rf"(?=\^?[{_ws}]|[{_punct}{_nl}])"
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/shell.py:308:    _number = rf"(?:-?(?:0[0-7]+|0x[\da-f]+|\d+){_token_terminator})"
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/shell.py:318:    _core_token = rf'(?:(?:(?:\^[{_nl}]?)?[^"{_nlws}{_punct}])+)'
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/shell.py:319:    _core_token_compound = rf'(?:(?:(?:\^[{_nl}]?)?[^"{_nlws}{_punct})])+)'
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/shell.py:320:    _token = rf"(?:[{_punct}]+|{_core_token})"
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/shell.py:321:    _token_compound = rf"(?:[{_punct}]+|{_core_token_compound})"
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/shell.py:322:    _stoken = rf"(?:[{_punct}]+|(?:{_string}|{_variable}|{_core_token})+)"
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/shell.py:326:        _core_token=_core_token,
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/shell.py:327:        _core_token_compound=_core_token_compound,
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/shell.py:334:        _stoken=_stoken,
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/shell.py:335:        _token_terminator=_token_terminator,
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/shell.py:348:            _token_terminator = rf"(?:(?=\))|{_token_terminator})"
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/shell.py:354:                else (rf"\)((?=\()|{_token_terminator}){rest_of_line}", Comment.Single)
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/shell.py:364:                rf"(?<=m))(?:(?=\()|{_token_terminator})))({_space}?{_core_token_compound if compound else _core_token}?(?:\^[{_nl}]?)?/(?:\^[{_nl}]?)?\?)",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/shell.py:428:                rf"(for{_token_terminator}(?!\^))({_space})(/f{_token_terminator})",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/shell.py:433:                rf"(for{_token_terminator}(?!\^))({_space})(/l{_token_terminator})",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/shell.py:437:            (rf"for{_token_terminator}(?!\^)", Keyword, ("for2", "for")),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/shell.py:444:                rf"(if(?:(?=\()|{_token_terminator})(?!\^))({_space}?)((?:/i{_token_terminator})?)({_space}?)((?:not{_token_terminator})?)({_space}?)",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/shell.py:456:                rf"rem(((?=\()|{_token_terminator}){_space}?{_stoken}?.*|{_keyword_terminator}{rest_of_line_compound if compound else rest_of_line})",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/shell.py:498:        _token=_token,
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/shell.py:499:        _token_compound=_token_compound,
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/shell.py:594:        _core_token_compound=_core_token_compound,
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/shell.py:597:        _stoken=_stoken,
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/shell.py:603:        stoken_compound = rf"(?:[{_punct}]+|(?:{_string}|{_variable}|{_core_token_compound})+)"
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/shell.py:610:                rf"((?:(?<=[{_nlws}])(?<!\^[{_nl}])\d)?)(>>?|<)({_space}?{stoken_compound if compound else _stoken})",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/shell.py:615:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/shell.py:664:                rf"({_space})(do{_token_terminator})",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/shell.py:690:                rf"((?:cmdextversion|errorlevel){_token_terminator})({_space})(\d+)",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/shell.py:695:                rf"(defined{_token_terminator})({_space})({_stoken})",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/shell.py:700:                rf"(exist{_token_terminator})({_space}{_stoken})",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/shell.py:711:            (_stoken, using(this, state="text"), ("#pop", "if2")),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/shell.py:715:                rf"({_space}?)(==)({_space}?{_stoken})",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/shell.py:720:                rf"({_space})({_opword})({_space}{_stoken})",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/shell.py:732:            (rf"else{_token_terminator}", Keyword, "#pop"),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/shell.py:769:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/shell.py:914:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/shell.py:1001:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/shell.py:1077:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/arrow.py:12:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/arrow.py:41:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/supercollider.py:14:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/supercollider.py:41:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/qlik.py:20:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/qlik.py:47:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/praat.py:12:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/praat.py:421:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/sophia.py:14:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/sophia.py:90:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lean.py:14:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lean.py:47:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/lean.py:396:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/tablegen.py:13:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/tablegen.py:127:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/tablegen.py:149:            # numbers, and we want to parse 1X as one name token as opposed to
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/erlang.py:23:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/erlang.py:235:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/erlang.py:315:    def get_tokens_unprocessed(self, text):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/erlang.py:329:                    yield from do_insertions(insertions, erlexer.get_tokens_unprocessed(curcode))
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/erlang.py:337:            yield from do_insertions(insertions, erlexer.get_tokens_unprocessed(curcode))
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/erlang.py:340:def gen_elixir_string_rules(name, symbol, token):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/erlang.py:343:        (rf"[^#{symbol}\\]+", token),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/erlang.py:345:        (r"\\.", token),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/erlang.py:346:        (rf"({symbol})", bygroups(token), "#pop"),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/erlang.py:352:def gen_elixir_sigstr_rules(term, term_class, token, interpol=True):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/erlang.py:355:            (rf"[^#{term_class}\\]+", token),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/erlang.py:357:            (r"\\.", token),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/erlang.py:358:            (rf"{term}[a-zA-Z]*", token, "#pop"),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/erlang.py:363:            (rf"[^{term_class}\\]+", token),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/erlang.py:364:            (r"\\.", token),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/erlang.py:365:            (rf"{term}[a-zA-Z]*", token, "#pop"),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/erlang.py:467:    def get_tokens_unprocessed(self, text):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/erlang.py:468:        for index, token, value in RegexLexer.get_tokens_unprocessed(self, text):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/erlang.py:469:            if token is Name:
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/erlang.py:485:                    yield index, token, value
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/erlang.py:487:                yield index, token, value
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/erlang.py:505:        token = String.Other
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/erlang.py:512:                    bygroups(token, String.Heredoc),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/erlang.py:517:                    bygroups(token, String.Heredoc),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/erlang.py:523:                (r"[a-zA-Z]+", token, "#pop"),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/erlang.py:537:                (r"~[a-z]" + lterm, token, name + "-intp"),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/erlang.py:538:                (r"~[A-Z]" + lterm, token, name + "-no-intp"),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/erlang.py:540:            states[name + "-intp"] = gen_elixir_sigstr_rules(rterm, rterm_class, token)
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/erlang.py:542:                rterm, rterm_class, token, interpol=False
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/erlang.py:562:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/erlang.py:658:    tokens.update(gen_elixir_string_rules("double", '"', String.Double))
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/erlang.py:659:    tokens.update(gen_elixir_string_rules("single", "'", String.Single))
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/erlang.py:660:    tokens.update(gen_elixir_string_rules("double_atom", '"', String.Symbol))
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/erlang.py:661:    tokens.update(gen_elixir_string_rules("single_atom", "'", String.Symbol))
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/erlang.py:662:    tokens.update(gen_elixir_sigil_rules())
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/erlang.py:691:    def get_tokens_unprocessed(self, text):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/erlang.py:713:                            insertions, exlexer.get_tokens_unprocessed(curcode)
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/erlang.py:717:                    token = Generic.Error if in_error else Generic.Output
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/erlang.py:718:                    yield match.start(), token, line
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/erlang.py:720:            yield from do_insertions(insertions, exlexer.get_tokens_unprocessed(curcode))
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/jvm.py:25:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/jvm.py:72:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/jvm.py:209:    def get_tokens_unprocessed(self, text):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/jvm.py:210:        for index, token, value in JavaLexer.get_tokens_unprocessed(self, text):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/jvm.py:211:            if token is Name and value in self.aj_keywords:
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/jvm.py:213:            elif token is Name.Label and value in self.aj_inter_type:
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/jvm.py:216:            elif token is Name.Decorator and value in self.aj_inter_type_annotation:
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/jvm.py:219:                yield index, token, value
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/jvm.py:319:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/jvm.py:589:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/jvm.py:667:    def get_tokens_unprocessed(self, text):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/jvm.py:670:        yield from lexer.get_tokens_unprocessed(text, stack)
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/jvm.py:687:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/jvm.py:763:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/jvm.py:1285:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/jvm.py:1353:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/jvm.py:1404:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/jvm.py:1508:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/jvm.py:1679:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/jvm.py:1741:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/jvm.py:1780:                r"TOKENIZE)\b",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/jvm.py:1813:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/jvm.py:1908:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/jvm.py:2382:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/freefem.py:12:from pygments.token import Comment, Keyword, Name, Operator
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/freefem.py:935:    def get_tokens_unprocessed(self, text, stack=("root",)):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/freefem.py:936:        for index, token, value in CppLexer.get_tokens_unprocessed(self, text, stack):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/freefem.py:954:                yield index, token, value
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/ml.py:14:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/ml.py:123:    # Callbacks for distinguishing tokens and reserved words
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/ml.py:126:            token = Error
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/ml.py:128:            token = Name.Namespace
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/ml.py:129:        yield match.start(1), token, match.group(1)
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/ml.py:134:            token = Error
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/ml.py:136:            token = Error
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/ml.py:138:            token = Name
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/ml.py:139:        yield match.start(1), token, match.group(1)
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/ml.py:144:            token = Keyword.Reserved
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/ml.py:146:            token = Punctuation
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/ml.py:148:            token = Name
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/ml.py:149:        yield match.start(1), token, str
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/ml.py:151:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/ml.py:464:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/ml.py:569:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/ml.py:948:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/ml.py:1125:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/minecraft.py:23:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/minecraft.py:49:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/minecraft.py:119:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/minecraft.py:314:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/smalltalk.py:12:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/smalltalk.py:40:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/smalltalk.py:179:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/jsx.py:15:from pygments.token import Name, Operator, Punctuation, String, Text, Whitespace
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/jsx.py:74:    # Use same tokens as `JavascriptLexer`, but with tags and attributes support
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/jsx.py:75:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/jsx.py:96:    # Use same tokens as `TypescriptLexer`, but with tags and attributes support
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/jsx.py:97:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/esoteric.py:12:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/esoteric.py:46:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/esoteric.py:104:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/esoteric.py:133:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/esoteric.py:266:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/esoteric.py:395:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/esoteric.py:426:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/smv.py:12:from pygments.token import Comment, Keyword, Name, Number, Operator, Punctuation, Text
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/smv.py:29:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/x10.py:12:from pygments.token import Comment, Keyword, String, Text
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/x10.py:95:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/ambient.py:14:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/ambient.py:64:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/thingsdb.py:12:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/thingsdb.py:38:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/thingsdb.py:107:                r"collections_info|del_collection|del_expired|del_node|del_token|"
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/thingsdb.py:108:                r"del_user|grant|has_collection|has_node|has_token|has_user|"
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/thingsdb.py:109:                r"new_collection|new_node|new_token|new_user|rename_collection|"
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/thingsdb.py:110:                r"rename_user|restore|revoke|set_password|set_time_zone|"
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/oberon.py:14:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/oberon.py:42:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/rdf.py:14:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/rdf.py:116:    # Lexer token definitions ::
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/rdf.py:118:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/rdf.py:282:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/rdf.py:451:    # Lexer token definitions ::
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/rdf.py:453:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/phix.py:14:from pygments.token import Comment, Keyword, Name, Operator, String, Text, Whitespace
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/phix.py:1160:        "CURLOPT_PASSWORD",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/phix.py:1395:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/d.py:12:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/d.py:37:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/d.py:246:            # -- TokenString
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/d.py:247:            (r"q\{", String, "token_string"),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/d.py:250:            # Tokens
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/d.py:271:        "token_string": [
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/d.py:272:            (r"\{", Punctuation, "token_string_nest"),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/d.py:276:        "token_string_nest": [
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/d.py:336:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/d.py:408:            # Tokens
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/varnish.py:12:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/varnish.py:51:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/varnish.py:272:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/asn1.py:14:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/asn1.py:121:def word_sequences(tokens):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/asn1.py:122:    return "(" + "|".join(token.replace(" ", r"\s+") for token in tokens) + r")\b"
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/asn1.py:138:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/pascal.py:18:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/pascal.py:48:    def get_tokens_unprocessed(self, text):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/pascal.py:49:        return self.lexer.get_tokens_unprocessed(text)
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/pascal.py:1298:    def get_tokens_unprocessed(self, text):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/pascal.py:1304:        next_token_is_function = False
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/pascal.py:1305:        next_token_is_property = False
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/pascal.py:1311:            token = Error
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/pascal.py:1315:                    token = Whitespace
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/pascal.py:1318:                        token = Comment.Preproc
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/pascal.py:1320:                        token = Comment.Multiline
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/pascal.py:1322:                    token = Comment.Single
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/pascal.py:1326:                    token = Operator
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/pascal.py:1328:                    token = Operator
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/pascal.py:1333:                    token = Punctuation
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/pascal.py:1335:                    next_token_is_function = False
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/pascal.py:1351:                        token = Name.Builtin.Pseudo
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/pascal.py:1353:                        token = Keyword
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/pascal.py:1360:                                next_token_is_function = True
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/pascal.py:1378:                                next_token_is_property = True
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/pascal.py:1387:                                next_token_is_function = True
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/pascal.py:1396:                        token = Keyword.Pseudo
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/pascal.py:1404:                        token = Keyword.Pseudo
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/pascal.py:1405:                        next_token_is_function = True
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/pascal.py:1406:                    # if the last iteration set next_token_is_function
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/pascal.py:1409:                    elif next_token_is_function:
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/pascal.py:1410:                        # Look if the next token is a dot. If yes it's
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/pascal.py:1414:                            token = Name.Class
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/pascal.py:1417:                            token = Name.Function
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/pascal.py:1418:                            next_token_is_function = False
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/pascal.py:1424:                    elif not self.is_portugol and next_token_is_property:
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/pascal.py:1425:                        token = Name.Property
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/pascal.py:1426:                        next_token_is_property = False
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/pascal.py:1427:                    # Highlight this token as label and add it
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/pascal.py:1430:                        token = Name.Label
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/pascal.py:1434:                        token = Name.Label
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/pascal.py:1436:                        token = Keyword.Type
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/pascal.py:1438:                        token = Keyword.Type
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/pascal.py:1440:                        token = Keyword.Pseudo
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/pascal.py:1441:                    # builtins are just builtins if the token
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/pascal.py:1444:                        token = Name.Builtin
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/pascal.py:1446:                        token = Name
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/pascal.py:1448:                    token = String
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/pascal.py:1451:                    token = String
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/pascal.py:1454:                    token = String.Char
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/pascal.py:1456:                    token = Number.Hex
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/pascal.py:1458:                    token = Number.Integer
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/pascal.py:1460:                    token = Number.Float
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/pascal.py:1470:                        token = String.Escape
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/pascal.py:1472:                        token = String
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/pascal.py:1475:                        token = String
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/pascal.py:1481:                        token = String.Escape
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/pascal.py:1483:                        token = String
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/pascal.py:1486:                        token = String
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/pascal.py:1492:                    token = Whitespace
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/pascal.py:1494:                    token = Keyword
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/pascal.py:1498:                        token = Comment.Preproc
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/pascal.py:1500:                        token = Comment.Multiline
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/pascal.py:1502:                    token = Comment.Single
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/pascal.py:1504:                    token = String
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/pascal.py:1507:                    token = Name.Label
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/pascal.py:1511:                        token = Keyword
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/pascal.py:1513:                        token = Name.Builtin
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/pascal.py:1515:                        token = Name
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/pascal.py:1517:                    token = Operator
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/pascal.py:1519:                    token = Punctuation
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/pascal.py:1521:                    token = Number.Hex
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/pascal.py:1523:                    token = Number.Integer
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/pascal.py:1525:                    token = Number.Float
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/pascal.py:1534:            yield scanner.start_pos, token, scanner.match or ""
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/graphics.py:12:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/graphics.py:46:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/graphics.py:343:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/graphics.py:832:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/graphics.py:1015:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/graphics.py:1118:    def get_tokens_unprocessed(self, text):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/graphics.py:1121:        for index, token, value in RegexLexer.get_tokens_unprocessed(self, text):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/graphics.py:1122:            if token is Name and value in ASYFUNCNAME:
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/graphics.py:1123:                token = Name.Function
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/graphics.py:1124:            elif token is Name and value in ASYVARNAME:
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/graphics.py:1125:                token = Name.Variable
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/graphics.py:1126:            yield index, token, value
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/graphics.py:1150:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/graphics.py:1210:            # don't add the newline to the Comment token
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/graphics.py:1474:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/savi.py:12:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/savi.py:56:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/rita.py:12:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/rita.py:37:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/bibtex.py:14:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/bibtex.py:64:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/bibtex.py:136:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/mojo.py:24:from pygments.token import (  # Error,
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/mojo.py:98:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/ul4.py:16:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/ul4.py:51:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/nix.py:14:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/nix.py:77:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_lasso_builtins.py:29:        "curltoken",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_lasso_builtins.py:433:        "client_password",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_lasso_builtins.py:502:        "curle_bad_password_entered",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_lasso_builtins.py:520:        "curle_ftp_user_password_incorrect",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_lasso_builtins.py:760:        "email_token",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_lasso_builtins.py:1093:        "json_consume_token",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_lasso_builtins.py:1604:        "token_value",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_lasso_builtins.py:2079:        "client_password",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_lasso_builtins.py:2232:        "email_token",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_lasso_builtins.py:2298:        "error_code_invalidpassword",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_lasso_builtins.py:2339:        "error_invalidpassword",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_lasso_builtins.py:2380:        "error_msg_invalidpassword",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_lasso_builtins.py:2981:        "token_value",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_lasso_builtins.py:3149:        "addpasswordfield",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_lasso_builtins.py:3568:        "encodepassword",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_lasso_builtins.py:3949:        "hostpassword",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_lasso_builtins.py:4363:        "pop_token",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_lasso_builtins.py:4780:        "token",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_lasso_builtins.py:4891:        "addpasswordfield",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/resource.py:14:from pygments.token import Comment, Keyword, Name, Number, Operator, String, Text
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/resource.py:31:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/func.py:12:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/func.py:44:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_googlesql_builtins.py:914:    "TOKENLIST",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/yara.py:12:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/yara.py:39:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/graphql.py:16:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/graphql.py:74:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/graphql.py:75:        "ignored_tokens": [
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/graphql.py:81:            include("ignored_tokens"),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/graphql.py:96:            include("ignored_tokens"),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/graphql.py:101:            include("ignored_tokens"),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/graphql.py:112:            include("ignored_tokens"),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/graphql.py:119:            include("ignored_tokens"),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/graphql.py:125:            include("ignored_tokens"),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/graphql.py:133:            include("ignored_tokens"),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/graphql.py:139:            include("ignored_tokens"),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/graphql.py:154:            include("ignored_tokens"),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/graphql.py:158:            include("ignored_tokens"),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/graphql.py:165:            include("ignored_tokens"),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/graphql.py:173:            include("ignored_tokens"),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/graphql.py:178:            include("ignored_tokens"),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/chapel.py:12:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/chapel.py:131:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/chapel.py:172:            # tokens
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/trafficscript.py:12:from pygments.token import Comment, Keyword, Name, Number, Operator, String, Text
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/trafficscript.py:28:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/blueprint.py:14:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/blueprint.py:41:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_scilab_builtins.py:953:    "tokens",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_scilab_builtins.py:2891:    "tokenpos",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/bqn.py:12:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/bqn.py:42:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/bqn.py:65:            # This token type is used for diamond, commas
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/bqn.py:71:            # Since this token type is important in BQN, it is not included in
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/bqn.py:72:            # the punctuation token type but rather in the following one
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/openscad.py:12:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/openscad.py:38:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/pawn.py:12:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/pawn.py:44:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/pawn.py:190:    def get_tokens_unprocessed(self, text):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/pawn.py:191:        for index, token, value in RegexLexer.get_tokens_unprocessed(self, text):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/pawn.py:192:            if token is Name:
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/pawn.py:195:                        token = Keyword.Type
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/pawn.py:197:                        token = Name.Builtin
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/pawn.py:198:            yield index, token, value
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/pawn.py:218:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/basic.py:15:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/basic.py:62:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/basic.py:254:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/basic.py:431:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/basic.py:562:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/basic.py:870:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/basic.py:971:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/basic.py:1233:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/html.py:27:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/html.py:66:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/html.py:152:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/html.py:188:            (r"CDATA|IDREFS|IDREF|ID|NMTOKENS|NMTOKEN|ENTITIES|ENTITY|NOTATION", Keyword.Constant),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/html.py:235:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/html.py:315:    def get_tokens_unprocessed(self, text):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/html.py:316:        for index, token, value in XmlLexer.get_tokens_unprocessed(self, text):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/html.py:319:            if token is Name.Tag and m and m.group(1) in self.EXTRA_KEYWORDS:
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/html.py:322:                yield index, token, value
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/html.py:350:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/html.py:452:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/html.py:553:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/html.py:651:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/html.py:671:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/perl.py:23:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/perl.py:53:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/perl.py:538:        "token",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/perl.py:1842:    def brackets_callback(token_class):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/perl.py:1894:            yield match.start(), token_class, text[match.start() : end_pos + n_chars]
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/perl.py:1906:        # below a token state, it means we need to increment
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/perl.py:1908:        # we should return to the token rules.
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/perl.py:1909:        if len(stack) > 2 and stack[-2] == "token":
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/perl.py:1910:            context.perl6_token_nesting_level += 1
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/perl.py:1919:        # below a token state, it means we need to check the nesting
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/perl.py:1920:        # level to see if we need to return to the token state.
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/perl.py:1921:        if len(stack) > 2 and stack[-2] == "token":
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/perl.py:1922:            context.perl6_token_nesting_level -= 1
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/perl.py:1923:            if context.perl6_token_nesting_level == 0:
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/perl.py:1927:        context.perl6_token_nesting_level = 1
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/perl.py:1936:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/perl.py:1949:                r"(regex|token|rule)(\s*" + PERL6_IDENTIFIER_RANGE + "+:sym)",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/perl.py:1951:                "token-sym-brackets",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/perl.py:1954:                r"(regex|token|rule)(?!"
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/perl.py:1960:                "pre-token",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/perl.py:2017:        "pre-token": [
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/perl.py:2019:            (r"\{", Text, ("#pop", "token")),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/perl.py:2022:        "token-sym-brackets": [
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/perl.py:2026:                ("#pop", "pre-token"),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/perl.py:2028:            default(("#pop", "pre-token")),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/perl.py:2030:        "token": [
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/theorem.py:17:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/theorem.py:354:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/theorem.py:410:            # Consume comments like ***** as one token
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/theorem.py:838:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/sql.py:56:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/sql.py:128:        yield from lx.get_tokens_unprocessed(match.group(4))
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/sql.py:142:    had, _tokens could be created on this ancestor and not updated for the
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/sql.py:147:    def get_tokens_unprocessed(self, text, *args):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/sql.py:150:        yield from super().get_tokens_unprocessed(text, *args)
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/sql.py:187:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/sql.py:248:    tokens = {name: state[:] for (name, state) in PostgresLexer.tokens.items()}
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/sql.py:251:    for i, pattern in enumerate(tokens["root"]):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/sql.py:253:            tokens["root"][i] = (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/sql.py:265:    tokens["root"][:0] = [
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/sql.py:285:    tokens = {name: state[:] for (name, state) in PostgresLexer.tokens.items()}
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/sql.py:287:    tokens["root"].append((r"\\[^\s]+", Keyword.Pseudo, "psql-command"))
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/sql.py:288:    tokens["psql-command"] = [
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/sql.py:345:    def get_tokens_unprocessed(self, data):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/sql.py:361:                    yield from lexer.get_tokens_unprocessed(line)
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/sql.py:379:            yield from do_insertions(insertions, sql.get_tokens_unprocessed(curcode))
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/sql.py:382:            out_token = Generic.Output
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/sql.py:393:                        out_token = Generic.Error
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/sql.py:395:                    yield (mmsg.start(2), out_token, mmsg.group(2))
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/sql.py:397:                    yield (0, out_token, line)
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/sql.py:414:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/sql.py:664:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/sql.py:708:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/sql.py:732:            # tokens starting with a digit have already been recognized
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/sql.py:793:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/sql.py:857:            # Exceptions; these words tokenize differently in different contexts.
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/sql.py:860:            # In all other known cases, "SET" is tokenized by MYSQL_DATATYPES.
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/sql.py:929:        # additional styles based on the token name. This gives users
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/sql.py:973:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/sql.py:1031:            # Exceptions; these words tokenize differently in different contexts.
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/sql.py:1097:        # additional styles based on the token name. This gives users
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/sql.py:1108:        tokens = collections.Counter(text.split())
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/sql.py:1109:        return 0.001 * sum(count for t, count in tokens.items() if t in googlesql_identifiers)
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/sql.py:1125:    def get_tokens_unprocessed(self, data):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/sql.py:1139:                    yield from do_insertions(insertions, sql.get_tokens_unprocessed(curcode))
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/sql.py:1147:            yield from do_insertions(insertions, sql.get_tokens_unprocessed(curcode))
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/sql.py:1163:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/zig.py:12:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/zig.py:148:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/procfile.py:12:from pygments.token import Name, Number, Punctuation, String, Text
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/procfile.py:31:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/felix.py:12:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/felix.py:242:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/kuin.py:12:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/kuin.py:38:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/nit.py:12:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/nit.py:36:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/testing.py:12:from pygments.token import Comment, Generic, Keyword, Name, Number, String, Text
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/testing.py:34:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/testing.py:152:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/tlb.py:12:from pygments.token import Comment, Name, Number, Operator, Punctuation, Whitespace
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/tlb.py:28:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/parasail.py:14:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/parasail.py:43:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/mosel.py:13:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/mosel.py:407:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/grammar_notation.py:12:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/grammar_notation.py:61:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/grammar_notation.py:106:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/grammar_notation.py:155:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/grammar_notation.py:251:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_postgres_builtins.py:302:    "PASSWORD",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/webidl.py:12:from pygments.token import Comment, Keyword, Name, Number, Punctuation, String, Text
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/webidl.py:65:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/installers.py:14:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/installers.py:50:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/installers.py:193:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/installers.py:264:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/installers.py:291:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/installers.py:337:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/cddl.py:16:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/cddl.py:111:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/cddl.py:138:            # Token type is String as barewords are always interpreted as such.
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/cddl.py:161:            # (r";.+$", Token.Other),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/smithy.py:12:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/smithy.py:70:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/c_like.py:16:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/c_like.py:55:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/c_like.py:156:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/c_like.py:224:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/c_like.py:280:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/c_like.py:359:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/c_like.py:607:    def get_tokens_unprocessed(self, text, stack=("root",)):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/c_like.py:608:        for index, token, value in CLexer.get_tokens_unprocessed(self, text, stack):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/c_like.py:609:            if token is Name:
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/c_like.py:611:                    token = Keyword.Type
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/c_like.py:613:                    token = Keyword.Type
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/c_like.py:615:                    token = Name.Builtin
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/c_like.py:617:                    token = Keyword.Pseudo
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/c_like.py:619:                    token = Keyword.Reserved
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/c_like.py:621:                    token = Name.Function
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/c_like.py:622:            yield index, token, value
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/c_like.py:637:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/c_like.py:769:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/c_like.py:1279:    def get_tokens_unprocessed(self, text, stack=("root",)):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/c_like.py:1280:        for index, token, value in CppLexer.get_tokens_unprocessed(self, text, stack):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/c_like.py:1292:                yield index, token, value
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/c_like.py:1307:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/c_like.py:1382:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/c_like.py:1571:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/data.py:12:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/data.py:51:    def something(token_class):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/data.py:52:        """Do not produce empty tokens."""
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/data.py:58:            yield match.start(), token_class, text
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/data.py:63:    def reset_indent(token_class):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/data.py:72:            yield match.start(), token_class, text
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/data.py:77:    def save_indent(token_class, start=False):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/data.py:94:                yield match.start(), token_class, text
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/data.py:96:                yield match.start() + len(text), token_class.Error, extra
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/data.py:101:    def set_indent(token_class, implicit=False):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/data.py:111:            yield match.start(), token_class, text
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/data.py:116:    def set_block_scalar_indent(token_class):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/data.py:130:                yield match.start(), token_class, text
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/data.py:135:    def parse_block_scalar_empty_line(indent_token_class, content_token_class):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/data.py:142:                    yield match.start(), indent_token_class, text
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/data.py:146:                yield match.start(), indent_token_class, indentation
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/data.py:147:                yield (match.start() + context.block_scalar_indent, content_token_class, content)
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/data.py:152:    def parse_block_scalar_indent(token_class):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/data.py:169:                yield match.start(), token_class, text
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/data.py:174:    def parse_plain_scalar_indent(token_class):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/data.py:184:                yield match.start(), token_class, text
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/data.py:189:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/data.py:245:            # whitespaces separating tokens
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/data.py:435:    def get_tokens_unprocessed(self, text=None, context=None):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/data.py:438:        return super().get_tokens_unprocessed(text, context)
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/data.py:467:    # sets, the token will be considered valid. For example,
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/data.py:480:    def get_tokens_unprocessed(self, text):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/data.py:498:        # The queue is used to store data that may need to be tokenized
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/data.py:500:        # keys are tokenized differently than string values, but cannot
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/data.py:510:        #     (start_index, token_type, text)
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/data.py:512:        # By default the token type of text in double quotes is
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/data.py:513:        # String.Double. The token type will be replaced if a colon
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/data.py:624:                # Exhaust the queue. Accept the existing token types.
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/data.py:640:                # Exhaust the queue. Accept the existing token types.
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/data.py:647:                # Exhaust the queue. Accept the existing token types.
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/data.py:654:                # Yield from the queue. Replace string token types.
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/data.py:655:                for _start, _token, _text in queue:
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/data.py:656:                    # There can be only three types of tokens before a ':':
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/data.py:660:                    # Otherwise, we yield the original token.
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/data.py:664:                    if _token is String.Double:
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/data.py:667:                        yield _start, _token, _text
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/data.py:673:                # Exhaust the queue. Accept the existing token types.
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/data.py:684:                # Exhaust the queue. Accept the existing token types.
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/data.py:769:    def get_tokens_unprocessed(self, text):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/data.py:770:        for start, token, value in super().get_tokens_unprocessed(text):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/data.py:771:            if token is Name.Tag and value in self.json_ld_keywords:
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/data.py:774:                yield start, token, value
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/javascript.py:28:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/javascript.py:88:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/javascript.py:200:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/javascript.py:244:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/javascript.py:440:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/javascript.py:565:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/javascript.py:686:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/javascript.py:770:                r"Error_InvalidDatabase|Error_InvalidPassword|"
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/javascript.py:939:    def get_tokens_unprocessed(self, text):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/javascript.py:943:        for index, token, value in RegexLexer.get_tokens_unprocessed(self, text, stack):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/javascript.py:945:                token is Name.Other
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/javascript.py:947:                or token is Name.Other.Member
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/javascript.py:952:            yield index, token, value
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/javascript.py:982:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/javascript.py:1214:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/javascript.py:1333:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/javascript.py:1434:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/javascript.py:1742:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/javascript.py:1832:    def get_tokens_unprocessed(self, text):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/javascript.py:1856:                    yield from do_insertions(insertions, jslexer.get_tokens_unprocessed(curcode))
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/javascript.py:1861:                yield from do_insertions([], jslexer.get_tokens_unprocessed(line))
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/javascript.py:1864:            yield from do_insertions(insertions, jslexer.get_tokens_unprocessed(curcode))
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_scheme_builtins.py:1408:    "string-tokenize",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/jslt.py:12:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/jslt.py:41:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/business.py:15:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/business.py:61:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/business.py:674:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/business.py:695:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/business.py:919:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/business.py:982:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/business.py:1026:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/idl.py:14:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/idl.py:946:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/ecl.py:14:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/ecl.py:42:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/ecl.py:75:                r"QSTRING|REAL|RECORD|RULE|SET OF|STRING|TOKEN|UDECIMAL|UNICODE|"
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/ecl.py:209:                        "TOKEN",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/promql.py:12:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/promql.py:136:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/scdoc.py:14:from pygments.token import Comment, Generic, Keyword, String, Text
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/scdoc.py:31:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/slash.py:12:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/slash.py:40:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/spice.py:12:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/spice.py:39:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/spice.py:108:            # tokens
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_stata_builtins.py:468:    "gettoken",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_stata_builtins.py:1489:    "token",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_stata_builtins.py:1490:    "tokeni",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_stata_builtins.py:1491:    "tokeniz",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/_stata_builtins.py:1492:    "tokenize",
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/matlab.py:23:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/matlab.py:53:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/matlab.py:2804:    def get_tokens_unprocessed(self, text):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/matlab.py:2828:                token = (0, Generic.Traceback, line)
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/matlab.py:2829:                insertions.append((idx, [token]))
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/matlab.py:2842:                    yield from do_insertions(insertions, mlexer.get_tokens_unprocessed(curcode))
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/matlab.py:2856:            yield from do_insertions(insertions, mlexer.get_tokens_unprocessed(curcode))
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/matlab.py:4081:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/matlab.py:4219:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/monte.py:12:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/monte.py:202:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/c_cpp.py:23:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/c_cpp.py:73:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/c_cpp.py:491:    def get_tokens_unprocessed(self, text, stack=("root",)):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/c_cpp.py:492:        for index, token, value in RegexLexer.get_tokens_unprocessed(self, text, stack):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/c_cpp.py:493:            if token is Name:
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/c_cpp.py:495:                    token = Keyword.Type
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/c_cpp.py:497:                    token = Keyword.Type
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/c_cpp.py:499:                    token = Keyword.Type
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/c_cpp.py:501:                    token = Keyword.Type
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/c_cpp.py:502:            yield index, token, value
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/c_cpp.py:537:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/c_cpp.py:615:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/solidity.py:12:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/solidity.py:45:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/actionscript.py:14:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/actionscript.py:42:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/actionscript.py:378:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/actionscript.py:499:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/amdgpu.py:14:from pygments.token import Comment, Keyword, Name, Number, Text, Whitespace
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/amdgpu.py:32:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/dax.py:12:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/dax.py:39:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/python.py:25:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/python.py:130:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/python.py:746:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/python.py:1200:    Code tokens are output as ``Token.Other.Code``, traceback tokens as
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/python.py:1201:    ``Token.Other.Traceback``.
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/python.py:1203:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/python.py:1270:        # different tokens.  TODO: DelegatingLexer should support this
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/python.py:1272:        # distinguishing tokens. Then we wouldn't need this intermediary
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/python.py:1297:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/python.py:1359:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/python.py:1402:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/python.py:1727:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/python.py:2298:    def get_tokens_unprocessed(self, text):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/python.py:2299:        for index, token, value in PythonLexer.get_tokens_unprocessed(self, text):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/python.py:2300:            if token is Name and value in self.EXTRA_KEYWORDS:
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/python.py:2303:                yield index, token, value
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/devicetree.py:12:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/devicetree.py:42:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/whiley.py:12:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/whiley.py:41:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/wowtoc.py:16:from pygments.token import Comment, Keyword, Name, Punctuation, String, Text
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/wowtoc.py:30:def _create_tag_line_token(inner_pattern, inner_token, ignore_case=False):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/wowtoc.py:32:    # have a different pattern and different token. otherwise, everything about a tag
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/wowtoc.py:39:            inner_token,
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/wowtoc.py:60:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/wowtoc.py:64:            _create_tag_line_token(
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/wowtoc.py:70:            _create_tag_line_token(
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/wowtoc.py:78:            _create_tag_line_token(
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/wowtoc.py:84:            _create_tag_line_token(
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/teraterm.py:14:from pygments.token import Comment, Error, Keyword, Name, Number, Operator, String, Text
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/teraterm.py:31:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/teraterm.py:86:                r"delpassword|"
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/teraterm.py:141:                r"getpassword|"
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/teraterm.py:154:                r"ispassword|"
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/teraterm.py:174:                r"passwordbox|"
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/teraterm.py:208:                r"setpassword|"
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/teraterm.py:326:        if re.search(TeraTermLexer.tokens["commands"][0][0], text):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/fortran.py:14:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/fortran.py:49:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/fortran.py:600:        for index, token, value in lexer.get_tokens_unprocessed(text):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/fortran.py:603:                yield index, token, value
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/fortran.py:605:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/jmespath.py:12:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/jmespath.py:37:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/email.py:13:from pygments.token import Comment, Keyword, Name, Number, String, Text
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/email.py:30:    def get_x_header_tokens(self, match):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/email.py:36:            default_actions = self.get_tokens_unprocessed(match.group(2), stack=("root", "header"))
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/email.py:43:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/email.py:46:            (r"^(X-(?:\w[\w\-]*:))([\s\S]*?\n)(?![ \t])", get_x_header_tokens),
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/ooc.py:12:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/ooc.py:38:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/ezhil.py:14:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/ezhil.py:42:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/r.py:14:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/r.py:45:    def get_tokens_unprocessed(self, text):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/r.py:64:                        insertions, slexer.get_tokens_unprocessed(current_code_block)
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/r.py:76:            yield from do_insertions(insertions, slexer.get_tokens_unprocessed(current_code_block))
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/r.py:100:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/r.py:197:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/julia.py:19:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/julia.py:53:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/julia.py:282:    def get_tokens_unprocessed(self, text):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/julia.py:306:                    yield from do_insertions(insertions, jllexer.get_tokens_unprocessed(curcode))
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/julia.py:318:            yield from do_insertions(insertions, jllexer.get_tokens_unprocessed(curcode))
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/gcodelexer.py:12:from pygments.token import Comment, Keyword, Name, Number, Text
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/gcodelexer.py:28:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/bdd.py:12:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/bdd.py:43:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/dns.py:14:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/dns.py:53:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/configs.py:25:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/configs.py:83:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/configs.py:140:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/configs.py:191:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/configs.py:236:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/configs.py:286:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/configs.py:375:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/configs.py:417:            # Skip blank lines after help token, if any
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/configs.py:454:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/configs.py:532:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/configs.py:849:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/configs.py:884:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/configs.py:930:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/configs.py:963:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/configs.py:1184:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/configs.py:1280:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/configs.py:1328:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/configs.py:1370:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/configs.py:1411:    but it yield error token. It is because pacman.conf has
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/configs.py:1430:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/configs.py:1472:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/configs.py:1538:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/configs.py:1674:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/configs.py:1764:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexers/configs.py:1807:    tokens = {
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:18:from pygments.token import Error, Other, Text, Whitespace, _TokenType
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:274:    def get_tokens(self, text, unfiltered=False):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:278:        iterable of ``(tokentype, value)`` pairs from `text`.
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:282:        (`stripnl`, `stripall` and so on), and then yields all tokens
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:283:        from `get_tokens_unprocessed()`, with the ``index`` dropped.
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:291:            for _, t, v in self.get_tokens_unprocessed(text):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:299:    def get_tokens_unprocessed(self, text):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:302:        ``(index, tokentype, value)`` tuples where ``index`` is the starting
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:303:        position of the token within the input text.
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:315:    lexer, afterwards all ``Other`` tokens are lexed using the root
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:327:    def get_tokens_unprocessed(self, text):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:331:        for i, t, v in self.language_lexer.get_tokens_unprocessed(text):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:341:        return do_insertions(insertions, self.root_lexer.get_tokens_unprocessed(buffered))
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:418:            elif type(action) is _TokenType:
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:479:            for i, t, v in lx.get_tokens_unprocessed(match.group(), **gt_kwargs):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:492:            for i, t, v in lx.get_tokens_unprocessed(match.group(), **gt_kwargs):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:503:    For example default('#pop') is equivalent to ('', Token, '#pop')
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:532:    Metaclass for RegexLexer, creates the self._tokens attribute from
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:533:    self.tokens on the first instantiation.
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:537:        """Preprocess the regular expression component of a token definition."""
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:542:    def _process_token(cls, token):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:543:        """Preprocess the token component of a token definition."""
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:544:        assert type(token) is _TokenType or callable(
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:545:            token
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:546:        ), f"token type must be simple type or callable, not {token!r}"
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:547:        return token
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:550:        """Preprocess the state transition action of a token definition."""
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:567:            itokens = []
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:570:                itokens.extend(cls._process_state(unprocessed, processed, istate))
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:571:            processed[tmp_state] = itokens
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:589:        tokens = processed[state] = []
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:595:                tokens.extend(cls._process_state(unprocessed, processed, str(tdef)))
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:604:                tokens.append((re.compile("").match, None, new_state))
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:616:            token = cls._process_token(tdef[1])
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:623:            tokens.append((rex, token, new_state))
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:624:        return tokens
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:626:    def process_tokendef(cls, name, tokendefs=None):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:627:        """Preprocess a dictionary of token definitions."""
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:628:        processed = cls._all_tokens[name] = {}
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:629:        tokendefs = tokendefs or cls.tokens[name]
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:630:        for state in list(tokendefs):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:631:            cls._process_state(tokendefs, processed, state)
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:634:    def get_tokendefs(cls):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:636:        Merge tokens from superclasses in MRO order, returning a single tokendef
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:646:        tokens = {}
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:649:            toks = c.__dict__.get("tokens", {})
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:652:                curitems = tokens.get(state)
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:658:                    tokens[state] = items
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:681:        return tokens
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:684:        """Instantiate cls after preprocessing its token definitions."""
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:685:        if "_tokens" not in cls.__dict__:
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:686:            cls._all_tokens = {}
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:688:            if hasattr(cls, "token_variants") and cls.token_variants:
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:692:                cls._tokens = cls.process_tokendef("", cls.get_tokendefs())
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:711:    #: Dict of ``{'state': [(regex, tokentype, new_state), ...], ...}``
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:730:    tokens = {}
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:732:    def get_tokens_unprocessed(self, text, stack=("root",)):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:734:        Split ``text`` into (tokentype, text) pairs.
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:739:        tokendefs = self._tokens
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:741:        statetokens = tokendefs[statestack[-1]]
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:743:            for rexmatch, action, new_state in statetokens:
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:747:                        if type(action) is _TokenType:
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:775:                        statetokens = tokendefs[statestack[-1]]
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:778:                # We are here only if all state tokens have been considered
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:784:                        statetokens = tokendefs["root"]
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:814:    def get_tokens_unprocessed(self, text=None, context=None):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:816:        Split ``text`` into (tokentype, text) pairs.
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:819:        tokendefs = self._tokens
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:822:            statetokens = tokendefs["root"]
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:825:            statetokens = tokendefs[ctx.stack[-1]]
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:828:            for rexmatch, action, new_state in statetokens:
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:832:                        if type(action) is _TokenType:
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:839:                                statetokens = tokendefs[ctx.stack[-1]]
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:862:                        statetokens = tokendefs[ctx.stack[-1]]
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:871:                        statetokens = tokendefs["root"]
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:881:def do_insertions(insertions, tokens):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:886:    ``insertions`` is a list of ``(index, itokens)`` pairs.
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:887:    Each ``itokens`` iterable should be inserted at position
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:888:    ``index`` into the token stream given by the ``tokens``
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:891:    The result is a combined token stream.
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:897:        index, itokens = next(insertions)
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:900:        yield from tokens
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:906:    # iterate over the token stream where we want to insert
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:907:    # the tokens from the insertion list.
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:908:    for i, t, v in tokens:
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:918:            for it_index, it_token, it_value in itokens:
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:919:                yield realpos, it_token, it_value
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:923:                index, itokens = next(insertions)
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:931:    # leftover tokens
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:933:        # no normal tokens, set realpos to zero
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:935:        for p, t, v in itokens:
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:939:            index, itokens = next(insertions)
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:973:    def get_tokens_unprocessed(self, text, stack=("root",)):
./.venv_tmp/lib/python3.12/site-packages/pygments/lexer.py:976:        yield from RegexLexer.get_tokens_unprocessed(self, text, stack)
./.venv_tmp/lib/python3.12/site-packages/pygments/cmdline.py:572:        help="Add a filter to the token stream.  (Query names with -L.) "
./.venv_tmp/lib/python3.12/site-packages/pygments/__init__.py:39:    and return an iterable of tokens. Currently, this only calls
./.venv_tmp/lib/python3.12/site-packages/pygments/__init__.py:40:    `lexer.get_tokens()`.
./.venv_tmp/lib/python3.12/site-packages/pygments/__init__.py:43:        return lexer.get_tokens(code)
./.venv_tmp/lib/python3.12/site-packages/pygments/__init__.py:53:def format(tokens, formatter, outfile=None):  # pylint: disable=redefined-builtin
./.venv_tmp/lib/python3.12/site-packages/pygments/__init__.py:55:    Format ``tokens`` (an iterable of tokens) with the formatter ``formatter``
./.venv_tmp/lib/python3.12/site-packages/pygments/__init__.py:65:            formatter.format(tokens, realoutfile)
./.venv_tmp/lib/python3.12/site-packages/pygments/__init__.py:68:            formatter.format(tokens, outfile)
./.venv_tmp/lib/python3.12/site-packages/pygments/filters/__init__.py:16:from pygments.token import (
./.venv_tmp/lib/python3.12/site-packages/pygments/filters/__init__.py:23:    string_to_tokentype,
./.venv_tmp/lib/python3.12/site-packages/pygments/filters/__init__.py:723:    """Highlight a normal Name (and Name.*) token with a different token type.
./.venv_tmp/lib/python3.12/site-packages/pygments/filters/__init__.py:729:            tokentype=Name.Function,
./.venv_tmp/lib/python3.12/site-packages/pygments/filters/__init__.py:733:    as functions. `Name.Function` is the default token type.
./.venv_tmp/lib/python3.12/site-packages/pygments/filters/__init__.py:738:      A list of names that should be given the different token type.
./.venv_tmp/lib/python3.12/site-packages/pygments/filters/__init__.py:740:    `tokentype` : TokenType or string
./.venv_tmp/lib/python3.12/site-packages/pygments/filters/__init__.py:741:      A token type or a string containing a token type name that is
./.venv_tmp/lib/python3.12/site-packages/pygments/filters/__init__.py:749:        tokentype = options.get("tokentype")
./.venv_tmp/lib/python3.12/site-packages/pygments/filters/__init__.py:750:        if tokentype:
./.venv_tmp/lib/python3.12/site-packages/pygments/filters/__init__.py:751:            self.tokentype = string_to_tokentype(tokentype)
./.venv_tmp/lib/python3.12/site-packages/pygments/filters/__init__.py:753:            self.tokentype = Name.Function
./.venv_tmp/lib/python3.12/site-packages/pygments/filters/__init__.py:758:                yield self.tokentype, value
./.venv_tmp/lib/python3.12/site-packages/pygments/filters/__init__.py:763:class ErrorToken(Exception):
./.venv_tmp/lib/python3.12/site-packages/pygments/filters/__init__.py:767:class RaiseOnErrorTokenFilter(Filter):
./.venv_tmp/lib/python3.12/site-packages/pygments/filters/__init__.py:768:    """Raise an exception when the lexer generates an error token.
./.venv_tmp/lib/python3.12/site-packages/pygments/filters/__init__.py:774:      The default is `pygments.filters.ErrorToken`.
./.venv_tmp/lib/python3.12/site-packages/pygments/filters/__init__.py:781:        self.exception = options.get("excclass", ErrorToken)
./.venv_tmp/lib/python3.12/site-packages/pygments/filters/__init__.py:818:    `wstokentype` : bool
./.venv_tmp/lib/python3.12/site-packages/pygments/filters/__init__.py:819:      If true, give whitespace the special `Whitespace` token type.  This allows
./.venv_tmp/lib/python3.12/site-packages/pygments/filters/__init__.py:839:        self.wstt = get_bool_opt(options, "wstokentype", True)
./.venv_tmp/lib/python3.12/site-packages/pygments/filters/__init__.py:901:            # Remove ``left`` tokens from first line, ``n`` from all others.
./.venv_tmp/lib/python3.12/site-packages/pygments/filters/__init__.py:912:class TokenMergeFilter(Filter):
./.venv_tmp/lib/python3.12/site-packages/pygments/filters/__init__.py:913:    """Merges consecutive tokens with the same token type in the output
./.venv_tmp/lib/python3.12/site-packages/pygments/filters/__init__.py:941:    "raiseonerror": RaiseOnErrorTokenFilter,
./.venv_tmp/lib/python3.12/site-packages/pygments/filters/__init__.py:944:    "tokenmerge": TokenMergeFilter,
./.venv_tmp/lib/python3.12/site-packages/markdown_it/tree.py:1:"""A tree representation of a linear markdown-it token stream.
./.venv_tmp/lib/python3.12/site-packages/markdown_it/tree.py:12:from .token import Token
./.venv_tmp/lib/python3.12/site-packages/markdown_it/tree.py:15:class _NesterTokens(NamedTuple):
./.venv_tmp/lib/python3.12/site-packages/markdown_it/tree.py:16:    opening: Token
./.venv_tmp/lib/python3.12/site-packages/markdown_it/tree.py:17:    closing: Token
./.venv_tmp/lib/python3.12/site-packages/markdown_it/tree.py:27:    `markdown-it-py` token stream.
./.venv_tmp/lib/python3.12/site-packages/markdown_it/tree.py:31:      - a single unnested `Token`
./.venv_tmp/lib/python3.12/site-packages/markdown_it/tree.py:32:      - a `Token` "_open" and "_close" token pair, and the tokens nested in
./.venv_tmp/lib/python3.12/site-packages/markdown_it/tree.py:36:    def __init__(self, tokens: Sequence[Token] = (), *, create_root: bool = True) -> None:
./.venv_tmp/lib/python3.12/site-packages/markdown_it/tree.py:37:        """Initialize a `SyntaxTreeNode` from a token stream.
./.venv_tmp/lib/python3.12/site-packages/markdown_it/tree.py:41:        # Only nodes representing an unnested token have self.token
./.venv_tmp/lib/python3.12/site-packages/markdown_it/tree.py:42:        self.token: Token | None = None
./.venv_tmp/lib/python3.12/site-packages/markdown_it/tree.py:44:        # Only containers have nester tokens
./.venv_tmp/lib/python3.12/site-packages/markdown_it/tree.py:45:        self.nester_tokens: _NesterTokens | None = None
./.venv_tmp/lib/python3.12/site-packages/markdown_it/tree.py:50:        # Empty list unless a non-empty container, or unnested token that has
./.venv_tmp/lib/python3.12/site-packages/markdown_it/tree.py:55:            self._set_children_from_tokens(tokens)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/tree.py:58:        if not tokens:
./.venv_tmp/lib/python3.12/site-packages/markdown_it/tree.py:60:                "Can only create root from empty token sequence." " Set `create_root=True`."
./.venv_tmp/lib/python3.12/site-packages/markdown_it/tree.py:62:        elif len(tokens) == 1:
./.venv_tmp/lib/python3.12/site-packages/markdown_it/tree.py:63:            inline_token = tokens[0]
./.venv_tmp/lib/python3.12/site-packages/markdown_it/tree.py:64:            if inline_token.nesting:
./.venv_tmp/lib/python3.12/site-packages/markdown_it/tree.py:65:                raise ValueError("Unequal nesting level at the start and end of token stream.")
./.venv_tmp/lib/python3.12/site-packages/markdown_it/tree.py:66:            self.token = inline_token
./.venv_tmp/lib/python3.12/site-packages/markdown_it/tree.py:67:            if inline_token.children:
./.venv_tmp/lib/python3.12/site-packages/markdown_it/tree.py:68:                self._set_children_from_tokens(inline_token.children)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/tree.py:70:            self.nester_tokens = _NesterTokens(tokens[0], tokens[-1])
./.venv_tmp/lib/python3.12/site-packages/markdown_it/tree.py:71:            self._set_children_from_tokens(tokens[1:-1])
./.venv_tmp/lib/python3.12/site-packages/markdown_it/tree.py:85:    def to_tokens(self: _NodeType) -> list[Token]:
./.venv_tmp/lib/python3.12/site-packages/markdown_it/tree.py:86:        """Recover the linear token stream."""
./.venv_tmp/lib/python3.12/site-packages/markdown_it/tree.py:88:        def recursive_collect_tokens(node: _NodeType, token_list: list[Token]) -> None:
./.venv_tmp/lib/python3.12/site-packages/markdown_it/tree.py:91:                    recursive_collect_tokens(child, token_list)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/tree.py:92:            elif node.token:
./.venv_tmp/lib/python3.12/site-packages/markdown_it/tree.py:93:                token_list.append(node.token)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/tree.py:95:                assert node.nester_tokens
./.venv_tmp/lib/python3.12/site-packages/markdown_it/tree.py:96:                token_list.append(node.nester_tokens.opening)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/tree.py:98:                    recursive_collect_tokens(child, token_list)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/tree.py:99:                token_list.append(node.nester_tokens.closing)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/tree.py:101:        tokens: list[Token] = []
./.venv_tmp/lib/python3.12/site-packages/markdown_it/tree.py:102:        recursive_collect_tokens(self, tokens)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/tree.py:103:        return tokens
./.venv_tmp/lib/python3.12/site-packages/markdown_it/tree.py:124:        return not (self.token or self.nester_tokens)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/tree.py:130:        Returns `True` if the node represents a `Token` pair and tokens in the
./.venv_tmp/lib/python3.12/site-packages/markdown_it/tree.py:131:        sequence between them, where `Token.nesting` of the first `Token` in
./.venv_tmp/lib/python3.12/site-packages/markdown_it/tree.py:132:        the pair is 1 and nesting of the other `Token` is -1.
./.venv_tmp/lib/python3.12/site-packages/markdown_it/tree.py:134:        return bool(self.nester_tokens)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/tree.py:151:        - `Token.type` if the node represents an unnested token
./.venv_tmp/lib/python3.12/site-packages/markdown_it/tree.py:152:        - `Token.type` of the opening token, with "_open" suffix stripped, if
./.venv_tmp/lib/python3.12/site-packages/markdown_it/tree.py:153:            the node represents a nester token pair
./.venv_tmp/lib/python3.12/site-packages/markdown_it/tree.py:157:        if self.token:
./.venv_tmp/lib/python3.12/site-packages/markdown_it/tree.py:158:            return self.token.type
./.venv_tmp/lib/python3.12/site-packages/markdown_it/tree.py:159:        assert self.nester_tokens
./.venv_tmp/lib/python3.12/site-packages/markdown_it/tree.py:160:        return self.nester_tokens.opening.type.removesuffix("_open")
./.venv_tmp/lib/python3.12/site-packages/markdown_it/tree.py:186:        tokens: Sequence[Token],
./.venv_tmp/lib/python3.12/site-packages/markdown_it/tree.py:189:        child = type(self)(tokens, create_root=False)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/tree.py:193:    def _set_children_from_tokens(self, tokens: Sequence[Token]) -> None:
./.venv_tmp/lib/python3.12/site-packages/markdown_it/tree.py:194:        """Convert the token stream to a tree structure and set the resulting
./.venv_tmp/lib/python3.12/site-packages/markdown_it/tree.py:196:        reversed_tokens = list(reversed(tokens))
./.venv_tmp/lib/python3.12/site-packages/markdown_it/tree.py:197:        while reversed_tokens:
./.venv_tmp/lib/python3.12/site-packages/markdown_it/tree.py:198:            token = reversed_tokens.pop()
./.venv_tmp/lib/python3.12/site-packages/markdown_it/tree.py:200:            if not token.nesting:
./.venv_tmp/lib/python3.12/site-packages/markdown_it/tree.py:201:                self._add_child([token])
./.venv_tmp/lib/python3.12/site-packages/markdown_it/tree.py:203:            if token.nesting != 1:
./.venv_tmp/lib/python3.12/site-packages/markdown_it/tree.py:204:                raise ValueError("Invalid token nesting")
./.venv_tmp/lib/python3.12/site-packages/markdown_it/tree.py:206:            nested_tokens = [token]
./.venv_tmp/lib/python3.12/site-packages/markdown_it/tree.py:208:            while reversed_tokens and nesting:
./.venv_tmp/lib/python3.12/site-packages/markdown_it/tree.py:209:                token = reversed_tokens.pop()
./.venv_tmp/lib/python3.12/site-packages/markdown_it/tree.py:210:                nested_tokens.append(token)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/tree.py:211:                nesting += token.nesting
./.venv_tmp/lib/python3.12/site-packages/markdown_it/tree.py:213:                raise ValueError(f"unclosed tokens starting {nested_tokens[0]}")
./.venv_tmp/lib/python3.12/site-packages/markdown_it/tree.py:215:            self._add_child(nested_tokens)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/tree.py:240:        The order mimics the order of the underlying linear token
./.venv_tmp/lib/python3.12/site-packages/markdown_it/tree.py:250:    # of the underlying `Token`s. A root node does not translate to a `Token`
./.venv_tmp/lib/python3.12/site-packages/markdown_it/tree.py:254:    # There is no mapping for `Token.nesting` because the `is_nested` property
./.venv_tmp/lib/python3.12/site-packages/markdown_it/tree.py:257:    def _attribute_token(self) -> Token:
./.venv_tmp/lib/python3.12/site-packages/markdown_it/tree.py:258:        """Return the `Token` that is used as the data source for the
./.venv_tmp/lib/python3.12/site-packages/markdown_it/tree.py:260:        if self.token:
./.venv_tmp/lib/python3.12/site-packages/markdown_it/tree.py:261:            return self.token
./.venv_tmp/lib/python3.12/site-packages/markdown_it/tree.py:262:        if self.nester_tokens:
./.venv_tmp/lib/python3.12/site-packages/markdown_it/tree.py:263:            return self.nester_tokens.opening
./.venv_tmp/lib/python3.12/site-packages/markdown_it/tree.py:269:        return self._attribute_token().tag
./.venv_tmp/lib/python3.12/site-packages/markdown_it/tree.py:274:        return self._attribute_token().attrs
./.venv_tmp/lib/python3.12/site-packages/markdown_it/tree.py:278:        return self._attribute_token().attrGet(name)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/tree.py:283:        map_ = self._attribute_token().map
./.venv_tmp/lib/python3.12/site-packages/markdown_it/tree.py:285:            # Type ignore because `Token`s attribute types are not perfect
./.venv_tmp/lib/python3.12/site-packages/markdown_it/tree.py:292:        return self._attribute_token().level
./.venv_tmp/lib/python3.12/site-packages/markdown_it/tree.py:298:        return self._attribute_token().content
./.venv_tmp/lib/python3.12/site-packages/markdown_it/tree.py:303:        return self._attribute_token().markup
./.venv_tmp/lib/python3.12/site-packages/markdown_it/tree.py:308:        return self._attribute_token().info
./.venv_tmp/lib/python3.12/site-packages/markdown_it/tree.py:313:        return self._attribute_token().meta
./.venv_tmp/lib/python3.12/site-packages/markdown_it/tree.py:317:        """True for block-level tokens, false for inline tokens."""
./.venv_tmp/lib/python3.12/site-packages/markdown_it/tree.py:318:        return self._attribute_token().block
./.venv_tmp/lib/python3.12/site-packages/markdown_it/tree.py:324:        return self._attribute_token().hidden
./.venv_tmp/lib/python3.12/site-packages/markdown_it/utils.py:44:    """Store link label in link/image token's metadata (under Token.meta['label']).
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/paragraph.py:52:    token = state.push("paragraph_open", "p", 1)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/paragraph.py:53:    token.map = [startLine, state.line]
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/paragraph.py:55:    token = state.push("inline", "", 0)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/paragraph.py:56:    token.content = content
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/paragraph.py:57:    token.map = [startLine, state.line]
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/paragraph.py:58:    token.children = []
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/paragraph.py:60:    token = state.push("paragraph_close", "p", -1)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/fence.py:95:    token = state.push("fence", "code", 0)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/fence.py:96:    token.info = params
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/fence.py:97:    token.content = state.getLines(startLine + 1, nextLine, length, True)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/fence.py:98:    token.markup = markup
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/fence.py:99:    token.map = [startLine, state.line]
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/lheading.py:72:    token = state.push("heading_open", "h" + str(level), 1)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/lheading.py:73:    token.markup = marker
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/lheading.py:74:    token.map = [startLine, state.line]
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/lheading.py:76:    token = state.push("inline", "", 0)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/lheading.py:77:    token.content = content
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/lheading.py:78:    token.map = [startLine, state.line - 1]
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/lheading.py:79:    token.children = []
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/lheading.py:81:    token = state.push("heading_close", "h" + str(level), -1)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/lheading.py:82:    token.markup = marker
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/table.py:156:    token = state.push("table_open", "table", 1)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/table.py:157:    token.map = tableLines = [startLine, 0]
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/table.py:159:    token = state.push("thead_open", "thead", 1)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/table.py:160:    token.map = [startLine, startLine + 1]
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/table.py:162:    token = state.push("tr_open", "tr", 1)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/table.py:163:    token.map = [startLine, startLine + 1]
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/table.py:166:        token = state.push("th_open", "th", 1)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/table.py:168:            token.attrs = {"style": "text-align:" + aligns[i]}
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/table.py:170:        token = state.push("inline", "", 0)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/table.py:172:        # since it is helpful to propagate to children tokens
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/table.py:173:        token.map = [startLine, startLine + 1]
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/table.py:174:        token.content = columns[i].strip()
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/table.py:175:        token.children = []
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/table.py:177:        token = state.push("th_close", "th", -1)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/table.py:179:    token = state.push("tr_close", "tr", -1)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/table.py:180:    token = state.push("thead_close", "thead", -1)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/table.py:214:            token = state.push("tbody_open", "tbody", 1)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/table.py:215:            token.map = tbodyLines = [startLine + 2, 0]
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/table.py:217:        token = state.push("tr_open", "tr", 1)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/table.py:218:        token.map = [nextLine, nextLine + 1]
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/table.py:221:            token = state.push("td_open", "td", 1)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/table.py:223:                token.attrs = {"style": "text-align:" + aligns[i]}
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/table.py:225:            token = state.push("inline", "", 0)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/table.py:227:            # since it is helpful to propagate to children tokens
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/table.py:228:            token.map = [nextLine, nextLine + 1]
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/table.py:230:                token.content = columns[i].strip() if columns[i] else ""
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/table.py:232:                token.content = ""
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/table.py:233:            token.children = []
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/table.py:235:            token = state.push("td_close", "td", -1)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/table.py:237:        token = state.push("tr_close", "tr", -1)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/table.py:242:        token = state.push("tbody_close", "tbody", -1)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/table.py:245:    token = state.push("table_close", "table", -1)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/state_block.py:7:from ..token import Token
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/state_block.py:15:    def __init__(self, src: str, md: MarkdownIt, env: EnvType, tokens: list[Token]) -> None:
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/state_block.py:27:        self.tokens = tokens
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/state_block.py:114:            f"(line={self.line},level={self.level},tokens={len(self.tokens)})"
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/state_block.py:117:    def push(self, ttype: str, tag: str, nesting: Literal[-1, 0, 1]) -> Token:
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/state_block.py:118:        """Push new token to "stream"."""
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/state_block.py:119:        token = Token(ttype, tag, nesting)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/state_block.py:120:        token.block = True
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/state_block.py:123:        token.level = self.level
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/state_block.py:126:        self.tokens.append(token)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/state_block.py:127:        return token
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/hr.py:52:    token = state.push("hr", "hr", 0)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/hr.py:53:    token.map = [startLine, state.line]
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/hr.py:54:    token.markup = marker * (cnt + 1)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/blockquote.py:230:            # normally if you call `tokenize(state, startLine, nextLine)`,
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/blockquote.py:261:    token = state.push("blockquote_open", "blockquote", 1)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/blockquote.py:262:    token.markup = ">"
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/blockquote.py:263:    token.map = lines = [startLine, 0]
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/blockquote.py:265:    state.md.block.tokenize(state, startLine, nextLine)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/blockquote.py:267:    token = state.push("blockquote_close", "blockquote", -1)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/blockquote.py:268:    token.markup = ">"
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/code.py:32:    token = state.push("code_block", "code", 0)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/code.py:33:    token.content = state.getLines(startLine, last, 4 + state.blkIndent, False) + "\n"
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/code.py:34:    token.map = [startLine, state.line]
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/reference.py:163:        token = state.push("definition", "", 0)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/reference.py:164:        token.meta = {
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/reference.py:170:        token.map = [startLine, state.line]
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/list.py:92:    length = len(state.tokens) - 2
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/list.py:94:        if state.tokens[i].level == level and state.tokens[i].type == "paragraph_open":
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/list.py:95:            state.tokens[i + 2].hidden = True
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/list.py:96:            state.tokens[i].hidden = True
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/list.py:164:    listTokIdx = len(state.tokens)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/list.py:167:        token = state.push("ordered_list_open", "ol", 1)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/list.py:169:            token.attrs = {"start": markerValue}
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/list.py:172:        token = state.push("bullet_list_open", "ul", 1)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/list.py:174:    token.map = listLines = [startLine, 0]
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/list.py:175:    token.markup = markerChar
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/list.py:224:        # Run subparser & write tokens
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/list.py:225:        token = state.push("list_item_open", "li", 1)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/list.py:226:        token.markup = markerChar
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/list.py:227:        token.map = itemLines = [startLine, 0]
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/list.py:229:            token.info = state.src[start : posAfterMarker - 1]
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/list.py:259:            # state.md.block.tokenize(state, startLine, endLine, True)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/list.py:260:            # but  tokeniz does not take the final parameter
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/list.py:261:            state.md.block.tokenize(state, startLine, endLine)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/list.py:277:        token = state.push("list_item_close", "li", -1)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/list.py:278:        token.markup = markerChar
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/list.py:323:        token = state.push("ordered_list_close", "ol", -1)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/list.py:325:        token = state.push("bullet_list_close", "ul", -1)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/list.py:327:    token.markup = markerChar
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/html_block.py:84:    token = state.push("html_block", "", 0)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/html_block.py:85:    token.map = [startLine, nextLine]
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/html_block.py:86:    token.content = state.getLines(startLine, nextLine, state.blkIndent, True)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/heading.py:57:    token = state.push("heading_open", "h" + str(level), 1)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/heading.py:58:    token.markup = "########"[:level]
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/heading.py:59:    token.map = [startLine, state.line]
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/heading.py:61:    token = state.push("inline", "", 0)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/heading.py:62:    token.content = state.src[pos:maximum].strip()
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/heading.py:63:    token.map = [startLine, state.line]
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/heading.py:64:    token.children = []
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/heading.py:66:    token = state.push("heading_close", "h" + str(level), -1)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_block/heading.py:67:    token.markup = "########"[:level]
./.venv_tmp/lib/python3.12/site-packages/markdown_it/token.py:10:    """Convert Token.attrs set as ``None`` or ``[[key, value], ...]`` to a dict.
./.venv_tmp/lib/python3.12/site-packages/markdown_it/token.py:22:class Token:
./.venv_tmp/lib/python3.12/site-packages/markdown_it/token.py:24:    """Type of the token (string, e.g. "paragraph_open")"""
./.venv_tmp/lib/python3.12/site-packages/markdown_it/token.py:48:    children: list[Token] | None = None
./.venv_tmp/lib/python3.12/site-packages/markdown_it/token.py:49:    """Array of child nodes (inline and img tokens)."""
./.venv_tmp/lib/python3.12/site-packages/markdown_it/token.py:59:    - Info string for "fence" tokens
./.venv_tmp/lib/python3.12/site-packages/markdown_it/token.py:60:    - The value "auto" for autolink "link_open" and "link_close" tokens
./.venv_tmp/lib/python3.12/site-packages/markdown_it/token.py:61:    - The string value of the item marker for ordered-list "list_item_open" tokens
./.venv_tmp/lib/python3.12/site-packages/markdown_it/token.py:68:    """True for block-level tokens, false for inline tokens.
./.venv_tmp/lib/python3.12/site-packages/markdown_it/token.py:82:            "Token.attrIndex should not be used, since Token.attrs is a dictionary",
./.venv_tmp/lib/python3.12/site-packages/markdown_it/token.py:109:        Useful to operate with token classes.
./.venv_tmp/lib/python3.12/site-packages/markdown_it/token.py:119:    def copy(self, **changes: Any) -> Token:
./.venv_tmp/lib/python3.12/site-packages/markdown_it/token.py:132:        """Return the token as a dictionary.
./.venv_tmp/lib/python3.12/site-packages/markdown_it/token.py:137:        :param meta_serializer: hook for serializing ``Token.meta``
./.venv_tmp/lib/python3.12/site-packages/markdown_it/token.py:169:    def from_dict(cls, dct: MutableMapping[str, Any]) -> Token:
./.venv_tmp/lib/python3.12/site-packages/markdown_it/token.py:170:        """Convert a dict to a Token."""
./.venv_tmp/lib/python3.12/site-packages/markdown_it/token.py:171:        token = cls(**dct)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/token.py:172:        if token.children:
./.venv_tmp/lib/python3.12/site-packages/markdown_it/token.py:173:            token.children = [cls.from_dict(c) for c in token.children]  # type: ignore[arg-type]
./.venv_tmp/lib/python3.12/site-packages/markdown_it/token.py:174:        return token
./.venv_tmp/lib/python3.12/site-packages/markdown_it/parser_inline.py:1:"""Tokenizes paragraph content."""
./.venv_tmp/lib/python3.12/site-packages/markdown_it/parser_inline.py:11:from .token import Token
./.venv_tmp/lib/python3.12/site-packages/markdown_it/parser_inline.py:22:`silent` disables token generation, useful for lookahead.
./.venv_tmp/lib/python3.12/site-packages/markdown_it/parser_inline.py:30:    ("strikethrough", rules_inline.strikethrough.tokenize),
./.venv_tmp/lib/python3.12/site-packages/markdown_it/parser_inline.py:31:    ("emphasis", rules_inline.emphasis.tokenize),
./.venv_tmp/lib/python3.12/site-packages/markdown_it/parser_inline.py:49:    # rules for pairs separate '**' into its own text tokens, which may be left unused,
./.venv_tmp/lib/python3.12/site-packages/markdown_it/parser_inline.py:65:    def skipToken(self, state: StateInline) -> None:
./.venv_tmp/lib/python3.12/site-packages/markdown_it/parser_inline.py:66:        """Skip single token by running all rules in validation mode;
./.venv_tmp/lib/python3.12/site-packages/markdown_it/parser_inline.py:82:                # It's harmless to do here, because no tokens are created.
./.venv_tmp/lib/python3.12/site-packages/markdown_it/parser_inline.py:107:    def tokenize(self, state: StateInline) -> None:
./.venv_tmp/lib/python3.12/site-packages/markdown_it/parser_inline.py:108:        """Generate tokens for input range."""
./.venv_tmp/lib/python3.12/site-packages/markdown_it/parser_inline.py:119:            # - update `state.tokens`
./.venv_tmp/lib/python3.12/site-packages/markdown_it/parser_inline.py:139:    def parse(self, src: str, md: MarkdownIt, env: EnvType, tokens: list[Token]) -> list[Token]:
./.venv_tmp/lib/python3.12/site-packages/markdown_it/parser_inline.py:140:        """Process input string and push inline tokens into `tokens`"""
./.venv_tmp/lib/python3.12/site-packages/markdown_it/parser_inline.py:141:        state = StateInline(src, md, env, tokens)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/parser_inline.py:142:        self.tokenize(state)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/parser_inline.py:146:        return state.tokens
./.venv_tmp/lib/python3.12/site-packages/markdown_it/main.py:14:from .token import Token
./.venv_tmp/lib/python3.12/site-packages/markdown_it/main.py:217:        """Add a rule for rendering a particular Token type.
./.venv_tmp/lib/python3.12/site-packages/markdown_it/main.py:231:            def func(tokens, idx):
./.venv_tmp/lib/python3.12/site-packages/markdown_it/main.py:232:                tokens[idx].content = tokens[idx].content.replace('foo', 'bar')
./.venv_tmp/lib/python3.12/site-packages/markdown_it/main.py:239:    def parse(self, src: str, env: EnvType | None = None) -> list[Token]:
./.venv_tmp/lib/python3.12/site-packages/markdown_it/main.py:240:        """Parse the source string to a token stream
./.venv_tmp/lib/python3.12/site-packages/markdown_it/main.py:245:        Parse input string and return list of block tokens (special token type
./.venv_tmp/lib/python3.12/site-packages/markdown_it/main.py:246:        "inline" will contain list of inline tokens).
./.venv_tmp/lib/python3.12/site-packages/markdown_it/main.py:260:        return state.tokens
./.venv_tmp/lib/python3.12/site-packages/markdown_it/main.py:276:    def parseInline(self, src: str, env: EnvType | None = None) -> list[Token]:
./.venv_tmp/lib/python3.12/site-packages/markdown_it/main.py:283:        block tokens list with the single `inline` element, containing parsed inline
./.venv_tmp/lib/python3.12/site-packages/markdown_it/main.py:284:        tokens in `children` property. Also updates `env` object.
./.venv_tmp/lib/python3.12/site-packages/markdown_it/main.py:294:        return state.tokens
./.venv_tmp/lib/python3.12/site-packages/markdown_it/port.yaml:19:      `Token.attrs` is a dictionary, instead of a list of lists.
./.venv_tmp/lib/python3.12/site-packages/markdown_it/port.yaml:23:      to manipulate `Token.attrs`, which have an identical signature to those upstream.
./.venv_tmp/lib/python3.12/site-packages/markdown_it/port.yaml:40:      `func(tokens, idx, options, env, slf)` to
./.venv_tmp/lib/python3.12/site-packages/markdown_it/port.yaml:41:      `func(self, tokens, idx, options, env)`
./.venv_tmp/lib/python3.12/site-packages/markdown_it/port.yaml:48:    - inline tokens in tables are assigned a map (this is helpful for propagation to children)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/escape.py:51:        token = state.push("text_special", "", 0)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/escape.py:52:        token.content = escapedStr if ch1 in _ESCAPED else origStr
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/escape.py:53:        token.markup = origStr
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/escape.py:54:        token.info = "escape"
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/entity.py:28:                token = state.push("text_special", "", 0)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/entity.py:29:                token.content = (
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/entity.py:32:                token.markup = match.group(0)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/entity.py:33:                token.info = "entity"
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/entity.py:41:                token = state.push("text_special", "", 0)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/entity.py:42:                token.content = entities[match.group(1)]
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/entity.py:43:                token.markup = match.group(0)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/entity.py:44:                token.info = "entity"
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/link.py:125:    # so all that's left to do is to call tokenizer.
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/link.py:131:        token = state.push("link_open", "a", 1)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/link.py:132:        token.attrs = {"href": href}
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/link.py:135:            token.attrSet("title", title)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/link.py:139:            token.meta["label"] = label
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/link.py:142:        state.md.inline.tokenize(state)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/link.py:145:        token = state.push("link_close", "a", -1)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/linkify.py:49:        token = state.push("link_open", "a", 1)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/linkify.py:50:        token.attrs = {"href": full_url}
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/linkify.py:51:        token.markup = "linkify"
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/linkify.py:52:        token.info = "auto"
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/linkify.py:54:        token = state.push("text", "", 0)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/linkify.py:55:        token.content = state.md.normalizeLinkText(url)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/linkify.py:57:        token = state.push("link_close", "a", -1)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/linkify.py:58:        token.markup = "linkify"
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/linkify.py:59:        token.info = "auto"
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/image.py:5:from ..token import Token
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/image.py:126:    # so all that's left to do is to call tokenizer.
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/image.py:131:        tokens: list[Token] = []
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/image.py:132:        state.md.inline.parse(content, state.md, state.env, tokens)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/image.py:134:        token = state.push("image", "img", 0)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/image.py:135:        token.attrs = {"src": href, "alt": ""}
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/image.py:136:        token.children = tokens or None
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/image.py:137:        token.content = content
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/image.py:140:            token.attrSet("title", title)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/image.py:144:            token.meta["label"] = label
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/emphasis.py:8:def tokenize(state: StateInline, silent: bool) -> bool:
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/emphasis.py:9:    """Insert each marker as a separate text token, and add it to delimiter list"""
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/emphasis.py:22:        token = state.push("text", "", 0)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/emphasis.py:23:        token.content = marker
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/emphasis.py:28:                token=len(state.tokens) - 1,
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/emphasis.py:67:            and delimiters[i - 1].token == startDelim.token - 1
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/emphasis.py:69:            and delimiters[startDelim.end + 1].token == endDelim.token + 1
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/emphasis.py:74:        token = state.tokens[startDelim.token]
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/emphasis.py:75:        token.type = "strong_open" if isStrong else "em_open"
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/emphasis.py:76:        token.tag = "strong" if isStrong else "em"
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/emphasis.py:77:        token.nesting = 1
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/emphasis.py:78:        token.markup = ch + ch if isStrong else ch
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/emphasis.py:79:        token.content = ""
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/emphasis.py:81:        token = state.tokens[endDelim.token]
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/emphasis.py:82:        token.type = "strong_close" if isStrong else "em_close"
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/emphasis.py:83:        token.tag = "strong" if isStrong else "em"
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/emphasis.py:84:        token.nesting = -1
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/emphasis.py:85:        token.markup = ch + ch if isStrong else ch
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/emphasis.py:86:        token.content = ""
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/emphasis.py:89:            state.tokens[delimiters[i - 1].token].content = ""
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/emphasis.py:90:            state.tokens[delimiters[startDelim.end + 1].token].content = ""
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/emphasis.py:97:    """Walk through delimiter list and replace text tokens with tags."""
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/emphasis.py:100:    for token in state.tokens_meta:
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/emphasis.py:101:        if token and "delimiters" in token:
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/emphasis.py:102:            _postProcess(state, token["delimiters"])
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/autolink.py:41:            token = state.push("link_open", "a", 1)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/autolink.py:42:            token.attrs = {"href": fullUrl}
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/autolink.py:43:            token.markup = "autolink"
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/autolink.py:44:            token.info = "auto"
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/autolink.py:46:            token = state.push("text", "", 0)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/autolink.py:47:            token.content = state.md.normalizeLinkText(url)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/autolink.py:49:            token = state.push("link_close", "a", -1)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/autolink.py:50:            token.markup = "autolink"
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/autolink.py:51:            token.info = "auto"
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/autolink.py:62:            token = state.push("link_open", "a", 1)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/autolink.py:63:            token.attrs = {"href": fullUrl}
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/autolink.py:64:            token.markup = "autolink"
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/autolink.py:65:            token.info = "auto"
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/autolink.py:67:            token = state.push("text", "", 0)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/autolink.py:68:            token.content = state.md.normalizeLinkText(url)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/autolink.py:70:            token = state.push("link_close", "a", -1)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/autolink.py:71:            token.markup = "autolink"
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/autolink.py:72:            token.info = "auto"
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/fragments_join.py:6:    Clean up tokens after emphasis and strikethrough postprocessing:
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/fragments_join.py:7:    merge adjacent text nodes into one and re-calculate all token levels
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/fragments_join.py:10:    are treated as their own separate text tokens. Then emphasis rule either
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/fragments_join.py:15:    maximum = len(state.tokens)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/fragments_join.py:21:        if state.tokens[curr].nesting < 0:
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/fragments_join.py:23:        state.tokens[curr].level = level
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/fragments_join.py:24:        if state.tokens[curr].nesting > 0:
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/fragments_join.py:28:            state.tokens[curr].type == "text"
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/fragments_join.py:30:            and state.tokens[curr + 1].type == "text"
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/fragments_join.py:33:            state.tokens[curr + 1].content = (
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/fragments_join.py:34:                state.tokens[curr].content + state.tokens[curr + 1].content
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/fragments_join.py:38:                state.tokens[last] = state.tokens[curr]
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/fragments_join.py:43:        del state.tokens[last:]
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/state_inline.py:9:from ..token import Token
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/state_inline.py:24:    # A position of the token this delimiter corresponds to.
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/state_inline.py:25:    token: int
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/state_inline.py:43:    def __init__(self, src: str, md: MarkdownIt, env: EnvType, outTokens: list[Token]) -> None:
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/state_inline.py:47:        self.tokens = outTokens
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/state_inline.py:48:        self.tokens_meta: list[dict[str, Any] | None] = [None] * len(outTokens)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/state_inline.py:77:            f"(pos=[{self.pos} of {self.posMax}], token={len(self.tokens)})"
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/state_inline.py:80:    def pushPending(self) -> Token:
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/state_inline.py:81:        token = Token("text", "", 0)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/state_inline.py:82:        token.content = self.pending
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/state_inline.py:83:        token.level = self.pendingLevel
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/state_inline.py:84:        self.tokens.append(token)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/state_inline.py:86:        return token
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/state_inline.py:88:    def push(self, ttype: str, tag: str, nesting: Literal[-1, 0, 1]) -> Token:
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/state_inline.py:89:        """Push new token to "stream".
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/state_inline.py:90:        If pending text exists - flush it as text token
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/state_inline.py:95:        token = Token(ttype, tag, nesting)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/state_inline.py:96:        token_meta = None
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/state_inline.py:103:        token.level = self.level
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/state_inline.py:110:            token_meta = {"delimiters": self.delimiters}
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/state_inline.py:113:        self.tokens.append(token)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/state_inline.py:114:        self.tokens_meta.append(token_meta)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/state_inline.py:115:        return token
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/html_inline.py:34:        token = state.push("html_inline", "", 0)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/html_inline.py:35:        token.content = state.src[pos : pos + len(match.group(0))]
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/html_inline.py:37:        if isLinkOpen(token.content):
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/html_inline.py:39:        if isLinkClose(token.content):
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/backticks.py:51:                token = state.push("code_inline", "code", 0)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/backticks.py:52:                token.markup = marker
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/backticks.py:53:                token.content = state.src[pos:matchStart].replace("\n", " ")
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/backticks.py:55:                    token.content.startswith(" ")
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/backticks.py:56:                    and token.content.endswith(" ")
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/backticks.py:57:                    and len(token.content.strip()) > 0
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/backticks.py:59:                    token.content = token.content[1:-1]
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/text.py:4:# Skip text characters for text token, place those to pending buffer
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/balance_pairs.py:1:"""Balance paired characters (*, _, etc) in inline tokens."""
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/balance_pairs.py:18:    lastTokenIdx = -2  # needs any value lower than -1
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/balance_pairs.py:27:        #  - they have adjacent tokens
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/balance_pairs.py:30:        if delimiters[headerIdx].marker != closer.marker or lastTokenIdx != closer.token - 1:
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/balance_pairs.py:32:        lastTokenIdx = closer.token
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/balance_pairs.py:99:                    # treat next token as start of run,
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/balance_pairs.py:101:                    lastTokenIdx = -2
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/balance_pairs.py:123:    tokens_meta = state.tokens_meta
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/balance_pairs.py:124:    maximum = len(state.tokens_meta)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/balance_pairs.py:130:        curr_meta = tokens_meta[curr]
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/strikethrough.py:7:def tokenize(state: StateInline, silent: bool) -> bool:
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/strikethrough.py:8:    """Insert each marker as a separate text token, and add it to delimiter list"""
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/strikethrough.py:25:        token = state.push("text", "", 0)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/strikethrough.py:26:        token.content = ch
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/strikethrough.py:31:        token = state.push("text", "", 0)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/strikethrough.py:32:        token.content = ch + ch
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/strikethrough.py:37:                token=len(state.tokens) - 1,
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/strikethrough.py:69:        token = state.tokens[startDelim.token]
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/strikethrough.py:70:        token.type = "s_open"
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/strikethrough.py:71:        token.tag = "s"
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/strikethrough.py:72:        token.nesting = 1
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/strikethrough.py:73:        token.markup = "~~"
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/strikethrough.py:74:        token.content = ""
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/strikethrough.py:76:        token = state.tokens[endDelim.token]
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/strikethrough.py:77:        token.type = "s_close"
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/strikethrough.py:78:        token.tag = "s"
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/strikethrough.py:79:        token.nesting = -1
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/strikethrough.py:80:        token.markup = "~~"
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/strikethrough.py:81:        token.content = ""
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/strikethrough.py:84:            state.tokens[endDelim.token - 1].type == "text"
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/strikethrough.py:85:            and state.tokens[endDelim.token - 1].content == "~"
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/strikethrough.py:87:            loneMarkers.append(endDelim.token - 1)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/strikethrough.py:101:        while (j < len(state.tokens)) and (state.tokens[j].type == "s_close"):
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/strikethrough.py:107:            token = state.tokens[j]
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/strikethrough.py:108:            state.tokens[j] = state.tokens[i]
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/strikethrough.py:109:            state.tokens[i] = token
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/strikethrough.py:113:    """Walk through delimiter list and replace text tokens with tags."""
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/strikethrough.py:114:    tokens_meta = state.tokens_meta
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/strikethrough.py:115:    maximum = len(state.tokens_meta)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_inline/strikethrough.py:121:            curr_meta = tokens_meta[curr]
./.venv_tmp/lib/python3.12/site-packages/markdown_it/renderer.py:4:Generates HTML from parsed token stream. Each instance has independent
./.venv_tmp/lib/python3.12/site-packages/markdown_it/renderer.py:6:rules if you create plugin and adds new token types.
./.venv_tmp/lib/python3.12/site-packages/markdown_it/renderer.py:16:from .token import Token
./.venv_tmp/lib/python3.12/site-packages/markdown_it/renderer.py:23:    def render(self, tokens: Sequence[Token], options: OptionsDict, env: EnvType) -> Any: ...
./.venv_tmp/lib/python3.12/site-packages/markdown_it/renderer.py:27:    """Contains render rules for tokens. Can be updated and extended.
./.venv_tmp/lib/python3.12/site-packages/markdown_it/renderer.py:36:            def token_type_name(self, tokens, idx, options, env) {
./.venv_tmp/lib/python3.12/site-packages/markdown_it/renderer.py:43:            def strong_open(self, tokens, idx, options, env):
./.venv_tmp/lib/python3.12/site-packages/markdown_it/renderer.py:45:            def strong_close(self, tokens, idx, options, env):
./.venv_tmp/lib/python3.12/site-packages/markdown_it/renderer.py:65:    def render(self, tokens: Sequence[Token], options: OptionsDict, env: EnvType) -> str:
./.venv_tmp/lib/python3.12/site-packages/markdown_it/renderer.py:66:        """Takes token stream and generates HTML.
./.venv_tmp/lib/python3.12/site-packages/markdown_it/renderer.py:68:        :param tokens: list on block tokens to render
./.venv_tmp/lib/python3.12/site-packages/markdown_it/renderer.py:75:        for i, token in enumerate(tokens):
./.venv_tmp/lib/python3.12/site-packages/markdown_it/renderer.py:76:            if token.type == "inline":
./.venv_tmp/lib/python3.12/site-packages/markdown_it/renderer.py:77:                if token.children:
./.venv_tmp/lib/python3.12/site-packages/markdown_it/renderer.py:78:                    result += self.renderInline(token.children, options, env)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/renderer.py:79:            elif token.type in self.rules:
./.venv_tmp/lib/python3.12/site-packages/markdown_it/renderer.py:80:                result += self.rules[token.type](tokens, i, options, env)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/renderer.py:82:                result += self.renderToken(tokens, i, options, env)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/renderer.py:86:    def renderInline(self, tokens: Sequence[Token], options: OptionsDict, env: EnvType) -> str:
./.venv_tmp/lib/python3.12/site-packages/markdown_it/renderer.py:87:        """The same as ``render``, but for single token of `inline` type.
./.venv_tmp/lib/python3.12/site-packages/markdown_it/renderer.py:89:        :param tokens: list on block tokens to render
./.venv_tmp/lib/python3.12/site-packages/markdown_it/renderer.py:95:        for i, token in enumerate(tokens):
./.venv_tmp/lib/python3.12/site-packages/markdown_it/renderer.py:96:            if token.type in self.rules:
./.venv_tmp/lib/python3.12/site-packages/markdown_it/renderer.py:97:                result += self.rules[token.type](tokens, i, options, env)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/renderer.py:99:                result += self.renderToken(tokens, i, options, env)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/renderer.py:103:    def renderToken(
./.venv_tmp/lib/python3.12/site-packages/markdown_it/renderer.py:105:        tokens: Sequence[Token],
./.venv_tmp/lib/python3.12/site-packages/markdown_it/renderer.py:110:        """Default token renderer.
./.venv_tmp/lib/python3.12/site-packages/markdown_it/renderer.py:114:        :param idx: token index to render
./.venv_tmp/lib/python3.12/site-packages/markdown_it/renderer.py:119:        token = tokens[idx]
./.venv_tmp/lib/python3.12/site-packages/markdown_it/renderer.py:122:        if token.hidden:
./.venv_tmp/lib/python3.12/site-packages/markdown_it/renderer.py:132:        if token.block and token.nesting != -1 and idx and tokens[idx - 1].hidden:
./.venv_tmp/lib/python3.12/site-packages/markdown_it/renderer.py:135:        # Add token name, e.g. `<img`
./.venv_tmp/lib/python3.12/site-packages/markdown_it/renderer.py:136:        result += ("</" if token.nesting == -1 else "<") + token.tag
./.venv_tmp/lib/python3.12/site-packages/markdown_it/renderer.py:139:        result += self.renderAttrs(token)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/renderer.py:142:        if token.nesting == 0 and options["xhtmlOut"]:
./.venv_tmp/lib/python3.12/site-packages/markdown_it/renderer.py:146:        if token.block:
./.venv_tmp/lib/python3.12/site-packages/markdown_it/renderer.py:149:            if token.nesting == 1 and (idx + 1 < len(tokens)):
./.venv_tmp/lib/python3.12/site-packages/markdown_it/renderer.py:150:                nextToken = tokens[idx + 1]
./.venv_tmp/lib/python3.12/site-packages/markdown_it/renderer.py:152:                if nextToken.type == "inline" or nextToken.hidden:
./.venv_tmp/lib/python3.12/site-packages/markdown_it/renderer.py:157:                elif nextToken.nesting == -1 and nextToken.tag == token.tag:
./.venv_tmp/lib/python3.12/site-packages/markdown_it/renderer.py:167:    def renderAttrs(token: Token) -> str:
./.venv_tmp/lib/python3.12/site-packages/markdown_it/renderer.py:168:        """Render token attributes to string."""
./.venv_tmp/lib/python3.12/site-packages/markdown_it/renderer.py:171:        for key, value in token.attrItems():
./.venv_tmp/lib/python3.12/site-packages/markdown_it/renderer.py:178:        tokens: Sequence[Token] | None,
./.venv_tmp/lib/python3.12/site-packages/markdown_it/renderer.py:187:        :param tokens: list on block tokens to render
./.venv_tmp/lib/python3.12/site-packages/markdown_it/renderer.py:193:        for token in tokens or []:
./.venv_tmp/lib/python3.12/site-packages/markdown_it/renderer.py:194:            if token.type == "text":
./.venv_tmp/lib/python3.12/site-packages/markdown_it/renderer.py:195:                result += token.content
./.venv_tmp/lib/python3.12/site-packages/markdown_it/renderer.py:196:            elif token.type == "image":
./.venv_tmp/lib/python3.12/site-packages/markdown_it/renderer.py:197:                if token.children:
./.venv_tmp/lib/python3.12/site-packages/markdown_it/renderer.py:198:                    result += self.renderInlineAsText(token.children, options, env)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/renderer.py:199:            elif token.type == "softbreak":
./.venv_tmp/lib/python3.12/site-packages/markdown_it/renderer.py:207:        self, tokens: Sequence[Token], idx: int, options: OptionsDict, env: EnvType
./.venv_tmp/lib/python3.12/site-packages/markdown_it/renderer.py:209:        token = tokens[idx]
./.venv_tmp/lib/python3.12/site-packages/markdown_it/renderer.py:210:        return "<code" + self.renderAttrs(token) + ">" + escapeHtml(tokens[idx].content) + "</code>"
./.venv_tmp/lib/python3.12/site-packages/markdown_it/renderer.py:214:        tokens: Sequence[Token],
./.venv_tmp/lib/python3.12/site-packages/markdown_it/renderer.py:219:        token = tokens[idx]
./.venv_tmp/lib/python3.12/site-packages/markdown_it/renderer.py:223:            + self.renderAttrs(token)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/renderer.py:225:            + escapeHtml(tokens[idx].content)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/renderer.py:231:        tokens: Sequence[Token],
./.venv_tmp/lib/python3.12/site-packages/markdown_it/renderer.py:236:        token = tokens[idx]
./.venv_tmp/lib/python3.12/site-packages/markdown_it/renderer.py:237:        info = unescapeAll(token.info).strip() if token.info else ""
./.venv_tmp/lib/python3.12/site-packages/markdown_it/renderer.py:248:            highlighted = options.highlight(token.content, langName, langAttrs) or escapeHtml(
./.venv_tmp/lib/python3.12/site-packages/markdown_it/renderer.py:249:                token.content
./.venv_tmp/lib/python3.12/site-packages/markdown_it/renderer.py:252:            highlighted = escapeHtml(token.content)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/renderer.py:257:        # If language exists, inject class gently, without modifying original token.
./.venv_tmp/lib/python3.12/site-packages/markdown_it/renderer.py:258:        # May be, one day we will add .deepClone() for token and simplify this part, but
./.venv_tmp/lib/python3.12/site-packages/markdown_it/renderer.py:261:            # Fake token just to render attributes
./.venv_tmp/lib/python3.12/site-packages/markdown_it/renderer.py:262:            tmpToken = Token(type="", tag="", nesting=0, attrs=token.attrs.copy())
./.venv_tmp/lib/python3.12/site-packages/markdown_it/renderer.py:263:            tmpToken.attrJoin("class", options.langPrefix + langName)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/renderer.py:265:            return "<pre><code" + self.renderAttrs(tmpToken) + ">" + highlighted + "</code></pre>\n"
./.venv_tmp/lib/python3.12/site-packages/markdown_it/renderer.py:267:        return "<pre><code" + self.renderAttrs(token) + ">" + highlighted + "</code></pre>\n"
./.venv_tmp/lib/python3.12/site-packages/markdown_it/renderer.py:271:        tokens: Sequence[Token],
./.venv_tmp/lib/python3.12/site-packages/markdown_it/renderer.py:276:        token = tokens[idx]
./.venv_tmp/lib/python3.12/site-packages/markdown_it/renderer.py:280:        if token.children:
./.venv_tmp/lib/python3.12/site-packages/markdown_it/renderer.py:281:            token.attrSet("alt", self.renderInlineAsText(token.children, options, env))
./.venv_tmp/lib/python3.12/site-packages/markdown_it/renderer.py:283:            token.attrSet("alt", "")
./.venv_tmp/lib/python3.12/site-packages/markdown_it/renderer.py:285:        return self.renderToken(tokens, idx, options, env)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/renderer.py:288:        self, tokens: Sequence[Token], idx: int, options: OptionsDict, env: EnvType
./.venv_tmp/lib/python3.12/site-packages/markdown_it/renderer.py:293:        self, tokens: Sequence[Token], idx: int, options: OptionsDict, env: EnvType
./.venv_tmp/lib/python3.12/site-packages/markdown_it/renderer.py:297:    def text(self, tokens: Sequence[Token], idx: int, options: OptionsDict, env: EnvType) -> str:
./.venv_tmp/lib/python3.12/site-packages/markdown_it/renderer.py:298:        return escapeHtml(tokens[idx].content)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/renderer.py:301:        self, tokens: Sequence[Token], idx: int, options: OptionsDict, env: EnvType
./.venv_tmp/lib/python3.12/site-packages/markdown_it/renderer.py:303:        return tokens[idx].content
./.venv_tmp/lib/python3.12/site-packages/markdown_it/renderer.py:306:        self, tokens: Sequence[Token], idx: int, options: OptionsDict, env: EnvType
./.venv_tmp/lib/python3.12/site-packages/markdown_it/renderer.py:308:        return tokens[idx].content
./.venv_tmp/lib/python3.12/site-packages/markdown_it/parser_block.py:1:"""Block-level tokenizer."""
./.venv_tmp/lib/python3.12/site-packages/markdown_it/parser_block.py:12:from .token import Token
./.venv_tmp/lib/python3.12/site-packages/markdown_it/parser_block.py:24:`silent` disables token generation, useful for lookahead.
./.venv_tmp/lib/python3.12/site-packages/markdown_it/parser_block.py:60:    def tokenize(self, state: StateBlock, startLine: int, endLine: int) -> None:
./.venv_tmp/lib/python3.12/site-packages/markdown_it/parser_block.py:61:        """Generate tokens for input range."""
./.venv_tmp/lib/python3.12/site-packages/markdown_it/parser_block.py:84:            # - update `state.tokens`
./.venv_tmp/lib/python3.12/site-packages/markdown_it/parser_block.py:106:        self, src: str, md: MarkdownIt, env: EnvType, outTokens: list[Token]
./.venv_tmp/lib/python3.12/site-packages/markdown_it/parser_block.py:107:    ) -> list[Token] | None:
./.venv_tmp/lib/python3.12/site-packages/markdown_it/parser_block.py:108:        """Process input string and push block tokens into `outTokens`."""
./.venv_tmp/lib/python3.12/site-packages/markdown_it/parser_block.py:111:        state = StateBlock(src, md, env, outTokens)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/parser_block.py:112:        self.tokenize(state, state.line, state.lineMax)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/parser_block.py:113:        return state.tokens
./.venv_tmp/lib/python3.12/site-packages/markdown_it/helpers/parse_link_label.py:29:        state.md.inline.skipToken(state)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/helpers/parse_link_label.py:33:                # which is not a part of any token
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/block.py:1:from ..token import Token
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/block.py:7:        token = Token("inline", "", 0)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/block.py:8:        token.content = state.src
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/block.py:9:        token.map = [0, 1]
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/block.py:10:        token.children = []
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/block.py:11:        state.tokens.append(token)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/block.py:13:        state.md.block.parse(state.src, state.md, state.env, state.tokens)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/text_join.py:1:"""Join raw text tokens with the rest of the text
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/text_join.py:11:from ..token import Token
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/text_join.py:16:    """Join raw text for escape sequences (`text_special`) tokens with the rest of the text"""
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/text_join.py:18:    for inline_token in state.tokens[:]:
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/text_join.py:19:        if inline_token.type != "inline":
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/text_join.py:23:        new_tokens: list[Token] = []
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/text_join.py:24:        for child_token in inline_token.children or []:
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/text_join.py:25:            if child_token.type == "text_special":
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/text_join.py:26:                child_token.type = "text"
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/text_join.py:27:            if child_token.type == "text" and new_tokens and new_tokens[-1].type == "text":
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/text_join.py:28:                new_tokens[-1].content += child_token.content
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/text_join.py:30:                new_tokens.append(child_token)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/text_join.py:31:        inline_token.children = new_tokens
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/linkify.py:7:from ..token import Token
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/linkify.py:23:    for inline_token in state.tokens:
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/linkify.py:24:        if inline_token.type != "inline" or not state.md.linkify.pretest(inline_token.content):
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/linkify.py:27:        tokens = inline_token.children
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/linkify.py:33:        assert tokens is not None
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/linkify.py:34:        i = len(tokens)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/linkify.py:37:            assert isinstance(tokens, list)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/linkify.py:38:            currentToken = tokens[i]
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/linkify.py:41:            if currentToken.type == "link_close":
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/linkify.py:43:                while tokens[i].level != currentToken.level and tokens[i].type != "link_open":
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/linkify.py:48:            if currentToken.type == "html_inline":
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/linkify.py:49:                if isLinkOpen(currentToken.content) and htmlLinkLevel > 0:
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/linkify.py:51:                if isLinkClose(currentToken.content):
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/linkify.py:56:            if currentToken.type == "text" and state.md.linkify.test(currentToken.content):
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/linkify.py:57:                text = currentToken.content
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/linkify.py:62:                level = currentToken.level
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/linkify.py:68:                if links and links[0].index == 0 and i > 0 and tokens[i - 1].type == "text_special":
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/linkify.py:92:                        token = Token("text", "", 0)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/linkify.py:93:                        token.content = text[lastPos:pos]
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/linkify.py:94:                        token.level = level
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/linkify.py:95:                        nodes.append(token)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/linkify.py:97:                    token = Token("link_open", "a", 1)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/linkify.py:98:                    token.attrs = {"href": fullUrl}
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/linkify.py:99:                    token.level = level
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/linkify.py:101:                    token.markup = "linkify"
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/linkify.py:102:                    token.info = "auto"
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/linkify.py:103:                    nodes.append(token)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/linkify.py:105:                    token = Token("text", "", 0)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/linkify.py:106:                    token.content = urlText
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/linkify.py:107:                    token.level = level
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/linkify.py:108:                    nodes.append(token)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/linkify.py:110:                    token = Token("link_close", "a", -1)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/linkify.py:112:                    token.level = level
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/linkify.py:113:                    token.markup = "linkify"
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/linkify.py:114:                    token.info = "auto"
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/linkify.py:115:                    nodes.append(token)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/linkify.py:120:                    token = Token("text", "", 0)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/linkify.py:121:                    token.content = text[lastPos:]
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/linkify.py:122:                    token.level = level
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/linkify.py:123:                    nodes.append(token)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/linkify.py:125:                inline_token.children = tokens = arrayReplaceAt(tokens, i, nodes)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/replacements.py:22:from ..token import Token
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/replacements.py:63:def replace_scoped(inlineTokens: list[Token]) -> None:
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/replacements.py:66:    for token in inlineTokens:
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/replacements.py:67:        if token.type == "text" and not inside_autolink:
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/replacements.py:68:            token.content = SCOPED_ABBR_RE.sub(replaceFn, token.content)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/replacements.py:70:        if token.type == "link_open" and token.info == "auto":
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/replacements.py:73:        if token.type == "link_close" and token.info == "auto":
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/replacements.py:77:def replace_rare(inlineTokens: list[Token]) -> None:
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/replacements.py:80:    for token in inlineTokens:
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/replacements.py:81:        if token.type == "text" and (not inside_autolink) and RARE_RE.search(token.content):
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/replacements.py:83:            token.content = PLUS_MINUS_RE.sub("Â±", token.content)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/replacements.py:86:            token.content = ELLIPSIS_RE.sub("â€¦", token.content)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/replacements.py:89:            token.content = ELLIPSIS_QUESTION_EXCLAMATION_RE.sub("\\1..", token.content)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/replacements.py:90:            token.content = QUESTION_EXCLAMATION_RE.sub("\\1\\1\\1", token.content)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/replacements.py:93:            token.content = COMMA_RE.sub(",", token.content)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/replacements.py:96:            token.content = EM_DASH_RE.sub("\\1\u2014", token.content)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/replacements.py:99:            token.content = EN_DASH_RE.sub("\\1\u2013", token.content)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/replacements.py:100:            token.content = EN_DASH_INDENT_RE.sub("\\1\u2013", token.content)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/replacements.py:102:        if token.type == "link_open" and token.info == "auto":
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/replacements.py:105:        if token.type == "link_close" and token.info == "auto":
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/replacements.py:113:    for token in state.tokens:
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/replacements.py:114:        if token.type != "inline":
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/replacements.py:116:        if token.children is None:
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/replacements.py:119:        if SCOPED_ABBR_RE.search(token.content):
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/replacements.py:120:            replace_scoped(token.children)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/replacements.py:122:        if RARE_RE.search(token.content):
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/replacements.py:123:            replace_rare(token.children)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/inline.py:6:    for token in state.tokens:
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/inline.py:7:        if token.type == "inline":
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/inline.py:8:            if token.children is None:
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/inline.py:9:                token.children = []
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/inline.py:10:            state.md.inline.parse(token.content, state.md, state.env, token.children)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/smartquotes.py:9:from ..token import Token
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/smartquotes.py:24:def process_inlines(tokens: list[Token], state: StateCore) -> None:
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/smartquotes.py:27:    for i, token in enumerate(tokens):
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/smartquotes.py:28:        thisLevel = token.level
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/smartquotes.py:41:        if token.type != "text":
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/smartquotes.py:44:        text = token.content
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/smartquotes.py:67:                    if tokens[j].type == "softbreak" or tokens[j].type == "hardbreak":
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/smartquotes.py:69:                    # should skip all tokens except 'text', 'html_inline' or 'code_inline'
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/smartquotes.py:70:                    if not tokens[j].content:
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/smartquotes.py:73:                    lastChar = charCodeAt(tokens[j].content, len(tokens[j].content) - 1)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/smartquotes.py:83:                for j in range(i + 1, len(tokens)):
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/smartquotes.py:85:                    if tokens[j].type == "softbreak" or tokens[j].type == "hardbreak":
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/smartquotes.py:87:                    # should skip all tokens except 'text', 'html_inline' or 'code_inline'
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/smartquotes.py:88:                    if not tokens[j].content:
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/smartquotes.py:91:                    nextChar = charCodeAt(tokens[j].content, 0)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/smartquotes.py:134:                    token.content = replaceAt(token.content, t.start(0) + lastIndex, APOSTROPHE)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/smartquotes.py:153:                        # replace token.content *before* tokens[item.token].content,
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/smartquotes.py:154:                        # because, if they are pointing at the same token, replaceAt
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/smartquotes.py:156:                        token.content = replaceAt(token.content, t.start(0) + lastIndex, closeQuote)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/smartquotes.py:157:                        tokens[item["token"]].content = replaceAt(
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/smartquotes.py:158:                            tokens[item["token"]].content, item["pos"], openQuote
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/smartquotes.py:162:                        if item["token"] == i:
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/smartquotes.py:165:                        text = token.content
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/smartquotes.py:178:                        "token": i,
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/smartquotes.py:185:                token.content = replaceAt(token.content, t.start(0) + lastIndex, APOSTROPHE)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/smartquotes.py:192:    for token in state.tokens:
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/smartquotes.py:193:        if token.type != "inline" or not QUOTE_RE.search(token.content):
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/smartquotes.py:195:        if token.children is not None:
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/smartquotes.py:196:            process_inlines(token.children, state)
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/state_core.py:6:from ..token import Token
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/state_core.py:19:        tokens: list[Token] | None = None,
./.venv_tmp/lib/python3.12/site-packages/markdown_it/rules_core/state_core.py:24:        self.tokens: list[Token] = tokens or []
./.venv_tmp/lib/python3.12/site-packages/markdown_it/common/utils.py:49:    Useful for some operations with tokens
./.venv_tmp/lib/python3.12/site-packages/pip_requirements_parser.py:1464:    tokens = line.split(" ")
./.venv_tmp/lib/python3.12/site-packages/pip_requirements_parser.py:1466:    options = tokens[:]
./.venv_tmp/lib/python3.12/site-packages/pip_requirements_parser.py:1467:    for token in tokens:
./.venv_tmp/lib/python3.12/site-packages/pip_requirements_parser.py:1468:        if token.startswith("-") or token.startswith("--"):
./.venv_tmp/lib/python3.12/site-packages/pip_requirements_parser.py:1471:            args.append(token)
./.venv_tmp/lib/python3.12/site-packages/pip_requirements_parser.py:1751:            # includes a username and password.
./.venv_tmp/lib/python3.12/site-packages/pip_requirements_parser.py:2402:    Returns: (netloc, (username, password)).
./.venv_tmp/lib/python3.12/site-packages/pip_requirements_parser.py:2409:    # the password attribute of urlsplit()'s return value).
./.venv_tmp/lib/python3.12/site-packages/pip_requirements_parser.py:2415:        # using the password attribute of the return value)
./.venv_tmp/lib/python3.12/site-packages/click/core.py:216:    :param token_normalize_func: an optional function that is used to
./.venv_tmp/lib/python3.12/site-packages/click/core.py:217:                                 normalize tokens (options, choices,
./.venv_tmp/lib/python3.12/site-packages/click/core.py:232:        Click 9.0. ``args`` will contain remaining unparsed tokens.
./.venv_tmp/lib/python3.12/site-packages/click/core.py:255:        ``token_normalize_func`` parameters.
./.venv_tmp/lib/python3.12/site-packages/click/core.py:278:        token_normalize_func: t.Callable[[str], str] | None = None,
./.venv_tmp/lib/python3.12/site-packages/click/core.py:384:        if token_normalize_func is None and parent is not None:
./.venv_tmp/lib/python3.12/site-packages/click/core.py:385:            token_normalize_func = parent.token_normalize_func
./.venv_tmp/lib/python3.12/site-packages/click/core.py:387:        #: An optional normalization function for tokens.  This is
./.venv_tmp/lib/python3.12/site-packages/click/core.py:389:        self.token_normalize_func: t.Callable[[str], str] | None = token_normalize_func
./.venv_tmp/lib/python3.12/site-packages/click/core.py:437:            " 'args' will contain remaining unparsed tokens.",
./.venv_tmp/lib/python3.12/site-packages/click/core.py:1869:        if cmd is None and ctx.token_normalize_func is not None:
./.venv_tmp/lib/python3.12/site-packages/click/core.py:1870:            cmd_name = ctx.token_normalize_func(cmd_name)
./.venv_tmp/lib/python3.12/site-packages/click/core.py:2569:        will be hidden from the user. This is useful for password input.
./.venv_tmp/lib/python3.12/site-packages/click/parser.py:117:    if ctx is None or ctx.token_normalize_func is None:
./.venv_tmp/lib/python3.12/site-packages/click/parser.py:120:    return f"{prefix}{ctx.token_normalize_func(opt)}"
./.venv_tmp/lib/python3.12/site-packages/click/decorators.py:399:def password_option(*param_decls: str, **kwargs: t.Any) -> t.Callable[[FC], FC]:
./.venv_tmp/lib/python3.12/site-packages/click/decorators.py:400:    """Add a ``--password`` option which prompts for a password, hiding
./.venv_tmp/lib/python3.12/site-packages/click/decorators.py:404:        value ``"--password"``.
./.venv_tmp/lib/python3.12/site-packages/click/decorators.py:408:        param_decls = ("--password",)
./.venv_tmp/lib/python3.12/site-packages/click/types.py:276:        By default uses :meth:`Context.token_normalize_func` and if not case
./.venv_tmp/lib/python3.12/site-packages/click/types.py:283:        if ctx is not None and ctx.token_normalize_func is not None:
./.venv_tmp/lib/python3.12/site-packages/click/types.py:284:            normed_value = ctx.token_normalize_func(normed_value)
./.venv_tmp/lib/python3.12/site-packages/click/shell_completion.py:458:    incomplete escape sequence and uses the partial token as-is.
./.venv_tmp/lib/python3.12/site-packages/click/shell_completion.py:481:        for token in lex:
./.venv_tmp/lib/python3.12/site-packages/click/shell_completion.py:482:            out.append(token)
./.venv_tmp/lib/python3.12/site-packages/click/shell_completion.py:485:        # the partial token as-is. The quote or escape character is in
./.venv_tmp/lib/python3.12/site-packages/click/shell_completion.py:486:        # lex.state, not lex.token.
./.venv_tmp/lib/python3.12/site-packages/click/shell_completion.py:487:        out.append(lex.token)
./.venv_tmp/lib/python3.12/site-packages/click/__init__.py:29:    password_option as password_option,
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:43:    PARSE_UNKNOWN_TOKEN,
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:44:    TOKEN_AND,
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:45:    TOKEN_LPAR,
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:46:    TOKEN_OR,
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:47:    TOKEN_RPAR,
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:48:    TOKEN_SYMBOL,
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:51:from license_expression._pyahocorasick import Token, Trie as AdvancedTokenizer
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:91:# Used for tokenizing
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:95:# id for the "WITH" token which is not a proper boolean symbol but an expression
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:97:TOKEN_WITH = 10
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:101:KW_LPAR = Keyword("(", TOKEN_LPAR)
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:102:KW_RPAR = Keyword(")", TOKEN_RPAR)
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:103:KW_AND = Keyword("and", TOKEN_AND)
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:104:KW_OR = Keyword("or", TOKEN_OR)
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:105:KW_WITH = Keyword("with", TOKEN_WITH)
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:119:_simple_tokenizer = re.compile(
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:293:        # Aho-Corasick automaton-based Advanced Tokenizer
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:294:        self.advanced_tokenizer = None
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:500:        If ``simple`` is True, parsing will use a simple tokenizer that assumes
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:530:            tokens = list(
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:531:                self.tokenize(
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:537:            expression = super(Licensing, self).parse(tokens)
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:541:                token_type=e.token_type,
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:542:                token_string=e.token_string,
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:555:    def tokenize(self, expression, strict=False, simple=False):
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:557:        Return an iterable of 3-tuple describing each token given an
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:558:        ``expression`` string. See boolean.BooleanAlgreba.tokenize() for API
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:561:        This 3-tuple contains these items: (token, token string, position):
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:562:        - token: either a Symbol instance or one of TOKEN_* token types..
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:563:        - token string: the original token string.
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:564:        - position: the starting index of the token string in the `expr` string.
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:570:        If ``simple`` is True, use a simple tokenizer that assumes that license
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:580:            tokens = self.simple_tokenizer(expression)
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:582:            advanced_tokenizer = self.get_advanced_tokenizer()
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:583:            tokens = advanced_tokenizer.tokenize(expression)
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:585:        # Assign symbol for unknown tokens
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:586:        tokens = build_symbols_from_unknown_tokens(tokens)
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:588:        # skip whitespace-only tokens
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:589:        tokens = (t for t in tokens if t.string and t.string.strip())
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:592:        tokens = replace_with_subexpression_by_license_symbol(tokens, strict)
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:595:        for token in tokens:
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:596:            pos = token.start
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:597:            token_string = token.string
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:598:            token_value = token.value
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:600:            if isinstance(token_value, BaseSymbol):
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:601:                token_obj = token_value
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:602:            elif isinstance(token_value, Keyword):
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:603:                token_obj = token_value.type
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:607:            yield token_obj, token_string, pos
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:609:    def get_advanced_tokenizer(self):
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:611:        Return an AdvancedTokenizer instance for this Licensing either cached or
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:615:        tokenizer will recognize known symbol keys and aliases (ignoring case)
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:616:        when tokenizing expressions.
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:621:        if self.advanced_tokenizer is not None:
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:622:            return self.advanced_tokenizer
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:624:        self.advanced_tokenizer = tokenizer = AdvancedTokenizer()
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:626:        add_item = tokenizer.add
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:637:                # normalize spaces for each alias. The AdvancedTokenizer will
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:643:        tokenizer.make_automaton()
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:644:        return tokenizer
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:646:    def advanced_tokenizer(self, expression):
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:648:        Return an iterable of Token from an ``expression`` string.
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:650:        tokenizer = self.get_advanced_tokenizer()
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:651:        return tokenizer.tokenize(expression)
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:653:    def simple_tokenizer(self, expression):
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:655:        Return an iterable of Token from an ``expression`` string.
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:658:        symbol token, e.g. a typically license key or license id (that contains
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:662:        tokenizer will recognize known symbol keys (ignoring case) when
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:663:        tokenizing expressions.
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:668:        for match in _simple_tokenizer(expression):
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:678:                yield Token(start, end, space, None)
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:682:                yield Token(start, end, lpar, KW_LPAR)
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:686:                yield Token(start, end, rpar, KW_RPAR)
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:694:                    yield Token(start, end, sym_or_op, operator)
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:699:                    yield Token(start, end, sym_or_op, sym)
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:784:            expression_info.invalid_symbols.append(e.token_string)
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:878:def build_symbols_from_unknown_tokens(tokens):
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:880:    Yield Token given a ``token`` sequence of Token replacing unmatched
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:881:    contiguous tokens by a single token with a LicenseSymbol.
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:883:    tokens = list(tokens)
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:887:    def build_token_with_symbol():
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:889:        Build and return a new Token from accumulated unmatched tokens or None.
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:904:            yield Token(start, end, string, toksym)
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:909:    for tok in tokens:
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:911:            for symtok in build_token_with_symbol():
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:922:    for symtok in build_token_with_symbol():
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:926:def build_token_groups_for_with_subexpression(tokens):
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:928:    Yield tuples of Token given a ``tokens`` sequence of Token such that:
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:929:     - all "XXX WITH YYY" sequences of 3 tokens are grouped in a three-tuple
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:930:     - single tokens are just wrapped in a tuple for consistency.
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:934:    # exp otherwise: yield each single token as a group
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:936:    tokens = list(tokens)
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:938:    # check three contiguous tokens that may form "lic WITh exception" sequence
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:942:    if len(tokens) < triple_len:
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:943:        for tok in tokens:
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:947:    # accumulate three contiguous tokens
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:953:    for tok in tokens:
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:972:def is_with_subexpression(tokens_tripple):
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:974:    Return True if a ``tokens_tripple`` Token tripple is a "WITH" license sub-
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:977:    lic, wit, exc = tokens_tripple
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:985:def replace_with_subexpression_by_license_symbol(tokens, strict=False):
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:987:    Given a ``tokens`` iterable of Token, yield updated Token(s) replacing any
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:996:    token_groups = build_token_groups_for_with_subexpression(tokens)
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:998:    for token_group in token_groups:
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:999:        len_group = len(token_group)
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:1006:            # a single token
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:1007:            token = token_group[0]
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:1008:            tval = token.value
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:1011:                if tval.type == TOKEN_WITH:
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:1016:                        token_type=TOKEN_WITH,
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:1017:                        token_string=token.string,
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:1018:                        position=token.start,
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:1025:                        token_type=TOKEN_SYMBOL,
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:1026:                        token_string=token.string,
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:1027:                        position=token.start,
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:1033:                raise Exception(f"Licensing.tokenize is internally confused...: {tval!r}")
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:1035:            yield token
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:1040:            string = " ".join([tok.string for tok in token_group])
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:1041:            start = token_group[0].start
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:1043:                token_type=TOKEN_SYMBOL,
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:1044:                token_string=string,
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:1049:        # from now on we have a tripple of tokens: a WITH sub-expression such as
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:1050:        # "A with B" seq of three tokens
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:1051:        lic_token, WITH, exc_token = token_group
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:1053:        lic = lic_token.string
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:1054:        exc = exc_token.string
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:1056:        token_string = f"{lic} {WITH} {exc}"
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:1059:        lic_sym = lic_token.value
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:1064:                token_type=TOKEN_SYMBOL,
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:1065:                token_string=lic_token.string,
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:1066:                position=lic_token.start,
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:1072:                token_type=TOKEN_SYMBOL,
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:1073:                token_string=lic_token.string,
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:1074:                position=lic_token.start,
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:1079:        exc_sym = exc_token.value
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:1083:                token_type=TOKEN_SYMBOL,
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:1084:                token_string=lic_sym.string,
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:1091:                token_type=TOKEN_SYMBOL,
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:1092:                token_string=exc_token.string,
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:1093:                position=exc_token.start,
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:1103:        token = Token(
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:1104:            start=lic_token.start,
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:1105:            end=exc_token.end,
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:1106:            string=token_string,
./.venv_tmp/lib/python3.12/site-packages/license_expression/__init__.py:1109:        yield token
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:15:use in the license_expression library for advanced tokenization:
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:55:    __slots__ = ["token", "output", "fail", "children"]
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:57:    def __init__(self, token, output=nil):
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:58:        # token of a tokens string added to the Trie as a string
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:59:        self.token = token
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:75:            return "TrieNode(%r, %r)" % (self.token, self.output)
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:77:            return "TrieNode(%r)" % self.token
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:92:        # set of any unique tokens in the trie, updated on each addition we keep
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:93:        # track of the set of tokens added to the trie to build the automaton
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:95:        self._known_tokens = set()
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:100:    def add(self, tokens_string, value=None):
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:102:        Add a new tokens_string and its associated value to the trie. If the
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:103:        tokens_string already exists in the Trie, its value is replaced with the
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:104:        provided value, typically a Token object. If a value is not provided,
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:105:        the tokens_string is used as value.
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:107:        A tokens_string is any string. It will be tokenized when added
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:115:        if not tokens_string or not isinstance(tokens_string, str):
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:118:        tokens = [t for t in get_tokens(tokens_string) if t.strip()]
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:120:        # we keep track of the set of tokens added to the trie to build the
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:124:        self._known_tokens.update(tokens)
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:127:        for token in tokens:
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:129:                node = node.children[token]
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:131:                child = TrieNode(token)
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:132:                node.children[token] = child
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:135:        node.output = (tokens_string, value or tokens_string)
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:137:    def __get_node(self, tokens_string):
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:139:        Return a node for this tokens_string or None if the trie does not
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:140:        contain the tokens_string. Private function retrieving a final node of
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:141:        the Trie for a given tokens_string.
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:143:        if not tokens_string or not isinstance(tokens_string, str):
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:146:        tokens = [t for t in get_tokens(tokens_string) if t.strip()]
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:148:        for token in tokens:
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:150:                node = node.children[token]
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:155:    def get(self, tokens_string, default=nil):
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:157:        Return the output value found associated with a `tokens_string`. If
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:158:        there is no such tokens_string in the Trie, return the default value
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:162:        node = self.__get_node(tokens_string)
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:169:                raise KeyError(tokens_string)
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:193:        def walk(node, tokens):
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:197:            tokens = [t for t in tokens + [node.token] if t]
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:208:                    walk(child, tokens)
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:210:        walk(self.root, tokens=[])
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:214:    def exists(self, tokens_string):
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:218:        node = self.__get_node(tokens_string)
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:223:    def is_prefix(self, tokens_string):
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:225:        Return True if tokens_string is a prefix of any existing tokens_string in the trie.
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:227:        return bool(self.__get_node(tokens_string) is not None)
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:238:        # characters from all the added tokens), failing to root.
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:240:        for token in self._known_tokens:
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:241:            if token in self.root.children:
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:242:                node = self.root.children[token]
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:247:                self.root.children[token] = self.root
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:256:                while node.token not in state.children:
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:258:                node.fail = state.children.get(node.token, self.root)
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:263:    def iter(self, tokens_string, include_unmatched=False, include_space=False):
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:265:        Yield Token objects for matched strings by performing the Aho-Corasick
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:268:        The Token start and end positions in the searched string are such that
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:269:        the matched string is "tokens_string[start:end+1]". And the start is
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:274:        The Token.value is an object associated with a matched string.
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:284:        >>> tokens_string = 'a bcdef ghij kl m'
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:285:        >>> strings = Token.sort(a.iter(tokens_string))
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:287:        ...     Token(2, 6, u'bcdef', u'BCDEF'),
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:288:        ...     Token(13, 14, u'kl', u'KL')
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:300:        if not tokens_string:
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:303:        tokens = get_tokens(tokens_string)
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:307:            logger_debug("Trie.iter() with:", repr(tokens_string))
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:308:            logger_debug(" tokens:", tokens)
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:311:        for token_string in tokens:
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:312:            end_pos += len(token_string)
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:315:                logger_debug("token_string", repr(token_string))
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:318:            if not include_space and not token_string.strip():
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:323:            if token_string not in self._known_tokens:
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:328:                    n = len(token_string)
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:330:                    tok = Token(
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:333:                        string=tokens_string[start_pos : end_pos + 1],
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:343:            # search for a matching token_string in the children, starting at root
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:344:            while token_string not in state.children:
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:347:            # we have a matching starting token_string
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:348:            state = state.children.get(token_string, self.root)
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:359:                    yield Token(
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:360:                        start_pos, end_pos, tokens_string[start_pos : end_pos + 1], output_value
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:366:                    logger_debug("  unmatched but known token")
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:367:                n = len(token_string)
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:369:                tok = Token(start_pos, end_pos, tokens_string[start_pos : end_pos + 1], None)
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:376:    def tokenize(self, string, include_unmatched=True, include_space=False):
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:378:        Tokenize a string for matched and unmatched sub-sequences and yield non-
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:379:        overlapping Token objects performing a modified Aho-Corasick search
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:393:        Each Token contains the start and end position, the corresponding string
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:405:        >>> tokens = list(a.tokenize(string, include_space=True))
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:408:        ...     Token(0, 0, u'a', None),
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:409:        ...     Token(1, 1, u' ', None),
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:410:        ...     Token(2, 6, u'bcdef', u'BCDEF'),
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:411:        ...     Token(7, 7, u' ', None),
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:412:        ...     Token(8, 11, u'ghij', None),
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:413:        ...     Token(12, 12, u' ', None),
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:414:        ...     Token(13, 14, u'kl', u'KL')
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:416:        >>> tokens == expected
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:419:        tokens = self.iter(string, include_unmatched=include_unmatched, include_space=include_space)
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:420:        tokens = list(tokens)
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:422:            logger_debug("tokenize.tokens:", tokens)
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:424:            tokens = [t for t in tokens if t.string.strip()]
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:425:        tokens = filter_overlapping(tokens)
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:426:        return tokens
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:429:def filter_overlapping(tokens):
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:431:    Return a new list from an iterable of `tokens` discarding contained and
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:432:    overlaping Tokens using these rules:
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:434:    - skip a token fully contained in another token.
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:435:    - keep the biggest, left-most token of two overlapping tokens and skip the other
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:438:    >>> tokens = [
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:439:    ...     Token(0, 0, 'a'),
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:440:    ...     Token(1, 5, 'bcdef'),
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:441:    ...     Token(2, 4, 'cde'),
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:442:    ...     Token(3, 7, 'defgh'),
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:443:    ...     Token(4, 7, 'efgh'),
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:444:    ...     Token(8, 9, 'ij'),
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:445:    ...     Token(10, 13, 'klmn'),
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:446:    ...     Token(11, 15, 'lmnop'),
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:447:    ...     Token(16, 16, 'q'),
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:451:    ...     Token(0, 0, 'a'),
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:452:    ...     Token(1, 5, 'bcdef'),
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:453:    ...     Token(8, 9, 'ij'),
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:454:    ...     Token(11, 15, 'lmnop'),
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:455:    ...     Token(16, 16, 'q'),
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:458:    >>> filtered = list(filter_overlapping(tokens))
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:462:    tokens = Token.sort(tokens)
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:464:    # compare pair of tokens in the sorted sequence: current and next
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:466:    while i < len(tokens) - 1:
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:468:        while j < len(tokens):
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:469:            curr_tok = tokens[i]
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:470:            next_tok = tokens[j]
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:473:            # disjoint tokens: break, there is nothing to do
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:478:            # contained token: discard the contained token
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:481:                del tokens[j]
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:484:            # overlap: Keep the longest token and skip the smallest overlapping
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:485:            # tokens. In case of length tie: keep the left most
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:489:                    del tokens[j]
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:493:                    del tokens[i]
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:497:    return tokens
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:500:class Token(object):
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:502:    A Token is used to track the tokenization an expression with its
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:507:    - `string` is the matched substring from the original string for this Token.
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:508:    - `value` is the corresponding object for this token as one of:
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:539:        return isinstance(other, Token) and (
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:551:    def sort(cls, tokens):
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:553:        Return a new sorted sequence of tokens given a sequence of tokens. The
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:555:        Therefore if two tokens have the same start, the longer token will sort
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:559:        >>> tokens = [Token(0, 0), Token(5, 5), Token(1, 1), Token(2, 4), Token(2, 5)]
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:560:        >>> expected = [Token(0, 0), Token(1, 1), Token(2, 5), Token(2, 4), Token(5, 5)]
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:561:        >>> expected == Token.sort(tokens)
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:571:        return sorted(tokens, key=key)
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:575:        Return True if this token is after the other token.
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:578:        >>> Token(1, 2).is_after(Token(5, 6))
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:580:        >>> Token(5, 6).is_after(Token(5, 6))
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:582:        >>> Token(2, 3).is_after(Token(1, 2))
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:584:        >>> Token(5, 6).is_after(Token(3, 4))
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:594:        Return True if this token contains the other token.
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:597:        >>> Token(5, 7) in Token(5, 7)
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:599:        >>> Token(6, 8) in Token(5, 7)
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:601:        >>> Token(6, 6) in Token(4, 8)
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:603:        >>> Token(3, 9) in Token(4, 8)
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:605:        >>> Token(4, 8) in Token(3, 9)
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:612:        Return True if this token and the other token overlap.
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:615:        >>> Token(1, 2).overlap(Token(5, 6))
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:617:        >>> Token(5, 6).overlap(Token(5, 6))
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:619:        >>> Token(4, 5).overlap(Token(5, 6))
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:621:        >>> Token(4, 5).overlap(Token(5, 7))
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:623:        >>> Token(4, 5).overlap(Token(6, 7))
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:631:# tokenize to separate text from parens
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:632:_tokenizer = re.compile(
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:644:def get_tokens(tokens_string):
./.venv_tmp/lib/python3.12/site-packages/license_expression/_pyahocorasick.py:648:    return [match for match in _tokenizer.split(tokens_string.lower()) if match]
./.venv_tmp/lib/python3.12/site-packages/bandit/plugins/weak_cryptographic_key.py:26:    36  dsa.generate_private_key(512,
./.venv_tmp/lib/python3.12/site-packages/bandit/plugins/weak_cryptographic_key.py:28:    38  rsa.generate_private_key(3,
./.venv_tmp/lib/python3.12/site-packages/bandit/plugins/weak_cryptographic_key.py:90:        "cryptography.hazmat.primitives.asymmetric.dsa." "generate_private_key": "DSA",
./.venv_tmp/lib/python3.12/site-packages/bandit/plugins/weak_cryptographic_key.py:91:        "cryptography.hazmat.primitives.asymmetric.rsa." "generate_private_key": "RSA",
./.venv_tmp/lib/python3.12/site-packages/bandit/plugins/weak_cryptographic_key.py:92:        "cryptography.hazmat.primitives.asymmetric.ec." "generate_private_key": "EC",
./.venv_tmp/lib/python3.12/site-packages/bandit/plugins/huggingface_unsafe_download.py:25:- ``AutoTokenizer.from_pretrained("org/model-name")``
./.venv_tmp/lib/python3.12/site-packages/bandit/plugins/huggingface_unsafe_download.py:26:- ``AutoTokenizer.from_pretrained("org/model-name", revision="main")``
./.venv_tmp/lib/python3.12/site-packages/bandit/plugins/huggingface_unsafe_download.py:27:- ``AutoTokenizer.from_pretrained("org/model-name", revision="v3.3.0")``
./.venv_tmp/lib/python3.12/site-packages/bandit/plugins/trojansource.py:33:from tokenize import detect_encoding
./.venv_tmp/lib/python3.12/site-packages/bandit/plugins/general_hardcoded_password.py:11:RE_WORDS = "(pas+wo?r?d|pass(phrase)?|pwd|token|secrete?)"
./.venv_tmp/lib/python3.12/site-packages/bandit/plugins/general_hardcoded_password.py:19:        cwe=issue.Cwe.HARD_CODED_PASSWORD,
./.venv_tmp/lib/python3.12/site-packages/bandit/plugins/general_hardcoded_password.py:20:        text=f"Possible hardcoded password: '{value}'",
./.venv_tmp/lib/python3.12/site-packages/bandit/plugins/general_hardcoded_password.py:26:def hardcoded_password_string(context):
./.venv_tmp/lib/python3.12/site-packages/bandit/plugins/general_hardcoded_password.py:27:    """**B105: Test for use of hard-coded password strings**
./.venv_tmp/lib/python3.12/site-packages/bandit/plugins/general_hardcoded_password.py:29:    The use of hard-coded passwords increases the possibility of password
./.venv_tmp/lib/python3.12/site-packages/bandit/plugins/general_hardcoded_password.py:33:    - assigned to a variable that looks like a password
./.venv_tmp/lib/python3.12/site-packages/bandit/plugins/general_hardcoded_password.py:34:    - assigned to a dict key that looks like a password
./.venv_tmp/lib/python3.12/site-packages/bandit/plugins/general_hardcoded_password.py:35:    - assigned to a class attribute that looks like a password
./.venv_tmp/lib/python3.12/site-packages/bandit/plugins/general_hardcoded_password.py:36:    - used in a comparison with a variable that looks like a password
./.venv_tmp/lib/python3.12/site-packages/bandit/plugins/general_hardcoded_password.py:38:    Variables are considered to look like a password if they have match any one
./.venv_tmp/lib/python3.12/site-packages/bandit/plugins/general_hardcoded_password.py:41:    - "password"
./.venv_tmp/lib/python3.12/site-packages/bandit/plugins/general_hardcoded_password.py:45:    - "secret"
./.venv_tmp/lib/python3.12/site-packages/bandit/plugins/general_hardcoded_password.py:46:    - "token"
./.venv_tmp/lib/python3.12/site-packages/bandit/plugins/general_hardcoded_password.py:47:    - "secrete"
./.venv_tmp/lib/python3.12/site-packages/bandit/plugins/general_hardcoded_password.py:59:        >> Issue: Possible hardcoded password '(root)'
./.venv_tmp/lib/python3.12/site-packages/bandit/plugins/general_hardcoded_password.py:62:           Location: ./examples/hardcoded-passwords.py:5
./.venv_tmp/lib/python3.12/site-packages/bandit/plugins/general_hardcoded_password.py:63:        4 def someFunction2(password):
./.venv_tmp/lib/python3.12/site-packages/bandit/plugins/general_hardcoded_password.py:64:        5     if password == "root":
./.venv_tmp/lib/python3.12/site-packages/bandit/plugins/general_hardcoded_password.py:69:        - https://www.owasp.org/index.php/Use_of_hard-coded_password
./.venv_tmp/lib/python3.12/site-packages/bandit/plugins/general_hardcoded_password.py:116:def hardcoded_password_funcarg(context):
./.venv_tmp/lib/python3.12/site-packages/bandit/plugins/general_hardcoded_password.py:117:    """**B106: Test for use of hard-coded password function arguments**
./.venv_tmp/lib/python3.12/site-packages/bandit/plugins/general_hardcoded_password.py:119:    The use of hard-coded passwords increases the possibility of password
./.venv_tmp/lib/python3.12/site-packages/bandit/plugins/general_hardcoded_password.py:122:    assigned local variable does not look like a password.
./.venv_tmp/lib/python3.12/site-packages/bandit/plugins/general_hardcoded_password.py:124:    Variables are considered to look like a password if they have match any one
./.venv_tmp/lib/python3.12/site-packages/bandit/plugins/general_hardcoded_password.py:127:    - "password"
./.venv_tmp/lib/python3.12/site-packages/bandit/plugins/general_hardcoded_password.py:131:    - "secret"
./.venv_tmp/lib/python3.12/site-packages/bandit/plugins/general_hardcoded_password.py:132:    - "token"
./.venv_tmp/lib/python3.12/site-packages/bandit/plugins/general_hardcoded_password.py:133:    - "secrete"
./.venv_tmp/lib/python3.12/site-packages/bandit/plugins/general_hardcoded_password.py:145:        >> Issue: [B106:hardcoded_password_funcarg] Possible hardcoded
./.venv_tmp/lib/python3.12/site-packages/bandit/plugins/general_hardcoded_password.py:146:        password: 'blerg'
./.venv_tmp/lib/python3.12/site-packages/bandit/plugins/general_hardcoded_password.py:149:           Location: ./examples/hardcoded-passwords.py:16
./.venv_tmp/lib/python3.12/site-packages/bandit/plugins/general_hardcoded_password.py:151:        16    doLogin(password="blerg")
./.venv_tmp/lib/python3.12/site-packages/bandit/plugins/general_hardcoded_password.py:155:        - https://www.owasp.org/index.php/Use_of_hard-coded_password
./.venv_tmp/lib/python3.12/site-packages/bandit/plugins/general_hardcoded_password.py:172:def hardcoded_password_default(context):
./.venv_tmp/lib/python3.12/site-packages/bandit/plugins/general_hardcoded_password.py:173:    """**B107: Test for use of hard-coded password argument defaults**
./.venv_tmp/lib/python3.12/site-packages/bandit/plugins/general_hardcoded_password.py:175:    The use of hard-coded passwords increases the possibility of password
./.venv_tmp/lib/python3.12/site-packages/bandit/plugins/general_hardcoded_password.py:178:    the argument does not look like a password.
./.venv_tmp/lib/python3.12/site-packages/bandit/plugins/general_hardcoded_password.py:180:    Variables are considered to look like a password if they have match any one
./.venv_tmp/lib/python3.12/site-packages/bandit/plugins/general_hardcoded_password.py:183:    - "password"
./.venv_tmp/lib/python3.12/site-packages/bandit/plugins/general_hardcoded_password.py:187:    - "secret"
./.venv_tmp/lib/python3.12/site-packages/bandit/plugins/general_hardcoded_password.py:188:    - "token"
./.venv_tmp/lib/python3.12/site-packages/bandit/plugins/general_hardcoded_password.py:189:    - "secrete"
./.venv_tmp/lib/python3.12/site-packages/bandit/plugins/general_hardcoded_password.py:203:        >> Issue: [B107:hardcoded_password_default] Possible hardcoded
./.venv_tmp/lib/python3.12/site-packages/bandit/plugins/general_hardcoded_password.py:204:        password: 'Admin'
./.venv_tmp/lib/python3.12/site-packages/bandit/plugins/general_hardcoded_password.py:207:           Location: ./examples/hardcoded-passwords.py:1
./.venv_tmp/lib/python3.12/site-packages/bandit/plugins/general_hardcoded_password.py:209:        1    def someFunction(user, password="Admin"):
./.venv_tmp/lib/python3.12/site-packages/bandit/plugins/general_hardcoded_password.py:214:        - https://www.owasp.org/index.php/Use_of_hard-coded_password
./.venv_tmp/lib/python3.12/site-packages/bandit/blacklists/calls.py:193:purposes. Consider using the secrets module instead:
./.venv_tmp/lib/python3.12/site-packages/bandit/blacklists/calls.py:194:https://docs.python.org/library/secrets.html
./.venv_tmp/lib/python3.12/site-packages/bandit/core/issue.py:20:    HARD_CODED_PASSWORD = 259
./.venv_tmp/lib/python3.12/site-packages/bandit/core/manager.py:13:import tokenize
./.venv_tmp/lib/python3.12/site-packages/bandit/core/manager.py:301:                tokens = tokenize.tokenize(fdata.readline)
./.venv_tmp/lib/python3.12/site-packages/bandit/core/manager.py:304:                    for toktype, tokval, (lineno, _), _, _ in tokens:
./.venv_tmp/lib/python3.12/site-packages/bandit/core/manager.py:305:                        if toktype == tokenize.COMMENT:
./.venv_tmp/lib/python3.12/site-packages/bandit/core/manager.py:308:            except tokenize.TokenError:
./.venv_tmp/lib/python3.12/site-packages/pytokens/__main__.py:1:"""Support executing the CLI by doing `python -m pytokens`."""
./.venv_tmp/lib/python3.12/site-packages/pytokens/__main__.py:5:from pytokens.cli import cli
./.venv_tmp/lib/python3.12/site-packages/pytokens/cli.py:1:"""CLI interface for pytokens."""
./.venv_tmp/lib/python3.12/site-packages/pytokens/cli.py:8:import tokenize
./.venv_tmp/lib/python3.12/site-packages/pytokens/cli.py:12:import pytokens
./.venv_tmp/lib/python3.12/site-packages/pytokens/cli.py:43:                encoding, read_bytes = tokenize.detect_encoding(file.readline)
./.venv_tmp/lib/python3.12/site-packages/pytokens/cli.py:46:                    # Broken `# coding` comment, tokenizer bails, skip file
./.venv_tmp/lib/python3.12/site-packages/pytokens/cli.py:65:            for token in pytokens.tokenize(
./.venv_tmp/lib/python3.12/site-packages/pytokens/cli.py:69:                token_source = source_str[token.start_index : token.end_index]
./.venv_tmp/lib/python3.12/site-packages/pytokens/cli.py:70:                print(repr(token_source), token)
./.venv_tmp/lib/python3.12/site-packages/pytokens/cli.py:75:class TokenTuple(NamedTuple):
./.venv_tmp/lib/python3.12/site-packages/pytokens/cli.py:98:    # For that last newline token that exists on an imaginary line sometimes
./.venv_tmp/lib/python3.12/site-packages/pytokens/cli.py:102:    builtin_tokens = tokenize.tokenize(source_file.readline)
./.venv_tmp/lib/python3.12/site-packages/pytokens/cli.py:103:    # drop the encoding token
./.venv_tmp/lib/python3.12/site-packages/pytokens/cli.py:104:    next(builtin_tokens)
./.venv_tmp/lib/python3.12/site-packages/pytokens/cli.py:107:        expected_tokens_unprocessed = [
./.venv_tmp/lib/python3.12/site-packages/pytokens/cli.py:108:            TokenTuple(tokenize.tok_name[token.type], token.start, token.end)
./.venv_tmp/lib/python3.12/site-packages/pytokens/cli.py:109:            for token in builtin_tokens
./.venv_tmp/lib/python3.12/site-packages/pytokens/cli.py:111:    except tokenize.TokenError:
./.venv_tmp/lib/python3.12/site-packages/pytokens/cli.py:115:    expected_tokens = [expected_tokens_unprocessed[0]]
./.venv_tmp/lib/python3.12/site-packages/pytokens/cli.py:116:    for index, token in enumerate(expected_tokens_unprocessed[1:], start=1):
./.venv_tmp/lib/python3.12/site-packages/pytokens/cli.py:117:        last_token = expected_tokens[-1]
./.venv_tmp/lib/python3.12/site-packages/pytokens/cli.py:119:        current_token = token
./.venv_tmp/lib/python3.12/site-packages/pytokens/cli.py:120:        # Merge consecutive FSTRING_MIDDLE tokens. it's weird cpython has it like that.
./.venv_tmp/lib/python3.12/site-packages/pytokens/cli.py:121:        if current_token.type == last_token.type == "FSTRING_MIDDLE":
./.venv_tmp/lib/python3.12/site-packages/pytokens/cli.py:122:            expected_tokens.pop()
./.venv_tmp/lib/python3.12/site-packages/pytokens/cli.py:123:            current_token = TokenTuple(
./.venv_tmp/lib/python3.12/site-packages/pytokens/cli.py:124:                current_token.type,
./.venv_tmp/lib/python3.12/site-packages/pytokens/cli.py:125:                last_token.start,
./.venv_tmp/lib/python3.12/site-packages/pytokens/cli.py:126:                current_token.end,
./.venv_tmp/lib/python3.12/site-packages/pytokens/cli.py:129:        if index + 1 < len(expected_tokens_unprocessed):
./.venv_tmp/lib/python3.12/site-packages/pytokens/cli.py:131:            # the last { char as well as its end index, so we get a `x{` token
./.venv_tmp/lib/python3.12/site-packages/pytokens/cli.py:132:            # instead of the expected `x{{` token. This fixes that case. Pretty
./.venv_tmp/lib/python3.12/site-packages/pytokens/cli.py:136:            next_token = expected_tokens_unprocessed[index + 1]
./.venv_tmp/lib/python3.12/site-packages/pytokens/cli.py:138:                (current_token.type == "FSTRING_MIDDLE" and next_token.type == "OP")
./.venv_tmp/lib/python3.12/site-packages/pytokens/cli.py:139:                or (current_token.type == "FSTRING_MIDDLE" and next_token.type == "FSTRING_END")
./.venv_tmp/lib/python3.12/site-packages/pytokens/cli.py:140:                and next_token.start[0] == current_token.end[0]
./.venv_tmp/lib/python3.12/site-packages/pytokens/cli.py:141:                and next_token.start[1] > current_token.end[1]
./.venv_tmp/lib/python3.12/site-packages/pytokens/cli.py:143:                expected_tokens.append(
./.venv_tmp/lib/python3.12/site-packages/pytokens/cli.py:144:                    TokenTuple(
./.venv_tmp/lib/python3.12/site-packages/pytokens/cli.py:145:                        current_token.type,
./.venv_tmp/lib/python3.12/site-packages/pytokens/cli.py:146:                        current_token.start,
./.venv_tmp/lib/python3.12/site-packages/pytokens/cli.py:147:                        next_token.start,
./.venv_tmp/lib/python3.12/site-packages/pytokens/cli.py:152:        expected_tokens.append(current_token)
./.venv_tmp/lib/python3.12/site-packages/pytokens/cli.py:155:    our_tokens = (
./.venv_tmp/lib/python3.12/site-packages/pytokens/cli.py:156:        TokenTuple(
./.venv_tmp/lib/python3.12/site-packages/pytokens/cli.py:157:            token.type.to_python_token(),
./.venv_tmp/lib/python3.12/site-packages/pytokens/cli.py:158:            (token.start_line, token.start_col),
./.venv_tmp/lib/python3.12/site-packages/pytokens/cli.py:159:            (token.end_line, token.end_col),
./.venv_tmp/lib/python3.12/site-packages/pytokens/cli.py:161:        for token in pytokens.tokenize(source_string, issue_128233_handling=issue_128233_handling)
./.venv_tmp/lib/python3.12/site-packages/pytokens/cli.py:162:        if token.type != pytokens.TokenType.whitespace
./.venv_tmp/lib/python3.12/site-packages/pytokens/cli.py:165:    for builtin_token, our_token in zip(expected_tokens, our_tokens, strict=True):
./.venv_tmp/lib/python3.12/site-packages/pytokens/cli.py:166:        mismatch = builtin_token != our_token
./.venv_tmp/lib/python3.12/site-packages/pytokens/cli.py:168:            print("EXPECTED", builtin_token)
./.venv_tmp/lib/python3.12/site-packages/pytokens/cli.py:169:            print("---- GOT", our_token)
./.venv_tmp/lib/python3.12/site-packages/pytokens/cli.py:174:            # raise AssertionError("Tokens do not match")
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:1:"""pytokens - A Fast, spec compliant Python 3.12+ tokenizer that runs on older Pythons."""
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:11:class TokenizeError(Exception): ...
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:14:class IndentationError(TokenizeError): ...
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:23:class UnterminatedString(TokenizeError): ...
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:26:class UnexpectedEOF(TokenizeError): ...
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:29:class UnexpectedCharacterAfterBackslash(TokenizeError): ...
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:38:class TokenType(enum.IntEnum):
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:46:    _op_start = 7  # marker used to check if a token is an operator
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:56:    _op_end = 17  # marker used to check if a token is an operator
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:71:    errortoken = 28
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:74:        return f"TokenType.{self.name}"
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:76:    def to_python_token(self) -> str:
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:86:        return TokenType._op_start < self < TokenType._op_end
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:90:class Token:
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:91:    type: TokenType
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:104:            (self.type == TokenType.newline or self.type == TokenType.nl)
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:112:            self.type == TokenType.dedent
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:119:        if self.type == TokenType.endmarker:
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:176:class TokenIterator:
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:190:    prev_token: Token | None = None
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:202:    # present, the next token becomes an OP. regardless of what it is.
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:253:    def make_token(self, tok_type: TokenType) -> Token:
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:255:            if tok_type == TokenType.fstring_start:
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:256:                tok_type = TokenType.tstring_start
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:257:            elif tok_type == TokenType.fstring_middle:
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:258:                tok_type = TokenType.tstring_middle
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:259:            elif tok_type == TokenType.fstring_end:
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:260:                tok_type = TokenType.tstring_end
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:262:        token_type = (
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:263:            TokenType.op
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:266:            and tok_type not in (TokenType.number, TokenType.string)
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:272:            # as the next character, i.e. when the token is '\r ',
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:276:            # ' ' token in it anyway so it doesn't classify it as
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:279:            token_str = self.source[self.prev_index : self.current_index]
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:280:            if token_str == "\r ":
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:284:        token = Token(
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:285:            type=token_type,
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:293:        if tok_type == TokenType.newline or tok_type == TokenType.nl:
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:295:        elif tok_type == TokenType.whitespace or tok_type == TokenType.comment:
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:300:        self.prev_token = token
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:306:        return token
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:327:    def newline(self) -> Token:
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:331:        token_type = (
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:332:            TokenType.nl
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:339:            else TokenType.newline
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:341:        token = self.make_token(token_type)
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:343:        return token
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:345:    def endmarker(self) -> Token:
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:351:            return self.make_token(TokenType.dedent)
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:353:        return self.make_token(TokenType.endmarker)
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:355:    def decimal(self) -> Token:
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:361:        # TODO: this is too lax; 1__2 tokenizes successfully
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:398:        # TODO: this is too lax; 1__2 tokenizes successfully
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:424:            return self.make_token(TokenType.op)
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:426:        return self.make_token(TokenType.number)
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:428:    def binary(self) -> Token:
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:451:        return self.make_token(TokenType.number)
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:453:    def octal(self) -> Token:
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:476:        return self.make_token(TokenType.number)
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:478:    def hexadecimal(self) -> Token:
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:499:        return self.make_token(TokenType.number)
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:502:        # Quotes should always be within 3 chars of the beginning of the string token
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:527:    def fstring(self) -> Token:
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:540:            return self.make_token(TokenType.fstring_start)
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:581:                        # If fstring-middle is empty, skip it by returning the next step token
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:585:                        return self.make_token(TokenType.fstring_middle)
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:590:                    # If fstring-middle is empty, skip it by returning the next step token
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:594:                    return self.make_token(TokenType.fstring_middle)
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:605:            return self.make_token(TokenType.lbrace)
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:611:            token = self.make_token(TokenType.fstring_end)
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:614:            return token
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:628:                    # If fstring-middle is empty, skip it by returning the next step token
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:632:                    return self.make_token(TokenType.fstring_middle)
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:635:                    return self.make_token(TokenType.fstring_middle)
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:643:    def string(self) -> Token:
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:647:            return self.make_token(tok_type=TokenType.op)
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:676:                return self.make_token(TokenType.string)
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:682:    def indent(self) -> Token:
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:701:            return self.make_token(TokenType.whitespace)
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:707:            return self.make_token(TokenType.whitespace)
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:713:            return self.make_token(TokenType.whitespace)
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:724:            return self.make_token(TokenType.whitespace)
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:729:            return self.make_token(TokenType.indent)
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:743:            return self.make_token(TokenType.whitespace)
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:764:    def name(self) -> Token:
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:767:            return self.make_token(TokenType.identifier)
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:769:        # According to PEP 3131, any non-ascii character is valid in a NAME token.
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:780:        return self.make_token(TokenType.identifier)
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:782:    def __iter__(self) -> TokenIterator:
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:785:    def __next__(self) -> Token:
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:786:        if self.prev_token is not None and self.prev_token.type == TokenType.endmarker:
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:791:            if self.prev_token is None:
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:794:            if self.prev_token.type in {
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:795:                TokenType.newline,
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:796:                TokenType.nl,
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:797:                TokenType.dedent,
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:818:        # then we produce identical tokens to CPython.
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:827:                if self.prev_token is not None and self.prev_token.type == TokenType.comment:
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:834:                return self.make_token(TokenType.comment)
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:840:            return self.make_token(TokenType.comment)
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:845:            return self.make_token(TokenType.dedent)
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:871:                    # Move to next line without creating a newline token. But,
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:888:            return self.make_token(TokenType.whitespace)
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:898:                indent_token = self.indent()
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:900:                indent_token = None
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:902:            if indent_token is not None:
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:903:                return indent_token
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:908:            return self.make_token(TokenType.whitespace)
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:914:            return self.make_token(TokenType.op)
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:921:                return self.make_token(TokenType.op)
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:927:            return self.make_token(TokenType.op)
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:935:            return self.make_token(TokenType.op)
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:943:            return self.make_token(TokenType.op)
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:951:            return self.make_token(TokenType.op)
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:958:                return self.make_token(TokenType.op)
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:963:            return self.make_token(TokenType.op)
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:967:            return self.make_token(TokenType.op)
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:973:            return self.make_token(TokenType.op)
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:978:            return self.make_token(TokenType.lparen)
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:985:            return self.make_token(TokenType.rparen)
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:990:            return self.make_token(TokenType.lbracket)
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:997:            return self.make_token(TokenType.rbracket)
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:1002:            return self.make_token(TokenType.lbrace)
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:1014:            return self.make_token(TokenType.rbrace)
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:1020:                return self.make_token(TokenType.op)
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:1024:                return self.make_token(TokenType.op)
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:1084:def tokenize(
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:1087:    fstring_tokens: bool = True,
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:1089:) -> Iterator[Token]:
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:1090:    token_iterator = TokenIterator(source, issue_128233_handling=issue_128233_handling)
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:1091:    if fstring_tokens:
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:1092:        return iter(token_iterator)
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:1094:    return merge_fstring_tokens(token_iterator)
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:1097:def merge_fstring_tokens(token_iterator: TokenIterator) -> Iterator[Token]:
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:1098:    """Turn post-Python-3.12 FSTRING-* tokens back to a single STRING token."""
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:1099:    for token in token_iterator:
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:1100:        if token.type not in (TokenType.fstring_start, TokenType.tstring_start):
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:1101:            yield token
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:1104:        start_token = token
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:1105:        end_token = token
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:1109:        for token in token_iterator:
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:1110:            if token.type in (TokenType.fstring_start, TokenType.tstring_start):
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:1112:            if token.type in (TokenType.fstring_end, TokenType.tstring_end):
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:1116:                end_token = token
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:1119:        yield Token(
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:1120:            type=TokenType.string,
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:1121:            start_index=start_token.start_index,
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:1122:            start_line=start_token.start_line,
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:1123:            start_col=start_token.start_col,
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:1124:            end_index=end_token.end_index,
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:1125:            end_line=end_token.end_line,
./.venv_tmp/lib/python3.12/site-packages/pytokens/__init__.py:1126:            end_col=end_token.end_col,
./.venv_tmp/lib/python3.12/site-packages/cyclonedx/model/crypto.py:742:    PASSWORD = "password"  # nosec
./.venv_tmp/lib/python3.12/site-packages/cyclonedx/model/crypto.py:743:    PRIVATE_KEY = "private-key"
./.venv_tmp/lib/python3.12/site-packages/cyclonedx/model/crypto.py:746:    SECRET_KEY = "secret-key"  # nosec
./.venv_tmp/lib/python3.12/site-packages/cyclonedx/model/crypto.py:748:    SHARED_SECRET = "shared-secret"  # nosec
./.venv_tmp/lib/python3.12/site-packages/cyclonedx/model/crypto.py:751:    TOKEN = "token"  # nosec
./.venv_tmp/lib/python3.12/site-packages/cyclonedx/model/crypto.py:1536:        and related cryptographic material like keys, tokens, secrets or passwords are other cryptographic assets to be
./.venv_tmp/lib/python3.12/site-packages/cyclonedx/model/component.py:1186:    @serializable.xml_string(serializable.XmlStringSerializationType.TOKEN)
./.venv_tmp/lib/python3.12/site-packages/cyclonedx/model/__init__.py:533:    @serializable.xml_string(serializable.XmlStringSerializationType.TOKEN)
./.venv_tmp/lib/python3.12/site-packages/py_serializable/xml.py:22:__all__ = ["xs_normalizedString", "xs_token"]
./.venv_tmp/lib/python3.12/site-packages/py_serializable/xml.py:51:# region token
./.venv_tmp/lib/python3.12/site-packages/py_serializable/xml.py:54:__TOKEN_MULTISTRING_SEARCH = re_compile(r" {2,}")
./.venv_tmp/lib/python3.12/site-packages/py_serializable/xml.py:55:__TOKEN_MULTISTRING_REPLACE = " "
./.venv_tmp/lib/python3.12/site-packages/py_serializable/xml.py:58:def xs_token(s: str) -> str:
./.venv_tmp/lib/python3.12/site-packages/py_serializable/xml.py:59:    """Make a ``token``, adhering XML spec.
./.venv_tmp/lib/python3.12/site-packages/py_serializable/xml.py:62:       *token* represents tokenized strings.
./.venv_tmp/lib/python3.12/site-packages/py_serializable/xml.py:63:       The `Â·value spaceÂ· <https://www.w3.org/TR/xmlschema-2/#dt-value-space>`_ of token is the set of strings that do
./.venv_tmp/lib/python3.12/site-packages/py_serializable/xml.py:66:       The `Â·lexical spaceÂ· <https://www.w3.org/TR/xmlschema-2/#dt-lexical-space>`_ of token is the set of strings that
./.venv_tmp/lib/python3.12/site-packages/py_serializable/xml.py:69:       The `Â·base typeÂ· <https://www.w3.org/TR/xmlschema-2/#dt-basetype>`_ of token is
./.venv_tmp/lib/python3.12/site-packages/py_serializable/xml.py:72:       -- the `XML schema spec <http://www.w3.org/TR/xmlschema-2/#token>`_
./.venv_tmp/lib/python3.12/site-packages/py_serializable/xml.py:74:    return __TOKEN_MULTISTRING_SEARCH.sub(
./.venv_tmp/lib/python3.12/site-packages/py_serializable/xml.py:75:        __TOKEN_MULTISTRING_REPLACE, xs_normalizedString(s).strip()
./.venv_tmp/lib/python3.12/site-packages/py_serializable/__init__.py:49:from .xml import xs_normalizedString, xs_token
./.venv_tmp/lib/python3.12/site-packages/py_serializable/__init__.py:148:    TOKEN = 3
./.venv_tmp/lib/python3.12/site-packages/py_serializable/__init__.py:150:    as `token`.
./.venv_tmp/lib/python3.12/site-packages/py_serializable/__init__.py:151:    see http://www.w3.org/TR/xmlschema-2/#token"""
./.venv_tmp/lib/python3.12/site-packages/py_serializable/__init__.py:155:    # - https://www.w3.org/TR/xmlschema-2/#NMTOKEN
./.venv_tmp/lib/python3.12/site-packages/py_serializable/__init__.py:163:    XmlStringSerializationType.TOKEN: xs_token,
./PHASE_COMPLETION_INDEX.md:45:â”œâ”€â”€ test_secrets_scanning.py          (Gitleaks integration)
./PHASE_COMPLETION_INDEX.md:285:âœ… Secrets Scanning: Gitleaks integration
./PHASE_COMPLETION_INDEX.md:374:âœ… Gitleaks detection (secret leaks)
./PHASE1_COVERAGE_STATUS.md:177:- Secrets scanning (14 tests) âœ…
./ENTERPRISE_AUDIT_COMPLETE.md:22:- [x] Hardcoded secrets (none found)
./ENTERPRISE_AUDIT_COMPLETE.md:53:| **Security** | âœ… Excellent | 95/100: No eval, exec, pickle, unsafe yaml, or hardcoded secrets found |
./ENTERPRISE_AUDIT_REPORT.md:25:| **Secrets Management** | âš ï¸ REVIEW NEEDED | MEDIUM | 1 default secret |
./ENTERPRISE_AUDIT_REPORT.md:34:2. **ðŸŸ¡ P1 - Remove hardcoded default secret** in `src/firsttry/license.py`
./ENTERPRISE_AUDIT_REPORT.md:104:### 1.3 Secrets Detection âš ï¸
./ENTERPRISE_AUDIT_REPORT.md:111:1. **ðŸ”´ Default Shared Secret**
./ENTERPRISE_AUDIT_REPORT.md:114:   DEFAULT_SHARED_SECRET = os.getenv("FIRSTTRY_SHARED_SECRET", "dev-secret-change-me")
./ENTERPRISE_AUDIT_REPORT.md:116:   - **Risk:** HIGH - Hardcoded fallback secret in production code
./ENTERPRISE_AUDIT_REPORT.md:125:   - Found legitimate use of env vars for tokens/credentials
./ENTERPRISE_AUDIT_REPORT.md:126:   - No hardcoded private keys or API tokens detected
./ENTERPRISE_AUDIT_REPORT.md:132:FIRSTTRY_SHARED_SECRET = os.environ.get("FIRSTTRY_SHARED_SECRET")
./ENTERPRISE_AUDIT_REPORT.md:133:if not FIRSTTRY_SHARED_SECRET:
./ENTERPRISE_AUDIT_REPORT.md:135:        FIRSTTRY_SHARED_SECRET = "dev-only-insecure"
./ENTERPRISE_AUDIT_REPORT.md:136:        warnings.warn("Using insecure dev secret")
./ENTERPRISE_AUDIT_REPORT.md:138:        raise EnvironmentError("FIRSTTRY_SHARED_SECRET required in production")
./ENTERPRISE_AUDIT_REPORT.md:240:- No hardcoded secrets in YAML
./ENTERPRISE_AUDIT_REPORT.md:351:- Shared secret mechanism for internal services (`FIRSTTRY_SHARED_SECRET`)
./ENTERPRISE_AUDIT_REPORT.md:352:- âš ï¸ Default secret issue (see Section 1.3)
./ENTERPRISE_AUDIT_REPORT.md:355:- Rotate secrets regularly (document rotation policy)
./ENTERPRISE_AUDIT_REPORT.md:356:- Use secret management service (AWS Secrets Manager, HashiCorp Vault, GitHub Secrets)
./ENTERPRISE_AUDIT_REPORT.md:357:- Implement secret expiration/rotation for production keys
./ENTERPRISE_AUDIT_REPORT.md:378:**Secrets Management:** GitHub Secrets (encrypted)
./ENTERPRISE_AUDIT_REPORT.md:447:| Default secret in code | HIGH | HIGH | MEDIUM | ðŸ”´ P0 - This sprint |
./ENTERPRISE_AUDIT_REPORT.md:472:**2. Fix Hardcoded Secret**
./ENTERPRISE_AUDIT_REPORT.md:476:git commit -m "security: remove hardcoded default secret, enforce env var"
./ENTERPRISE_AUDIT_REPORT.md:528:| **OWASP Top 10** | âœ… 8/10 | A04 (Insecure Design) - default secret issue |
./ENTERPRISE_AUDIT_REPORT.md:558:2. **Remove default secret** - Active security risk
./ENTERPRISE_AUDIT_REPORT.md:599:- âš ï¸ Hardcoded default secret
./ENTERPRISE_AUDIT_REPORT.md:610:2. One hardcoded default secret
./ENTERPRISE_AUDIT_REPORT.md:642:| grep | System | Pattern-based secrets detection |
./ENTERPRISE_SUITE_FINAL_REPORT.md:33:  - Secrets scanning (14 tests)
./ENTERPRISE_SUITE_FINAL_REPORT.md:61:- **Secrets Scanning:** AWS keys, GitHub tokens, private keys detected
./ENTERPRISE_SUITE_FINAL_REPORT.md:101:  - test_secrets_scanning.py (14)
./ENTERPRISE_SUITE_FINAL_REPORT.md:131:- [x] Secrets scanning integrated
./ENTERPRISE_SUITE_FINAL_REPORT.md:189:- Secrets patterns detected: 20+
./ENTERPRISE_SUITE_FINAL_REPORT.md:211:- Secrets scanning configuration
./ENTERPRISE_SUITE_FINAL_REPORT.md:276:âœ… **Security** - Secrets scanning integrated  
./PHASE2_VERIFICATION_REPORT.md:147:#### 2.D.3 Secrets Scanning (12 tests âœ…)
./PHASE2_VERIFICATION_REPORT.md:149:**File:** `tests/enterprise/test_secrets_scanning.py`
./PHASE2_VERIFICATION_REPORT.md:153:âœ… test_aws_secret_detection
./PHASE2_VERIFICATION_REPORT.md:154:âœ… test_api_key_detection
./PHASE2_VERIFICATION_REPORT.md:155:âœ… test_private_key_detection
./PHASE2_VERIFICATION_REPORT.md:161:- AWS secret detection âœ…
./PHASE2_VERIFICATION_REPORT.md:234:| Secrets Scanning | 12 | âœ… PASS | 100% |
./PHASE2_VERIFICATION_REPORT.md:350:- âœ… Secrets scanning (gitleaks + custom patterns)
./PHASE2_VERIFICATION_REPORT.md:380:### 3. Secrets Detection Coverage ðŸ”§
./S3_INTEGRATION_COMPLETE.md:33:- Clear separation between code and secrets
./S3_INTEGRATION_COMPLETE.md:47:export S3_SECRET_ACCESS_KEY="your-secret"
./S3_INTEGRATION_COMPLETE.md:115:    S3_ACCESS_KEY_ID: ${{ secrets.S3_ACCESS_KEY_ID }}
./S3_INTEGRATION_COMPLETE.md:116:    S3_SECRET_ACCESS_KEY: ${{ secrets.S3_SECRET_ACCESS_KEY }}
./S3_INTEGRATION_COMPLETE.md:117:    S3_ENDPOINT_URL: ${{ secrets.S3_ENDPOINT_URL }}
./S3_INTEGRATION_COMPLETE.md:118:    S3_BUCKET_NAME: ${{ secrets.S3_BUCKET_NAME }}
./S3_INTEGRATION_COMPLETE.md:186:- GitHub Actions secrets configuration
./S3_INTEGRATION_COMPLETE.md:239:   - Add S3 credentials to GitHub Secrets
./ENTERPRISE_DELIVERY_EXEC_SUMMARY.md:160:| gitleaks | Secrets scanning | High |
./S3_INTEGRATION_GUIDE.md:10:- Environment variable-based configuration (no hardcoded secrets)
./S3_INTEGRATION_GUIDE.md:43:export S3_SECRET_ACCESS_KEY="your-secret-access-key"
./S3_INTEGRATION_GUIDE.md:60:export S3_SECRET_ACCESS_KEY="wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY"
./S3_INTEGRATION_GUIDE.md:69:export S3_SECRET_ACCESS_KEY="e3090f0c5b2ec021aa979c91e7816a810bafb6c6b77fffc36de06d6bd3c5c022"
./S3_INTEGRATION_GUIDE.md:154:    secret_access_key="your-secret",
./S3_INTEGRATION_GUIDE.md:208:          S3_ACCESS_KEY_ID: ${{ secrets.S3_ACCESS_KEY_ID }}
./S3_INTEGRATION_GUIDE.md:209:          S3_SECRET_ACCESS_KEY: ${{ secrets.S3_SECRET_ACCESS_KEY }}
./S3_INTEGRATION_GUIDE.md:210:          S3_ENDPOINT_URL: ${{ secrets.S3_ENDPOINT_URL }}
./S3_INTEGRATION_GUIDE.md:211:          S3_BUCKET_NAME: ${{ secrets.S3_BUCKET_NAME }}
./S3_INTEGRATION_GUIDE.md:364:export S3_SECRET_ACCESS_KEY="your-secret"
./S3_INTEGRATION_GUIDE.md:408:| `S3_SECRET_ACCESS_KEY` | âœ“ | - | S3 secret key |
./FIRSTTRY_ENTERPRISE_AUDIT_UPDATED.md:58:- Medium: test harness sets `FIRSTTRY_LICENSE_KEY = "TEST-KEY-OK"` in `tools/ft_bench_harness.py` for pro-tier benching when no key present â€” intentional for testing; ensure production keys are provided via secrets.
./FIRSTTRY_ENTERPRISE_AUDIT_UPDATED.md:59:- Low: no obvious `eval()`/`exec()` misuse, no clear hardcoded secrets in main code (static grep performed).
./.pre-commit-config.yaml:65:      - id: ft-secret-guard
./.pre-commit-config.yaml:66:        name: FirstTry hardcoded secret guard
./.pre-commit-config.yaml:67:        entry: bash -c 'BAD=$(grep -RIn --exclude-dir=.git --exclude-dir=dist --exclude-dir=build --exclude-dir=.mypy_cache --exclude-dir=.pytest_cache --exclude-dir=.ruff_cache --exclude-dir=node_modules --exclude-dir=__pycache__ --exclude="*.md" --exclude="*.json" --exclude="*.lock" --exclude="*.yaml" --exclude="*.yml" -E "dev-secret-change-me" src/ tests/ || true); if [ -n "$BAD" ]; then echo "Hardcoded secret detected:"; echo "$BAD"; exit 1; fi'
./OPERATIONAL_STATUS_REPORT_OLD.md:193:- `STRIPE_WEBHOOK_SECRET`: Stripe signature validation
./OPERATIONAL_STATUS_REPORT_OLD.md:194:- `LEMON_SQUEEZY_WEBHOOK_SECRET`: LemonSqueezy signature validation
./OPERATIONAL_STATUS_REPORT_OLD.md:489:| `STRIPE_WEBHOOK_SECRET` | Stripe signature validation | `""` | webhooks.py |
./OPERATIONAL_STATUS_REPORT_OLD.md:490:| `LEMON_SQUEEZY_WEBHOOK_SECRET` | LemonSqueezy signature | `""` | webhooks.py |
./MERGE_READINESS_SUMMARY.md:75:- **Secrets Management**: Test keys properly isolated
./CONTRIBUTING.md:86:security: remove hardcoded default secret
./PHASE1_INDEX.md:295:- Secrets scanning (gitleaks)
./DEMO_LICENSE_DELIVERY.md:139:âœ… **Safe** - Environment variable-based, no hardcoded secrets
./ENTERPRISE_DELIVERY_INDEX.md:105:- **gitleaks** - Secrets scanning
./PHASE3_AUDIT_SCHEMA_STATUS.md:229:- Location: `tests/enterprise/test_secrets_scanning.py`
./PHASE3_AUDIT_SCHEMA_STATUS.md:232:  - AWS secret detection
./FIRSTTRY_ENTERPRISE_AUDIT.md:192:- âœ… No hardcoded secrets in code (license keys stored in env/config)
./tools/ft_bench_s3.py:32:    secret_access_key: str
./tools/ft_bench_s3.py:45:        - S3_SECRET_ACCESS_KEY or AWS_SECRET_ACCESS_KEY
./tools/ft_bench_s3.py:59:        secret_access_key = os.getenv("S3_SECRET_ACCESS_KEY") or os.getenv("AWS_SECRET_ACCESS_KEY")
./tools/ft_bench_s3.py:64:        if not all([access_key_id, secret_access_key, endpoint_url, bucket_name]):
./tools/ft_bench_s3.py:69:            secret_access_key=secret_access_key,
./tools/ft_bench_s3.py:78:        """Return config as dict (without secrets for logging)."""
./tools/ft_bench_s3.py:121:                aws_secret_access_key=self.config.secret_access_key,
./tools/ft_bench_s3.py:340:        print("  S3_SECRET_ACCESS_KEY")
./tools/ci_self_check.py:74:            # Check no hardcoded secrets
./tools/ci_self_check.py:263:            ("id-token: write", "OIDC token permission"),
./tools/ci_self_check.py:279:        if "AWS_ACCESS_KEY_ID" in content or "AWS_SECRET_ACCESS_KEY" in content:
./tools/audit_emit.py:279:            "secrets_found": 0,
./tools/audit_emit.py:293:                    "check": "No hardcoded secrets",
./tools/audit_emit.py:295:                    "details": "Secrets stored in env variables",
./tools/firsttry/tests/conftest.py:33:        "AWS_SECRET_ACCESS_KEY",
./tools/firsttry/firsttry/mapper.py:12:    tokens: set[str] = set()
./tools/firsttry/firsttry/mapper.py:20:            tokens.add(stem)
./tools/firsttry/firsttry/mapper.py:21:    if not tokens:
./tools/firsttry/firsttry/mapper.py:24:    ors = " or ".join(sorted(tokens))
./SUPPLY_CHAIN_SECURITY_REMEDIATION.md:174:- **Secrets Scan:** Review `FIRSTTRY_SHARED_SECRET` default in `src/firsttry/license.py`
./ENTERPRISE_IMPLEMENTATION_FINAL.md:85:- âœ… Secrets Scanning (gitleaks)
./STABILITY.md:13:- **`FIRSTTRY_SHARED_SECRET` test fixture behavior.**
./STABILITY.md:14:  - Tests rely on a deterministic shared secret in dev/test runs. We provide an
./STABILITY.md:17:    if it is not already present â€” it does not override a real secret.
./PHASE2_QUICK_REF.md:132:1. **gitleaks** - Secrets scanning
./BACKUP_RECOVERY_GUIDE.md:143:**Manual Steps** (since `gh repo create` requires admin token):
./src/firsttry/ci_mapper.py:226:    # Replace standalone 'python' or 'python3' tokens with the preferred interpreter
./src/firsttry/models.py:29:    name: str  # "Lint / Style", "Types", "Security / Secrets", "Tests & Coverage"
./src/firsttry/mapper.py:15:    If no interesting tokens, return "" (means: run full suite).
./src/firsttry/mapper.py:17:    tokens: list[str] = []
./src/firsttry/mapper.py:24:        if stem and stem not in tokens:
./src/firsttry/mapper.py:25:            tokens.append(stem)
./src/firsttry/mapper.py:28:        if parent and parent not in tokens:
./src/firsttry/mapper.py:29:            tokens.append(parent)
./src/firsttry/mapper.py:31:    if not tokens:
./src/firsttry/mapper.py:34:    tokens_sorted = sorted(tokens)
./src/firsttry/mapper.py:35:    return " or ".join(tokens_sorted)
./src/firsttry/ci_parity/util.py:39:    # Final safety: drop stray '\\' tokens that can appear from malformed splits
./src/firsttry/ci_parity/util.py:104:def tokenize_shell_line(line: str) -> List[str]:
./src/firsttry/ci_parity/util.py:106:    Tokenize a shell command line robustly.
./src/firsttry/ci_parity/util.py:109:    - Avoids leaking stray backslashes as tokens.
./src/firsttry/ci_parity/util.py:122:        # Drop standalone backslash tokens (defensive)
./src/firsttry/ci_parity/util.py:125:        token = c.strip('"').strip("'")
./src/firsttry/ci_parity/util.py:126:        # Ignore common shell redirection tokens that may be present when
./src/firsttry/ci_parity/util.py:129:        if "/dev/null" in token:
./src/firsttry/ci_parity/util.py:131:        if token in {">", "2>", "2>&1", "&>", "1>", "1>&2"}:
./src/firsttry/ci_parity/util.py:134:        if re.match(r"^\d+>&?\d*$", token) or re.match(r"^\d+>.*", token):
./src/firsttry/ci_parity/util.py:136:        out.append(token)
./src/firsttry/ci_parity/detector.py:7:from .util import current_repo_root, normalize_cmd, tokenize_shell_line
./src/firsttry/ci_parity/detector.py:47:    def _split_on_separators(tokens: List[str]) -> List[List[str]]:
./src/firsttry/ci_parity/detector.py:48:        """Split a token list on shell command separators like &&, ; and ||."""
./src/firsttry/ci_parity/detector.py:51:        for t in tokens:
./src/firsttry/ci_parity/detector.py:73:                    toks = normalize_cmd(tokenize_shell_line(rest.strip()))
./src/firsttry/ci_parity/detector.py:88:                            toks = normalize_cmd(tokenize_shell_line(b))
./src/firsttry/ci_parity/detector.py:97:                toks = normalize_cmd(tokenize_shell_line(b))
./src/firsttry/ci_parity/detector.py:146:def _is_runner_cmd(tokens: list[str]) -> bool:
./src/firsttry/ci_parity/detector.py:148:    if not tokens:
./src/firsttry/ci_parity/detector.py:150:    t0 = tokens[0]
./src/firsttry/ci_parity/detector.py:153:        for i in range(1, len(tokens) - 1):
./src/firsttry/ci_parity/detector.py:154:            if tokens[i] == "-m" and tokens[i + 1] == "firsttry.ci_parity.runner":
./src/firsttry/scanner.py:272:# SECURITY / SECRETS (bandit)
./src/firsttry/scanner.py:289:            name="Security / Secrets",
./src/firsttry/scanner.py:410:        name="Security / Secrets",
./src/firsttry/scanner.py:620:        name="Security / Secrets",
./src/firsttry/security/secret_scan.py:10:class SecretFinding:
./src/firsttry/security/secret_scan.py:21:    "AWS_SECRET_ACCESS_KEY": re.compile(r"AKIA[0-9A-Z]{16}"),
./src/firsttry/security/secret_scan.py:28:# High-entropy token-ish pattern (base64-like) â€” long strings of [A-Za-z0-9+/=]{40,}
./src/firsttry/security/secret_scan.py:40:def scan_text_for_secrets(text: str) -> List[str]:
./src/firsttry/security/secret_scan.py:51:def scan_files(paths: Iterable[str]) -> List[SecretFinding]:
./src/firsttry/security/secret_scan.py:52:    findings: List[SecretFinding] = []
./src/firsttry/security/secret_scan.py:73:                    findings.append(SecretFinding(str(path), i, name, masked))
./src/firsttry/security/secret_scan.py:77:                token = me.group(0)
./src/firsttry/security/secret_scan.py:78:                masked = token[:8] + "*" * (len(token) - 12) + token[-4:]
./src/firsttry/security/secret_scan.py:79:                findings.append(SecretFinding(str(path), i, "high_entropy", masked))
./src/firsttry/security/secret_scan.py:84:def scan_changed_files(changed_files: Iterable[str]) -> List[SecretFinding]:
./src/firsttry/security/secret_scan.py:95:__all__ = ["SecretFinding", "scan_files", "scan_changed_files", "scan_text_for_secrets"]
./src/firsttry/report.py:35:    # Security / Secrets ... âš ï¸  0 autofixable / 42 manual
./src/firsttry/report.py:101:            "Keep them from leaking secrets / unsafe exec over time.",
./src/firsttry/device.py:68:    # prefer to store only an obfuscated token in future; store raw for now
./src/firsttry/cli.py:765:        # Run a quick secret scan on changed files and abort early with a distinct code
./src/firsttry/cli.py:767:            from firsttry.security.secret_scan import scan_changed_files
./src/firsttry/cli.py:769:            # Exclude CI parity lock and other CI metadata from secret scanning
./src/firsttry/cli.py:772:            secrets = scan_changed_files(files_for_scan)
./src/firsttry/cli.py:773:            if secrets:
./src/firsttry/cli.py:774:                click.echo("\nSecret scan: potential secrets found in changed files:", err=True)
./src/firsttry/cli.py:775:                for s in secrets:
./src/firsttry/cli.py:777:                # Distinct exit code used by CI/demo to indicate secret detection
./src/firsttry/cli.py:1912:                "AWS_SECRET_ACCESS_KEY",
./src/firsttry/cli.py:1913:                "AWS_SESSION_TOKEN",
./src/firsttry/license.py:206:# SECURITY: Secret MUST be provided via environment variable in production.
./src/firsttry/license.py:208:_raw_secret = os.getenv("FIRSTTRY_SHARED_SECRET")
./src/firsttry/license.py:209:if not _raw_secret:
./src/firsttry/license.py:212:        _raw_secret = "dev-only-insecure-fallback"
./src/firsttry/license.py:216:            "FIRSTTRY_SHARED_SECRET not set - using insecure dev fallback. "
./src/firsttry/license.py:217:            "Set FIRSTTRY_SHARED_SECRET in production!",
./src/firsttry/license.py:223:            "FIRSTTRY_SHARED_SECRET environment variable is required in production. "
./src/firsttry/license.py:227:DEFAULT_SHARED_SECRET = _raw_secret
./src/firsttry/license.py:238:    secret: str,
./src/firsttry/license.py:241:    mac = hmac.new(secret.encode("utf-8"), msg.encode("utf-8"), hashlib.sha256).digest()
./src/firsttry/license.py:245:def verify_sig(payload: dict[str, Any], secret: str = DEFAULT_SHARED_SECRET) -> bool:
./src/firsttry/license.py:250:        secret,
./src/firsttry/license.py:260:    secret: str = DEFAULT_SHARED_SECRET,
./src/firsttry/license.py:263:    payload["sig"] = _sign_payload(valid, plan, expiry, secret)
./src/firsttry/config/compat_alias.py:21:# token in source. This lets callers import the legacy symbol while keeping
./src/firsttry/config/compat_alias.py:22:# the raw token out of repository files (scanners search for the token).
./src/firsttry/config/__init__.py:81:# literal token in source (so repository-wide token scans don't trigger).
./src/firsttry/pro_features.py:104:    DANGEROUS_TOKENS = [
./src/firsttry/pro_features.py:114:        for bad in DANGEROUS_TOKENS:
./src/firsttry/executor/key_builder.py:11:    - special tokens: "TOOL:ruff", "TOOL:mypy", "CONF:pyproject.toml"
./src/firsttry/executor/key_builder.py:14:    tokens: list[str] = []
./src/firsttry/executor/key_builder.py:19:            tokens.append(f"TV:{tool}:{tool_version_hash([tool, '--version'])}")
./src/firsttry/executor/key_builder.py:33:        sorted(file_hashes) + sorted(tokens) + [env, "CMD:" + hash_bytes(cmd_sig)],
./src/firsttry/reports/tier_map.py:50:        "secrets-scan",
./src/firsttry/reports/tier_map.py:74:        "secrets-scan",
./src/firsttry/reports/ui.py:132:            print("  â€¢ Coverage and secrets scanning (Team/Enterprise)")
./ENTERPRISE_COMPLETION_INDEX.md:40:- **[PHASE2D_ENTERPRISE_FEATURES.md](PHASE2D_ENTERPRISE_FEATURES.md)** - Secrets scanning, dependencies, performance, SBOM
./ENTERPRISE_COMPLETION_INDEX.md:42:  - `tests/enterprise/test_secrets_scanning.py` (14 tests)
./ENTERPRISE_COMPLETION_INDEX.md:121:| Secrets Scanning | 2.D.1 | 14 | âœ… | PHASE2D_ENTERPRISE_FEATURES.md |
./ENTERPRISE_COMPLETION_INDEX.md:139:- Secret patterns detected: **20+**
./ENTERPRISE_COMPLETION_INDEX.md:171:3. [ ] Enable secrets scanning (gitleaks)
./ENTERPRISE_COMPLETION_INDEX.md:272:â”‚   â”‚   â”œâ”€â”€ test_secrets_scanning.py (14 tests)
./IMPLEMENTATION_INDEX.md:73:- **tests/enterprise/test_secrets_scanning.py** (12 tests)
./IMPLEMENTATION_INDEX.md:163:- Secrets Scanning: 12 tests for gitleaks integration
./PHASE2D5_COMMIT_VALIDATION.md:87:This commit adds JWT token validation to all protected endpoints.
./PHASE2D_ENTERPRISE_FEATURES.md:14:### Feature 1: Secrets Scanning (gitleaks) âœ…
./PHASE2D_ENTERPRISE_FEATURES.md:19:- `tests/enterprise/test_secrets_scanning.py` - 14 comprehensive tests
./PHASE2D_ENTERPRISE_FEATURES.md:26:| `test_secret_detection_aws_key` | AWS access key detection |
./PHASE2D_ENTERPRISE_FEATURES.md:27:| `test_secret_detection_github_token` | GitHub token detection |
./PHASE2D_ENTERPRISE_FEATURES.md:28:| `test_secret_detection_private_key` | Private key detection |
./PHASE2D_ENTERPRISE_FEATURES.md:30:| `test_gitleaks_ci_blocking` | CI/CD blocking on secrets |
./PHASE2D_ENTERPRISE_FEATURES.md:31:| `test_secret_pattern_customization` | Custom pattern support |
./PHASE2D_ENTERPRISE_FEATURES.md:35:| `test_policy_enforcement_with_secrets` | Policy integration |
./PHASE2D_ENTERPRISE_FEATURES.md:37:| `test_enterprise_secret_vault_integration` | Vault integration |
./PHASE2D_ENTERPRISE_FEATURES.md:38:| `test_ci_pipeline_secret_scanning` | CI integration |
./PHASE2D_ENTERPRISE_FEATURES.md:42:- âœ… GitHub Tokens: Pattern `ghp_[0-9a-zA-Z]+`
./PHASE2D_ENTERPRISE_FEATURES.md:43:- âœ… Private Keys: Pattern `-----BEGIN RSA PRIVATE KEY-----`
./PHASE2D_ENTERPRISE_FEATURES.md:168:Secrets Scanning (gitleaks):      14 tests âœ…
./PHASE2D_ENTERPRISE_FEATURES.md:199:tests/enterprise/test_secrets_scanning.py      14 tests  âœ…
./PHASE2D_ENTERPRISE_FEATURES.md:217:| **Secrets Scanning** | âœ… Complete | AWS/GitHub/PKI patterns | 14 |
./PHASE2D_ENTERPRISE_FEATURES.md:231:â”œâ”€â”€ test_secrets_scanning.py        âœ… (14 tests, Phase 2.D.1)
./PHASE2D_ENTERPRISE_FEATURES.md:244:### Secrets Scanning (gitleaks)
./PHASE2D_ENTERPRISE_FEATURES.md:250:  - Blocks: AWS keys, GitHub tokens, private keys
./PHASE2D_ENTERPRISE_FEATURES.md:274:- Secrets scanning: MUST PASS
./PHASE2D_ENTERPRISE_FEATURES.md:306:Secrets Scanning:
./PHASE2D_ENTERPRISE_FEATURES.md:340:- `.gitleaks.toml` - Secret patterns (to create)
./PHASE2D_ENTERPRISE_FEATURES.md:345:- Secrets: 14 tests
./PHASE2D_ENTERPRISE_FEATURES.md:359:- [x] Secrets scanning implemented
./PHASE2D_ENTERPRISE_FEATURES.md:378:| Security features | âœ… | Secrets + dependencies |
./PHASE2D_ENTERPRISE_FEATURES.md:392:- âœ… Secrets scanning (14 tests)
./PHASE4_CI_CD_DEPLOYMENT.md:65:| **Secrets** | Gitleaks | Latest | Secret detection | Advisory gate |
./PHASE4_CI_CD_DEPLOYMENT.md:133:   - No hardcoded secrets audit
./PHASE4_CI_CD_DEPLOYMENT.md:263:- [x] No hardcoded secrets in workflows
./PHASE4_CI_CD_DEPLOYMENT.md:266:- [x] Secret scanning integrated (gitleaks)
./PHASE4_CI_CD_DEPLOYMENT.md:370:| **Secret Detection** | âœ… | Gitleaks prevents credential leaks |
./PHASE4_CI_CD_DEPLOYMENT.md:439:# (GitHub Actions can assume IAM role without secrets)
./PHASE4_CI_CD_DEPLOYMENT.md:444:### Step 3: Set Secrets (if using S3)
./PHASE4_CI_CD_DEPLOYMENT.md:447:# GitHub Settings > Secrets > New repository secret
./TELEMETRY.md:36:- It does not read or transmit repository source code files, test file contents, or secrets (there is no code that collects or sends raw file contents or environment variable values).
./S3_INTEGRATION_SETUP.md:16:2. **Generate API Token**
./S3_INTEGRATION_SETUP.md:17:   - Go to Account Settings â†’ API Tokens
./S3_INTEGRATION_SETUP.md:18:   - Create token with R2 permissions
./S3_INTEGRATION_SETUP.md:23:   export S3_ACCESS_KEY_ID="your-token-id"
./S3_INTEGRATION_SETUP.md:24:   export S3_SECRET_ACCESS_KEY="your-token-secret"
./S3_INTEGRATION_SETUP.md:49:export S3_SECRET_ACCESS_KEY="test"
./S3_INTEGRATION_SETUP.md:71:export S3_SECRET_ACCESS_KEY="your-secret"
./S3_INTEGRATION_SETUP.md:110:export S3_SECRET_ACCESS_KEY="test"
./S3_INTEGRATION_SETUP.md:149:export S3_SECRET_ACCESS_KEY="..."
./S3_INTEGRATION_SETUP.md:167:### GitHub Actions Secrets
./S3_INTEGRATION_SETUP.md:169:1. Go to Settings â†’ Secrets and variables â†’ Actions
./S3_INTEGRATION_SETUP.md:170:2. Add secrets:
./S3_INTEGRATION_SETUP.md:172:   - `S3_SECRET_ACCESS_KEY`
./S3_INTEGRATION_SETUP.md:180:    S3_ACCESS_KEY_ID: ${{ secrets.S3_ACCESS_KEY_ID }}
./S3_INTEGRATION_SETUP.md:181:    S3_SECRET_ACCESS_KEY: ${{ secrets.S3_SECRET_ACCESS_KEY }}
./S3_INTEGRATION_SETUP.md:182:    S3_ENDPOINT_URL: ${{ secrets.S3_ENDPOINT_URL }}
./S3_INTEGRATION_SETUP.md:183:    S3_BUCKET_NAME: ${{ secrets.S3_BUCKET_NAME }}
./PHASE2D6_RELEASE_SBOM.md:75:signer = SupplyChainSigner(private_key="org-private-key")
./PHASE2D6_RELEASE_SBOM.md:279:            --key-id "${{ secrets.RELEASE_KEY_ID }}" \
./PHASE2D6_RELEASE_SBOM.md:365:    key_file: "/etc/secrets/release.key"
./PHASE2D6_RELEASE_SBOM.md:461:# Store in secrets manager
./CI_WORKFLOWS_WARM_CACHE.md:104:        run: echo "${{ secrets.GITHUB_TOKEN }}" | gh auth login --with-token
./CI_WORKFLOWS_WARM_CACHE.md:332:Set these in GitHub Settings â†’ Secrets:
./CI_WORKFLOWS_WARM_CACHE.md:334:- `GITHUB_TOKEN` - Already available (for gh CLI)
